<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>机器之心</title>
    <link>http://www.iwgc.cn/list/670</link>
    <description>人与科技的美好关系</description>
    <item>
      <title>机器学习算法集锦：从贝叶斯到深度学习及各自优缺点</title>
      <link>http://www.iwgc.cn/link/</link>
      <description>
&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;选自static.coggle.it&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;在我们日常生活中所用到的推荐系统、智能图片美化应用和聊天机器人等应用中，各种各样的机器学习和数据处理算法正尽职尽责地发挥着自己的功效。本文筛选并简单介绍了一些最常见算法类别，还为每一个类别列出了一些实际的算法并简单介绍了它们的优缺点。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;https://static.coggle.it/diagram/WHeBqDIrJRk-kDDY&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/f8a2d14b7a8f1265f9d56ffb3218f442d34043b8"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目录&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;正则化算法（Regularization Algorithms）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;集成算法（Ensemble Algorithms）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;决策树算法（Decision Tree Algorithm）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;回归（Regression）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;人工神经网络（Artificial Neural Network）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;深度学习（Deep Learning）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;支持向量机（Support Vector Machine）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;降维算法（Dimensionality Reduction Algorithms）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;聚类算法（Clustering Algorithms）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;基于实例的算法（Instance-based Algorithms）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;贝叶斯算法（Bayesian Algorithms）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;关联规则学习算法（Association Rule Learning Algorithms）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;图模型（Graphical Models）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;正则化算法（Regularization Algorithms）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/d84f497c3c1356ed625047b3e3494489c4981a67"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;它是另一种方法（通常是回归方法）的拓展，这种方法会基于模型复杂性对其进行惩罚，它喜欢相对简单能够更好的泛化的模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;岭回归（Ridge Regression）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;最小绝对收缩与选择算子（LASSO）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;GLASSO&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;弹性网络（Elastic Net）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;最小角回归（Least-Angle Regression）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;其惩罚会减少过拟合&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;总会有解决方法&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;惩罚会造成欠拟合&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;很难校准&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;集成算法（Ensemble algorithms）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/426f07943f43204e4ee4cf05c5744a854553d2e8"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;集成方法是由多个较弱的模型集成模型组，其中的模型可以单独进行训练，并且它们的预测能以某种方式结合起来去做出一个总体预测。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该算法主要的问题是要找出哪些较弱的模型可以结合起来，以及结合的方法。这是一个非常强大的技术集，因此广受欢迎。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Boosting&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Bootstrapped Aggregation（Bagging）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;AdaBoost&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;层叠泛化（Stacked Generalization）（blending）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;梯度推进机（Gradient Boosting Machines，GBM）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;梯度提升回归树（Gradient Boosted Regression Trees，GBRT）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;随机森林（Random Forest）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;当先最先进的预测几乎都使用了算法集成。它比使用单个模型预测出来的结果要精确的多&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;需要大量的维护工作&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;决策树算法（Decision Tree Algorithm）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/dd5f116079cde79755d2116486f4c83965cf13cf"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;决策树学习使用一个决策树作为一个预测模型，它将对一个 item（表征在分支上）观察所得映射成关于该 item 的目标值的结论（表征在叶子中）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;树模型中的目标是可变的，可以采一组有限值，被称为分类树；在这些树结构中，叶子表示类标签，分支表示表征这些类标签的连接的特征。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;分类和回归树（Classification and Regression Tree，CART）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Iterative Dichotomiser 3（ID3）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;C4.5 和 C5.0（一种强大方法的两个不同版本）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;容易解释&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;非参数型&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;趋向过拟合&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;可能或陷于局部最小值中&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;没有在线学习&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;回归（Regression）算法&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/05255719683d9ebb24e7e05bdeb456a667014355"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;回归是用于估计两种变量之间关系的统计过程。当用于分析因变量和一个 多个自变量之间的关系时，该算法能提供很多建模和分析多个变量的技巧。具体一点说，回归分析可以帮助我们理解当任意一个自变量变化，另一个自变量不变时，因变量变化的典型值。最常见的是，回归分析能在给定自变量的条件下估计出因变量的条件期望。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;回归算法是统计学中的主要算法，它已被纳入统计机器学习。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;普通最小二乘回归（Ordinary Least Squares Regression，OLSR）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;线性回归（Linear Regression）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;逻辑回归（Logistic Regression）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;逐步回归（Stepwise Regression）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;多元自适应回归样条（Multivariate Adaptive Regression Splines，MARS）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;本地散点平滑估计（Locally Estimated Scatterplot Smoothing，LOESS）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;直接、快速&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;知名度高&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;要求严格的假设&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;需要处理异常值&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;人工神经网络&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/d6e5c216ef2c1995991a2407b982ba6ae64cfd50"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人工神经网络是受生物神经网络启发而构建的算法模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;它是一种模式匹配，常被用于回归和分类问题，但拥有庞大的子域，由数百种算法和各类问题的变体组成。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;感知器&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;反向传播&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Hopfield 网络&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;径向基函数网络（Radial Basis Function Network，RBFN）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在语音、语义、视觉、各类游戏（如围棋）的任务中表现极好。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;算法可以快速调整，适应新的问题。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;需要大量数据进行训练&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;训练要求很高的硬件配置&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;模型处于「黑箱状态」，难以理解内部机制&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;元参数（Metaparameter）与网络拓扑选择困难。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;深度学习（Deep Learning）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/ecd4966b8790e2ac1c640b92fbbe34a63d4fa1d4"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度学习是人工神经网络的最新分支，它受益于当代硬件的快速发展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;众多研究者目前的方向主要集中于构建更大、更复杂的神经网络，目前有许多方法正在聚焦半监督学习问题，其中用于训练的大数据集只包含很少的标记。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;深玻耳兹曼机（Deep Boltzmann Machine，DBM）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Deep Belief Networks（DBN）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;卷积神经网络（CNN）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Stacked Auto-Encoders&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点/缺点&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：见神经网络&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;支持向量机（Support Vector Machines）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/88831bc8ad0b0624c1d98295e25f9155887a4cef"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;给定一组训练事例，其中每个事例都属于两个类别中的一个，支持向量机（SVM）训练算法可以在被输入新的事例后将其分类到两个类别中的一个，使自身成为非概率二进制线性分类器。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;SVM 模型将训练事例表示为空间中的点，它们被映射到一幅图中，由一条明确的、尽可能宽的间隔分开以区分两个类别。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随后，新的示例会被映射到同一空间中，并基于它们落在间隔的哪一侧来预测它属于的类别。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在非线性可分问题上表现优秀&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;非常难以训练&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;很难解释&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;降维算法（Dimensionality Reduction Algorithms）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/f3b8236410e4d8d8aa94b51bc68f6845f38eb293"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;和集簇方法类似，降维追求并利用数据的内在结构，目的在于使用较少的信息总结或描述数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这一算法可用于可视化高维数据或简化接下来可用于监督学习中的数据。许多这样的方法可针对分类和回归的使用进行调整。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;主成分分析（Principal Component Analysis (PCA)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;主成分回归（Principal Component Regression (PCR)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;偏最小二乘回归（Partial Least Squares Regression (PLSR)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Sammon 映射（Sammon Mapping）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;多维尺度变换（Multidimensional Scaling (MDS)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;投影寻踪（Projection Pursuit）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;线性判别分析（Linear Discriminant Analysis (LDA)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;混合判别分析（Mixture Discriminant Analysis (MDA)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;二次判别分析（Quadratic Discriminant Analysis (QDA)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;灵活判别分析（Flexible Discriminant Analysis (FDA)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;可处理大规模数据集&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;无需在数据上进行假设&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;难以搞定非线性数据&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;难以理解结果的意义&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;聚类算法（Clustering Algorithms）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/e47270e40957f8c0b48f0f0badb73f2ea9917638"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;聚类算法是指对一组目标进行分类，属于同一组（亦即一个类，cluster）的目标被划分在一组中，与其他组目标相比，同一组目标更加彼此相似（在某种意义上）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;K-均值（k-Means）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;k-Medians 算法&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Expectation Maximi 封层 ation (EM)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;最大期望算法（EM）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;分层集群（Hierarchical Clstering）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;让数据变得有意义&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;结果难以解读，针对不寻常的数据组，结果可能无用。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;基于实例的算法（Instance-based Algorithms）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/25639eb05f7f7fb1dabf2d6522db35fa646e44a7"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基于实例的算法（有时也称为基于记忆的学习）是这样学 习算法，不是明确归纳，而是将新的问题例子与训练过程中见过的例子进行对比，这些见过的例子就在存储器中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;之所以叫基于实例的算法是因为它直接从训练实例中建构出假设。这意味这，假设的复杂度能随着数据的增长而变化：最糟的情况是，假设是一个训练项目列表，分类一个单独新实例计算复杂度为 O（n）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;K 最近邻（k-Nearest Neighbor (kNN)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;学习向量量化（Learning Vector Quantization (LVQ)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;自组织映射（Self-Organizing Map (SOM)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;局部加权学习（Locally Weighted Learning (LWL)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;算法简单、结果易于解读&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;内存使用非常高&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;计算成本高&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;不可能用于高维特征空间&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;贝叶斯算法（Bayesian Algorithms）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/b3d91267f38f52801b65c70ce708b370df32a497"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;贝叶斯方法是指明确应用了贝叶斯定理来解决如分类和回归等问题的方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;朴素贝叶斯（Naive Bayes）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;高斯朴素贝叶斯（Gaussian Naive Bayes）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;多项式朴素贝叶斯（Multinomial Naive Bayes）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;平均一致依赖估计器（Averaged One-Dependence Estimators (AODE)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;贝叶斯信念网络（Bayesian Belief Network (BBN)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;贝叶斯网络（Bayesian Network (BN)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;快速、易于训练、给出了它们所需的资源能带来良好的表现&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如果输入变量是相关的，则会出现问题&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;关联规则学习算法（Association Rule Learning Algorithms）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/6d720e446249498b469a04fec4cda2d7527c0f17"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;关联规则学习方法能够提取出对数据中的变量之间的关系的最佳解释。比如说一家超市的销售数据中存在规则 {洋葱，土豆}=&amp;gt; {汉堡}，那说明当一位客户同时购买了洋葱和土豆的时候，他很有可能还会购买汉堡肉。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Apriori 算法（Apriori algorithm）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Eclat 算法（Eclat algorithm）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;FP-growth&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图模型（Graphical Models）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/c86b7f4292c3f340016b6d1df3fb45c245b55abb"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;图模型或概率图模型（PGM/probabilistic graphical model）是一种概率模型，一个图（graph）可以通过其表示随机变量之间的条件依赖结构（conditional dependence structure）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;贝叶斯网络（Bayesian network）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;马尔可夫随机域（Markov random field）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;链图（Chain Graphs）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;祖先图（Ancestral graph）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;模型清晰，能被直观地理解&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;确定其依赖的拓扑很困难，有时候也很模糊&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&amp;copy;本文为机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Mon, 20 Feb 2017 12:00:06 +0800</pubDate>
    </item>
    <item>
      <title>独家 | NOR-NET技术详解：AI技术落地移动端新时代即将崛起</title>
      <link>http://www.iwgc.cn/link/</link>
      <description>
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：高静宜&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;技术指导：杨浩进&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote style="color: rgb(62, 62, 62); font-size: 16px; white-space: normal; max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/blockquote&gt;&lt;blockquote style="color: rgb(62, 62, 62); font-size: 16px; white-space: normal; max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;在这个时代，人类生活离不开智能设备。无论是随身携带的手机，还是腕上的智能手表，都与人工智能息息相关。同时，人类的生活方式也在不断化繁为简，现代人出行只需带上一只手机，便可以有效解决社交沟通、交易支付、出行交通等一系列问题。随着社会便携化、智能化的发展需求，在移动端实现人工智能也已经成为大势所趋。然而，人工智能的实现可能不仅需要在硬件配备方面进行大量投入，还需要大型数据中心的支撑。那么，如何在移动端建立可以遍布人类生活的 AI 技术力量呢？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2017 年 2 月 2 日，位于美国西雅图的 AI 创业公司 xnor.ai 宣布获得来自麦德罗纳风险投资集团（Madrona Venture Group）和艾伦人工智能研究所（Allen Institute for Artificial Intelligence）的 260 万美元的种子融资。这个平台致力于开发不依赖于数据中心或互联网连接的，可以直接有效地在移动端或嵌入式设备（例如手机、无人驾驶车辆等）上运行的深度学习模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Xnor.ai 平台的这项针对移动端部署深度学习研发技术，无论是从响应性、速度还是可靠性上来说，都可以达到前所未有的水平。而且，由于数据全部存储于移动端设备上，个人隐私可以得到高水平保障。例如，就物体检测的性能而言，业界完全可以把这项技术应用于手机上，实现物体的实时检测。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;事实上，xnor.ai 团队就曾将 XNOR-Net 部署在价值 5 美元的 Raspberry Pi Zero 上，通过连接一个摄像头实现了实时视频分析，这段网站上的 demo 展示出的实时检测分析效果十分引人注意，给人很强的视觉冲击力。如果在类似于 Raspberry Pi Zero 这样的移动设备上都能进行对枪支和刀具的实时监测并及时报警，那么人们完全可以利用这项技术针对性地开发出更多 AI 安防产品，拓展 AI 安防领域，更不用说这项技术在其他领域中潜在的巨大商业价值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen="" class="video_iframe" data-vidtype="1" frameborder="0" height="417" src="https://v.qq.com/iframe/preview.html?vid=v03750s4kee&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0" width="556"&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;众所周知，深度学习模型大量的矩阵运算使 GPU 加速成了必不可少的硬件支持，这使得深度网络难以在运算资源有限的移动设备上面实现。那么，xnor.ai 又是如何将深度网络部署于移动端的呢？在这里，不得不提到二值神经网络这个概念。&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随着对神经网络研究深度不断推进，学界研究人员发现传统的神经网络对计算成本和内存容量要求较高，而二值化则可以有效地改善这些问题。二值化网络不仅有助于减小模型的存储大小，节省存储容量，而且能加快运算速度，降低计算成本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2015 年 11 月，来自于 Yoshua Bengio 教授带领的加拿大蒙特利尔大学实验室团队的 Matthie Courbariaux 发表了关于二值神经网络 BinaryConnect 的相关论文（BinaryConnect：Training Deep Learing Neural Networks with binary weight during propagations），引起了广泛关注，开启了崭新的二值化网络时代。论文中提出了 BinaryConnect 算法的关键在于仅在前向传播和反向传播中对权重进行二值化 1 或-1，而在参数更新过程保持权重的全精度（即仍为浮点数），这样的做法可以省去接近三分之二的矩阵运算，训练时间和内存空间都得到了大幅度优化，同时，BinaryConnect 在 MNISIST,CIFAR-10 和 SVHN 图像分类数据集上的实验效果可以达到当时世界领先水平。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在此基础上，Matthieu 和 Itay 随后联合发表的论文（Binarized Neural Networks：Training Networks with Weights and Activations Constrained to +1 or -1）提出了更完善的网络模型&amp;mdash;&amp;mdash;BinaryNet，将权值和隐藏层激活值同时进行二值化，并利用 xnorcount 和 popcount 运算操作代替网络中传统的算术运算。这个算法在常用图像数据集上的模型二值化实验也比较成功，可以减少约 60% 的计算复杂度，甚至可以在保证分类准确率的情况下，减少七倍的 GPU 运行时间。同时，实验团队也公开了在 CUDA、Theano 以及 Torch 上的代码，不过很可惜的是，这个算法并没有在如 ImageNet 的大数据集上证明精度是否可以维持。在尽可能减小模型准确率损失的情况下，BinaryNet 的出现通过缩减模型大小，简化运算难度对算法进行加速。这使得深度网络部署于移动端的前景初见曙光。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;值得一提的是，随着神经网络技术的蓬勃发展，许多教授、学者投身工业界，像 Matthie Courbariaux 现已投身 Google，并负责在 TensorFlow 框架中实现对深度模型的量化任务。不同于 Matthie 在论文中的二值化概念 (即不丢失模型准确率，只压缩模型大小), 实际投入应用的量化更适合被理解为离散化。一般来说，在训练神经网络的时候，要对权重做一些微小的调整，而这些微小的调整需要浮点精度才能正常工作，而低精度计算会被网络当做一种噪声。深度网络的一个奇妙之处就在于它可以很好地应对输入噪音，因为网络可以把低精度计算当做一种噪声，这使得量化后的网络在具备较少信息的数值格式下，仍能产生精确的结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量化网络有两个动机，一是缩小尺寸，这是通过存储每层的最大和最小值，然后把每一个浮点值压缩成 8-bit 整数来表示。二是降低资源需求，这需要整个计算都用 8-bit 输入和输出来实现。量化压缩是存在风险的，目前的版本似乎还不是很成熟，Github 上面有很多开发人员认为利用这种方法量化后的模型效率较低。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;2016 年 3 月，Mohammad Rastegari 等人在论文 (XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks) 中首次提出了 XNOR-Net 的概念。这篇论文旨在利用二值化操作寻找到最优的简化网络，并分别介绍了两种有效的网络：Binary-Weight-Networks 和 XNOR-Networks。Binary-Weight-Networks 是对 CNN 中所有的权重做近似二值化，可以节省 32 倍的存储空间。而且，由于权重被二值化，卷积过程只剩加减算法，不再包括乘法运算，可以提高约两倍的运算速度，这促使 CNN 可以在不牺牲准确率的情况下在小存储设备上使用，包括便携式设备。Binary-Weight-Networks 区别于 BinaryNet 的地方在于它进行二值化的方法和网络结构。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;XNOR-Networks 算法则是对 CNN 中所有的权重和输入同时做近似二值化，如果卷积运算中的所有操作数都是二进制的，那么两个二进制向量的点乘就可以等同于同或运算和位运算。作者在这篇文章里主要有两个贡献：一是引入比例因子，大幅度提升精度；二是对典型常规的 CNN 构成进行改动。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;XNOR-Net 算法的基本思路如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Step1：定义一个 L 层的 CNN 结构，使用三个元素 I,W,* 来表示，I 表示卷积输入，W 表示滤波器，*表示卷积算子；利用比例因子&amp;alpha;帮助二值化滤波器去近似全精度滤波器权重，利用比例因子&amp;beta;帮助二值化的输入去近似全精度输入值。这有点类似于批规范化（Batch Normalization）中的仿射参数（Affine Parameters），但不同的是，这里不是通过为网络学习获得的，而是通过计算平均值得到的。在探索计算比例因子&amp;beta;的时候，要对每一次卷积产生的子张量都计算一个&amp;beta;，这一步骤会产生很多冗余的计算。为了降低计算量，作者把输入的所有 channels 计算一个绝对值的平均值矩阵 A，然后通过一个二维的滤波器 k 和 A 的卷积生成 K，那么 K 就包含了针对输入 I 在所有子张量上的比例因子。通过这样一系列数学推导，输入 I 与权重 W 的二值化卷积可以被近似为：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/d2f1f47fa8a6ccd924b0ffd856bc04889f804b52"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;具体过程如下图：&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/59182724bf853619a206eb68a9f7290221dfb5c2"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;图 1：XNOR-Net 近似二值化卷积过程（Mohammad Rastegari et al.）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Step2：一个典型的 CNN 具有卷积、批规范化、激活、池化这样的四层结构，其中，池化层可以对输入运用任何种类的池化方式。但在二值化的输入（-1,1）进入到池化过程时，会产生大量的信息丢失。例如，对二值化输入进行 max-pooling 时，会导致大部分输入只剩+1，使得消息消减，精度降低。为了解决这个问题，作者改善了网络结构，改变这几层的顺序，首先实行批规范化，保证 0 均值，然后进行二值化激活，使数据都是+1 和-1，再做二值化卷积，此时由于比例因子的作用输出的不再是-1 和+1，这会相对减少信息丢失。在这里，作者建议在二值化卷积后加一个非二值化激活步骤（如 ReLU），这可以帮助训练比较复杂的网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;具体过程如下图：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/2be183502f93ea4bc4a403fb273ca69b3647c578"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;图 2：典型的 CNN 与 XNOR-Net 结构（Mohammad Rastegari et al.）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;XNOR-Net 团队在自主搭建的轻型神经网络框架 DarkNet 中实现了在 CPU 上 58 倍速度的提升，这意味着 XNOR-Net 可以在小内存设备上完成实时任务。事实上，在 2016 计算机视觉大会上，XNOR-Net 团队把 yolo object detection 算法的 xnor 版本在 iphone 上面做到了实时探测就成了一大亮点。XNOR-Net 的出现弥补了 BinaryNet 文章的缺失，首次让二值神经网络在 ImageNet 上面完了实验。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有趣的是，XNOR-Net 团队曾在 Github 上公开代码，甚至包括让其名声大噪的 Yolo network，但在公开发表的论文中却并没有公开 C/C++源代码而只是做了 Torch 的版本公开，曾经发布在 Github 的版本也昙花一下，不久便被撤回了。现在看来，这些举动都无疑都是为之后 XNOR-Net 的商业化做准备。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Xnor.ai 的一系列举措不禁让人联想到去年大火的 Prisma APP 离线版的上架以及新一代智能手表操作系统 Andriod Wear 2.0 的发布。这些都是在移动端实现深度学习的经典产品实例，而这些技术成果与研发产品都验证了 Facebook 对未来十年的重点研发战略领域的远见卓识&amp;mdash;&amp;mdash;连接世界、普及网络；人工智能；虚拟现实和增强现实。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除此之外，去年年底 Facebook 发布的 Caffe2Go 也是可以嵌入、部署于移动设备的深度学习框架，具有规模小、训练速度快、对计算机性能要求低等性能。其精华在于 Facebook 硬件优化工程师和算法专家（以贾扬清为代表）做了大量的针对性能上的优化，才使 Caffe2Go 可以顺利部署于手机上。类似的还有 Google 发布于 Github 上的 TensorFlow android camera demo，在这里，Google 将较为复杂的 inception v3 图片分类网络模型进行量化压缩减小 4 倍左右，然后部署于安卓手机上，也可以完成手机端的物体识别、行人检测等任务。虽然这些优化似乎更多是工程意义上的，而不是算法本身具备创新性，但是这些互联网巨头公司的行动无疑会带给我们一些启示：将深度学习框架部署于移动端是未来的一个主流发展趋势。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前，深度学习框架的开发及优化发展迅速，种类也不少，不过，可以支持移动端的框架还是相对少数的，到底哪种框架是部署于移动端的最佳选择，这还有待于考证。相较于 TensorFlow 这种比较复杂的主流深度学习框架，MXNET 作为一种十分灵活、对内存要求较少的深度开源框架也被业界看好，而且它本身就提供了对多种移动端的支持。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;去年 6 月，国内 Face++推出了关于 DoReFa-Net 算法的文章 ( DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients)。DoReLa-Net 对比例因子的设计更为简单，这里并没有针对卷积层输出的每一个过滤映射计算比例因子，而是对卷积层的整体输出计算一个均值常量作为比例因子。这样的做法可以简化反向运算，因为在他们反向计算时也要实现量化。DoReLa-Net 的贡献在于提供了不同量化因子的实验结果，即 2,4,8,16,32 bit 的权重、激活函数量化，同时在后向反馈中也实现了梯度的量化。对于梯度二值化问题，XNOR-Net 中只提出了理论的计算方法，未实现 4~16 bit 的量化实验，也没有在反向梯度计算中使用二值运算。在 SVHN 和 ImageNet 上的实验都可以说明 DoReFa-Net 在有效地应用于 CPU,FPGA,ASIC 和 GPU 上，具有很大的潜力和可行性。但是 DoReLa-Net 并没有使用 xnor 和 popcount 运算，因此实验结果只具备精度参考价值，没有任何加速的效果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人工智能现如今已渗透在医疗、安防、车辆交通、教育等方方面面的领域，将 AI 技术移植到普罗大众的便携式生活中成为必然，未来更多致力于实现神经网络嵌入于移动端的产品将会应运而生，例如，车辆上的导航设施、游戏的手机客户端以及各种各样的手机 APP。这一方面是源于，在移动端实现人工智能十分方便、便携，它可以随时随地满足人们的各种需求；另一方面，在离线的情况下，数据无需上传下传，降低了信息传递时间，同时还能增强用户隐私空间。人们有意愿、有需求直接把 AI 掌控在自己手中，从而达到进一步改善生活品质，甚至于改变生活方式的目的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;同时，从工业发展的角度，人工智能移动端的推行也势在必行。工业机器人、家居机器人等工业化产品也需要依托于具有可移植功能的嵌入式芯片。在硬件条件的发展限制了深度学习运行速度的时候，软件算法技术改进将会不断革新，在这个革新过程中，终端设备智能化已经初见曙光。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，在此过程中，还有一些有待于攻克的瓶颈和存在的问题。比如，如何改善二值神经网络模型在大规模数据库上的表现存在不足的问题；如何对现有的二值化网络算法进行精度和速度上的优化；而进行二值化的网络模型相比于全精度的网络，存在的信息损失这个缺陷，是否可以被三值网络来弥补；还有一个在工业领域十分重要的问题，如何将理论算法高效地落地，甚至是否可以开发出具备落地性的网络模型或是框架。这些问题都将是未来研发人员的关注焦点和研究方向。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;到底 xnor.ai 是否会在人工智能领域掀起一场腥风血雨，我们让时间来解答这个问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&amp;copy;本文为机器之心原创，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Mon, 20 Feb 2017 12:00:06 +0800</pubDate>
    </item>
    <item>
      <title>专栏 | 千人千面智能淘宝店铺背后的算法研究登陆人工智能顶级会议AAAI 2017</title>
      <link>http://www.iwgc.cn/link/</link>
      <description>
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;机器之心专栏&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;作者：周畅（钟煌）、刘效飞（翼升）等&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote style="color: rgb(62, 62, 62); font-size: 16px; white-space: normal; max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;千人千面模块上线，每一家淘宝店铺从此都可能有一个隐形智能导购，推荐算法再升级。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;电商时代，消费者对推荐系统已经不再陌生。「蓦然回首」，你发现喜欢的商品就在首页显眼处。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如今，不仅仅是电商网站首页会给你贴心推荐。你逛进一家淘宝商家的店铺，也很有可能享受到推荐算法的服务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是阿里商家事业部推出的智能店铺「千人千面」模块。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;阿里商家事业部相关负责人介绍，单纯通过算法做出的商品推荐，未必符合商家利益。常有商家抱怨，自家想卖的商品得不到推荐，营销被算法牵着鼻子走。而「千人千面」，就是先让商家给出他们想要推送的商品集，算法再从指定候选集中为进入某家商铺的消费者做个性化推荐。如此一来，算法可以为商家的营销服务，为商家既定的 营销计划「锦上添花」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不过要做到这一点并不简单。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;业界推荐系统往往由 Matching 和 Ranking 两部分组成。Matching 部分会根据全网用户的浏览、加购、收藏等行为数据，在一个庞大的商品池中找出较小的候选集。Ranking 则是利用综合用户 Profile，偏好，以及商品特征等信息训练得出的一个打分排序模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是，阿里电商目前拥有百万级别的活跃店铺，单个用户在单个特定店铺内的行为记录非常匮乏，很难按传统方法有效进行 matching。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对此，阿里商家事业部提出一种高可扩展性的 Graph Embedding（图嵌入）方法，并创新性地将它应用到商品的 embedding 中。它能够以非常小的存储空间来计算任意两个商品的相似度。就算你此前从未踏足这家店铺，算法也能根据你此前在别家的浏览记录，从店铺里挑出你可能喜欢的商品，摆在你面前。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;模块投入使用后，商家的商品点击率提升了 30%，成交量提升 60%。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从学术层面来说，该 Graph Embedding 方法可学习到能够描述图中节点间高阶的、非对称相似度的低维 Embedding 向量，并且可以在理论上解释这种基于机器学习的方法和基于预定义的传统节点间相似度的关系，相关论文已被人工智能领域的顶级会议 AAAI'2017 接收。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来是对该论文的中文讲解，完整论文PDF可点击阅读原文下载：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;工业界的推荐系统通常由 Matching 和 Ranking 两个部分组成，Matching 部分会根据全网用户的浏览、加购、收藏等行为数据，利用协同过滤一类的算法（例如基于商品的 ItemCF）在一个庞大的商品池中找出一个足够小的候选集，以缩小后续算法需要评估的范围。Ranking 则是利用综合用户 Profile，偏好，以及商品特征等额外信息训练得出的一个打分排序模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的推荐场景，即对于店铺私域内的千人千面推荐模块来说，其与公网推荐的重要区别在于，推荐的目标仅限于很小的一部分商家指定的商品集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;传统的 Matching 这部分所遇到的难题在于，阿里电商目前拥有百万级别的活跃店铺，这使得单个用户在单个店铺内的行为记录非常稀疏。而在很多情况下，用户在近期首次进入某商铺主页时，由于缺乏店内的行为信息（如足迹商品），很难有效利用店内 ItemCF 来进行推荐。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;ItemCF 的核心问题之一在于如何有效衡量与计算 item 与 item 之间的相似度\parencite{recsurvey05}。对于全网推荐的应用场景，由于商品数量太大，通常我们会离线计算出每个 item 前 k 个相似的 item list\parencite{itemcftopk}，来用于在线打分的推荐方案。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，如果我们直接用全网 topk item 相似度的数据，对于每个商品来说，与他相似的商品数目其实可能很多，但由于 topk 的限制（通常小于 200），只有极少数店铺的商品才能够被召回，即基于全网 top-k 的商品相似度在同店推荐中的召回能力比较有限。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当然，我们可以使用同样的方法，对于每个店铺，仅计算店铺内部的 i2i 数据，来完成推荐。这样做的缺陷在于，完全无法覆盖用户没有店内足迹的情况。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，为了提高相似商品的召回，以覆盖用户没有店内足迹的情况，我们使用了图嵌入算法 APP 来基于用户浏览记录来做商品嵌入&amp;mdash;&amp;mdash;试图将商品嵌入到一个低维空间中，同时保存一些商品之间的结构特征，即商品相似度。这样就可以用稳定、较小的代价在线算出任意两个商品之间的相似度了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「旺铺智能版智能模块」是一款面向中小商家的、商家可运营的个性化商品装修模块。在商家侧算法提供面向场景的选品，同时允许商家对算法商品池进行调整，或者完全手动建立商品池；在消费者端，个性化算法基于商家设置的商品池对访客进行实时投放。产品设计上一定程度上满足了商家确定性需求，在此基础上通过个性化算法提升成交转化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们研究 Graph Embedding 的初衷是为旺铺模块千人千面场景提供覆盖率高的 Match 支持。因为用户在店铺内部的行为稀疏，传统的基于 I2I 的 match 覆盖率较低。而通过 Embedding 可以计算出任意两个商品之间的 Match 分数，极大改善覆盖率问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们提出一种高可扩展性的 Graph Embedding 方法，该方法可学习到能够可描述图中节点间高阶的、非对称相似度的低维 Embedding 向量。同时我们提供理论上的解释，来阐述这种基于机器学习的方法和基于预定义的传统节点 I2I 相似度的关系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1.背景介绍 &amp;amp; 相关工作&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;图是一种抽象程度高、表达能力强的数据结构，它通过对节点和边的定义来描述实体和实体之间的关联关系。常用的图有社交关系网络，通信网络，商品网络，知识图谱等等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而如何衡量图中节点之间的相似度，对于朋友推荐、商品推荐、以及常见的分类聚类问题来说都是一个很重要的前置步骤。Graph Embedding 可以理解成是一种降维技术，它可以将图中的节点映射到一个低维空间里，我们只需要通过计算低维向量之间的关系，就可以得到原来节点之间的关联关系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管传统 Embedding 技术被研究了很久，但他们的复杂度往往都在 N^2 级别以上，难以适应大规模数据。最近的一系列可扩展性较强的 Graph Embedding 工作主要是从 DeepWalk【6】开始，后面有 Line【7】，Node2vec【2】等等。DeepWalk 在原图中做了一些路径采样，然后将路径当作一个句子，路径中的点当作单词，之后就采用 word2vec 中提出的 Skip-Gram with Negative-Sampling【5】方式进行训练，得到每一个节点的 embedding 向量。Line 只针对边进行采样。Node2vec 可以调节参数来进行 BFS 或者 DFS 的抽样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而图中的路径采样在概率上有着非常严重的非对称性，之前的这些方法并没有注意到这件事，也没有从理论上来思考为什么这么干不太科学。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;例如在有向图（图 1）中，对于 A 来说，可能并不关心 C，而对于 C 来说，A 很可能是他的兴趣点。即使在无向图中（图 2），也有同样的现象。这样的节点非对称性关系是由于节点周围的图结构不同造成的。而从 C 出发的路径 C-&amp;gt;B-&amp;gt;A 和从 A 出发的路径 A-&amp;gt;B-&amp;gt;C 有着完全不相同的概率（0.5，0.08）。因此我们不能认为 C-&amp;gt;B-&amp;gt;A 这条路径的产生会带来一个（A-&amp;gt;C）的正样本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/3cdc7e66c9bb3b6e00c609b03c494039e9d42039"/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;图 1 有向图中的非对称性&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/ccf75790e80f02f5e920e4d3fe90167feb451542"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 2 无向图中的非对称性&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.我们的工作&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的工作所做的改进其实非常简单，首先为了有能力表达非对称性相似度，我们为每个节点引入了两种 Embedding 向量，分别是 Source 向量和 Target 向量，如图一所示。我们将对于 A 来说 B 的相似度记为 sim(A，B)，并使用 Source(A) 与 Target(B) 的点积来表示，图一中我们可以从 Embedding 中算出 sim(A，C)&amp;lt;sim(C, A)。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/8543b60289b32da836ff7c5bd90a845e274c58a8"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 3 节点的两种 Embedding 身份&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其次我们遵循了一种标准的、用来估计 Rooted PageRank【3】的蒙特卡洛随机游走的方法【1】【8】来进行正例的采样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;节点 u 对于节点 v 的 Rooted PageRank（PPR）值代表了从 v 出发落在 u 点的概率。我们认为以这种方式生成图中节点对的正样例是更加自然、合理、有说法的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这类游走方法都是基于常见的 Random Ｗalk with Restart，即从一个点出发以（1-alpha）的概率选择邻居进行跳转，另外 alpha 的概率跳转回自己。那么现有的几种方法稍有一些区别：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;例如 Monte Carlo End Point 只保留首次跳转之前的节点，Monte Carlo Full Path 保留路径上的所有节点，将路径的后缀也当作有效的采样【1】。因为这两条路径对于起始点来说可以看作是相互独立的。在最新的工作中也有对前缀路径进行重用的【8】，就不再此展开。值得注意的是，后两种的采样效率相对于 1 来说要更高，尽管这三种方法都在各自的文章中被证明是正确且有 Bound 的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们遵循这类游走方法，企图给图中的节点对创造一些正样本。对于每一个被标记为正例的样本（A, B）我们会根据目标函数更新 A 的 source 向量和 B 的 target 向量。并且随机采样其他的节点作为负样本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们定义给定节点 u，可以预测到节点 v 的概率&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/55c33a01ee6ee09b22e37af7e7a5e0bb3eb9ec83"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;利用 Skip-Gram with Negative-Sampling【5】，近似等价于优化&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/26fa13de0f09c4b5c311bc1a04d0e8811852a2df"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;K 是负采样数，P_D（n）在图中可用均匀分布替代。则总的目标函数如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/e378f320304814d6558d3387e10db5f4bcf6b7e6"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面我们来解释一个有趣的现象，我们非对称的点积最终会是以学习出两点之间的 PPR 的对数为目标。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/5028183426e29bd34e3d6d7f747e1191dddb1cfa"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这里，类似于 Levy【4】的证明，当维数充分大时，可看作互相独立的变量。于是另下式为 0：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/42f0a21deacb30585cc3077f5ae658e5ab0890e0"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;得到：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/f8424293e9519e301909b4dc90690ca4f5135c8c"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于|V|, k 均为常数，我们可以看出 x 只跟 Rooted PageRank 的模拟值 Sim_u(v) 呈对数关系。通过以上证明，论证了该方法可以保持非对称的、高阶相似度的说法，因为 Rooted PageRank 就是一种非对称的、高阶的相似度度量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3.小数据集上的实验&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Link Prediction Task（AUC）：Embedding 方法相对于传统 Pre-defined i2i 指标来说，在 AUC 上很占便宜。因为传统指标大多基于 2 跳以内的关系，包括阿里内部使用的 Swing。这样就有很多正例的结果是 0&amp;mdash;&amp;mdash;完全无法和负例分开，AUC 不高。可以看出我们的方法（APP）在比现有的方法要好一些。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/c130bb6ad2340683fd28780b85172a2cece52f15"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下表是为了体现非对称性的优势，而在负样本中加大了单向边的比例，即 A-&amp;gt;B 有边，B-&amp;gt;A 无边。可以看出我们与之前的方法在 LinkPrediction 任务上有显著提升。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/0d86de282bc9b60bacd5f1559e7cee1a7c7b3968"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Node Recommendation：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/70dc493b9927e1734ecf1c6018399c02689141f4"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;值得注意的是，在寻找 topk 的这个问题当中，我们发现之前的 Embedding 方法似乎并没有传统指标靠谱。但我们的方法可以比较好的反应 Topk 的相似关系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;4.在模块千人千面中的实践&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了缓解用户在店铺内部行为的稀疏性，我们将用户 Session 中的全网点商品击序列转化成一个全网商品点击转换图。之后应用我们的 Graph Embedding 方法得到商品向量。该向量可以用来计算用户点击行为所产生的商品之间的相似度。下图是我们与传统 topk i2i 方法在真实场景中的点击率比较。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/6bc4a1121f0b54245f975d4538186304983ba15f"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的这项工作目前还只是作为 Match 打分的基础算法，我们正在尝试进一步融合一些外部信息，如商品文本属性、类目信息等，提高长尾商品的结构化 Embedding 质量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;1.Fogaras, D.; R&amp;acute;acz, B.; Csalog&amp;acute;any, K.; and Sarl&amp;acute;os, T. 2005. Towards scaling fully personalized pagerank: Algorithms, lower bounds, and experiments. Internet Mathematics 2(3):333&amp;ndash;358.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;2.Grover, A., and Leskovec, J. 2016. node2vec: Scalable feature learning for networks. In International Conference on Knowledge Discovery and Data Mining. ACM.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;3.Haveliwala, T. H. 2002. Topic-sensitive pagerank. In Proceedings of the 11th international conference on World Wide Web, 517&amp;ndash;526. ACM.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;4.Levy, O., and Goldberg, Y. 2014. Neural word embedding as implicit matrix factorization. In Advances in neural information processing systems, 2177&amp;ndash;2185.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;5.Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, 3111&amp;ndash;3119.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;6.Perozzi, B.; Al-Rfou, R.; and Skiena, S. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, 701&amp;ndash;710. ACM.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;7.Tang, J.; Qu, M.;Wang, M.; Zhang, M.; Yan, J.; and Mei, Q. 2015. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, 1067&amp;ndash;1077. ACM.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;8.Liu, Q.; Li, Z.; Lui, J.; and Cheng, J. 2016. Powerwalk: Scalable personalized pagerank via random walks with vertex-centric decomposition. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, 195&amp;ndash;204. ACM.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&amp;copy;本文为机器之心专栏，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号或作者获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Mon, 20 Feb 2017 12:00:06 +0800</pubDate>
    </item>
    <item>
      <title>教程 | 从头开始：用Python实现决策树算法</title>
      <link>http://www.iwgc.cn/link/</link>
      <description>
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;选自Machine Learning Mastery&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：&amp;nbsp;Jason Brownlee&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：沈泽江、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;决策树算法是一个强大的预测方法，它非常流行。因为它们的模型能够让新手轻而易举地理解得和专家一样好，所以它们比较流行。同时，最终生成的决策树能够解释做出特定预测的确切原因，这使它们在实际运用中倍受亲睐。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;同时，决策树算法也为更高级的集成模型（如 bagging、随机森林及 gradient boosting）提供了基础。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这篇教程中，你将会从零开始，学习如何用 Python 实现《Classification And Regression Tree algorithm》中所说的内容。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在学完该教程之后，你将会知道：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如何计算并评价数据集中地候选分割点（Candidate Split Point）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如何在决策树结构中排分配这些分割点&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如何在实际问题中应用这些分类和回归算法&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;一、概要&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本节简要介绍了关于分类及回归树（Classification and Regression Trees）算法的一些内容，并给出了将在本教程中使用的钞票数据集（Banknote Dataset）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1.1 分类及回归树&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;分类及回归树（CART）是由 Leo Breiman 提出的一个术语，用来描述一种能被用于分类或者回归预测模型问题的回归树算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们将在本教程中主要讨论 CART 在分类问题上的应用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;二叉树（Binary Tree）是 CART 模型的代表之一。这里所说的二叉树，与数据结构和算法里面所说的二叉树别无二致，没有什么特别之处（每个节点可以有 0、1 或 2 个子节点）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;每个节点代表在节点处有一个输入变量被传入，并根据某些变量被分类（我们假定该变量是数值型的）。树的叶节点（又叫做终端节点，Terminal Node）由输出变量构成，它被用于进行预测。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在树被创建完成之后，每个新的数据样本都将按照每个节点的分割条件，沿着该树从顶部往下，直到输出一个最终决策。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;创建一个二元分类树实际上是一个分割输入空间的过程。递归二元分类（Recursive Binary Splitting）是一个被用于分割空间的贪心算法。这实际上是一个数值过程：当一系列的输入值被排列好后，它将尝试一系列的分割点，测试它们分类完后成本函数（Cost Function）的值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有最优成本函数（通常是最小的成本函数，因为我们往往希望该值最小）的分割点将会被选择。根据贪心法（greedy approach）原则，所有的输入变量和所有可能的分割点都将被测试，并会基于它们成本函数的表现被评估。（译者注：下面简述对回归问题和分类问题常用的成本函数。）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;回归问题&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对落在分割点确定区域内所有的样本取误差平方和（Sum Squared Error）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;分类问题&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：一般采用基尼成本函数（Gini Cost Function），它能够表明被分割之后每个节点的纯净度（Node Purity）如何。其中，节点纯净度是一种表明每个节点分类后训练数据混杂程度的指标。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;分割将一直进行，直到每个节点（分类后）都只含有最小数量的训练样本或者树的深度达到了最大值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1.2 Banknote 数据集&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Banknote 数据集，需要我们根据对纸币照片某些性质的分析，来预测该钞票的真伪。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集中含有 1372 个样本，每个样本由 5 个数值型变量构成。这是一个二元分类问题。如下列举 5 个变量的含义及数据性质：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 图像经小波变换后的方差（Variance）（连续值）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 图像经小波变换后的偏度（Skewness）（连续值）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3. 图像经小波变换后的峰度（Kurtosis）（连续值）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4. 图像的熵（Entropy）（连续值）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;5. 钞票所属类别（整数，离散值）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如下是数据集前五行数据的样本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;code&gt;3.6216,8.6661,-2.8073,-0.44699,0&lt;/code&gt;&lt;br&gt;&lt;code&gt;4.5459,8.1674,-2.4586,-1.4621,0&lt;/code&gt;&lt;br&gt;&lt;code&gt;3.866,-2.6383,1.9242,0.10645,0&lt;/code&gt;&lt;br&gt;&lt;code&gt;3.4566,9.5228,-4.0112,-3.5944,0&lt;/code&gt;&lt;br&gt;&lt;code&gt;0.32924,-4.4552,4.5718,-0.9888,0&lt;/code&gt;&lt;br&gt;&lt;code&gt;4.3684,9.6718,-3.9606,-3.1625,0&lt;/code&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;使用零规则算法（Zero Rule Algorithm）来预测最常出现类别的情况（译者注：也就是找到最常出现的一类样本，然后预测所有的样本都是这个类别），对该问的基准准确大概是 50%。&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你可以在这里下载并了解更多关于这个数据集的内容：UCI Machine Learning Repository。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请下载该数据集，放到你当前的工作目录，并重命名该文件为 data_banknote_authentication.csv。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;二、教程&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本教程分为五大部分：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 对基尼系数（Gini Index）的介绍&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2.（如何）创建分割点&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3.（如何）生成树模型&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4.（如何）利用模型进行预测&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;5. 对钞票数据集的案例研究&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些步骤能帮你打好基础，让你能够从零实现 CART 算法，并能将它应用到你子集的预测模型问题中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.1 基尼系数&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基尼系数是一种评估数据集分割点优劣的成本函数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;数据集的分割点是关于输入中某个属性的分割。对数据集中某个样本而言，分割点会根据某阈值对该样本对应属性的值进行分类。他能根据训练集中出现的模式将数据分为两类。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基尼系数通过计算分割点创建的两个类别中数据类别的混杂程度，来表现分割点的好坏。一个完美的分割点对应的基尼系数为 0（译者注：即在一类中不会出现另一类的数据，每个类都是「纯」的），而最差的分割点的基尼系数则为 1.0（对于二分问题，每一类中出现另一类数据的比例都为 50%，也就是数据完全没能被根据类别不同区分开）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面我们通过一个具体的例子来说明如何计算基尼系数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们有两组数据，每组有两行。第一组数据中所有行都属于类别 0（Class 0），第二组数据中所有的行都属于类别 1（Class 1）。这是一个完美的分割点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先我们要按照下式计算每组数据中各类别数据的比例：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;br&gt;&lt;tbody style="border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent;"&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;proportion = count(class_value) / count(rows)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那么，对本例而言，相应的比例为：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;group_1_class_0 = 2 / 2 = 1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;group_1_class_1 = 0 / 2 = 0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;group_2_class_0 = 0 / 2 = 0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;group_2_class_1 = 2 / 2 = 1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基尼系数按照如下公式计算：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;gini_index = sum(proportion * (1.0 - proportion))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;将本例中所有组、所有类数据的比例带入到上述公式：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;gini_index = (group_1_class_0 * (1.0 - group_1_class_0)) +&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;(group_1_class_1 * (1.0 - group_1_class_1)) +&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;(group_2_class_0 * (1.0 - group_2_class_0)) +&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;(group_2_class_1 * (1.0 - group_2_class_1))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;化简，得：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;gini_index = 0 + 0 + 0 + 0 = 0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如下是一个叫做 gini_index() 的函数，它能够计算给定数据的基尼系数（组、类别都以列表（list）的形式给出）。其中有些算法鲁棒性检测，能够避免对空组除以 0 的情况。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;# Calculate the Gini index for a split dataset&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;def gini_index(groups, class_values):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;gini = 0.0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;for class_value in class_values:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;for group in groups:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;size = len(group)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;if size == 0:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;continue&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;proportion = [row[-1] for row in group].count(class_value) / float(size)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;gini += (proportion * (1.0 - proportion))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;return gini&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以根据上例来测试该函数的运行情况，也可以测试最差分割点的情况。完整的代码如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;# Calculate the Gini index for a split dataset&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;def gini_index(groups, class_values):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;gini = 0.0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;for class_value in class_values:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;for group in groups:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;size = len(group)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;if size == 0:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;continue&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;proportion = [row[-1] for row in group].count(class_value) / float(size)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;gini += (proportion * (1.0 - proportion))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;return gini&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;# test Gini values&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;print(gini_index([[[1, 1], [1, 0]], [[1, 1], [1, 0]]], [0, 1]))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;print(gini_index([[[1, 0], [1, 0]], [[1, 1], [1, 1]]], [0, 1]))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;运行该代码，将会打印两个基尼系数，其中第一个对应的是最差的情况为 1.0，第二个对应的是最好的情况为 0.0。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;1.0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;0.0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.2 创建分割点&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个分割点由数据集中的一个属性和一个阈值构成。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以将其总结为对给定的属性确定一个分割数据的阈值。这是一种行之有效的分类数据的方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;创建分割点包括三个步骤，其中第一步已在计算基尼系数的部分讨论过。余下两部分分别为：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 分割数据集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 评价所有（可行的）分割点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们具体看一下每个步骤。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.2.1 分割数据集&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;分割数据集意味着我们给定数据集某属性（或其位于属性列表中的下表）及相应阈值的情况下，将数据集分为两个部分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一旦数据被分为两部分，我们就可以使用基尼系数来评估该分割的成本函数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;分割数据集需要对每行数据进行迭代，根据每个数据点相应属性的值与阈值的大小情况将该数据点放到相应的部分（对应树结构中的左叉与右叉）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如下是一个名为 test_split() 的函数，它能实现上述功能：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;# Split a dataset based on an attribute and an attribute value&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;def test_split(index, value, dataset):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;left, right = list(), list()&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;for row in dataset:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;if row[index] &amp;lt; value:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;left.append(row)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;else:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;right.append(row)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;return left, right&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;代码还是很简单的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意，在代码中，属性值大于或等于阈值的数据点被分类到了右组中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.2.2 评价所有分割点&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在基尼函数 gini_index() 和分类函数 test_split() 的帮助下，我们可以开始进行评估分割点的流程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对给定的数据集，对每一个属性，我们都要检查所有的可能的阈值使之作为候选分割点。然后，我们将根据这些分割点的成本（cost）对其进行评估，最终挑选出最优的分割点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当最优分割点被找到之后，我们就能用它作为我们决策树中的一个节点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而这也就是所谓的穷举型贪心算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在该例中，我们将使用一个词典来代表决策树中的一个节点，它能够按照变量名储存数据。当选择了最优分割点并使用它作为树的新节点时，我们存下对应属性的下标、对应分割值及根据分割值分割后的两部分数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;分割后地每一组数据都是一个更小规模地数据集（可以继续进行分割操作），它实际上就是原始数据集中地数据按照分割点被分到了左叉或右叉的数据集。你可以想象我们可以进一步将每一组数据再分割，不断循环直到建构出整个决策树。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如下是一个名为 get_split() 的函数，它能实现上述的步骤。你会发现，它遍历了每一个属性（除了类别值）以及属性对应的每一个值，在每次迭代中它都会分割数据并评估该分割点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当所有的检查完成后，最优的分割点将被记录并返回。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;# Select the best split point for a dataset&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;def get_split(dataset):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;class_values = list(set(row[-1] for row in dataset))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;b_index, b_value, b_score, b_groups = 999, 999, 999, None&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;for index in range(len(dataset[0])-1):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;for row in dataset:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;groups = test_split(index, row[index], dataset)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;gini = gini_index(groups, class_values)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;if gini &amp;lt; b_score:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;b_index, b_value, b_score, b_groups = index, row[index], gini, groups&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;return {'index':b_index, 'value':b_value, 'groups':b_groups}&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们能在一个小型合成的数据集上来测试这个函数以及整个数据集分割的过程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X1 X2 Y&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;2.771244718 1.784783929 0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;1.728571309 1.169761413 0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;3.678319846 2.81281357 0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;3.961043357 2.61995032 0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;2.999208922 2.209014212 0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;7.497545867 3.162953546 1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;9.00220326 3.339047188 1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;7.444542326 0.476683375 1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;10.12493903 3.234550982 1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;6.642287351 3.319983761 1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;同时，我们可以使用不同颜色标记不同的类，将该数据集绘制出来。由图可知，我们可以从 X1 轴（即图中的 X 轴）上挑出一个值来分割该数据集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/485be4b866949c75b669170183a2c3bdcd7318b7"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;范例所有的代码整合如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;# Split a dataset based on an attribute and an attribute value&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;def test_split(index, value, dataset):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;left, right = list(), list()&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;for row in dataset:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;if row[index] &amp;lt; value:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;left.append(row)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;else:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;right.append(row)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;return left, right&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;# Calculate the Gini index for a split dataset&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;def gini_index(groups, class_values):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;gini = 0.0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;for class_value in class_values:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;for group in groups:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;size = len(group)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;if size == 0:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;continue&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;proportion = [row[-1] for row in group].count(class_value) / float(size)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;gini += (proportion * (1.0 - proportion))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;return gini&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;# Select the best split point for a dataset&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;def get_split(dataset):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;class_values = list(set(row[-1] for row in dataset))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;b_index, b_value, b_score, b_groups = 999, 999, 999, None&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;for index in range(len(dataset[0])-1):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;for row in dataset:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;groups = test_split(index, row[index], dataset)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;gini = gini_index(groups, class_values)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;print('X%d &amp;lt; %.3f Gini=%.3f' % ((index+1), row[index], gini))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;if gini &amp;lt; b_score:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;b_index, b_value, b_score, b_groups = index, row[index], gini, groups&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;return {'index':b_index, 'value':b_value, 'groups':b_groups}&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;dataset = [[2.771244718,1.784783929,0],&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;[1.728571309,1.169761413,0],&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;[3.678319846,2.81281357,0],&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;[3.961043357,2.61995032,0],&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;[2.999208922,2.209014212,0],&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;[7.497545867,3.162953546,1],&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;[9.00220326,3.339047188,1],&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;[7.444542326,0.476683375,1],&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;[10.12493903,3.234550982,1],&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;[6.642287351,3.319983761,1]]&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;split = get_split(dataset)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;print('Split: [X%d &amp;lt; %.3f]' % ((split['index']+1), split['value']))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;优化后的 get_split() 函数能够输出每个分割点及其对应的基尼系数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;运行如上的代码后，它将 print 所有的基尼系数及其选中的最优分割点。在此范例中，它选中了 X1&amp;lt;6.642 作为最终完美分割点（它对应的基尼系数为 0）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X1 &amp;lt; 2.771 Gini=0.494&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X1 &amp;lt; 1.729 Gini=0.500&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X1 &amp;lt; 3.678 Gini=0.408&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X1 &amp;lt; 3.961 Gini=0.278&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X1 &amp;lt; 2.999 Gini=0.469&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X1 &amp;lt; 7.498 Gini=0.408&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X1 &amp;lt; 9.002 Gini=0.469&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X1 &amp;lt; 7.445 Gini=0.278&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X1 &amp;lt; 10.125 Gini=0.494&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X1 &amp;lt; 6.642 Gini=0.000&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X2 &amp;lt; 1.785 Gini=1.000&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X2 &amp;lt; 1.170 Gini=0.494&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X2 &amp;lt; 2.813 Gini=0.640&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X2 &amp;lt; 2.620 Gini=0.819&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X2 &amp;lt; 2.209 Gini=0.934&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X2 &amp;lt; 3.163 Gini=0.278&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X2 &amp;lt; 3.339 Gini=0.494&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X2 &amp;lt; 0.477 Gini=0.500&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X2 &amp;lt; 3.235 Gini=0.408&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X2 &amp;lt; 3.320 Gini=0.469&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Split: [X1 &amp;lt; 6.642]&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;既然我们现在已经能够找出数据集中最优的分割点，那我们现在就来看看我们能如何应用它来建立一个决策树。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.3 生成树模型&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;创建树的根节点（root node）是比较方便的，可以调用 get_split() 函数并传入整个数据集即可达到此目的。但向树中增加更多的节点则比较有趣。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;建立树结构主要分为三个步骤：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 创建终端节点&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 递归地分割&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3. 建构整棵树&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.3.1 创建终端节点&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们需要决定何时停止树的「增长」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以用两个条件进行控制：树的深度和每个节点分割后的数据点个数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最大树深度：这代表了树中从根结点算起节点数目的上限。一旦树中的节点树达到了这一上界，则算法将会停止分割数据、增加新的节点。更神的树会更为复杂，也更有可能过拟合训练集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最小节点记录数：这是某节点分割数据后分个部分数据个数的最小值。一旦达到或低于该最小值，则算法将会停止分割数据、增加新的节点。将数据集分为只有很少数据点的两个部分的分割节点被认为太具针对性，并很有可能过拟合训练集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这两个方法基于用户给定的参数，参与到树模型的构建过程中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此外，还有一个情况。算法有可能选择一个分割点，分割数据后所有的数据都被分割到同一组内（也就是左叉、右叉只有一个分支上有数据，另一个分支没有）。在这样的情况下，因为在树的另一个分叉没有数据，我们不能继续我们的分割与添加节点的工作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基于上述内容，我们已经有一些停止树「增长」的判别机制。当树在某一结点停止增长的时候，该节点被称为终端节点，并被用来进行最终预测。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;预测的过程是通过选择组表征值进行的。当遍历树进入到最终节点分割后的数据组中，算法将会选择该组中最普遍出现的值作为预测值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如下是一个名为 to_terminal() 的函数，对每一组收据它都能选择一个表征值。他能够返回一系列数据点中最普遍出现的值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;# Create a terminal node value&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;def to_terminal(group):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;outcomes = [row[-1] for row in group]&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;return max(set(outcomes), key=outcomes.count)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.3.2 递归分割&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在了解了如何及何时创建终端节点后，我们现在可以开始建立树模型了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;建立树地模型，需要我们对给定的数据集反复调用如上定义的 get_split() 函数，不断创建树中的节点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在已有节点下加入的新节点叫做子节点。对树中的任意节点而言，它可能没有子节点（则该节点为终端节点）、一个子节点（则该节点能够直接进行预测）或两个子节点。在程序中，在表示某节点的字典中，我们将一棵树的两子节点命名为 left 和 right。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一旦一个节点被创建，我们就可以递归地对在该节点被分割得到的两个子数据集上调用相同的函数，来分割子数据集并创建新的节点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如下是一个实现该递归过程的函数。它的输入参数包括：某一节点（node）、最大树深度（max_depth）、最小节点记录数（min_size）及当前树深度（depth）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;显然，一开始运行该函数时，根节点将被传入，当前深度为 1。函数的功能分为如下几步：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 首先，该节点分割的两部分数据将被提取出来以便使用，同时数据将被在节点中删除（随着分割工作的逐步进行，之前的节点不需要再使用相应的数据）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 然后，我们将会检查该节点的左叉及右叉的数据集是否为空。如果是，则其将会创建一个终端节点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3. 同时，我们会检查是否到达了最大深度。如果是，则其将会创建一个终端节点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4. 接着，我们将对左子节点进一步操作。若该组数据个数小于阈值，则会创建一个终端节点并停止进一步操作。否则它将会以一种深度优先的方式创建并添加节点，直到该分叉达到底部。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;5. 对右子节点同样进行上述操作，不断增加节点直到达到终端节点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/40faf3a05298f0b15a9552867ea437b5c595201f"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.3.3 建构整棵树&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们将所有的内容整合到一起。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;创建一棵树包括创建根节点及递归地调用 split() 函数来不断地分割数据以构建整棵树。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如下是实现上述功能的 bulid_tree() 函数的简化版本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;&lt;em&gt;&lt;code&gt;&lt;span&gt;# Build a decision tree&lt;/span&gt;&lt;/code&gt;&lt;br&gt;&lt;code&gt;&lt;span&gt;def &lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;build_tree&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;(&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;train&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;,&lt;/span&gt;&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&lt;span&gt;max_depth&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;,&lt;/span&gt;&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&lt;span&gt;min_size&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;)&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;:&lt;/span&gt;&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&lt;span&gt;root&lt;/span&gt;&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&lt;span&gt;=&lt;/span&gt;&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&lt;span&gt;get_split&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;(&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;dataset&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;)&lt;/span&gt;&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&lt;span&gt;split&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;(&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;root&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;,&lt;/span&gt;&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&lt;span&gt;max_depth&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;,&lt;/span&gt;&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&lt;span&gt;min_size&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;,&lt;/span&gt;&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&lt;span&gt;1&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;)&lt;/span&gt;&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&lt;span&gt;return&lt;/span&gt;&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&lt;span&gt;root&lt;/span&gt;&lt;/code&gt;&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以在如上所述的合成数据集上测试整个过程。如下是完整的案例。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在其中还包括了一个 print_tree() 函数，它能够递归地一行一个地打印出决策树的节点。经过它打印的不是一个明显的树结构，但它能给我们关于树结构的大致印象，并能帮助决策。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;code&gt;# Split a dataset based on an attribute and an attribute value&lt;/code&gt;&lt;br&gt;&lt;code&gt;def &lt;/code&gt;&lt;code&gt;test_split&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;index&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;value&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;dataset&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;left&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;right&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;list&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;list&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;for&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;row &lt;/code&gt;&lt;code&gt;in&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;dataset&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;if&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;row&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;index&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&amp;lt;&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;value&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;left&lt;/code&gt;&lt;code&gt;.&lt;/code&gt;&lt;code&gt;append&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;row&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;else&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;right&lt;/code&gt;&lt;code&gt;.&lt;/code&gt;&lt;code&gt;append&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;row&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;return&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;left&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;right&lt;/code&gt;&lt;br&gt; &lt;br&gt;&lt;code&gt;# Calculate the Gini index for a split dataset&lt;/code&gt;&lt;br&gt;&lt;code&gt;def &lt;/code&gt;&lt;code&gt;gini_index&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;groups&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;class_values&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;gini&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;0.0&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;for&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;class_value &lt;/code&gt;&lt;code&gt;in&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;class_values&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;for&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;group &lt;/code&gt;&lt;code&gt;in&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;groups&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;size&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;len&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;group&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;if&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;size&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;==&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;0&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;continue&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;proportion&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;row&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;-&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;for&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;row &lt;/code&gt;&lt;code&gt;in&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;group&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;.&lt;/code&gt;&lt;code&gt;count&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;class_value&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;/&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;float&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;size&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;gini&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;+=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;proportion *&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;1.0&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;-&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;proportion&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;return&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;gini&lt;/code&gt;&lt;br&gt; &lt;br&gt;&lt;code&gt;# Select the best split point for a dataset&lt;/code&gt;&lt;br&gt;&lt;code&gt;def &lt;/code&gt;&lt;code&gt;get_split&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;dataset&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;class_values&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;list&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;set&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;row&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;-&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;for&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;row &lt;/code&gt;&lt;code&gt;in&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;dataset&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;b_index&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;b_value&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;b_score&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;b_groups&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;999&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;999&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;999&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;None&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;for&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;index &lt;/code&gt;&lt;code&gt;in&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;range&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;len&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;dataset&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;0&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;-&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;for&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;row &lt;/code&gt;&lt;code&gt;in&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;dataset&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;groups&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;test_split&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;index&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;row&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;index&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;dataset&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;gini&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;gini_index&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;groups&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;class_values&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;if&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;gini&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&amp;lt;&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;b_score&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;b_index&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;b_value&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;b_score&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;b_groups&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;index&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;row&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;index&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;gini&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;groups&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;return&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;{&lt;/code&gt;&lt;code&gt;'index'&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;code&gt;b_index&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;'value'&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;code&gt;b_value&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;'groups'&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;code&gt;b_groups&lt;/code&gt;&lt;code&gt;}&lt;/code&gt;&lt;br&gt; &lt;br&gt;&lt;code&gt;# Create a terminal node value&lt;/code&gt;&lt;br&gt;&lt;code&gt;def &lt;/code&gt;&lt;code&gt;to_terminal&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;group&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;outcomes&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;row&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;-&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;for&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;row &lt;/code&gt;&lt;code&gt;in&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;group&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;return&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;max&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;set&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;outcomes&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;key&lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt;outcomes&lt;/code&gt;&lt;code&gt;.&lt;/code&gt;&lt;code&gt;count&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt; &lt;br&gt;&lt;code&gt;# Create child splits for a node or make terminal&lt;/code&gt;&lt;br&gt;&lt;code&gt;def &lt;/code&gt;&lt;code&gt;split&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;max_depth&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;min_size&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;depth&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;left&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;right&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'groups'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;del&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'groups'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;# check for a no split&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;if&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;not&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;left &lt;/code&gt;&lt;code&gt;or&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;not&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;right&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'left'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'right'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;to_terminal&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;left&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;+&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;right&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;return&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;# check for max depth&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;if&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;depth&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&amp;gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;max_depth&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'left'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'right'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;to_terminal&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;left&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;to_terminal&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;right&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;return&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;# process left child&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;if&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;len&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;left&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&amp;lt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;min_size&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'left'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;to_terminal&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;left&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;else&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'left'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;get_split&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;left&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;split&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'left'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;max_depth&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;min_size&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;depth&lt;/code&gt;&lt;code&gt;+&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;# process right child&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;if&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;len&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;right&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&amp;lt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;min_size&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'right'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;to_terminal&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;right&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;else&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'right'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;get_split&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;right&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;split&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'right'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;max_depth&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;min_size&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;depth&lt;/code&gt;&lt;code&gt;+&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt; &lt;br&gt;&lt;code&gt;# Build a decision tree&lt;/code&gt;&lt;br&gt;&lt;code&gt;def &lt;/code&gt;&lt;code&gt;build_tree&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;train&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;max_depth&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;min_size&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;root&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;get_split&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;dataset&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;split&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;root&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;max_depth&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;min_size&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;return&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;root&lt;/code&gt;&lt;br&gt; &lt;br&gt;&lt;code&gt;# Print a decision tree&lt;/code&gt;&lt;br&gt;&lt;code&gt;def &lt;/code&gt;&lt;code&gt;print_tree&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;depth&lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt;0&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;if&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;isinstance&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;dict&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;print&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;'%s[X%d &amp;lt; %.3f]'&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;%&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;depth*&lt;/code&gt;&lt;code&gt;' '&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'index'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;+&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'value'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;print_tree&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'left'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;depth&lt;/code&gt;&lt;code&gt;+&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;print_tree&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'right'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;depth&lt;/code&gt;&lt;code&gt;+&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;else&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;print&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;'%s[%s]'&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;%&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;depth*&lt;/code&gt;&lt;code&gt;' '&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt; &lt;br&gt;&lt;code&gt;dataset&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;2.771244718&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;1.784783929&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;0&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;1.728571309&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;1.169761413&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;0&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;3.678319846&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;2.81281357&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;0&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;3.961043357&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;2.61995032&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;0&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;2.999208922&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;2.209014212&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;0&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;7.497545867&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;3.162953546&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;9.00220326&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;3.339047188&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;7.444542326&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;0.476683375&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;10.12493903&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;3.234550982&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;6.642287351&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;3.319983761&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;br&gt;&lt;code&gt;tree&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;build_tree&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;dataset&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt;print_tree&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;tree&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;code&gt;&lt;br&gt;&lt;/code&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在运行过程中，我们能修改树的最大深度，并在打印的树上观察其影响。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当最大深度为 1 时（即调用 build_tree() 函数时第二个参数），我们可以发现该树使用了我们之前发现的完美分割点（作为树的唯一分割点）。该树只有一个节点，也被称为决策树桩。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;[X1 &amp;lt; 6.642]&lt;br&gt; [0]&lt;br&gt; [1]&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当最大深度加到 2 时，我们迫使输算法不需要分割的情况下强行分割。结果是，X1 属性在左右叉上被使用了两次来分割这个本已经完美分割的数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;[X1 &amp;lt; 6.642]&lt;br&gt; [X1 &amp;lt; 2.771]&lt;br&gt; [0]&lt;br&gt; [0]&lt;br&gt; [X1 &amp;lt; 7.498]&lt;br&gt; [1]&lt;br&gt; [1]&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，我们可以试试最大深度为 3 的情况：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;[X1 &amp;lt; 6.642]&lt;br&gt; [X1 &amp;lt; 2.771]&lt;br&gt; [0]&lt;br&gt; [X1 &amp;lt; 2.771]&lt;br&gt; [0]&lt;br&gt; [0]&lt;br&gt; [X1 &amp;lt; 7.498]&lt;br&gt; [X1 &amp;lt; 7.445]&lt;br&gt; [1]&lt;br&gt; [1]&lt;br&gt; [X1 &amp;lt; 7.498]&lt;br&gt; [1]&lt;br&gt; [1]&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些测试表明，我们可以优化代码来避免不必要的分割。请参见延伸章节的相关内容。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在我们已经可以（完整地）创建一棵决策树了，那么我们来看看如何用它来在新数据上做出预测吧。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.4 利用模型进行预测&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;使用决策树模型进行决策，需要我们根据给出的数据遍历整棵决策树。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;与前面相同，我们仍需要使用一个递归函数来实现该过程。其中，基于某分割点对给出数据的影响，相同的预测规则被应用到左子节点或右子节点上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们需要检查对某子节点而言，它是否是一个可以被作为预测结果返回的终端节点，又或是他是否含有下一层的分割节点需要被考虑。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如下是实现上述过程的名为 predict() 函数，你可以看到它是如何处理给定节点的下标与数值的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/ba8ef872c4938030ddc42a109ce3f17d2d6c68d0"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接着，我们使用合成的数据集来测试该函数。如下是一个使用仅有一个节点的硬编码树（即决策树桩）的案例。该案例中对数据集中的每个数据进行了预测。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/5712b635bbbc55843e925f5621b0634331173816"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;运行该例子，它将按照预期打印出每个数据的预测结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Expected=0, Got=0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Expected=0, Got=0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Expected=0, Got=0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Expected=0, Got=0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Expected=0, Got=0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Expected=1, Got=1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Expected=1, Got=1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Expected=1, Got=1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Expected=1, Got=1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Expected=1, Got=1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在，我们不仅掌握了如何创建一棵决策树，同时还知道如何用它进行预测。那么，我们就来试试在实际数据集上来应用该算法吧。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.5 对钞票数据集的案例研究&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该节描述了在钞票数据集上使用了 CART 算法的流程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第一步是导入数据，并转换载入的数据到数值形式，使得我们能够用它来计算分割点。对此，我们使用了辅助函数 load_csv() 载入数据及 str_column_to_float() 以转换字符串数据到浮点数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们将会使用 5 折交叉验证法（5-fold cross validation）来评估该算法的表现。这也就意味着，对一个记录，将会有 1273/5=274.4 即 270 个数据点。我们将会使用辅助函数 evaluate_algorithm() 来评估算法在交叉验证集上的表现，用 accuracy_metric() 来计算预测的准确率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;完成的代码如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/e3718591bcd4a650bea6a8c99aac09c77ddf86ef"/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/167dce3b1289127e521fa4f23bbb88faccef12c8"/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/97f787cee08888d52ddb9e8338690c106caa37b4"/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/373c180d199f8f7d5aa818dba8b05db2372a7bce"/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/dae518c198905eca6513f16ccc5555eb77248100"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;上述使用的参数包括：max_depth 为 5，min_size 为 10。经过了一些实现后，我们确定了上述 CART 算法的使用的参数，但这不代表所使用的参数就是最优的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;运行该案例，它将会 print 出对每一部分数据的平均分类准确度及对所有部分数据的平均表现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从数据中你可以发现，CART 算法选择的分类设置，达到了大约 83% 的平均分类准确率。其表现远远好于只有约 50% 正确率的零规则算法（Zero Rule algorithm）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Scores: [83.57664233576642, 84.30656934306569, 85.76642335766424, 81.38686131386861, 81.75182481751825]&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Mean Accuracy: 83.358%&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;三、延伸&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本节列出了关于该节的延伸项目，你可以根据此进行探索。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 算法调参（Algorithm Tuning）：在钞票数据集上使用的 CART 算法未被调参。你可以尝试不同的参数数值以获取更好的更优的结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 交叉熵（Cross Entropy）：另一个用来评估分割点的成本函数是交叉熵函数（对数损失）。你能够尝试使用该成本函数作为替代。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3. 剪枝（Tree Pruning）：另一个减少在训练过程中过拟合程度的重要方法是剪枝。你可以研究并尝试实现一些剪枝的方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4. 分类数据集（Categorical Dataset）：在上述例子中，其树模型被设计用于解决数值型或有序数据。你可以尝试修改树模型（主要修改分割的属性，用等式而非排序的形式），使之能够应对分类型的数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;5. 回归问题（Regression）：可以通过使用不同的成本函数及不同的创建终端节点的方法，来让该模型能够解决一个回归问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;6. 更多数据集：你可以尝试将该算法用于 UCI Machine Learning Repository 上其他的数据集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;原文：http://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&amp;copy;本文为机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Mon, 20 Feb 2017 12:00:06 +0800</pubDate>
    </item>
    <item>
      <title>学界 | YodaNN：一个用于超低功耗二值卷积神经网络加速的框架</title>
      <link>http://www.iwgc.cn/link/</link>
      <description>
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;选自arXiv.org&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：晏奇、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;去年，来自瑞士苏黎世联邦理工学院（ETH Zurich）和意大利博洛尼亚大学电气、电子与信息工程系的研究者提出一种用于超低功耗二值卷积神经网络加速的框架 YodaNN。近日，该研究团队对这个框架的论文进行了更新，机器之心在此对其进行了简单的摘要介绍，论文原文请点击文末「阅读原文」查看。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/503888f9c4ff2846381d61a759ff2396476548fb"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;摘要：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在过去几年里，卷积神经网络（CNN）已经为计算机视觉领域带来了革新，推动实现了超越人类准确度的图像分类。但是，我们需要非常高功耗的并行处理器或者通用计算图形处理器（GP-GPU）才能满足运行目前 CNN 的要求。最近在为系统级芯片集成（system-on-chip integration）的 CNN 加速器上的发展已经实现了显著的功耗降低。不幸的是，即便是这些经过高度优化的设备，其包络功率（power envelope）也因超过了移动设备和深层嵌入式应用从而面临因 CNN weight I/O 和 storage 导致的硬性限制。这也阻碍了未来在超低功耗物联网端节点中采用 CNN 来对近传感器（near-sensor）的分析工作。最近在算法和理论中的进展使具有竞争力的分类准确度成为可能&amp;mdash;&amp;mdash;即便当在训练中限制 CNN 使其使用二值权重（+1/-1）来计算也没问题。通过去除对大量乘法运算的需求和减少 I/O 带宽与存储，这些新发现为我们带来了在运算核心中进行重要优化的良机。本文中，我们提出了一种为二值 CNN 优化过的加速器，它在仅 1.33MGE（Million Gate Equivalent，百万级等效门）或 0.19 平方毫米的核心区域上在 1.2 V 的条件下实现了 1510 GOp/s 的速度，而且在 0.6 V 条件下使用 UMC 65 nm 技术时仅有 895 uW 的功率耗散。我们的加速器在能量效率和尺寸效率上的表现都显著超越了当前最佳水平，分别在 0.6 V 和 1.2 V 的条件下实现了 61.2 TOp/s/W 和 1135 GOp/s/MGE 的表现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/1fd3a071de9d7308e0adcab4a16c1d06cd086973"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;算法 1：该伪代码给出了卷积层处理所需的主要步骤的概览&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/6c53664fa856b91649ebb0d04ae86ccfa92af279"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 10：YodaNN 的 floorplan，其带有 9.2 KiB SCM 内存，可并行计算 32 个输出信道&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/812133e9a5cd34f23c727e2b83c786b6e7f55236"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 11：基线架构（定点 Q2.9、SRAM、8&amp;times;8 信道、固定 7&amp;times;7 滤波器）与最终的 YodaNN（二值的、SCM、32&amp;times;32 信道、支持多个滤波器）在内核能效和通量上的比较&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/6e7f19fdd534b0094c6cda8599384a7569f5ece9"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图.12. 固定点和若干二进制架构的核心功率击穿。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/26fcd237f0cd989bded01dbc3268da47d26df2c0"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图.13. 最先进的卷积神经网络加速器的核心区域效率和能源效率比较&amp;nbsp;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&amp;copy;本文为机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Mon, 20 Feb 2017 12:00:06 +0800</pubDate>
    </item>
    <item>
      <title>模型学习全面概述：利用机器学习查找软件漏洞</title>
      <link>http://www.iwgc.cn/link/</link>
      <description>
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;选自ACM&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Frits Vaandrager&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Communications of the ACM 近日发表一篇题为《Model Learning》的文章，详细介绍了模型学习及其研究现状和应用。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本文的要点是：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;模型学习的目标是通过提供输入和观察输出来构建软件和硬件系统的黑箱状态图模型（black box state diagram model）。模型学习的算法的设计师一个基本的研究问题。&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;模型学习正在成为一种高效的漏洞寻找技术，有银行卡、网络协议和遗产软件等领域的应用。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;iframe allowfullscreen="" class="video_iframe" data-vidtype="1" frameborder="0" height="417" src="https://v.qq.com/iframe/preview.html?vid=f0376e2xzl0&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0" width="556"&gt;&lt;/iframe&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在新算法的设计上，最新出现了很多新进展，既有有限状态图（Mealy 机）背景的进展，也有数据（register automata）背景的进展。通过抽象（abstraction）技术的使用，这些算法可以被应用到复杂系统上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;按下按键观察结果，这是我们学习一个装置或计算机程序的惯常做法。孩童尤其擅长这一点，无需借助手册他们便可以搞懂如何正确使用智能手机或微波炉。鉴于以上，我们建构了一个心智模型&amp;mdash;&amp;mdash;一个装置状态图：做一些实验，即可获知该装置的整体状态以及输入所对应的状态转换与输出结果。本文介绍了自动执行此任务的算法的设计与应用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有很多推断软件组件模型的方法，比如分析代码、挖掘系统日志、执行测试；有不同的模型被推断过，比如隐马尔可夫模型、变量之间的关系、类图（class diagrams）。在本文中，我们关注一种特定类型的模型，即状态图（state diagrams），其对于理解许多软件系统的行为至关重要，例如（安全和网络）协议和嵌入式控制软件。模型推断技术分为白箱和黑箱，区别在于是否需要访问代码。本文只讨论黑箱技术。这些技术的优点是相对容易使用，并可以应用在我们没有代码访问权限或足够的白箱工具的情况下。作为最终的限制，我们只考虑主动学习（active learning）的技术，即通过主动地对软件进行实验（测试）来完成它们的任务的技术。此外，还有一个广泛的被动学习（passive learning）工作，其中模型是从（一组组）运行的软件构建的。主动学习的优点是它提供了软件组件的完整行为模型，而不仅仅是在实际操作期间发生的特定运行的模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;状态图（或自动机（automaton））的主动性黑箱学习的基本问题的研究已经持续了几十年。1956 年，Moore 首先提出了学习有限自动机（finite automata）的问题，并提供了一个指数算法，还证明这个问题本质上是指数式的。后来，不同的组织以不同的名字对这个问题进行着研究：控制论学家把它称为「系统辨识（system identification）」；计算语言学家称之为「语法推理（grammatical inference）」；一些论文将其命名为「常规推理（regular inference）」、「常规外推（regular extrapolation）」、「主动性自动机学习（active automata learning）」；安全研究者造了个新术语「协议状态模糊（protocol state fuzzing）」。本文中，我们使用的术语「模型学习（model learning）」与经常使用的「机器检查（model checking）」类似。虽然「模型检查」被广泛用于分析有限状态模型，但「模型学习」则是通过观察输入-输出数据以构建模型的补充技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1987 年，Angluin 发表了一篇研讨论文，她表明可以使用所谓的会员查询（membership query）和等价查询（equivalence queries）来学习到有限自动机。自此之后，尽管提出了更快算法，但最有效的学习算法依然遵循着 Angluin 所提出的 MAT（minimally adequate teacher/最低限度足够的教师）的原则。在 MAT 框架中，学习被看作是一个博弈（game），其中学习器（learner）必须通过询问教师（teacher）来推断一个未知的状态图的行为。在我们的设定中，教师知道状态图，其被称为 Mealy 机（Mealy machine），简称：M。一开始，学习器只知道 M 的输入 I 和输出 O。学习器的任务是通过两种类型的查询学习 M：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用会员查询 (MQ/membership query)：学习器询问输入序列&amp;sigma; &amp;isin; I*对应的输出结果是什么。教师使用输出序列 AM(&amp;sigma;) 来回答。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;使用等价查询 (EQ/equivalence query)：学习器询问一个带有输入 I 和输出 O 的虚拟的 Mealy 机 H 是否正确，即：H 和 M 是否等同。如果情况属实，教师回答「是」。否则教师回答「否」，并提供一个反例&amp;sigma; &amp;isin; I*来区分 H 和 M。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Angluin 的 L*算法能够通过询问会员查询和等价查询的多项式数（多项式数的大小对应于典型的 Mealy 机）来学习 Mealy 机 M。在 Angluin 的算法中我们给 L*做了一个简化，实际的实现中（例如 LearnLib 和 libalf）则包含很多优化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Peled 等人作出了重大发现：MAT 框架可以用来学习软硬件组件的黑箱模型。假设我们有一个组件，我们称之为系统学习（SUL），其行为可以由（未知的）Mealy 机 M 描述。我们进一步假设，总是可以使 SUL 回到其初始状态。现在，通过使 SUL 回到初始状态并进一步观察给到 SUL 的输入序列所对应的输出结果可以实现会员查询。等价查询可以通过有限数量的测试查询（TQ/test queries）以使用一致性测试（CT/conformance testing）工具来接近。测试查询询问 SUL 对输入序列的响应，类似于会员查询。如果其中一个测试查询呈现反例，则等价查询的答案为否，否则答案为是。示意图如图 4 所示。在这种方法中，学习者的任务是构造假设，而一致性测试工具的任务是测试这些假设的有效性。由于测试工具只能构造有限数量的查询，因此我们无法确定一个学习模型的正确性。然而，如果我们假定机器 M 的状态数量有界限，则存在有限和完整的一致性测试套件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/dc5036990176d3b67a07eb1ae7c7fd3209211dc3"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图.4&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Peled 等人和 Steffen 等人的开创性工作在模型学习和正式方法的领域之间建立了迷人的联系，特别是模型检验和基于模型的测试。随后的研究已经证实，在没有反应系统的易处理的白箱模型的情况下，学习模型通常是可以以相对低的成本获得的优良的替代方案。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了检查学习模型的属性，可以使用模型检查（model checking）。事实上，Peled 等人已经在一种叫「黑箱检查（black box checking）」的方法中展示了如何完全整合模型学习与模型检查，其基本思想是使用模型检查器作为图 4 中一致性测试工具的「预处理器（preprocessor）」。当教师接收到学习器的假设时，首先运行模型检查器以验证假设模型是否满足 SUL 规定的所有属性。只有假设为真时，才转发给一致性测试器（conformance tester）。如果其中一个 SUL 属性不成立，那么模型检查器产生一个反例。现在有两种情况。第一种可能性是反例可以在 SUL 上再现。这意味着我们已经在 SUL（或其规定中）中展示了一个错误，我们停止学习。第二种可能性是反例不能在 SUL 上再现。在这种情况下，教师遵循假设是不正确的原则向学习器返回反例。在后来的工作中，黑箱检查方法已经进一步完善，并已成功应用于几个工业案例。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大多数学习算法的所需会员查询数量随着输入数量线性增长，并与状态数量成二次方。这意味着当输入数量增长时，学习算法规模相当好；换句话说，制定一个新的假设是容易的。然而，检查假设是否正确（一致性测试）会很快成为大量输入的瓶颈。如果当前假设具有 n 个状态，则 SUL 具有 n' 个状态，并且存在 k 个输入，则在最坏的情况下，需要运行包含 n'-n 个输入的所有可能序列的测试序列，即 k(n' &amp;minus; n) 个可能性。因此，模型学习目前只能应用于少于 100 个输入的情况下。因此，我们寻求帮助我们减少输入数量的方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;抽象（abstraction）是将模型学习方法扩展到现实应用程序的关键。Cho 等人通过在僵尸网络服务器和学习软件之间放置仿真器/映射器（emulator/mapper），将字母符号具体化为有效的网络消息并将它们发送到僵尸网络服务器（botnet servers），成功推断出现实僵尸网络命令和控制协议的模型。当接收到响应时，仿真器作反向处理：它将响应消息抽象为输出的字母，并将它们传递到学习软件。这种学习设置的示意图概述如图 5 所示。处理抽象的中间映射器组件的想法是非常自然的，并且在许多关于自动机学习的案例研究中被隐含地或明确地使用。Aarts 等人通过与谓词抽象（predicate abstraction）和抽象解释（abstract interpretation）建立连接，发展出了关于中间性抽象（intermediate abstraction）的数学理论。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/d7a65a7246a36154ea5a3f5a78481e7630e48394"/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;图.5&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个补充性的简单但实用的方法是将模型学习应用在多个更小的输入子集上。这将明显降低学习复杂性；还因为对于有限数量的刺激，可达状态的集合通常将更小。然后，对于输入的子集学习的模型可以用于在学习更大子集的模型时生成反例。例如，Chalupar 等人已经应用的另一种方法是将通常以特定顺序发生的多个输入动作合并成单个高级动作，从而减少输入的数量。再次，已经用少量高级输入学习的模型可以用于在后续实验中产生反例，其中这些输入被分解成它们的组成部分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;正如 C.A.R. Hoare 所说，一个人可以说，在每个大程序中都有一个小的状态机试图出去。通过选择适当的输入动作集合并定义适当的映射器/抽象，我们可以使这个小状态机对学习者可见。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;应用案例&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;近年来，模型学习已经成功应用于不同领域的许多实际案例。工业应用的例子有，例如，在西门子的电信系统的回归测试（regression testing），在法国电信的集成测试，在施普林格出版社线上会议的自动测试，在沃尔沃科技的线控制动系统的测试要求。下面，我将概述奈梅亨大学（Radboud University）在智能卡、网络协议和遗产软件（legacy software）方面进行的一些代表性案例研究。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;智能卡。&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;Chalupar 等人使用模型学习来反向工程 e.dentifier2&amp;mdash;&amp;mdash;一种用于网上银行的智能卡阅读器。为了能够学习 e.dentifier2 的模型，作者构建了一个由树莓派（Raspberry Pi）控制的乐高机器人，可以操作读取器的键盘（参见图 6）。从笔记本电脑控制所有这些之后，他们可以使用 LearnLib 学习 e.dentifier2 的模型。他们学习了一个版本的 e.dentifier2 的四态 Mealy 机，揭示了存在的一个安全缺陷，并且表明该缺陷不再存在于新版本设备的三态模型中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/f3be5c730ffb415a252d548f206abac9e4511271"/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;图.6&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在另一项研究中，Aarts 等人学习了银行卡上的 EMV 协议套件的实现模型，这些银行卡有来自几家荷兰和德国银行的，有荷兰和瑞典银行发行的万事达信用卡以及一张英国签证借记卡。为了学习模型，LearnLib 对每个卡执行 855 到 1696 个会员和测试查询，并生成 4 到 8 个状态的模型。（图 7 展示了其中一个学习的模型）。所有卡产生不同的模型，只有荷兰银行卡上的应用程序是相同的。所学到的模型没有揭示任何安全问题，虽然注意到一些怪异问题。作者认为，模型学习将作为安全评估的一部分发挥作用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/a1b8936f74b1f3c6d46db6e72c6b084a9b0994a8"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;图.7&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;网络协议。&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我们的社会已完全依赖于网络和安全协议的正确运作；这些协议中的错误或漏洞可能会导致安全漏洞甚至是彻底的网络故障。模型检查已被证明是一种用于发现此类错误与漏洞的有效技术。然而，由于针对协议实现的详尽模型检查通常不可行，因此模型检查通常会应用于根据协议标准开始人工制作的模型。这意味着由于协议实现不符合模型检查的规范，其出现的错误便无法被捕捉。研究证明，模型学习能够有效地找到此类错误，使这项技术得以与模型检查互补。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;例如，De Ruiter 和 Poll 使用支持多种密钥交换算法和客户端认证选项的测试工具分析了 TLS 协议的服务器端和客户端实现。结果表明模型学习（或称为协议状态模糊）可以捕获一类有趣的实现缺陷，而这种缺陷在安全协议实现中十分常见：在九个受测试的 TLS 实现中，有三个能够发现新的安全缺陷。如 Java Secure Socket Extension 便是一类学习了 Java 1.8.0.25 版本的模型。他们发现该模型包含两条通往应用程序数据交换的路径：常规 TLS 协议运行和另一意外运行。客户端以及服务器应用程序都以为它们处于安全的连接上交谈，但实际上任何人都能够通过利用这种行为读取并篡改客户端的数据。所以修复作为安全更新的一个关键部分而被发布，并且他们能够通过学习 JSSE 1.8.0.31 版本的模型来确认问题是否已解决。得益于人工构建的抽象/映射器，经验丰富的 Mealy 机包含 6 至 16 个状态并且规模都很小。另外，由于对不同的 TLS 实现的分析产生了独一无二的 Mealy 机，模型学习也可用于为 TLS 实现添加指纹印记。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Fiterau 等人在一个涉及 Linux、Windows 以及使用 TCP 服务器与客户端的 FreeBSD 实现的案例研究中将模型学习与模型检查进行了结合。模型学习用于推断不同组件的模型，而后应用模型检查来充分探索当这些组件（如 Linux 客户端和 Windows 服务器）交互时可能的情况。案例研究揭示了 TCP 实现中不符合其 RFC 规范的几个例子，具体示例参见图 8。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/bf065a80d1dd600eed33d1ba7af5cacdd70bfa3f"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图.8&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;遗产软件（Legacy software）&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;。遗产系统被定义为「不知如何处理却对组织至关重要的大型软件系统」（7）。通常这些系统的技术已经过时，并且文档存在限制，原始开发人员也已经离职。此外现有的回归测试将受限。鉴于以上特征，需要改变传统组件的创新存在风险。故而开发了几种技术用于提取隐藏在传统组件中的关键业务信息，并支持重构实现的结构。Margaria 等人（30）首先指出，模型学习可能有助于确认传统组件和重构实现具有相同的行为。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;例如 Schuts 等人在飞利浦的开发项目中使用模型学习来支持传统嵌入式软件的复兴。该项目涉及到一个新引入的硬件组件&amp;mdash;&amp;mdash;电源控制组件（PCC），用于启动和关闭介入放射学系统。系统中的所有计算机都具有软件组件，即在启动和关闭的执行期间通过内部控制网络与 PCC 通信的电源控制服务（PCS）。为了处理具有不同接口的 PCC 的新硬件，则需要 PCS 的新型实现。由于必须支持新型和旧型 PCC 硬件的不同配置，新型与旧型 PCS 软件需要具有完全相同的外部行为。图 9 说明了所遵循的方法。由传统的 A 实现以及重构的 B 实现可获得 Mealy 机器模型 MA，而使用模型学习可获得 MB；这些模型将使用等价检查器进行比较。当等价检查器发现反例&amp;sigma;时，我们将检查 A 和 MA 在输入&amp;sigma;上表现是否相同，同样检查 B 和 MB 在输入&amp;sigma;上是否相同。若 A 和 MA 或者 B 和 MB 存在差异，我们便会要求学习者基于反例&amp;sigma;构造一个改进的模型。否则&amp;sigma;便表示 A 和 B 之间的差异，而我们也会根据对于&amp;sigma;的响应表现差劲的行为来改变 A 或 B（或是两者）。为了解决可扩展性问题，往往通过增加刺激来学习以及迭代地检查实现。在组件集成之前的早期阶段，重构实现和传统实现都出现了问题。也正因如此，才能避免开发的后期阶段昂贵的重工现象。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/0ed2dd0fde8d121bb311178fea9335a6112c4712"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图.9&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;最新进展&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;近年来，有关模型学习的算法已取得显著进步，这对将这些技术应用扩展到更大的系统而言至关重要。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;基本算法&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;。自 1987 年以来，Angluin's 的 L *算法已得到显着改善。原始 L *对观察表中的每个条目执行成员资格查询；但这通常是多余的，因为该查询的唯一目的是区分状态（行）。因此，Kearns 和 Vazirani 通过鉴别树（discrimination trees）将 L *算法的观察表进行重设，而该鉴别树基本上是用于确定等价状态的决策树。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;L *的另一个低效的体现是将反例的所有前缀作为行添加到表格中。通过一致性测试或运行时监控获得的反例样本可能极长且极小，而这会导致大量多余的成员资格查询。Rivest 和 Schapire 发现不必将反例的所有前缀作为行添加到表格中，将一个选定的后缀添加为列便足够了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Isberner 等人提出的新 TTT 算法是目前用于主动学习的最有效的算法。该算法基于 Kearns 和 Vazirani 以及 Rivest 和 Schapire 的想法，但消除了通过清理内部数据结构及重新组织判别树来处理长型反例时过长的鉴别树。假设某 Mealy 机 M 具有 n 个状态和 k 个输入值，并且返回的最长反例长度为 m。然后在最坏的情况下 TTT 算法需要长度为 O（n + m）的 O（n）个等价查询和 O（kn2 + nlog m）个成员资格查询。这种最坏情况的查询和符号复杂度与 Rivest 和 Schapire 的算法一致，但实践中 TTT 的速度更快。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;TTT 算法通常比 L *算法产生更多的中度假设（intermediate hypotheses）。这表明单就 membership 查询中使用的输入符号数量可能并不是比较学习算法的适宜度量：我们还需考虑实现等价查询所需的测试查询的数量。membership 与测试查询中的输入符号的总数似乎是比较实践中学习方法的可靠度量。我的两个学生 J.Merman 和 A.Fedotov 在大量基准（协议、控制软件、回路等）上，比较了学习算法和测试算法的不同组合，并发现 TTT 使用的输入符号平均比 L*少 3.9 倍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当可以同时运行 SUL 的多个实例时，学习和测试能够很容易被并行化。能够加速学习的另一项技术是保存并恢复 SUL 的软件状态（检查点）。其中的益处是：当 learner 想要从保存的 q 状态中探索不同的外向转换时，仅仅需要恢复 q，这通常比复位系统，再经由输入序列返回到 q 要快得多。Henrix（21）报告了在实验中使用 DMTCP（分布式多线程检查点）进行检查点加速学习，系数为 1.7。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;寄存器自动机。&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;尽管我们已经看到学习状态机中基本算法的诸多进步，但这些算法仅能成功学习相对很小的状态机。为了能把这些算法扩展至现实应用领域，使用者一般需要手动构建抽象或者映射器。2这可能是个耗时的活动，需要几次迭代和 SUL 的专业知识。因此，最近已经进行了许多工作以将学习算法推广到结构更多结构、类型更丰富类的模型中去，特别是其中数据值可以被传送，存储和操纵的 EFSM 模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;模型学习算法的开发正是对寄存器自动机进行的特定扩展。11 &amp;nbsp;这些自动机具有有限的一组状态，但都用一组可用于存储数据值的寄存器进行扩展了。输入和输出动作被参数化为具体的数据值，可以在转换保护中进行相等测试并存储在寄存器中。图 10 给出了寄存器自动机的简单示例，即容量为 2 的 FIFO 集。一个 FIFO 集对应于一个只能存储不同值的队列。它有一个 Push（d）输入符，用来尝试在队列中插入值 d 的符号，还有一个 Pop 输入符，用来尝试从队列中取回一个值。如果输入值可以成功添加，则 Push 的对应输出为 OK，如果输入值已经在队列中或者队列为满，则 Push 的对应输出为 KO。如果以队列中最旧的值作为参数，则 Pop 的对应输出是 Out，如果队列是空的，则 Pop 的对应输出是 KO。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/8f111fd54618c02246493d0136066f86393d1aa1"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图.10 &lt;/span&gt;&lt;/em&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;抽象是将模型学习方法扩展至现实应用的关键&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在寄存器自动机中，所有数据值是完全对称的，且这种对称性是可以在学习过程中被利用的。在本文中已经探索了两种不同的方法。第一种方法，以 Cassel 等人为代表，12 &amp;nbsp;已在软件工具 LearnLib26 和 RALib 中得到实施。10 模型学习算法通常依赖于 Nerode 关系（Nerode relation）来识别已学过的自动机的状态和转换：如果两个词的残差语言（residual languages）吻合，这两个词则导致相同的状态。现在的基本思想是为寄存器自动机设计出一个类似 Nerode 的同余关系，而这决定了已推断的自动机的寄存器的状态、转换、和内容。这种实施方法的技术基础是所谓的符号决策树（symbolic decision trees），它可以用来总结许多用简明符号表达的测试结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;学习寄存器自动机的第二种方法，以 Aarts 等人为代表，已在软件工具 Tomte 中得到实施。这种方法使用反例引导的抽象提炼来自动构建适当的映射器。这种想法源于一个彻底抽象，即完全忽略在输入和输出动作中出现的数据值。当这种抽象过于粗糙时，learner 将观察到非确定性行为。比如在图 10 的示例中，输入序列 Push Push Pop Pop 大部分情况触发的输出为 OK OK Out KO，但有时为 OK OK Out Out。出现这种行为就需要对抽象进行提炼。在我们的示例中，比如就第二个 Push，我们至少需要两个抽象版本，因为这显然关乎该输入的数据值是否等于第一个 Push 的数据值。RALib 和 Tomte 在性能上都优于 LearnLib。Tomte 和 RALib 的性能大致相当。RALib 在一些基准测试中胜过 Tomte，但是 Tomte 能够学习一些 RALib 无法处理的寄存器自动机，例如容量为 40 的 FIFO 集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;研究挑战&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;即使模型学习已经在许多地方得到成功应用，但该研究领域仍处于起步阶段。模型学习的应用具有巨大的潜力，尤其是在传统控制软件领域，但是还需要对算法和工具进行更多的研究，以将模型学习从目前的学术原型水平转变为可实际使用的技术水平，从而方便应用于大量不同类型的系统。在这里，我将讨论一些主要的研究挑战。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;谓词与数据操作。&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;最近对寄存器自动机模型学习算法所做的扩展是一个突破，这可能使模型学习适用于更大范围的系统。由于不允许对数据进行操作的限制，可以被描述为寄存器自动机的系统类型很少，并且主要由一些学术样例构成，例如有界重传协议和一些简单数据结构等等。然而，如 Cassel 等人所指，12 使用 SMT 解决寄存器自动机的新学习算法可以被扩展为 EFSM 的形式，其中模型防护（guard）可能包含谓词，如 successor 和小于关系（less than relation）。目前已经有 RALib 的原型实现，而且我们正在接近可以自动学习真实世界协议模型的目标，这样的协议可以像 TCP、SIP、SSH 和 TLS 等等，而这些都不需要手动定义抽象。然而，我们对使用不同谓词和操作学习 EFSM 的算法的理解仍然有限，而且还有许多悬而未决的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;即使模型学习已经在许多地方得到成功应用，但该研究领域仍处于起步阶段。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Isberner24开发了一种用于可视下推自动机（VPA）的模型学习算法，该算法是 Alur 和 Madhusudan 提出的一种限制类型的下推自动机。5 这个结果在某种意义上与学习寄存器自动机上的结果正交：使用寄存器自动机学习，可以学习具有存储来自无限域的、带有有限容量存储值的堆栈，而使用 VPA 学习可以学习具有无限容量的堆栈，其存储来自有限域的数据值。从实践角度来看，开发一种通用于寄存器自动机和 VPA 这一类模型的学习算法将是有用的。许多协议中的消息可以缓冲，因此我们需要能够学习具有无限容量的队列的算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;超越 Mealy 机。&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;在 Mealy 机中，单个输入始终触发单个输出。然而，在实践中，系统可以用零个或多个输出来响应输入。此外，系统的行为通常是时序相关的，并且某个输出可能仅在某些输入未能在一定量的时间内得到时才发生。因此，模型学习的实际应用经常严重受限于 Mealy 机缺乏表达性。例如，为了将 TCP 实现为 Mealy 机，我们必须消除基于时序的行为以及重传（retransmissions）。17 &amp;nbsp;已经有一些初步的工作，将学习算法扩展到 I / O 自动机 4和事件记录自动机，18 但是仍然需要大量的工作将这些想法变成实际的工具。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于一个输入序列可能导致在不同运行中会有不同的输出事件，因此从这种意义上来说，系统通常是非确定的。然而，现有的模型学习工具只能够学习确定性的 Mealy 机。在实际应用中，我们有时可以通过将不同的具体输出事件抽象为单个抽象输出以消除非确定性，但在许多情况下这是不可能的。Volpato 和 Tretmans 38 提出了一种*L **对非确定性 I / O 自动机的主动学习的调整方案。他们的算法能够学习非确定性 SUL，并且它允许我们构造部分或近似模型。同样，还需要进行大量工作以将这些想法纳入最先进的工具，如 LearnLib、libalf、RALib 或 Tomte。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;模型质量&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;。由于模型学习算法生成的模型是通过有限数量的测试获得的，所以我们不能确保它们是正确的。然而，从实践的角度来看，我们希望能够对学习模型的质量进行定量说明，例如，断言假设高概率地近似正确。Angluin 6 &amp;nbsp;根据 Valiant 的 PAC 学习方法提出了这样的一种设定。她的想法是假设在输入字母表 I 上一组单词的一些（未知）概率分布。为了测试假设，一致性测试器（参见图 4）选择指定数量的输入词（这些是统计独立的事件），并检查每个词，无论 SUL 的输出结果是否和假设相一致。只有当完全一致时，一致性测试仪才会向 learner 返回答案&lt;em&gt; 'yes'&lt;/em&gt;。如果选择一个字符串所表现出来的差异的概率最多为&amp;epsilon;，则该假设被认为是 SUL 的&amp;epsilon;近似。给定 SUL 的状态数量的界限以及两个常数&amp;epsilon;和&amp;delta;，Angluin 的多项式算法产生模型，使得该模型是 SUL 的近似的概率为至少 1 -&amp;delta;。Angluin 的结果是优雅的，但在反应系统的设置中不现实，因为我们通常在输入词上没有固定的分布。（输入受 SUL 环境的控制，且此环境可能会发生改变。）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们使用传统一致性测试，可以设计出一个测试套件，该测试套件可在给定 SUL 状态数量上限的情况下保证学习模型的准确性。但是这种方法也不能令人满意，因为所需要的测试序列的数量会随着 SUL 的状态数量呈现指数型增长。因此，挑战在于如何在 Angluin 方法和传统的一致性测试之间建立一个折中。系统日志通常提供了一个输入词集合的概率分布，该输入词集合可被用来作为定义某个度量标准的启动点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;打开箱子。&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;使用黑箱模型学习技术可以有很多原因。例如，我们可能想要了解组件的行为，但是不能访问代码。或者我们可以访问代码，但没有合适的工具来分析它（例如，在旧版软件的情况下）。即使在「白箱」情况下，我们可以访问代码并有强大的代码分析工具，黑箱学习也是有意义的，例如因为黑箱模型可以用于生成回归测试，用于检查是标准是否一致，或作为更大的基于模型开发的系统的一部分。一个重要的研究挑战是结合黑箱和白箱模型提取技术，例如，使用白盒方法，如静态分析和 concolic 测试，以帮助回答由黑箱 learner 提出的等价性查询。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;致谢。&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;部分工作是在 STW 项目 11763（ITALIA）和 13859（SUMBAT）以及 NWO 项目 628.001.009（LEMMA）和 612.001.216（ALSEP）的背景下进行的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;1. Aarts, F., Fiterău-Broştean, P., Kuppens, H., Vaandrager, F. Learning register automata with fresh value generation. In ICTAC'15, LNCS 9399 (2015). Springer, 165&amp;ndash;183.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;2. Aarts, F., Jonsson, B., Uijen, J., Vaandrager, F. Generating models of infinite-state communication protocols using regular inference with abstraction. Formal Methods Syst. Des. 46, 1 (2015), 1&amp;ndash;41.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;3. Aarts, F., de Ruiter, J., Poll, E. Formal models of bank cards for free. In SECTEST'13 (2013). IEEE, 461&amp;ndash;468.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;4. Aarts, F., Vaandrager, F. Learning I/O automata. In CONCUR'10, LNCS 6269 (2010). Springer, 71&amp;ndash;85.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;5. Alur, R., Madhusudan, P. Visibly pushdown languages. In STOC'04 (2004). ACM, 202&amp;ndash;211.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;6. Angluin, D. Learning regular sets from queries and counterexamples. Inf. Comput. 75, 2 (1987), 87&amp;ndash;106.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;7. Bennett, K. Legacy systems: coping with success. IEEE Softw. 12, 1 (1995), 19&amp;ndash;23.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;8. Berg, T., Grinchtein, O., Jonsson, B., Leucker, M., Raffelt, H., Steffen, B. On the correspondence between conformance testing and regular inference. In FASE'05, LNCS 3442 (2005). Springer, 175&amp;ndash;189.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;9. Bollig, B., Katoen, J.-P., Kern, C., Leucker, M., Neider, D., Piegdon, D. libalf: The automata learning framework. In CAV'10, LNCS 6174 (2010). Springer, 360&amp;ndash;364.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;10. Cassel, S., Howar, F., Jonsson, B. RALib: A LearnLib extension for inferring EFSMs. In DIFTS 15 (2015).&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;11. Cassel, S., Howar, F., Jonsson, B., Merten, M., Steffen, B. A succinct canonical register automaton model. J. Log. Algebr. Meth. Program. 84, 1 (2015), 54&amp;ndash;66.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;12. Cassel, S., Howar, F., Jonsson, B., Steffen, B. Active learning for extended finite state machines. Formal Asp. Comput. 28, 2 (2016), 233&amp;ndash;263.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;13. Chalupar, G., Peherstorfer, S., Poll, E., Ruiter, J. Automated reverse engineering using Lego. In WOOT'14 (Aug. 2014). IEEE Computer Society.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;14. Cho, C., Babic, D., Shin, E., Song, D. Inference and analysis of formal models of botnet command and control protocols. In CCS'10 (2010). ACM, 426&amp;ndash;439.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;15. Clarke, E., Grumberg, O., Peled, D. Model Checking. MIT Press, Cambridge, MA, 1999.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;16. Feng, L., Lundmark, S., Meinke, K., Niu, F., Sindhu, M., Wong, P. Case studies in learning-based testing. In ICTSS'13, LNCS 8254 (2013). Springer, 164&amp;ndash;179.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;17. Fiterău-Broştean, P., Janssen, R., Vaandrager, F. Combining model learning and model checking to analyze TCP implementations. In CAV'16, LNCS 9780 (2016). Springer, 454&amp;ndash;471.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;18. Grinchtein, O., Jonsson, B., Leucker, M. Learning of event-recording automata. Theor. Comput. Sci. 411, 47 (2010), 4029&amp;ndash;4054.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;19. Groce, A., Peled, D., Yannakakis, M. Adaptive model checking. Logic J. IGPL 14, 5 (2006), 729&amp;ndash;744.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;20. Hagerer, A., Hungar, H., Niese, O., Steffen, B. Model generation by moderated regular extrapolation. In FASE'02, LNCS 2306 (2002). Springer, 80&amp;ndash;95.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;21. Henrix, M. Performance improvement in automata learning. Master thesis, Radboud University (2015).&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;22. Hungar, H., Niese, O., Steffen, B. Domain-specific optimization in automata learning. In CAV 2003, LNCS 2725 (2003). Springer, 315&amp;ndash;327.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;23. de la Higuera, C. Grammatical Inference: Learning Automata and Grammars. Cambridge University Press, 2010.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;24. Isberner, M. Foundations of active automata learning: An algorithmic perspective. PhD thesis, Technical University of Dortmund (2015).&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;25. Isberner, M., Howar, F., Steffen, B. The TTT algorithm: A redundancy-free approach to active automata learning. In RV'14, LNCS 8734 (2014). Springer, 307&amp;ndash;322.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;26. Isberner, M., Howar, F., Steffen, B. The open-source LearnLib - A framework for active automata learning. In CAV'15, LNCS 9206 (2015). Springer, 487&amp;ndash;495.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;27. Jhala, R., Majumdar, R. Software model checking. ACM Comput. Surv. 41, 4 (Oct. 2009), 21:1&amp;ndash;21:54.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;28. Kearns, M.J., Vazirani, U.V. An Introduction to Computational Learning Theory. MIT Press, 1994.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;29. Lee, D., Yannakakis, M. Principles and methods of testing finite state machines&amp;mdash;A survey. Proc. IEEE 84, 8 (1996), 1090&amp;ndash;1123.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;30. Margaria, T., Niese, O., Raffelt, H., Steffen, B. Efficient test-based model generation for legacy reactive systems. In HLDVT'04 (2004). IEEE Computer Society, 95&amp;ndash;100.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;31. Moore, E. Gedanken-experiments on sequential machines. In Automata Studies, Annals of Mathematics Studies 34 (1956). Princeton University Press, 129&amp;ndash;153.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;32. Peled, D., Vardi, M., Yannakakis, M. Black box checking. J. Autom. Lang. Comb. 7, 2 (2002), 225&amp;ndash;246.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;33. Rivest, R.L., Schapire, R.E. Inference of finite automata using homing sequences. Inf. Comput. 103, 2 (1993), 299&amp;ndash;347.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;34. de Ruiter, J., Poll, E. Protocol state fuzzing of TLS implementations. In USENIX Security'15 (2015). USENIX Association, 193&amp;ndash;206.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;35. Schuts, M., Hooman, J., Vaandrager, F. Refactoring of legacy software using model learning and equivalence checking: an industrial experience report. In iFM'16, LNCS 9681 (2016). Springer, 311&amp;ndash;325.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;36. Shahbaz, M., Groz, R. Analysis and testing of black-box component-based systems by inferring partial models. Softw. Test. Verif. Reliab. 24, 4 (2014), 253&amp;ndash;288.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;37. Valiant, L.G. A theory of the learnable. In STOC'84 (1984). ACM, 436&amp;ndash;445.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;38. Volpato, M., Tretmans, J. Approximate active learning of nondeterministic input output transition systems. Electron. Commun. EASST 72 (2015).&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;39. Windm&amp;uuml;ller, S., Neubauer, J., Steffen, B., Howar, F., Bauer, O. Active continuous quality control. In CBSE'13 (2013). ACM, 111&amp;ndash;120.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br&gt;原文链接：http://cacm.acm.org/magazines/2017/2/212445-model-learning/fulltext#F7&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&amp;copy;本文为机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Sun, 19 Feb 2017 12:19:19 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 机器学习很有趣Part6：怎样使用深度学习进行语音识别</title>
      <link>http://www.iwgc.cn/link/</link>
      <description>
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;选自Medium&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Adam Geitgey&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：邵明&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;还记得machine learning is fun吗？本文是该系列文章的第六部分，博主通俗细致地讲解了神经网络语音识别的整个过程， 是篇非常不错的入门级文章。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;语音识别正闯入我们的生活。它内置于我们的手机、游戏机和智能手表。它甚至正在让我们的家庭变得自动化。只需要 50 美元，你就可以买到亚马逊的 Echo Dot&amp;mdash;&amp;mdash;一个能允许你订购比萨饼，获得天气报告，甚至购买垃圾袋的魔法盒&amp;mdash;&amp;mdash;只要你大声说：「Alexa，订购一个大披萨！」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/ab0cdf534867bcd6e2fbbfdd09cc00bfa8266027"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Alexa, order a large pizza!&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Echo Dot 在这个假期很受欢迎，亚马逊似乎没有 Echo Dot 的库存了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;语音识别已经存在数十年了，但是为什么现在才刚刚开始成为主流呢？原因是深度学习让语音识别足够准确，能够让语音识别在需要精心控制的环境之外中使用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;吴恩达早就预测，当语音识别的准确率从 95％达到 99％时，语音识别将成为人与计算机交互的主要方式。4％的准确性差距就相当于「难以容忍的不可靠」到「令人难以置信的有用性」之间的差异。由于有深度学习，我们正在走向顶峰。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们学习怎样利用深度学习进行语音识别！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;机器学习并不总是黑箱&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你知道神经网络机器翻译怎样工作，你可能会猜到：我们可以简单地将声音录音输入神经网络，然后训练神经网络来生成文本：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/37157aa7103c398b2bddda58a3719fcafec52a4c"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是用深度学习进行语音识别的核心，但我们还没有完全做到（至少在我写这篇文章的时候没做到&amp;mdash;&amp;mdash;我打赌，在未来的几年我们可以做到）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最大的问题是语音会随着速度变化。一个人可能很快地说出「Hello！」，而另外一个人可能会很缓慢说「heeeelllllllllllllooooo!」。这就产生了一个更长的声音文件和更多的数据。这两个声音文件本应该被识别为完全相同的文本「hello！」而事实证明，把各种长度的音频文件自动对齐到一个固定长度的文本是很难的一件事情。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了解决这个问题，我们必须使用一些特殊的技巧和一些除了深度神经网络以外的额外处理。让我们看看它是如何工作的吧！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;将声音转换成「字节」&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;语音识别的第一步是很显而易见的&amp;mdash;&amp;mdash;我们需要将声波输入到计算机。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在第 3 章中（https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721#.tvzicp8bh），我们学习了如何把图像处理成数字序列，以便我们能直接将其输入进神经网络进行图像识别：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/48c812da665dea547eb405b0f6e3193aac8a3ed5"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图像仅是图片中每个像素值的数字编码数组&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;声音以波的形式传播。我们怎样将声波转换成数字呢？让我们使用我说的「hello」这个声音片段作为例子：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/e045cd4bfb3fb17d8ee10fea717978a677abbd7f"/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;音频「Hello」的波形&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;声波是一维的。在每个时刻，它有单一的高度值对应。让我们放大声波的一个小部分，看看：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/ae8ef99057e156f1a67ee7c9065572d340610423"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了将这个声波转换成数值，我们只记录波在等间隔点的高度值：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/2b9367fb15a6c9a6d3fc6941bcca500f872ee018"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;声波采样&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这被称为「采样」。我们采取每秒读取数千次的方式，并把声波在对应时刻的高度值记录下来。这基本上是一个未被压缩的.wav 音频文件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「CD 音质」以 44.1kHZ（每秒读取 44100 次）进行采样。但是对于语音识别，16kHz 的采样频率足以覆盖人类语言的频率范围。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们用 16kHz 的方式对「Hello」音频采样，这是前 100 个样本：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/63911371fa6b1d0c34ec63dd130f114fd3e93839"/&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136); font-size: 12px; text-align: center;"&gt;每个数字代表声波在第 1/16000 间隔处时刻的高度值。&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;数字采样快速入门助手&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你可能会认为：采样只是对原始声波的粗略近似，因为它只是间歇性读取数据，我们的读数之间有差距，所以我们丢失了数据，对吗？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/4f3e35c21efa7aff56b21bfc0a79c68a1366f4b9"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;数字采样能否完美重现原始声波？如何处理那些间距？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;借鉴奈奎斯特定理 (Nyquist theorem)，我们可以利用数学从间隔的采样中完美地重建原始声波&amp;mdash;&amp;mdash;只要以我们希望得到的最高频率的两倍来进行采样就可以实现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我提到这一点，是因为几乎每个人都会犯这个错误：认为使用更高的采样率总是能获得更好的音频质量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;采样声音数据的预处理&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们现在有一个数值数组，每个数值代表声波在间隔为 1 / 16,000 秒的时刻时的高度值（振幅）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以将这些数值输入神经网络。但是试图通过直接处理这些样本来识别语音模式是困难的。相反，我们可以通过对音频数据预处理来简化问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们将采样的音频以 20 毫秒时间段长进行分组。这是我们第一个 20 毫秒的样本音频，即我们的前 320 个样本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/3cb44a53d08b41d88c0b834d8598d87d95d96eb0"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;将这些数值绘制为简单的线图，给出了对于 20 毫秒时间段的原始声波的粗略近似：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/5c9945f4423a5d95df7f0def17b2fd820fc62441"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然这段录音只有 1/50 秒的时长，但即使这样短暂的时长也包含不同频率的声音。有低音、中音，甚至高音混在一起。但总的来说，这些不同频率的声音混合在一起构成了人类复杂的语音。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了使这些数据更容易进行神经网络处理，我们将这复杂的声波分成不同部分。我们将一步步分离低音部分，下一个最低音部分，以此类推。然后通过将每个频带（从低到高）中的能量相加，我们就为各个类别（音调）的音频片段创建了一个指纹。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;想象你有一段某人在钢琴上弹奏 C 大调的音频。这段音频是由三个音符组合而成的 - C，E 和 G &amp;ndash; 他们混合在一起组成一个复杂的音频。我们想把这个复杂的音频分解成单独的音符：C，E 和 G。这和我们语音识别的想法一样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以使用数学中的傅里叶变换来完成。傅里叶变换将复杂的声波分解成简单的声波，一旦我们得到了这些简单声波，我们将每一个声波包含的能量加在一起。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最终结果是从低音（即低音音符）到高音，每个频率范围的重要程度。下面的每个数字表示在我们的 20 毫秒音频剪辑中每个 50Hz 频带中的能量：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/58652dc023e40c8002c3270368bb6d97e57e416c"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;表中的每个数值表示每个 50Hz 频带中的能量&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当你把它以图表形式画出，你更容易看出：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/bd62d8b5a3e1a251f4613c64639d3cf2e7b968c8"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你可以看出，我们的 20 毫秒的声音片段中有很多低频能量，高频能量较少。这是典型的「男性」的声音。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果我们以每 20 毫秒的音频块重复这个过程，我们最终会得到一个频谱图（每一列从左到右都是一个 20 毫秒的块）：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/8c12fca7e1b7c5823c9263f61f554fdee654615b"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;「hello」声音片段的完整频谱图&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;频谱图很棒，因为你可以从音频数据中看到音符和其他音高模式。相比于原始声波，神经网络可以更加容易地从这种数据中找到规律。因此，这（频谱图）就是我们将实际输入到神经网络的数据表征方式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;从短声音中识别字符&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有了易于处理音频形式，我们将把它输入到深度神经网络。神经网络的输入是 20 毫秒的音频块，对于每个小的音频切片，神经网络都会试图找出与声音对应的字母。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/8046c2dc0832e3e93cb4ac632825f72d914866c9"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们将使用循环神经网络&amp;mdash;&amp;mdash;即是具有能影响未来预测的记忆的神经网络。因为它预测的每个字母都将影响它对下一个字母的预测。例如，如果我们已经说出「HEL」，那么接下来我们很可能说出「LO」以说出「Hello」。我们不太可能会说像「XYZ」这种根本无法发音的词。因此，具有先前预测的记忆将有助于神经网络对未来进行更准确的预测。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当我们在神经网络上运行我们的整个音频剪辑（一次一块）后，我们最终将得到与每个音频块最可能对应的字符的一个映射。这是一个看起来像是我说」Hello」时的映射：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/e2bae96e0e8aa701363b0f6f64147ab8452ca5e3"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的神经网络可能预测到我说的是「HHHEE_LL_LLLOOO」，也可能是「HHHUU_LL_LLLOOO」或甚至「AAAUU_LL_LLLOOO」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们用几个步骤来整理输出结果。首先，我们会用单个字符替换重复的的字符：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;HHHEE_LL_LLLOOO becomes HE_L_LO&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;HHHUU_LL_LLLOOO becomes HU_L_LO&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;AAAUU_LL_LLLOOO becomes AU_L_LO&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然后，我们将移除所有空白&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;HE_L_LO 变成 HELLO&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;HU_L_LO 变成 HULLO&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;AU_L_LO 变成 AULLO&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这让我们得到三种可能的转录 -「Hello」，「Hullo」和「Aullo」。如果你大声说出这些单词，这些转录的声音都类似于「Hello」。因为它每次只预测一个字符，神经网络会得出一些试探性的转录。例如，如果你说「He would not go」，它可能会给一个可能转录「He wud net go」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;技巧是将这些基于发音的预测与基于书写文本（书籍，新闻文章等）的大数据库的似然分数相结合。你抛出看起来最不可能是真的的转录，并保持转录看起来最现实。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在我们可能的转录「Hello」、「Hullo」和「Aullo」中，显然「Hello」在文本数据库中更频繁地出现（更不用说在我们原始的基于音频的训练数据中），因此可能是正确的。所以我们选择「Hello」作为我们的最后的转录。这就完成了！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;等一下！&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是如果有人说「Hullo」那又怎么样？「Hullo」是一个有效的词。也许「Hello」是错误的转录！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/ce206795bfab410a7e668ad6c9e84e456ce7ab01"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当然实际上可能有人说的是「Hullo」而不是「Hello」。但是这样的语音识别系统（基于美式英语训练）基本上不会将「Hullo」作为转录。相比」Hello「，用户不太可能说「Hullo」，即是你在说」Hullo「ullo，它也总是会认为你在说「Hello」，无论你发「U」的声音有多重。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;试试看！如果你的手机设置为美式英语，尝试让你的手机的数字助理识别「Hullo」。你不能达到目标！它会拒绝！它总是会理解为「Hello」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不识别「Hullo」是合理的，但有时你会发现令人讨厌的情况:你的手机就是不能理解你说的语句。这就是为什么这些语音识别模型总需要更多的数据训练来处理这些少数情况。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;我能建立自己的语音识别系统吗？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;机器学习最酷的事情之一就是它有时看起来十分简单。你得到一堆数据，将把它输入到机器学习算法当中去，然后就能神奇的得到一个运行在你的游戏笔记本电脑显卡上的世界级人工智能系统... 对吧？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有些情况下的确是这样，但是语音识别却并不如此简单。语音识别是一个难题，你必须克服无限的挑战：质量差的麦克风、背景噪声、混响和回声、口音变化等等。这些问题都需要呈现在你的训练数据中，以确保神经网络可以处理它们。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;还有另一个例子：你知道当你在一个大房间里说话时，你会不自觉地提高你的音调以便掩盖噪音吗？人类在什么情况下都可以理解你，但神经网络需要特殊训练来处理这些情况。所以你需要得到人们在噪音中大声说话的训练数据！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;要构建一个达到 Siri、Google Now！或 Alexa 等水平的语音识别系统，你需要得到大量的训练数据，如果没有雇佣成百上千的人为你记录数据，你很难做到。用户对低质量语音识别系统的容忍度很低，因此你不能吝啬语音数据。没有人想要一个只有 80% 的时间有效的语音识别系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;像谷歌或亚马逊这样的公司，现实生活中记录的成千上万小时的口语音频，对他们来说就是「黄金」。这就是将他们世界级语音识别系统与你自己的系统拉开差距的法宝。在手机上免费使用 Google Now! 和 Siri 或是不收取转录费且售价 50 美元的 Alexa，都是为了让你尽可能地使用它们。你说的每句话都将被这些系统所记录，然后这些数据将被用于训练未来的语音识别算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不相信我？如果你有一部安装了 Google Now！的安卓手机，点击这里去收听你对它说过的每一句话：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/268e4de2196b29148322fc655ddd91263ce46629"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你可以通过 Alexa 在亚马逊上找到相同的东西。然而不幸的是，苹果手机并不允许你利用你的 Siri 语音数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以，如果你正在寻找创业的想法，我不建议你建立自己的语音识别系统与 Google 竞争。相反，你应该找到一种能让人们将他们几个小时的录音给予你的方法。这种数据可以是你的产品。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;学习更多&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个被用来处理不同长度音频的算法被称为 Connectionist Temporal Classification（CTC）。你可以阅读来自 2006 年的原始文章：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;http://www.cs.toronto.edu/~graves/icml_2006.pdf。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;百度的 Adam Coates 在湾区深度学习学校做了关于「用深度学习做语音识别」的精彩演讲。你可以在 YouTube 上观看这段视频（https://youtu.be/9dXiAecyJrY?t=13874，他的演讲从 3 分 51 秒开始）。强烈推荐。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;原文链接：https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a#.34p9sntcc&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&amp;copy;本文为机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Sun, 19 Feb 2017 12:19:19 +0800</pubDate>
    </item>
    <item>
      <title>学界 | iPOP：首个基于个性化大数据的个性化医学研究</title>
      <link>http://www.iwgc.cn/link/</link>
      <description>
&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Genome Hunter&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：王灏、李亚洲、李泽南&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;该研究是关于 Mike Snyder 教授的「整合性个人组学图谱」（integrative personal omics profile，iPOP）；Mike Snyder 教授是该论文的通讯作者，也是斯坦福大学遗传学系主任。这是首项针对个人的健康与疾病状态进行的大规模 iPOP 研究。该论文于 2012 年发表在 Cell 上。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/1804ea9e90c9c0e4bce9aa8feb5e5f0638cf922b"/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;背景&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在基因组的层面上，我们与我们的邻居或者朋友具有 99.9% 的相似性。但正是这 0.1% 的不同，让我们每个人都与众不同。这些微小的遗传变异对于我们的健康具有巨大的影响。因此，疾病的发生过程和我们对于治疗的反应都与我们的基因组序列紧密相关。除了我们的基因组，人与人之间的差异也体现在 RNA、蛋白质和代谢产物的层面上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;长时间段的 iPOP 数据的收集&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;通过在 14 个月中持续收集 Snyder 教授的血液组分，将来自基因组、转录组、蛋白质组、代谢物组、抗体的图谱的结合起来，最终形成了 iPOP 数据集（图 1）。研究者使用了多种技术（包括全基因组测序、RNA 序列、人类细胞因子检测和质谱分析）来生成这一巨大的数据集（其包含采集自 20 多个时间点，总共超过 30 亿个数据点）。简单来说，基因组图谱提供了个体的基因组序列和种系变异。此外，转录组、蛋白质组、代谢物组和抗体图谱使得人们可以观测到个体在一段时间内基因表达趋势的动态变化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/0b55a9274a8508de6ce62f5f9d005fb1020d8543"/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;图 1. iPOP 的实验流程和数据分析方法。PBMC：外周血单核细胞（peripheral blood mononuclear cell）。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在该研究过程中，Snyder 教授受到了两次病毒性感染：（1）第一次是开始于第 0 天的人鼻病毒（human rhinovirus，HRV）感染；（2）第二次是开始于第 289 天的呼吸道合胞病毒（respiratory syncytial virus，RSV）感染。它为研究者提供了在病毒感染的反应期间研究基因表达动态变化的绝佳机会。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;iPOP 预测与疾病和药物相关的变异&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此前的数十年研究已经鉴定了众多与疾病和药物使用相关的遗传变异（即生物标志物）。因此，研究者首先分析了与疾病和药物应答有关的遗传变异。他们发现 Synder 教授的基因组序列包含多种与疾病相关的罕见变异，包括 2 型糖尿病以及一些与药物应答有关的变异（图 2）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/183c164da754f446e22e9544c75c981649024233"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 2. 一些重要的与疾病和药物相关的遗传变异示例&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;用 iPOP 监测糖尿病风险并帮助治疗&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在此项研究之前，Snyder 教授并没有与糖尿病相关的已知风险因素，并且从实验一开始时血糖水平是正常的。如上所述，Snyder 在研究过程中受到了 RSV 感染（从第 289 天开始）。显而易见，体内免疫反应激活了。令人意外的是，在其身体对病毒产生应答的同时，胰岛素信号通路表达水平下降，并且血糖水平同时升高，这是开始罹患糖尿病的标志（图 3）。在 RSV 感染后长达数月（第 301 天后），血糖水平持续处于高位。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/05094f8ea09d0a5312dc8a4b1cb990c3880bb98b"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;图 3. 本研究中的血糖水平趋势。有两次病毒感染：从第 0 天开始的 HRV 感染（红色箭头），以及从第 289 天开始的 RSV 感染（绿色箭头）&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Snyder教授在改变饮食和运动后，监测显示血糖水平呈逐渐下降的趋势。 这些结果表明，基因组序列可用于预估健康个体的患病风险，并且疾病的生物标志物（本文中的血糖）可用于监测和检测该疾病的治疗情况。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;整合性组学分析提供更多的生物医学信息&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了进一步利用转录组和基因组数据，作者对每个时间点的转录组、蛋白质组和代谢组学数据进行了整合分析，观察它们与不同生理状态的相关性（图 4）。特别地，他们着眼于系统地寻找随时间变化的相关模式。为了处理时间序列中的数据异质性和缺失数据，他们使用了一种傅立叶谱分析方法（Lomb-Scargle 变换）来为每个时间序列曲线构建周期图。Lomb-Scargle 方法已被成功应用于天文学中以处理非均匀采样的时间序列数据，也被用于多种形式的生物学问题上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/eb7a45bd4c88219972ed2b0efa53f6eaf83654fc"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 4. 转录组、蛋白质组和代谢组数据的整合分析。数据点被聚类以鉴定疾病相关的生物学通路&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集的整合分析证实了之前的发现。它表明，在感染 RSV 后的发病以及后续的应答过程中，机体产生了一个全身性的反应，包括在 RSV 感染后第 18 天有一个明显的应答。在随后的时间点中，多种与感染或者应激应答有关的生物学通路，以及与高血糖水平有关的生物学通路均受到影响，其中包括包括胰岛素应答通路。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;结论&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;iPOP 提供了一个针对健康状态的多维视角，包括健康状况、对病毒的应答、疾病恢复以及糖尿病发病。总而言之，该研究证明利用基于 iPOP 的方法是有助于实现个性化医学的：从基因组序列鉴定疾病风险，并且通过其他分子组分指标监测疾病状态。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;通过多种高通量技术将基因组信息与检测生理状态的常规方法结合起来，将有助于个性化医学的实现。从这项研究产生的丰富数据将是个性化医学发展领域的宝贵资源。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，该论文的作者创建了一个网站，以方便人们使用 iPOP 资源（http://snyderome.stanford.edu）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/c09d7855ed7ae47b50550defdd6340cf07d6f9af"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 5. 提供 iPOP 数据和结果的网站&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&amp;copy;本文为机器之心原创，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Sun, 19 Feb 2017 12:19:19 +0800</pubDate>
    </item>
    <item>
      <title>资源 | 5 个Python 库，照亮你的机器学习之路</title>
      <link>http://www.iwgc.cn/link/</link>
      <description>
&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;选自Infoworld&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Serdar Yegulalp&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：黄小天、李亚洲、微胖&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;这些 Python 库帮助你加速数据传输，通过 AWS Lambda 对大型计算工作做碎片化处理，并使用略低于 TensorFlow 的模型工作。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;机器学习令人兴奋，但具体工作复杂而困难。通常它涉及很多手动提升&amp;mdash;&amp;mdash;汇总工作流及传输渠道，设置数据源，以及在内部部署和云部署的资源之间来回分流。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用来提高工作效率的手头工具越多越好。庆幸的是，Python 是一个威力巨大的工具语言，在大数据和机器学习之中被广泛使用。下面是 5 个 Python 库，帮助你缓解来自交易提升的重负。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/47c2529a4976fde926318162fd181709da4bdc9f"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;PyWren&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;地址：https://github.com/ericmjonas/pywren&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;PyWren，一个带有强大前提的简单包，能使你运行基于 Python 的科学计算工作量，以作为 AWS Lambda 函数的多个例子。项目 At The New Stack 的简介这样描述 PyWren: 把 AWS Lambda 用作一个巨大的平行处理系统，以处理那些可被切割成诸多小任务的项目，这些小任务的运行不需要占用很多内存或硬盘。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Lambda 函数的一个缺点是运行时间最长不能超过 300 秒。但是，如果你需要一个只花费几分钟就能完成的工作，并在数据集中需要运行数千次，那么 PyWren 也许是一个好选择，它可以一种用户硬件上不可用的规模平行化云端的工作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌的 TensorFlow 框架正迈入伟大时刻，因为刚发布了 1.0。人们通常会问一个问题：如何利用在上面训练的模型而无需使用 TensorFlow 本身？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Tfdeploy&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;地址：https://github.com/riga/tfdeploy&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Tfdeploy 可以部分解决这个问题。将训练过的 TensorFlow 模型输出「一个简单的基于 Numpy 的可调用对象（callable）」，也就是说，借由 Tfdeploy，可以在 Python 中使用模型，而且 Numpy 的数学和统计库被作为唯一的依靠。几乎所有能在 TensorFlow 上跑的运行也能在 Tfdeploy 上跑，而且你可以通过标准 Python 隐喻方式来延伸库的行为（比如，超载一个类别）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在，坏的消息是：Tf 部署并不支持 GPU 加速，要是 Numpy 能克服那一点该多好。Tfdeploy 的创造者建议 gNumPy 项目是一个可行的替代。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Luigi&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;地址：https://github.com/spotify/luigi&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;编写成批作业通常只是处理海量数据的其中一步：你也不得不将所有这些工作串联起来，做成类似工作流程的东西。Luigi 是 Spotify 打造的，用于「解决所有通常与长期运行成批处理作业有关的管道问题。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有了 Luigi，研发人员就可以从事几个很难、与数据无关的任务处理&amp;mdash;&amp;mdash;「一个 Hive 询问，一个在 Jave 上完成的 Hadoop 任务，一个 Scala 上的 Spark 任务，一个从数据库中导出表格」&amp;mdash;&amp;mdash;创造一个端到端运行它们的工作流。对任务的整个描述以及依存性被打造为 Python 模块，和 XML 配置文档或其他数据形式不同，因此，可以被组合到其他以 Python 为中心的项目中去。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Kubelib&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;地址：https://github.com/safarijv/kubelib&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你采用 Kubernetes 作为完成机器学习工作的编排系统（orchestration system），你最不想要的就是它产生的问题比能解决的问题都多。Kubelib 为 Kubernetes 提供了一系列的 Python 接口，本来是用 Jekins scripting 作为帮助。但没有 Jenkins 的情况下也能够使用，它能够完成 暴露在 kubectl CLI 或者 Kubernetes API 中的所有事。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;a data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650722553&amp;amp;idx=1&amp;amp;sn=ce635e60fa8f1cc16982c5d6a9a6931b&amp;amp;chksm=871b1487b06c9d9180d7f881784e68d4b9785481c38aa86eccc183aed8254b2a452e073a0c9b&amp;amp;scene=21#wechat_redirect" href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650722553&amp;amp;idx=1&amp;amp;sn=ce635e60fa8f1cc16982c5d6a9a6931b&amp;amp;chksm=871b1487b06c9d9180d7f881784e68d4b9785481c38aa86eccc183aed8254b2a452e073a0c9b&amp;amp;scene=21#wechat_redirect" target="_blank"&gt;&lt;strong&gt;&lt;span&gt;PyTorch&lt;/span&gt;&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;地址：https://github.com/pytorch/pytorch&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不要忘记了最近发布的、引人注目的 Python 库新成员 Pytorch，这是 Torch 机器学习框架的一个工具。PyTorch 不仅为 Torch 添加了 Python 端口，也增加了许多其他的便利，比如 GPU 加速，共享内存完成多重处理（multiprocessing，特别是多核上隔离开的工作。) 最好的是，它们能为 Numpy 中的无加速功能提供 GPU 驱动的替代选择。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;原文链接：http://www.infoworld.com/article/3171654/artificial-intelligence/5-python-libraries-to-lighten-your-machine-learning-load.html&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&amp;copy;本文为机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Sun, 19 Feb 2017 12:19:19 +0800</pubDate>
    </item>
    <item>
      <title>一周论文 | 增强学习在Image Caption任务上的应用</title>
      <link>http://www.iwgc.cn/link/</link>
      <description>
&lt;h1 style=" max-width: 100%; color: rgb(62, 62, 62) ; ; ; ; ; ; ; ; ; ; ; ; "&gt;&lt;span&gt;&lt;strong&gt;引言&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第二十二期的PaperWeekly对Image Captioning进行了综述。今天这篇文章中，我们会介绍一些近期的工作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Image Captioning的模型一般是encoder-decoder的模型。模型对$p(S|I)$进行建模，$S$是描述，$I$是图片。模型的训练目标是最大化log似然：$\max_\theta\sum_i \log P(S_i|I_i, \theta)$。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而使用最大似然训练有两个问题：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1、虽然训练时最大化后验概率，但是在评估时使用的测度则为BLEU，METEOR，ROUGE，CIDER等。这里有训练loss和评估方法不统一的问题。而且log似然可以认为对每个单词都给予一样的权重，然而实际上有些单词可能更重要一些（比如说一些表示内容的单词）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2、第二个问题为Exposure bias。训练的时候，每个时刻的输入都是来自于真实的caption。而生成的时候，每个时刻的输入来自于前一时刻的输出；所以一旦有一个单词生成的不好，错误可能会接着传递，使得生成的越来越糟糕。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如何解决这两个问题呢？很显而易见的想法就是尽量使得训练和评估时的情形一样。我们可以在训练的时候不优化log似然，而是直接最大化CIDER（或者BLEU，METEOR，ROUGE等）。并且，在训练时也和测试时一样使用前一时刻的输入，而不是全使用ground truth输入。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而这有什么难点呢？第一，CIDER或者这一些metric并不是可直接求导。（这就是为什么在分类问题中，我们把0-1 error近似成log loss，hinge loss的原因）。其次从前一时刻输出获得后一时刻的输入涉及到采样操作，这也是不可微的。为了能够解决这些不可微的问题，人们就想到了Reinforcement learning。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h1 style=" max-width: 100%; color: rgb(62, 62, 62) ; ; ; ; ; ; ; ; ; ; ; ; "&gt;&lt;span&gt;&lt;strong&gt;RL基本概念&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;RL中有一些比较重要的基本概念：状态（state），行为（action），回报（reward）和决策（policy）。决策是一个状态到动作的函数，一般是需要学习的东西。拿打游戏的例子介绍RL最简单。如果说是玩flappy bird，RL要学习的就是在什么位置跳，能使得最后得到的分数越高。在这个例子里，最后的分数就是回报，位置就是状态，跳或者不跳就是行为，而什么时候跳就是学到的策略。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果放在Image captioning中，状态就是你看到的图片和已生成的单词，而动作就是下一个单词生成什么，回报就是CIDER等metric。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h1 style=" max-width: 100%; color: rgb(62, 62, 62) ; ; ; ; ; ; ; ; ; ; ; ; "&gt;&lt;span&gt;&lt;strong&gt;相关文献&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最近已经有很多工作将RL用在NLP相关的问题上。[1]第一次将REINFORCE算法用在image caption和seq2seq问题上。[5]将使用了更先进的RL算法 &amp;mdash; Actor-critic &amp;mdash; 来做machine translation上。[2,4]将[1]的算法进行稍许改进（仍旧是REINFORCE算法），使用在了image captioning上。[3]将REINFORCE用在序列生成GAN中，解决了之前序列生成器输出为离散不可微的问题。[6]将RL用在自然对话系统中。这篇文章中我们主要介绍[1,2,4]。&lt;/span&gt;&lt;/p&gt;&lt;h1 style=" max-width: 100%; color: rgb(62, 62, 62) ; ; ; ; ; ; ; ; ; ; ; "&gt;&lt;br&gt;&lt;/h1&gt;&lt;h1 style=" max-width: 100%; color: rgb(62, 62, 62) ; ; ; ; ; ; ; ; ; ; ; ; "&gt;&lt;span&gt;&lt;strong&gt;RL算法背景&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这三篇文章使用的是REINFORCE算法，属于增强学习中Policy Gradient的一种。我们需要将deterministic的策略形式 $a=\pi(s,\theta)$转化为概率形式，$p(a) = \pi(a|s, \theta)$。Policy Gradient就是对参数$\theta$求梯度的方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;直观的想，如果我们希望最后的决策能获得更高的reward，最简单的就是使得高reward的行为有高概率，低reward的行为有低概率。所以REINFORCE的更新目标为&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;$$\max_{\theta} \sum R(a,s)\log \pi(a|s, \theta)$$&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;$R(s,a)$是回报函数。有了目标，我们可以通过随机梯度下降来更新$\theta$来获得更大的回报。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而这个方法有一个问题，训练时梯度的方差过大，导致训练不稳定。我们可以思考一下，如果reward的值为100到120之间，现在的方法虽然能更大地提高reward为120的行为的概率，但是也还是会提升低reward的行为的概率。所以为了克服这个问题，又有了REINFORCE with baseline。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;$$\max_{\theta} \sum (R(a,s) - b(s))\log \pi(a|s, \theta)$$&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;$b(s)$在这里就是baseline，目的是通过给回报一个基准来减少方差。假设还是100到120的回报，我们将baseline设为110，那么只有100回报的行为就会被降低概率，而120回报的行为则会被提升概率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h1 style=" max-width: 100%; color: rgb(62, 62, 62) ; ; ; ; ; ; ; ; ; ; ; ; "&gt;&lt;span&gt;&lt;strong&gt;三篇paper&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第一篇是FAIR在ICLR2016发表的[1]。这篇文章是第一个将RL的算法应用的离散序列生成的文章。文章中介绍了三种不同的方法，这里我们只看最后一种算法，Mixed Incremental Cross-Entropy Reinforce。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大体的想法就是用REINFORCE with baseline来希望直接优化BLEU4分数。具体训练的时候，他们先用最大似然方法做预训练，然后用REINFORCE finetune。在REINFORCE阶段，生成器不再使用任何ground truth信息，而是直接从RNN模型随机采样，最后获得采样的序列的BLEU4的分数r作为reward来更新整个序列生成器。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这里他们使用baseline在每个时刻是不同的；是每个RNN隐变量的一个线性函数。这个线性函数也会在训练中更新。他们的系统最后能比一般的的cross extropy loss，和scheduled sampling等方法获得更好的结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们在github开源了基于torch的代码，&lt;/span&gt;&lt;a rel="external" style="color: rgb(67, 149, 245); max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important; text-decoration: underline;" target="_blank"&gt;&lt;span&gt;https://github.com/facebookresearch/MIXER&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第二篇论文是今年CVPR的投稿。这篇文章在[1]的基础上改变了baseline的选取。他们并没有使用任何函数来对baseline进行建模，而是使用了greedy decoding的结果的回报作为baseline。他们声称这个baseline减小了梯度的variance。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个baseline理解起来也很简单：如果采样得到句子没有greedy decoding的结果好，那么降低这句话的概率，如果比greedy decoding还要好，则提高它的概率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个方法的好处在于避免了训练一个模型，并且这个baseline也极易获得。有一个很有意思的现象是，一旦使用了这样的训练方法，beam search和greedy decoding的结果就几乎一致了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前这篇文章的结果是COCO排行榜上第一名。他们使用CIDEr作为优化的reward，并且发现优化CIDEr能够使所有其他metric如BLEU，ROUGE，METEOR都能提高。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们的附录中有一些captioning的结果。他们发现他们的模型在一些非寻常的图片上表现很好，比如说有一张手心里捧着一个长劲鹿的图。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第三篇论文[4]也是这次CVPR的投稿。这篇文章则是在$R(a,s)$这一项动了手脚。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;前两篇都有一个共同特点，对所有时刻的单词，他们的$R(a,s)$都是一样的。然而这篇文章则给每个时刻的提供了不同的回报。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其实这个动机很好理解。比如说，定冠词a，无论生成的句子质量如何，都很容易在句首出现。假设说在一次采样中，a在句首，且最后的获得回报减去baseline后为负，这时候a的概率也会因此被调低，但是实际上大多数情况a对最后结果的好坏并没有影响。所以这篇文章采用了在每个时刻用$Q(w_{1:t})$来代替了原来一样的$R$。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个$Q$的定义为，&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;$Q\theta(w{1:t}) = \mathbb{E}{w{t+1:T}}[R(w{1:t}, w{t+1:T})]$&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;也就是说，当前时刻的回报，为固定了前t个单词的期望回报。考虑a的例子，由于a作为句首生成的结果有好有坏，最后的Q值可能接近于baseline，所以a的概率也就不会被很大地更新。实际使用中，这个Q值可以通过rollout来估计：固定前t个词后，随机采样K个序列，取他们的平均回报作为Q值。文中K为3。这篇文章中的baseline则跟[1]中类似。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从实验结果上，第三篇并没有第二篇好，但是很大一部分原因是因为使用的模型和特征都比较老旧。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h1 style=" max-width: 100%; color: rgb(62, 62, 62) ; ; ; ; ; ; ; ; ; ; ; ; "&gt;&lt;span&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;将RL用在序列生成上似乎是现在新的潮流。但是现在使用的大多数的RL方法还比较简单，比如本文中的REINFORCE算法可追溯到上个世纪。RL本身也是一个很火热的领域，所以可以预计会有更多的论文将二者有机地结合。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h1 style=" max-width: 100%; color: rgb(62, 62, 62) ; ; ; ; ; ; ; ; ; ; ; ; "&gt;&lt;span&gt;&lt;strong&gt;参考文献&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;[1] Ranzato, Marc&amp;rsquo;Aurelio, Sumit Chopra, Michael Auli, and Wojciech Zaremba. &amp;ldquo;Sequence level training with recurrent neural networks.&amp;rdquo;&amp;nbsp;arXiv preprint arXiv:1511.06732&amp;nbsp;(2015).&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;[2] Rennie, Steven J., Etienne Marcheret, Youssef Mroueh, Jarret Ross, and Vaibhava Goel. &amp;ldquo;Self-critical Sequence Training for Image Captioning.&amp;rdquo;&amp;nbsp;arXiv preprint arXiv:1612.00563&amp;nbsp;(2016).&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;[3] Yu, Lantao, Weinan Zhang, Jun Wang, and Yong Yu. &amp;ldquo;Seqgan: sequence generative adversarial nets with policy gradient.&amp;rdquo;&amp;nbsp;arXiv preprint arXiv:1609.05473&amp;nbsp;(2016).&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;[4] Liu, Siqi, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin Murphy. &amp;ldquo;Optimization of image description metrics using policy gradient methods.&amp;rdquo;&amp;nbsp;arXiv preprint arXiv:1612.00370&amp;nbsp;(2016).&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;[5] Bahdanau, Dzmitry, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. &amp;ldquo;An actor-critic algorithm for sequence prediction.&amp;rdquo;&amp;nbsp;arXiv preprint arXiv:1607.07086&amp;nbsp;(2016).&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;[6] Li, Jiwei, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. &amp;ldquo;Deep reinforcement learning for dialogue generation.&amp;rdquo;&amp;nbsp;arXiv preprint arXiv:1606.01541&amp;nbsp;(2016).&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h1 style=" max-width: 100%; color: rgb(62, 62, 62) ; ; ; ; ; ; ; ; ; ; ; ; "&gt;&lt;span&gt;&lt;strong&gt;作者&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;罗若天，&lt;/strong&gt;TTIC博士生 &amp;nbsp;研究方向&amp;nbsp;CV+NLP&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;博客&lt;/strong&gt;&lt;span&gt;：&lt;a style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;http://ruotianluo.github.io&lt;/a&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;专栏&lt;/strong&gt;&lt;span&gt;：&lt;a style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;https://zhuanlan.zhihu.com/c_73407294&lt;/a&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;github&lt;/strong&gt;&lt;span&gt;:&amp;nbsp;&lt;a style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;https://github.com/ruotianluo&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;关于PaperWeekly&lt;/span&gt;&lt;/strong&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;PaperWeekly是一个分享知识和交流学问的学术组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微信公众号：PaperWeekly&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/74e43d689e2973dc494ef5c2c85c981e72b56552"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微博账号：PaperWeekly（&lt;/span&gt;&lt;a rel="external" style="color: rgb(67, 149, 245); max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important; text-decoration: underline;" target="_blank"&gt;&lt;span&gt;http://weibo.com/u/2678093863&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&amp;nbsp;）&lt;br&gt;微信交流群：微信+ zhangjun168305（请备注：加群交流或参与写paper note）&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;
</description>
      <pubDate>Sun, 19 Feb 2017 12:19:19 +0800</pubDate>
    </item>
  </channel>
</rss>
