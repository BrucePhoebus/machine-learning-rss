<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>机器之心</title>
    <link>http://www.iwgc.cn/list/670</link>
    <description>人与科技的美好关系</description>
    <item>
      <title>入门级解读：小白也能看懂的TensorFlow介绍</title>
      <link>http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650723520&amp;idx=1&amp;sn=d204284574e9e56682b6ed6f9dcaff01&amp;chksm=871b10beb06c99a8ccb990b5299b4a2b2c6b7bdcf2b21813f294ed09c838f7ff03e94191c682&amp;scene=0#rd</link>
      <description>
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;选自medium&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;strong&gt;作者：Soon Hin Khor&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：Jane W、邵明、微胖&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote style="color: rgb(62, 62, 62); font-size: 16px; white-space: normal; max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;p&gt;&lt;span&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;本文是日本东京 TensorFlow 聚会联合组织者 Hin Khor 所写的 TensorFlow 系列介绍文章的Part 3 和 Part4，自称给出了关于 TensorFlow 的 gentlest 的介绍。&lt;a data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718466&amp;amp;idx=1&amp;amp;sn=016f111001e8354d49dd4ce279d283cd&amp;amp;scene=21#wechat_redirect" href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718466&amp;amp;idx=1&amp;amp;sn=016f111001e8354d49dd4ce279d283cd&amp;amp;scene=21#wechat_redirect" target="_blank"&gt;在之前发布的前两部分介绍中&lt;/a&gt;，作者谈到单一特征问题的线性回归问题以及训练（training）的含义，这两部分将讲解&amp;nbsp;TensorFlow（TF）进行多个特征的线性回归和逻辑回归。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;矩阵和多特征线性回归&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;快速回顾&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;之前文章的前提是：给定特征&amp;mdash;&amp;mdash;任何房屋面积（sqm），我们需要预测结果，也就是对应房价（$）。为了做到这一点，我们：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;我们找到一条「最拟合」所有数据点的直线（线性回归）。「最拟合」是当线性回归线确保实际数据点（灰色点）和预测值（内插在直线上的灰色点）之间的差异最小，即最小化多个蓝线之和。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用这条直线，我们可以预测任何房屋的价格。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/963481037d7566bb2b7ffe393f86589927fa23db"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;使用单一特征线性回归进行预测&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;strong&gt;&lt;span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;多特征线性回归概述&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;实际上，任何预测都依赖于多个特征，于是我们从单特征的线性回归进阶到 带有两个特征的线性回归；之所以选择两个特征，是为了让可视化和理解简明些，但这个思想可以推广到带有任何数量特征的线性回归。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们引进一个新的特征&amp;mdash;&amp;mdash;房间数量。当收集数据点时，现在我们需要在现有特征「房屋面积」之上收集新特征「房间数」的值，以及相应的结果「房屋价格」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的图表变成了 3 维的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/38ad5af7eb37dcd1f0288eb0b785be105dad89a9"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;结果「房屋价格」以及 2 个特征（「房间数」，「房屋面积」）的数据点空间&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然后，我们的目标变成：给定「房间数」和「房屋面积」，预测「房屋价格」（见下图）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/429f1ebc82fd87cc7be5162b24429688ffd629cf"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;由于缺少数据点，有时无法对给定的 2 个特征进行预测&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在单一特征的情形中，当没有数据点时，我们需要使用线性回归来创建一条直线，以帮助我们预测结果房屋价格。在 2 个特征的情形中，我们也可以使用线性回归，但是需要创建一个平面（而不是直线），以帮助我们预测（见下图）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/1ea37af70804bf4f8899cde87793485e8f026e31"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;使用线性回归在 2 个特征空间中的创建一个平面来做预测&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;多特征线性回归模型&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;回忆单一特征的线性回归（见下图左边），线性回归模型结果为 y，权重为 W，房屋大面积为 x，偏差为 b。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于 2 个特征的回归（参见下图右侧），我们引入另一个权重 W2，另一个自变量 x2 来代表房间数的特征值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/79fbe951113b74b0002019a394c4a8badebfbdad"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;单特征 vs. 2 个特征的线性回归方程&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如之前讨论的那样，当我们执行线性回归时，梯度下降算法能帮助学习系数 W、W2 和 b 的值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Tensorflow 的多特征线性回归&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1.快速回顾&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;单特征线性回归的 TF 代码由 3 部分组成（见下图）：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;构建模型（蓝色部分）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;基于模型构建成本函数（红色部分）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用梯度下降（绿色部分）最小化成本函数&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/a4c3ab928f3c22eb8b7ff9b4fb2c3d8f1cac3604"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;用于单特征线性回归的 Tensorflow 代码&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2.Tensorflow 的 2 个特征的线性回归&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;TF 代码中 2 个特征的线性回归方程（如上所述）的变化（相比单特征）用红色显示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/aa5dc2b9176ddcda4c3f12f9d2aa951165729fc9"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意，增加新特征的这种方式效率低；随着特征数量的增长，所需的变量系数和自变量的数量会增加。实际的模型有更多的特征，这恶化了这个问题。那么，如何能有效地表示特征呢？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;解决方法：矩阵&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先，让我们将表征两个特征的模型推广到表征 n 个特征的模型：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/c622645f91b9cee532afeea782011720ec9ae89a"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;复杂的 n 特征公式可以用矩阵简化，矩阵被内置于 TF 中，这是因为：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;数据可以用多维表示，这契合我们表征具有 n 个特征的数据点（左下方，也称为特征矩阵）以及具有 n 个权重模型（右下，也称为权重矩阵）的方式&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/29a188068963c1b73f2a9c68d704429ca869fc5e"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;单个数据点的 n 个特征与模型的矩阵形式的 n 个权重&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 TF 中，它们将被写为：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;x = tf.placeholder（tf.float，[1，n]）&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;W = tf.Variable（tf.zeros [n，1]）&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意：对于 W，我们使用 tf.zeros，它将所有 W1，W2，...，Wn 初始化为零。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在数学上，矩阵乘法是向量乘法的加总；因此自然地，特征（中间的一个）和权重（右边的）矩阵之间的矩阵乘法给出（左边的）结果，即等于 n 个特征的线性回归公式的第一部分（如上所述），没有截距项。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/7ac2e1e392b9f5b74f0085604971c7e97d895b59"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;特征和权重矩阵之间的矩阵乘法给出结果（未添加截距项）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 TF 中，这种乘法将表示为：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;y = tf.matmul(x, W)&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;多行特征矩阵（每行表示数据点的 n 个特征）之间的矩阵乘法返回多行结果，每行代表每个数据点的结果/预测（没有加入截距项）；因此一个矩阵乘法就可以将线性回归公式应用于多个数据点，并对应地产生多个预测（每个数据点对应一个结果）（见下文）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意：特征矩阵中的 x 表示变的更复杂，即我们使用 x1.1、x1.2，而不是 x1、x2 等，因为特征矩阵（中间矩阵）从表示 n 个特征（1 行 x，n 列）的单个数据点扩展到表示具有 n 个特征（m 行 x，n 列）的 m 个数据点。因此，我们扩展 x &amp;lt;n&amp;gt;（如 x1）到 x &amp;lt;m &amp;gt;.&amp;lt;n&amp;gt;（如 x1.1），其中，n 是特征数，m 是数据点的数量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/6cd712fff08efb1e637a2b03e80e9162cb1bb3bf"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;具有模型权重的多行矩阵乘法产生矩阵的多个行结果&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 TF 中，它们将被写为：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;x = tf.placeholder（tf.float，[m，n]）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;W = tf.Variable（tf.zeros [n，1]）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;y = tf.matmul（x，W）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;最后，向结果矩阵添加常数，也就是将常数添加到矩阵中的每一行&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 TF 中，用矩阵表示 x 和 W，无论模型的特征数量或要处理的数据点数量，矩阵都可以简化为：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;b = tf.Variable(tf.zeros[1])&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;y = tf.matmul(x, W) + b&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Tensorflow 的多特征备忘单&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们做一个从单一特征到多特征的线性回归的变化的并行比较：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/eb844cac0b776bfd2bc8f1e30523670b430acaf2"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Tensorflow 中的单特征与 n 个特征的线性回归模型&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;总结&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在本文中，我们介绍了多特征线性回归的概念，并展示了我们如何将模型和 TF 代码从单特征的线性回归模型扩展到 2 个特征的线性回归模型，并可以推广到 n 特征线性回归模型。最后我们为多特征的 TF 线性回归模型提供了一张备忘单。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;逻辑回归&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;逻辑回归综述&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们已经学会了如何使用 Tensorflow（TF）去实现线性回归以预测标量值得结果，例如给定一组特征，如住房大小，预测房价。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，有时我们需要对事物分类（classify）而不是去预测一个具体的数值，例如给定一张含有数字（0-9 十个数字中的一个）的图片，我们需要将其分类为 0，1，2，3，4，5，6，7，8，9 十类。或者，我们需要将一首歌曲进行归类，如归类为流行，摇滚，说唱等。集合 [0,1,2，...，9]、[流行，摇滚，说唱，等等] 中的每一个元素都可以表示一个类。在计算机中，我们通常用数字对抽象名词进行表示，比如，pop = 0, rock = 1, 等等。为了实现分类，我们使用 TF 来实现逻辑回归。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在本文中，我们将使用逻辑回归将数字图片归类为 0，1，2，3，4，5，6，7，8，9 这十类。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;逻辑回归的细节&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;线性回归中的许多概念仍然用于逻辑回归之中。我们可以再次使用公式 y = W.x + b，但是有一些不同的地方。让我们看看线性回归和逻辑回归的公式：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/eb5a2145fb3331650eeb918def1cebbb9b90daa1"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;线性回归与逻辑回归的区别与相似&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;区别：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;结果（y）：对于线性回归，结果是一个标量值（可以是任意一个符合实际的数值），例如 50000，23.98 等；对于逻辑回归，结果是一个整数（表示不同类的整数，是离散的），例如 0,1,2，... 9。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;特征（x）：对于线性回归，特征都表示为一个列向量；对于涉及二维图像的逻辑回归，特征是一个二维矩阵，矩阵的每个元素表示图像的像素值，每个像素值是属于 0 到 255 之间的整数，其中 0 表示黑色，255 表示白色，其他值表示具有某些灰度阴影。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;成本函数（成本）：对于线性回归，成本函数是表示每个预测值与其预期结果之间的聚合差异的某些函数；对于逻辑回归，是计算每次预测的正确或错误的某些函数。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;相似性：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;训练：线性回归和逻辑回归的训练目标都是去学习权重（W）和偏置（b）值。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;结果：线性回归与逻辑回归的目标都是利用学习到的权重和偏置值去预测/分类结果。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;协调逻辑回归与线性回归&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了使逻辑回归利用 y = W.b + x，我们需要做出一些改变以协调上述差异。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1.特征变换，x&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以将二维的图片特征（假设二维特征有 X 行，Y 列）转换成一维的行向量：将第一行以外的其它行数值依顺序放在第一行后面。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/1745831307c3ceccae44bdebf689d7b6c4393f1a"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;转换图像特征以适用于逻辑回归公式&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2.预测结果转换，y&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于逻辑回归，y 不能作为标量，因为预测可能最终为 2.3 或 11，这不在可能的类 [0,1，...，9] 中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了解决这个问题，y 应该被转换成列向量，该向量的每个元素代表逻辑回归模型认为属于某个特定类的得分。在下面的示例中，预测结果为类'1'，因为它具有最高得分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/0b9d136bf7486c4863bb6f5cc6a260a561df4e7b"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;每个类的分数和具有最高分数的类成为被预测的类&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于给定的图片，为求这个分数向量，每个像素都会贡献一组分数（针对每一类），分数表示系统认为这张图片属于某类的可能性，每个像素分数之和成为预测向量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/1ef15fd09376f861d2b68362c434789e301d4b48"/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;每个像素提供一个分数向量；每个类别有一个分数，最后变成预测向量。所有预测向量的总和变成最终预测。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3.成本函数的变换&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;涉及到预测结果和实际结果之间数值距离的任何函数都不能作为成本函数。对于数字图片「1」，这样的成本函数将使预测值「7」（7-1=6）更严重地惩罚预测值「2」（2-1=1），尽管两个预测结果都是错误的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们即将使用的成本函数，交叉熵（H），用以下几个步骤实现：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 将实际图片的类向量（y'）转化成 one-hot 向量，这是一个概率分布。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 将预测类 (y) 转化成概率分布。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3. 使用交叉熵函数去计算成本函数，这表示的是两个概率分布函数之间的差异。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第一步：One-hot 向量&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于我们已经将预测 (y) 转换成分数向量，因此，我们也应该将实际图片类（y』）转换成相同维数的向量；one-hot 向量是将对应于实际类的的元素为设为 1，其它元素为 0。下面，我们展示表示 0-9 十个类中一个类的 one-hot 向量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/6f7bd24e1fab945239a15fea1c8958ddbd23ea7a"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图片类和它们的 one-hot 向量表示&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;假设实际图像上是数字「1」(y')，它的 one-hot 向量是 [0,1,0,0,0,0,0,0,0,0]，假设其预测向量 (y) [1.3, 33, 2, 1.2, 3.2, 0.5, 3, 9.2, 1]，绘制比较如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/47de5f78b7bd3b18cc8f003e45b99628793af2c6"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;真实图片 one&amp;mdash;hot 向量（顶）预测类别概率&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第二步：用 softmax 实现概率分布&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了在数学上比较这两个「图」的相似性，交叉熵是一个好方法。（这里是一个很棒但比较长的解释，如果你对细节感兴趣的话。https://colah.github.io/posts/2015-09-Visual-Information/）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，为了利用交叉熵，我们需要将实际结果向量（y'）和预测结果向量（y）转换为「概率分布」，「概率分布」意味着：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;每个类的概率/分数值在 0-1 之间；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;所以类的概率/分数和必须是 1；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;实际结果向量（y'）如果是 one-hot 向量，满足了上述限制。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为预测结果向量（y）, 使用 softmax 将其转换为概率分布：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/5059bfe852e174850c2a618f040e2a9a0e51ab55"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;softmax 函数，这里 i 是表示 0, 1, 2, &amp;hellip;, 9 十类&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个过程只需要简单的两步，预测向量（y）中的每个分量是 exp(y_i) 除以所有分量的 exp() 的和。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/249c0f2ca5d0f6c5f6893bb66118e6bf46ceda3d"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意：softmax（y）图形在形状上与 prediction (y) 相似，但是仅仅有较大的最大值和较小的最小值&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/020dd3419516d64bb1cf369f4a4fd85cf44f2c41"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;使用 softmax 前后预测（y）曲线&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第三步：交叉熵&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在，我们将预测向量分数概率分布（y'）和实际向量分数概率分布 (y) 运用交叉熵。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;交叉熵公式：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/2e1a604b189e0130a6652254be9f44ab96317348"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;交叉熵作为我们想最小化的成本函数&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了快速理解这个复杂的公式，我们将其分为 3 部分（见下文）。注意，本文中的符号，我们使用 y_i 表示 y 的第 i 个分量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/315fdc75081652f64cbe386d973237d4d003da60"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;交叉熵（H）公式可视为三个部分：红，蓝，绿&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;蓝：实际图像类（y'）对应的 one-hot 图，参看 one-hot 向量部分：&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;红：由预测向量元素（y）经过softmax(y)，-og(softmax(y）一系列变化而来：&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;绿：每一图片类别 i，其中，i = 0, 1, 2, &amp;hellip;, 9, 红蓝部分相乘的结果&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以下图例会进一步简化理解。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;蓝色制图只是真实图片类别（y'）one-hot 向量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/06af77c4e66f08413c8355ffd8773056c1b85953"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;每个预测向量元素，y，转换成 -log(softmax(y)，就得到红图：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/fb89a0181847cc580d84b0488132c641bf337366"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;预测类别向量（y）一系列转换后，得到红图&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你想完全地理解第二个变换 -log(softmax(y)) 与 softmax(y) 为什么成反比，请点击 video or slides（参见文末资源部分）.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;交叉熵（H），这个绿色的部分是每个类别的蓝色值和红色值的乘积和，然后将它们做如下相加：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/8536e3265247dc8a2577d283a142d6c33001e5e7"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;交叉熵是每个图像类的蓝色值和红色值的乘积之和。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于这张蓝色图片对应一个 one-hot 向量，one-hot 向量仅仅有一个元素是 1，它对应一个正确的图片类，交叉熵的其它所有元素乘积为 0，交叉熵简化为：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/4e238dfa3195852a4d4662e006975be2465c7ac7"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;将所有部分放到一起&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有了三个转换后，现在，我们就可以将用于线性回归的技术用于逻辑回归。下面的代码片段展示的是本系列文章第三部分线性回归代码和代码适用逻辑回归所需要的变化之间的对比。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;逻辑回归的目标是最小化交叉熵（H），这意味着我们只需要最小化 -log（softmax（y_i）项；因为该项与 softmax（y_i）成反比，所以我们实际上是最大化该项。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;使用反向传播去最小化交叉熵 (H ) 将改变逻辑回归的权重 W 和偏置 b。因此，每张图片的像素值将会给出对应图片类最高分数/概率!（最高分数/概率对应于正确的图片类）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/697681a421b61e7ebef4054b46be857cc203419a"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;将线性回归方法用于逻辑回归之中，「total_class」是欲分类问题的总类数量，例如，在上文手写数字体识别例子中，total_class=10。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 将特征变换成一维特征；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 将预测结果向量、实际结果向量变化成 one-hot 向量；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3. 将成本函数从平方误差函数变化到交叉熵。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/fc6a122f2019982de9974d4510c8cbd013b557e5"/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;总结&lt;/span&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;线性回归对基于给定特征的预测（数值）是有帮助的，逻辑回归根据输入特征实现分类是有帮助的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们展示了如何调整线性回归 y = W.x + b 实现逻辑回归：（1）转换特征向量；2）转换预测/结果向量；（3）转换成本函数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当你掌握了 one-hot 向量，softmax，交叉熵的知识，你就可以处理谷歌上针对「初学者」的图片分类问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;资源：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;针对初学者的图像识别的谷歌代码：&lt;/span&gt;&lt;span&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;slideshare 上的幻灯片：&lt;/span&gt;&lt;span&gt;http://www.slideshare.net/KhorSoonHin/gentlest-introduction-to-tensorflow-part-3&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;油管上的视频：&lt;/span&gt;&lt;span&gt;https://www.youtube.com/watch?v=F8g_6TXKlxw&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;原文链接：https://medium.com/all-of-us-are-belong-to-machines/gentlest-intro-to-tensorflow-4-logistic-regression-2afd0cabc54#.glculhxzi&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&amp;copy;本文为机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Tue, 21 Feb 2017 12:26:02 +0800</pubDate>
    </item>
    <item>
      <title>教程 | 七个小贴士，顺利提升TensorFlow模型训练表现</title>
      <link>http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650723520&amp;idx=2&amp;sn=53680fb4b709beb19b28d071907958fe&amp;chksm=871b10beb06c99a8993cd1729197bb05ab5f341359bf21fd6b3d3f134f51051d948265ca4abb&amp;scene=0#rd</link>
      <description>
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;选自deeplearningweekly&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Malte Baumann&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀、蒋思源、微胖&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;初来乍到的 TensorFlow 在 2015 年推出之后已经飞速成长为了 2016 年被用得最多的深度学习框架。我（Malte Baumann）在 TensorFlow 推出后几个月入了坑，在我努力完成硕士论文阶段开始了我的深度学习之旅。我用了一段时间才适应计算图（computation graph）和会话模型（session model）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这篇短文并不是一篇 TensorFlow 介绍文章，而是介绍了一些有用的小提示，其中大多都是关于性能表现上的，这些提示揭示了一些常见的陷阱，能帮助你将你的模型和训练表现提升到新的层次。本文将从预处理和你的输入流程开始，然后介绍图构建，之后会谈到调试和性能优化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;预处理和输入流程：&lt;/span&gt;&lt;span&gt;让预处理整洁精简&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你是否疑惑过：为什么你的简单模型需要那么长的训练时间？一定是预处理做得不好！如果你的神经网络输入还需要做一些类似于转换数据这样的繁重的预处理工作，那么你的推理速度就会被大大减缓。我就遇到过这种情况，那是我正在创建了一个所谓的「距离地图（distance maps）」，其使用了一个自定义的 Python 函数而将「Deep Interactive Object Selection」所使用的灰度图像作为附加输入。我的训练速度顶多只达到了大约每秒 2.4 张图像&amp;mdash;&amp;mdash;即使我已经换用了一个远远更加强大的 GTX 1080。然后我注意到了这个瓶颈，经过修复之后，我将训练速度提升到了每秒 50 张图像。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你也遇到了类似的瓶颈，通常你的第一直觉是应该优化一些代码。但减少训练流程的计算时间的一个更有效的方法是将预处理作为一个一次性的操作先完成&amp;mdash;&amp;mdash;生成 TFRecord 文件。这样，你只需要执行一次繁重的预处理工作；有了预处理训练数据之后得到的 TFRecord 文件之后，你需要在训练阶段加载这些文件即可。即使你想引入某种形式的随机性来增强（augment）你的数据，你考虑的也应该是创建不同的变体，而不是用原始数据来填充你的训练流程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;注意你的队列&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一种关注成本高昂的预处理流程的方法是使用 TensorBoard 中的队列图（queue graph）。如果你使用框架 QueueRunners，那么就会自动生成队列图并将总结存储在一个文件中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该图能够显示你的机器是否能够保持队列充盈。如果你在该图中看到了负峰值，那就说明你的系统无法在你的机器想要处理一个批（batch）的时候生成新数据。这种情况的原因有很多，而根据我的经验，其中最常见的是 min_after_dequeue 的值太大。如果你的队列想要在内存中保存大量记录，那么这些记录会很快充满你的容量，这会导致 swapping 并显著减慢你的队列。其它原因还可能是硬件上的问题（比如磁盘速度太慢）或数据太大超过了系统处理能力。不管原因是什么，解决这些问题就能帮助你获得更高的训练速度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;图构建和训练：&lt;/span&gt;&lt;span&gt;完成你的图&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;TensorFlow 独立的图构建和图计算模型在日常编程中十分少见，并且可能让初学者感到困惑。这种独立的构架可以应用于在第一次构建图时代码所出现的漏洞和错误信息，然后在实际评估时再一次运行，这个是和你过去代码只会评估一次这样的直觉相反的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一个问题是与训练回路（training loops）结合的图构建。因为这些回路一般是「标准」的 Python 循环，因此能改变图并向其添加新的操作。在连续不断地评估过程中改变图会造成重大的性能损失，但一开始却很难引起注意。幸好 TensorFlow 有一个简单的解决方案，仅仅在开始训练回路前调用 tf.getDefaultGraph().finalize() 完成你的图即可。这一段语句将会锁定你的图，并且任何想要添加新操作的尝试都将会报错。这正是我们想要达到的效果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;分析你的图的性能&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;TensorFlow 一个少有人了解的功能是性能分析（profiling）。这是一种记录图操作的运行时间和内存消耗的机制。如果你正在寻找系统瓶颈或想要弄清楚模型能不能在不 swapping 到硬件驱动的情况下训练，这种功能会十分有效。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了生成性能分析数据，你需要在开启了 tracing 的情况下在你的图上执行一次运行。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;之后，会有一个 timeline.json 文件会保存到当前文件夹，tracing 数据在 TensorBoard 中也就可用了。现在你很容易就看到每个操作用了多长时间和多少内存。仅仅只需要在 TensorBoard 中打开图视窗（graph view），并在左边选定你最后的运行，然后你就可以在右边看到性能的详细记录。一方面，你能根据这些记录调整你的模型，从而尽可能地利用你机器的运算资源。另一方面，这些记录可以帮助你在训练流程（pipeline）中找到瓶颈。如果你比较喜欢时间轴视窗，可以在谷歌 Chrome 的事件追踪性能分析工具（Trace Event Profiling Tool，https://www.chromium.org/developers/how-tos/trace-event-profiling-tool）中加载 timeline.json 文件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一个很赞的工具就是 tfprof（http://dwz.cn/5lRNeQ），将同样的功能用于内存和执行时间分析，但能提供更加便利的功能。额外的统计需要变换代码。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;注意你的内存&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;就像前部分解释的，分析（profiling）能够让你跟踪特定运行的内存使用情况，不过，注意整个模型的内存消耗更重要些。需要始终确定没有超出机器内存，因为 swapping 肯定会让输入流程放慢，会让你的 GPU 开始坐等新数据。简单地 top，就像前文讲到的 TensorBoard 队列图就应当足够侦测到这样的行为。然后使用前文提过的 tracing，进行细节调查。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;调试：&lt;/span&gt;&lt;span&gt;print 会帮到你&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我主要用 tf.Print 来调试诸如停滞损失（stagnating loss）或奇怪的输出等问题。由于神经网络天性的缘故，观察模型内部张量原始值（raw value）通常并没多大意义。没人能够解释清楚数以百万的浮点数并搞清楚哪儿有问题。不过，专门 print 出数据形状（shape）或均值就能发现重要见解。如果你正在试着实现一些既有模型，你就能比较自己模型值和论文或文章中的模型值，这有利于解决棘手问题或发现论文中的书写错误。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有了 TensorFlow 1.0，我们也有了新的调试工具（http://suo.im/4FtjRy）&amp;mdash;&amp;mdash;这个看起来似乎还蛮有前途的。虽然我还没有用过，不过呢，肯定会在接下来的时间里尝试一下啦。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;设定一个运算执行的超时时间&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你已经实现了你的模型，载入了会话，但却没动静？这经常是有空列队（empty queues）造成的，但是如果你并不清哪个队列才是罪魁祸首，一个很简单的解决办法：在创造会话时，设定运行超时时间&amp;mdash;&amp;mdash;当运行超过你设定的时限，脚本就会崩溃。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;使用栈进行追踪，找出让你头疼的问题，解决问题然后继续训练。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;原文链接：http://www.deeplearningweekly.com/blog/tensorflow-quick-tips&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&amp;copy;本文为机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Tue, 21 Feb 2017 12:26:02 +0800</pubDate>
    </item>
    <item>
      <title>资源 | 如何用Kur训练百度的DeepSpeech模型？</title>
      <link>http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650723520&amp;idx=3&amp;sn=36224f6dca9ab65547629009288dc94c&amp;chksm=871b10beb06c99a80ea60aafa2f785188a78ab4099ff72aa3a7cc741f75fa5f50b8dcc74b096&amp;scene=0#rd</link>
      <description>
&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;选自deep gram&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李泽南、黄小天、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你想要训练一个可用于语音识别的深度神经网络吗？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我也是。两年前我获得了密歇根大学的博士学位成了一名粒子物理学家。我懂一些 C/C++和 Python，并且认识 Noah Shutty。我和 Noah 联合创建了 Deepgram，Noah 是一个精力充沛、学习速度极快的人。我们俩都没有语音识别背景知识，但是懂一点编程和机器学习应用的知识，有大量鼓捣数据处理/系统的经验。我们确实很清楚一件事&amp;mdash;&amp;mdash;如何快速解决问题（一个建造深度地下暗物质探测器时练就的本领）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那时，我们发现自己建造了世界上第一个基于深度学习的语音搜索引擎。为了发展我们需要一个可以理解语音的 DNN。我们成功了，现在你也可以。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;项目地址：http://blog.deepgram.com/how-to-train-baidus-deepspeech-model-with-kur/&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面是基本问题：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/fa9f5ca264baf4fe931e76d413f3386152177495"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;将下面这段音频：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;iframe allowfullscreen="" class="video_iframe" data-vidtype="1" frameborder="0" height="417" src="https://v.qq.com/iframe/preview.html?vid=x0377zo8zm9&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0" width="556"&gt;&lt;/iframe&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;一段普通人说：「I am a human saying human things」的声谱。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;转换成下面这句文本：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/dcf07c37fa1044f3f3879752ea254ab8f93977fd"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;听到了「I am a human saying human things」音频文件的一个 DNN 的预测结果&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该怎么做？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/8c40ec037a71b2fe4252f4c40c8cba72ff282466"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;在易于使用的 Kur 框架中训练一个 DNN&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;我们为什么这样做&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你可以这么想：我们正在一个搜索音频的谷歌，我们需要一个用于语音识别的深度学习模型来完成这一目标。两年前我们开始的时候，百度首次公开了关于 Deepspeech 的论文，这对我们来讲是一件大好事。这将帮助我们搞明白深度学习可以如何用于搜索语音。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;照片中是吴恩达，站在深度语音 RNN 的前面，看起来像是电影《A.I.》里的大坏蛋。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/1842de719babd2e98452fd0054664fa318df15ea"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;在英伟达的 GTC 大会上，来自百度的吴恩达正在做关于 Deepspeech 的演讲&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第一步就是建立一个端对端的深度学习语音识别系统。我们已经做这个超过一年了，现在我们拿出来共享，就像当初百度共享给我们一样（好吧，事实上百度共享给了全世界）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们将给每一个人一个完全有效的 Deepspeech DNN。在 Deepgram 的开源 Python 软件包 Kur 中：http://kur.deepogram.com/&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们不是东拼西凑胡搞一通让它工作的。我们打造了一个运行在 TensorFlow 上的高质量抽象框架，使深度学习变的真正容易起来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;插播一句：为了 Deepgram 的生存我们不得不打造 Kur。现在人工智能领域已是一片红海，除非胸有成竹，否则你不可能快速建立前沿的模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从前，运行 DNN 很麻烦，现在，这变的简单起来。我们的系统从基础做起，一步一步使其简单起来，使你可以描述模型，并且无需做繁杂的工作即可使其运转。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Kur 软件包刚刚发布。它免费且开源，以第一个神话之中的龙（dragon）来命名，Kur 由 Deepgram 人工智能小组全体成员倾力打造，希望你们会喜欢它。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;我们要创造一些人工智能&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;四个轻松的部分：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 安装 Kur&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 运行 Deepspeech 示例&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3. 观察你的计算机如何学习人类语音&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4. 你变成了邪恶的人工智能统领，请放声大笑&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介结束，现在进入实际操作！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一些小的说明：当你看到这起效时，你会认为自己是上帝。学习语音的训练网络是一种变革性的东西。考虑一下这种情况：你在自己的计算机上创造出来的人工智能现在能理解人类说出的话。你的所作所为要负责任。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;额外补充：训练端到端的语音识别深度学习模型需要很多计算。你需要耐心。毕竟你自己也不是在几分钟之内就学会了听人说话。如果你耐心有限，可以使用强大的 GPU&amp;mdash;&amp;mdash;Kur 支持。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;下载并安装 Kur（这很简单）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于安装，如果你已经安装了 Python3.4 或以上版本，你就只需要在你的终端运行$ pip install kur 即可。如果你需要指导，或者一个轻松的操作环境，请访问 kur.deepgram.com 查看完整的安装指导。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;运行 Deepspeech 例子&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;安装 Kur，运行$ kur -v train speech.yml，该文件应该保存在 kur/examples/目录中。如果你想要直接训练而不显示目前的工作状态，可以省略-v。当然，加入-v 可以让你了解 Kur 的工作方式，如果你希望了解更多的细节，请用-vv。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;你的模型会开始训练&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一开始输出的基本是乱码，随后，它的表现将会越来越好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;训练 1 小时后：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;真实转录： these vast buildings what were they&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;DNN 预测：he s ma tol ln wt r hett jzxzjxzjqzjqjxzq&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;训练 6 小时后：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;真实转录：the valkyrie kept off the coast steering to the westward&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;DNN 预测：the bak gerly cap dof the cost stkuarinte the west werd&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;训练 24 小时后：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;真实转录：it was a theatre ready made&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;DNN 预测：it was it theater readi made&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;经过训练，模型输出了真正的英语。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这个示例中，我总共训练了 48 小时。还记得「i am a human saying human things」文件吗？让我们看看人工智能对它的预测。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen="" class="video_iframe" data-vidtype="1" frameborder="0" height="417" src="https://v.qq.com/iframe/preview.html?vid=c0377wq80w7&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0" width="556"&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;训练 48 小时后：&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;True transcript: i am a human saying human things&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;DNN prediction:i am a human saying human things&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;仅仅诞生 48 个小时，它就可以和人一样听懂别人说的话了！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;看看这个表格吧：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/9441848be5261d7453f98a02d23ecaff5e6b9688"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;在 Kur speech.yml 例子中训练和验证数据的 loss 和 batch 的函数&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;就这样，你训练出了目前最先进的语音识别模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;很棒不是吗？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为什么会如此简单？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是因为我们为这项任务深度优化了整个模型，这就是 Kur 带来的提升。另一些优势则来自于 Kur 的可描述性。在这里，你可以直接表达自己想要的东西，它就会实现。阅读 Deepspeech Kurfile，你就会明白这是什么意思。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/d17e65951a94f51e299b0f9cade510d797c3e124"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;事例中 Deepspeech 的超参数&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些是构造 DNN 所需的超参数，有一个一维 CNN 对 FFT 输出的时间片进行操作。然后有一个 3 层的 RNN 组，每个有 1000 个节点。词汇的尺度取决于我们的选择（在本例中是 a 到 z，外加空格和撇号&amp;mdash;&amp;mdash;总共有 28 种字符）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;超参数在 Kurfile 的模型部分（speech.yml）中被抓取。CNN 层就是这样构建的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/d85e0095a5ab8490e476124ffb636b84b0402f09"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;CNN 层的规格&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这里，整流线性单元（ReLU）激活层上的几个明显的超参数直击单个 CNN 层。这些超参数使用 Jinja2（http://jinja.pocoo.org/docs/2.9/）模板引擎填充。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;RNN 层的堆栈是由一个 for 循环中构建的，因为 depth 超参数而有三个层。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/58bdef8c78c814ab5faa239d0a378a1b309a5145"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;RNN堆栈规格&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中，批规范化层用来保持权重分布的稳定，提高训练速度。RNN sequence 超参数只意味着你想要在输出文本与测时按顺序输出（语音片段时间顺序）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Quick Summary：CNN 层采集 FFT 输入，然后连接到 RNN，最终形成一个完整的、可预测 28 个字符的层。这就是 Deepspeech。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;工作方式概述&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当训练语音 DNN 时，你通常会将音频分为每个约 20 毫秒的小块，使用像快速傅里叶变换（FFT）这样的方法将这些分块按顺序输入 DNN 里，并生成对当前块的预测。这个过程不断持续，直到结束这个序列，处理完整个文件（其中的所有预测都被保存）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这就是 Deepspeech 在 Kur 中运行的方式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Kur 需要使用 wav 音频文件，它会抓取文件的频谱图（时间 FFT），并将其同 CNN 层和三个 RNN 层的堆叠一同插入 CNN 层里。输出拉丁语字符的概率预测，从而形成单词。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在模型训练时，将会有验证步骤为你提供随机音频文件的即时预测。你能看到系统每一步的预测。你可以观看当前预测输出的文本，了解神经网络是如何被训练的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一开始，它会先学习空格，随后它会了解元音和辅音的比例关系，从而开始学会一些简单词的表述（如 the、it、a、good），开始具有自己的词汇量。这是一个令人着迷的过程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/aea2839c4683d2b648d7c81f3d7175718b13b186"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;输入语音片段，预测即将出现的字母，从左到右以时间顺序排列。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关链接：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;a data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650723269&amp;amp;idx=3&amp;amp;sn=0fd9eb3e5d3856401739e1354bf6fd68&amp;amp;chksm=871b17bbb06c9eadba1f78353f5d4a33cc4746750a3d8dec7980ca96d2a34d7e5ea66d160a07&amp;amp;scene=21#wechat_redirect" href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650723269&amp;amp;idx=3&amp;amp;sn=0fd9eb3e5d3856401739e1354bf6fd68&amp;amp;chksm=871b17bbb06c9eadba1f78353f5d4a33cc4746750a3d8dec7980ca96d2a34d7e5ea66d160a07&amp;amp;scene=21#wechat_redirect" target="_blank"&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;The most cited deep learning papers&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;：https://github.com/terryum/awesome-deep-learning-papers&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Kur：Descriptive Deep Learninghttps://github.com/deepgram/kur&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;KurHub：http://www.kurhub.com/&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;原文链接：http://blog.deepgram.com/how-to-train-baidus-deepspeech-model-with-kur/&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&amp;copy;本文为机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Tue, 21 Feb 2017 12:26:02 +0800</pubDate>
    </item>
    <item>
      <title>专栏 | 卷积神经网络简介</title>
      <link>http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650723520&amp;idx=4&amp;sn=8ee14dd052766ca3e0afa60dcbb65b2d&amp;chksm=871b10beb06c99a81ef547319637a177142d33a40da5a85024fc6a3b623d60d3a7ac22e3efc3&amp;scene=0#rd</link>
      <description>
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;转自知乎&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;strong&gt;作者：张觉非 阿里巴巴 友盟+&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心经作者授权转载&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;链接：https://zhuanlan.zhihu.com/p/25249694&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;一、卷积&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们在 2 维上说话。有两个&lt;/span&gt;&lt;img src="http://img04.iwgc.cn/mpimg/cf56dc453b92ceaf1ae6d5227cd87fcdbcf0c32b"/&gt;&lt;span&gt;的函数 f(x,y) 和 g(x,y)。f 和 g 的卷积就是一个新的 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/cf56dc453b92ceaf1ae6d5227cd87fcdbcf0c32b"/&gt;&lt;span&gt;的函数。通过下式得到：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/ca6dcdf7edaee06e5e4139711407d7532d20403f"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这式子的含义是：遍览从负无穷到正无穷的全部 s 和 t 值，把 g 在 (x-s,y-t) 位置上的值乘上 f 在 (s,t) 位置上的值之后「加和」（积分意义上的加和）到一起，就是 c 在 (x,y) 上的值。说白了卷积就是一种「加权求和」。以 (x,y) 为中心，把 g 距离中心 (-s,-t) 位置上的值乘上 f 在 (s,t) 的值，最后加到一起。把卷积公式写成离散形式就更清楚了：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/8a85dae5f02fdfb809de7ae6b062e5c8be335410"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果 G 表示一幅 100 x 100 大小的灰度图像，G(x,y) 取值 [0,255] 区间内的整数，是图像在 (x,y) 的灰度值。范围外的位置上的 G 值全取 0。令 F 在 s 和 t 取 {-1,0,1} 的时候有值，其他位置全是 0。F 可以看作是一个 3 x 3 的网格。如下图：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/387da2c636f298ee51a335d53d7dc58319d4a7b7"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;G 每个小格子里的值就是图像在 (x,y) 的灰度值。F 每个小格子里的值就是 F 在 (s,t) 的值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/4376533b5e5d70acbcc556c4f6bfc8517dcc45c4"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 2&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如上图所示，将 F 的中心 (0,0) 对准 G 的 (5,6)。把 F 和 G 对应的 9 个位置上各自的函数值相乘，再将 9 个乘积加在一起，就得到了卷积值 C(5,6)。对 G 的每一个位置求 C 值，就得到了一幅新的图像。其中有两个问题：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如果 F 的所有值之和不等于 1.0，则 C 值有可能不落在 [0,255] 区间内，那就不是一个合法的图像灰度值。所以如果需要让结果是一幅图像，就得将 F 归一化&amp;mdash;&amp;mdash;令它的所有位置之和等于 1.0 ；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;对于 G 边缘上的点，有可能它的周围位置超出了图像边缘。此时可以把图像边缘之外的值当做 0。或者只计算其周围都不超边缘的点的 C。这样计算出来的图像就比原图像小一些。在上例中是小了一圈，如果 F 覆盖范围更大，那么小的圈数更多。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;上述操作其实就是对数字图像进行离散卷积操作，又叫滤波。F 称作卷积核或滤波器。不同的滤波器起不同的作用。想象一下，如果 F 的大小是 3 x 3，每个格子里的值都是 1/9。那么滤波就相当于对原图像每一个点计算它周围 3 x 3 范围内 9 个图像点的灰度平均值。这应该是一种模糊。看看效果：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/2d42eca75096693ee3e713bc4165c8fdc29f411b"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 3&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;左图是 lena 灰度原图。中图用 3 x 3 值都为 1/9 的滤波器去滤，得到一个轻微模糊的图像。模糊程度不高是因为滤波器覆盖范围小。右图选取了 9 x 9 值为 1/81 的滤波器，模糊效果就较明显了。滤波器还有许多其他用处。例如下面这个滤波器：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/33745fffea53568f259e7345a777482d9b7ec50a"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尝试用它来滤 lena 图。注意该滤波器没有归一化（和不是 1.0），故滤出来的值可能不在 [0,255] 之内。通过减去最小值、除以最大／最小值之差、再乘以 255 并取整，把结果值归一到 [0,255] 之内，使之成为一幅灰度图像。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/922804cd2c653b0f6fd474ccff4478180b1d7b9b"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 4&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该滤波器把图像的边缘检测出来了。它就是 Sobel 算子。图像模糊、边缘检测等等都是人们设计出来的、有专门用途的滤波器。如果搞一个 9 x 9 的随机滤波器，会是什么效果呢？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/1ede2bd2d89f07c08d48da51f3b2dd605615b1a2"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 5&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如上图，效果也类似于模糊。因为把一个像素点的值用它周围 9 x 9 范围的值随机加权求和，相当于「捣浆糊」。但可以看出模糊得并不润滑。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这时我们不禁要想，如果不是由人来设计一个滤波器，而是从一个随机滤波器开始，根据某种目标、用某种方法去逐渐调整它，直到它接近我们想要的样子，可行么？这就是卷积神经网络（Convolutional Neural Network, CNN）的思想了。可调整的滤波器是 CNN 的「卷积」那部分；如何调整滤波器则是 CNN 的「神经网络」那部分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;二、神经网络&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人工神经网络（Neural Network, NN）作为一个计算模型，其历史甚至要早于计算机。W.S. McCulloch 和 W. Pitts 在四十年代就提出了人工神经元模型。但是单个人工神经元甚至无法计算异或。人工智能领域的巨擘马文. 明斯基认为这个计算模型是没有前途的。在那时人们已经认识到将多个人工神经元连接成网络就能克服无法计算异或的问题，但是当时没有找到多层人工神经网络的训练方法，以至于人工神经网络模型被压抑多年。直到人们找到了多层人工神经网络的训练方法，人工神经网络才迎来了辉煌。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人工神经元就是用一个数学模型简单模拟人的神经细胞。人的神经细胞有多个树突和一个伸长的轴突。一个神经元的轴突连接到其他神经元的树突，并向其传导神经脉冲。一个神经元会根据来自它的若干树突的信号决定是否从其轴突向其他神经元发出神经脉冲。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/744f93d9b7bd7c15002519f6b68ea918bedf7bdf"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 6&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个人工神经元就是对生物神经元的数学建模。见下图。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/b6a5d806304ae4aed3ac3206304736ab36fb5b20"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 7&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/651291b27daf6b8fbfcddbf42f00af5e5d5324d4"/&gt;&lt;span&gt;是人工神经元的输入。a 是人工神经元的输出。人工神经元将输入&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/651291b27daf6b8fbfcddbf42f00af5e5d5324d4"/&gt;&lt;span&gt;加权求和后再加上偏置值 b，最后再施加一个函数 f，即：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/405269b37885368806c6415306557ef40e01857b"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;上式最后是这个式子的向量形式。P 是输入向量，W 是权值向量，b 是偏置值标量。f 称为「激活函数」。激活函数可以采用多种形式。例如 Sigmoid 函数：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/6808b325cfec8126c5d4f9cbae032f5949805c4d"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是单个人工神经元的定义。人工神经网络就是把这样的人工神经元互联成一个网络：一个神经元的输出作为另一个神经元的输入。神经网络可以有多种多样的拓扑结构。其中最简单的就是「多层全连接前向神经网络」。它的输入连接到网络第一层的每个神经元。前一层的每个神经元的输出连接到下一层每个神经元的输入。最后一层神经元的输出就是整个神经网络的输出。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如下图，是一个三层神经网络。它接受 10 个输入，也就是一个 10 元向量。第一层和第二层各有 12 个神经元。最后一层有 6 个神经元。就是说这个神经网络输出一个 6 元向量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/ae0a471aff9e243798bd2d0396e4389f74dcf388"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 8&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;整个神经网络的计算可以用矩阵式给出。我们给出人工神经网络单层的式子。每层的神经元个数不一样，输入／输出维度也就不一样，计算式中的矩阵和向量的行列数也就不一样，但形式是一致的。假设我们考虑的这一层是第 i 层。它接受 m 个输入，拥有 n 个神经元（n 个输出），那么这一层的计算如下式所示：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/e7ece040f06fefe1f4ca273f43bdafa6ab10ca58"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;上标 i 表示第 i 层。是输出向量，n 元，因为第 i 层有 n 个神经元。第 i 层的输入，即第 i-1 层的输出，是 m 元向量。权值矩阵 W 是 n x m 矩阵：n 个神经元，每个神经元有 m 个权值。W 乘以第 i-1 层输出的 m 向量，得到一个 n 向量，加上 n 元偏置向量 b，再对结果的每一个元素施以激活函数 f，最终得到第 i 层的 n 元输出向量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;若不嫌繁琐，可以将第 i-1 层的输出也展开，最终能写出一个巨大的式子。它就是整个全连接前向神经网络的计算式。可以看出整个神经网络其实就是一个向量到向量的函数。至于它是什么函数，就取决于网络拓扑结构和每一个神经元的权值和偏置值。如果随机给出权值和偏置值，那么这个神经网络是无用的。我们想要的是有用的神经网络。它应该表现出我们想要的行为。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;要达到这个目的，首先准备一个从目标函数采样的包含若干「输入－输出对儿」的集合&amp;mdash;&amp;mdash;训练集。把训练集的输入送给神经网络，得到的输出肯定不是正确的输出。因为一开始这个神经网络的行为是随机的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;把一个训练样本输入给神经网络，计算输出与正确输出的（向量）差的模平方（自己与自己的内积）。再把全部 n 个样本的差的模平方求平均，得到 e ：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/99b910739c246c2d20e77f07fb6dec5968190271"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;e 称为均方误差 mse。e 越小则神经网络的输出与正确输出越接近。神经网络的行为就与想要的行为越接近。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目标是使 e 变小。在这里 e 可以看做是全体权值和偏置值的一个函数。这就成为了一个无约束优化问题。如果能找到一个全局最小点，e 值在可接受的范围内，就可以认为这个神经网络训练好了。它能够很好地拟合目标函数。这里待优化的函数也可以是 mse 外的其他函数，统称 Cost Function，都可以用 e 表示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;经典的神经网络的训练算法是反向传播算法（Back Propagation, BP）。BP 算法属于优化理论中的梯度下降法（Gradient Descend）。将误差 e 作为全部权值和全部偏置值的函数。算法的目的是在自变量空间内找到 e 的全局极小点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先随机初始化全体权值和全体偏置值，之后在自变量空间中沿误差函数 e 在该点的梯度方向的反方向（该方向上方向导数最小，函数值下降最快）前进一个步长。步长称为学习速率（Learning Rate, LR）。如此反复迭代，最终（至少是期望）解运动到误差曲面的全局最小点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下图是用 matlab 训练一个极简单的神经网络。它只有单输入单输出。输入层有两个神经元，输出层有一个神经元。整个网络有 4 个权值加 3 个偏置。图中展示了固定其他权值，只把第一层第一个神经元的权值&lt;/span&gt;&lt;img src="http://img03.iwgc.cn/mpimg/181179dc12e22c6af7189a1dd03a9a531f3c4c96"/&gt;&lt;span&gt;和偏置&lt;/span&gt;&lt;img src="http://img03.iwgc.cn/mpimg/4f1d699233cf032ff30868562d758a25b41c17e8"/&gt;&lt;span&gt;做自变量时候的 e 曲面，以及随着算法迭代，解的运动轨迹。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/0a9d3914d306036f3496d3e652898732728ca1ba"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 9&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最终算法没有收敛到全局最优解（红 +）。但是解已经运动到了一个峡谷的底部。由于底部过于平缓，解「走不动」了。所得解比最优也差不到哪去。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于一个稍复杂的神经网络，e 对权值和偏置值的函数将是一个非常复杂的函数。求梯度需要计算该函数对每一个权值和偏置值的偏导数。所幸的是，每一个权值或偏置值的偏导数公式不会因为这个权值或偏置值距离输出层越远而越复杂。计算过程中有一个中间量&lt;/span&gt;&lt;img src="http://img04.iwgc.cn/mpimg/fa5596c2e0d440f4a8b7c6cccf55668934720464"/&gt;&lt;span&gt;，每层的权值和偏置值的偏导数都可根据后一层的&lt;/span&gt;&lt;img src="http://img05.iwgc.cn/mpimg/93930d127184e32537afc96bd7d221f68861d441"/&gt;&lt;span&gt;以统一形式计算出来。每层再把计算过程中产生的&lt;/span&gt;&lt;img src="http://img04.iwgc.cn/mpimg/fa5596c2e0d440f4a8b7c6cccf55668934720464"/&gt;&lt;span&gt;传递给前一层。这就是「反向传播」名称的由来&amp;mdash;&amp;mdash;&lt;/span&gt;&lt;img src="http://img03.iwgc.cn/mpimg/b238a668613c86a76cf7047972f818ca2b7a56ac"/&gt;&lt;span&gt;沿着反向向前传。这与计算网络输出时，计算结果向后传相反。如此可逐层计算出全部权值和偏置值的偏导数，得到梯度。具体推导这里不给出了，可以参考［1］第八章和［2］第十一章。正是反向传播能够让我们训练神经网络「深处」的参数，这就是「Deep Learning」的含义。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;梯度下降法有很多变体。通过调整学习速率 LR 可以提高收敛速度；通过增加冲量可以避免解陷入局部最优点。还可以每一次不计算全部样本的 e，而是随机取一部分样本，根据它们的 e 更新权值。这样可以减少计算量。梯度下降是基于误差函数的一阶性质。还有其他方法基于二阶性质进行优化，比如共轭法、牛顿法等等。优化作为一门应用数学学科，是机器学习的一个重要理论基础，在理论和实现上均有众多结论和方法。参考［1］。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;三、卷积神经网络&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在把卷积滤波器和神经网络两个思想结合起来。卷积滤波器无非就是一套权值。而神经网络也可以有（除全连接外的）其它拓扑结构。可以构造如下图所示意的神经网络：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/6639ed96fa7c61be9b51c3780aef1fc7d965b63c"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 10&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该神经网络接受&lt;/span&gt;&lt;img src="http://img04.iwgc.cn/mpimg/c6f55b70e561370cbbaf9b0db2a7f7cf5bcd2345"/&gt;&lt;span&gt;个输入，产生&lt;/span&gt;&lt;img src="http://img05.iwgc.cn/mpimg/c6f55b70e561370cbbaf9b0db2a7f7cf5bcd2345"/&gt;&lt;span&gt;个输出。图中左边的平面包含 n x n 个格子，每个格子中是一个 [0,255] 的整数值。它就是输入图像，也是这个神经网络的输入。右边的平面也是 n x n 个格子，每个格子是一个神经元。每个神经元根据二维位置关系连接到输入上它周围 3 x 3 范围内的值。每个连接有一个权值。所有神经元都如此连接（图中只画了一个，出了输入图像边缘的连接就认为连接到常数 0）。右边层的&lt;/span&gt;&lt;img src="http://img04.iwgc.cn/mpimg/c6f55b70e561370cbbaf9b0db2a7f7cf5bcd2345"/&gt;&lt;span&gt;个神经元的输出就是该神经网络的输出。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个网络有两点与全连接神经网络不同。首先它不是全连接的。右层的神经元并非连接上全部输入，而是只连接了一部分。这里的一部分就是输入图像的一个局部区域。我们常听说 CNN 能够把握图像局部特征、AlphaGO 从棋局局部状态提取信息等等，就是这个意思。这样一来权值少了很多，因为连接就少了。权值其实还更少，因为每一个神经元的 9 个权值都是和其他神经元共享的。全部&lt;/span&gt;&lt;img src="http://img03.iwgc.cn/mpimg/c6f55b70e561370cbbaf9b0db2a7f7cf5bcd2345"/&gt;&lt;span&gt;个神经元都用这共同的一组 9 个权值，并且不要偏置值。那么这个神经网络其实一共只有 9 个参数需要调整。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;看了第一节的同学们都看出来了，这个神经网络不就是一个卷积滤波器么？只不过卷积核的参数未定，需要我们去训练&amp;mdash;&amp;mdash;它是一个「可训练滤波器」。这个神经网络就已经是一个拓扑结构特别简单的 CNN 了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;试着用 Sobel 算子滤出来的图片作为目标值去训练这个神经网络。给网络的输入是灰度 lena 图，正确输出是经过 Sobel 算子滤波的 lena 图，见图 4。这唯一的一对输入输出图片就构成了训练集。网络权值随机初始化，训练 2000 轮。如下图：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;img src="http://img03.iwgc.cn/mpimg/4f40bab43b22247d6e134810a4a174df92666f2b"/&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 11&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从左上到右下依次为：初始随机滤波器输出、每个 200 轮训练后的滤波器输出（10 幅）、最后一幅是 Sobel 算子的输出，也就是用作训练的目标图像。可以看到经过最初 200 轮后，神经网络的输出就已经和 Sobel 算子的输出看不出什么差别了。后面那些轮的输出基本一样。输入与输出的均方误差 mse 随着训练轮次的变化如下图：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/7687b79d3e22a560efbbf54480cd66678444fb20"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 12&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1500 轮过后，mse 基本就是 0 了。训练完成后网络的权值是：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/cf7fc93ce30a2c625abf7146f02ac18a7687ab25"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;与 Sobel 算子比较一下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/7103c31c34c73b7e29acbea797649acbc5ee5792"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意训练出来的滤波器负数列在右侧而不是左侧。因为用 Sobel 算子算卷积的时候也许库函数（scipy.ndimage.filters.convolve）是把滤波器「反着扣上去」的。这并不重要。关键是一正列、一负列，中间零值列。正／负列值之比近似 1:2:1。它就是近似的 Sobel 算子。我们以训练神经网络的方式把一个随机滤波器训练成了 Sobel 算子。这就是优化的魔力。AlphaGO 之神奇的核心也在于此&amp;mdash;&amp;mdash;优化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 CNN 中，这样的滤波器层叫做卷积层。一个卷积层可以有多个滤波器，每一个叫做一个 channel，或者叫做一个 feature map。可以给卷积层的输出施加某个激活函数：Sigmoid 、Tanh 等等。激活函数也构成 CNN 的一层&amp;mdash;&amp;mdash;激活层，这样的层没有可训练的参数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;还有一种层叫做 Pooling 层（采样层）。它也没有参数，起到降维的作用。将输入切分成不重叠的一些 n x n 区域。每一个区域就包含&lt;/span&gt;&lt;img src="http://img05.iwgc.cn/mpimg/c6f55b70e561370cbbaf9b0db2a7f7cf5bcd2345"/&gt;&lt;span&gt;个值。从这&lt;/span&gt;&lt;img src="http://img04.iwgc.cn/mpimg/c6f55b70e561370cbbaf9b0db2a7f7cf5bcd2345"/&gt;&lt;span&gt;个值计算出一个值。计算方法可以是求平均、取最大 max 等等。假设 n=2，那么 4 个输入变成一个输出。输出图像就是输入图像的 1/4 大小。若把 2 维的层展平成一维向量，后面可再连接一个全连接前向神经网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;通过把这些组件进行组合就得到了一个 CNN。它直接以原始图像为输入，以最终的回归或分类问题的结论为输出，内部兼有滤波图像处理和函数拟合，所有参数放在一起训练。这就是卷积神经网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;四、举个栗子&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;手写数字识别。数据集中一共有 42000 个 28 x 28 的手写数字灰度图片。十个数字（0～9）的样本数量大致相等。下图展示其中一部分（前 100 个）：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/5929ceb7971f3a6048b89b3ae0c74911d9645dea"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 13&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;将样本集合的 75% 用作训练，剩下的 25% 用作测试。构造一个结构如下图的 CNN ：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/3a810135a7c64256bcf1f65f1cdcb362980d589a"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 14&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该 CNN 共有 8 层（不包括输入层）。它接受 784 元向量作为输入，就是一幅 28 x 28 的灰度图片。这里没有将图片变形成 28 x 28 再输入，因为在 CNN 的第一层放了一个 reshape 层，它将 784 元的输入向量变形成 1 x 28 x 28 的阵列。最开始那个 1 x 表示只有一个 channel，因为这是灰度图像，并没有 RGB 三个 channel。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来放一个卷积层。它包含 32 个滤波器，所以它的输出维度是 32 x 28 x 28。32 个滤波器搞出来 32 幅图像（channel），每个都是 28 x 28 大小。后面又是一个 32 个滤波器的卷积层，输出维度也是 32 x 28 x 28。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;后面接上一个 Pooling 层，降降维。一个 2 x 2 的取平均值 Pooling 层，把输出维度减小了一半：32 x 14 x 14。接着是一个展平层，没有运算也没有参数，只变化一下数据形状：把 32 x 14 x 14 展平成了 6272 元向量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该 6272 元向量送给后面一个三层的全连接神经网络。该网络的神经元个数是 1000 x 1000 x 10。两个隐藏层各有 1000 个神经元，最后的输出层有 10 个神经元，代表 10 个数字。假如第六个输出为 1，其余输出为 0，就表示网络判定这个手写数字为「5」（数字「0」占第一个输出，所以「5」占第六个输出）。数字「5」就编码成了：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/fa985eb424ab13125f7e3b3347666e9ea173a7b6"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;训练集和测试集的数字标签都这么编码（one-hot 编码）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;全连接神经网络这部分的激活函数都采用了 Sigmoid。这出于我一个过时且肤浅的理解：用「弯弯绕」较多的 Sigmoid 给网络贡献非线性。实际上当代深度学习从生物神经的行为中得到启发，设计了其它一些表现优异的激活函数，比如单边线性 Relu。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;误差函数采用均方误差 mse。优化算法采用 rmsprop，这是梯度下降的一个变体。它动态调整学习速率 LR。训练过程持续 10 轮。注意这里 10 轮不是指当前解在解空间只运动 10 步。一轮是指全部 31500 个训练样本都送进网络迭代一次。每次权值更新以 32 个样本为一个 batch 提交给算法。下图展示了随着训练，mse 的下降情况：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/406b187881d6efef7f862ea3e38d3457ea8bf38e"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 15&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下图是分类正确率随着训练的变化情况：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/7be4d48aefb0b5245dbcdb3e1bf5ad8d9af07a89"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图16&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该 CNN 在测试集上的正确率（accuracy）是 96.7%，各数字的准确率 / 召回率 / f1-score 如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/3ec242826c6e5953cd6e0611226dbf7742ee5f58"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该 CNN 对测试集 10 种数字分类的混淆矩阵为：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/ca9050aca19ebd15cc721777b254eeae2399d8a0"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图17&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;训练完成神经网络后，最有趣的是将其内部权值以某种方式展现出来。看着那些神秘的、不明所以的连接强度最后竟产生表观上有意义的行为，不由让我们联想起大脑中的神经元连接竟构成了我们的记忆、人格、情感 ... 引人遐思。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 CNN 上就更适合做这种事情。因为卷积层训练出来的是滤波器。用这些滤波器把输入图像滤一滤，看看 CNN 到底「看到」了什么。下图用第一、二卷积层的 32 个滤波器滤了图 13 第 8 行第 8 列的那个手写数字「6」。32 个 channel 显示如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/14e080aebef72f2a472355808c88d88e57a6437a"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 18&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/6b33454935b8181dd3d7f976fa64c7d576d4bd4f"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 19&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中有些把边缘高亮（输出值较大），有些把「6」的圈圈高亮，等等。这些就是 CNN 第一步滤波后「看到」的信息。再经过后面的各神经层，抽象程度逐层提高，它就这样「认出」了手写数字。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后把代码附上。CNN 使用的是 keras 库。数据集来自 kaggle ：https://www.kaggle.com/c/digit-recognizer/data。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;import pandas as pd&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;from keras.models import Sequential&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;from keras.layers import Dense, Flatten, Reshape, AveragePooling2D, Convolution2D&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;from keras.utils.np_utils import to_categorical&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;from keras.utils.visualize_util import plot&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;from keras.callbacks import Callback&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;from sklearn.model_selection import train_test_split&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;from sklearn.metrics import classification_report, accuracy_score, confusion_matrix&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;class LossHistory(Callback):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;def __init__(self):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;Callback.__init__(self)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;self.losses = []&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;self.accuracies = []&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;def on_train_begin(self, logs=None):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;pass&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;def on_batch_end(self, batch, logs=None):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;self.losses.append(logs.get('loss'))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;self.accuracies.append(logs.get('acc'))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;history = LossHistory()&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;data = pd.read_csv("train.csv")&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;digits = data[data.columns.values[1:]].values&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;labels = data.label.values&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;train_digits, test_digits, train_labels, test_labels = train_test_split(digits, labels)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;train_labels_one_hot = to_categorical(train_labels)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;test_labels_one_hot = to_categorical(test_labels)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;model = Sequential()&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;model.add(Reshape(target_shape=(1, 28, 28), input_shape=(784,)))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;model.add(Convolution2D(nb_filter=32, nb_row=3, nb_col=3, dim_ordering="th", border_mode="same", bias=False, init="uniform"))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;model.add(Convolution2D(nb_filter=32, nb_row=3, nb_col=3, dim_ordering="th", border_mode="same", bias=False, init="uniform"))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;model.add(AveragePooling2D(pool_size=(2, 2), dim_ordering="th"))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;model.add(Flatten())&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;model.add(Dense(output_dim=1000, activation="sigmoid"))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;model.add(Dense(output_dim=1000, activation="sigmoid"))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;model.add(Dense(output_dim=10, activation="sigmoid"))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;with open("digits_model.json", "w") as f:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;f.write(model.to_json())&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;plot(model, to_file="digits_model.png", show_shapes=True)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;model.compile(loss="mse", optimizer="rmsprop", metrics=["accuracy"])&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;model.fit(train_digits, train_labels_one_hot, batch_size=32, nb_epoch=10, callbacks=[history])&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;model.save_weights("digits_model_weights.hdf5")&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;predict_labels = model.predict_classes(test_digits)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;print(classification_report(test_labels, predict_labels))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;print(accuracy_score(test_labels, predict_labels))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;print(confusion_matrix(test_labels, predict_labels))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;五、参考书目&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;［1］《最优化导论》（美）Edwin K. P. Chong（美）Stanislaw H. Zak&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;［2］《神经网络设计》（美）Martin T.Hagan（美）Howard B.Demuth（美）Mark Beale&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&amp;copy;本文为机器之心转载文章，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Tue, 21 Feb 2017 12:26:02 +0800</pubDate>
    </item>
    <item>
      <title>业界 | Bot完全指南：从与机器人平台的区别到知名框架</title>
      <link>http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650723520&amp;idx=5&amp;sn=3b8e06c29a320445dbb766d7b5979339&amp;chksm=871b10beb06c99a8ec5dd3960599db2ea4e49a09b783425c5f2f62425dd197d6e40a9536036f&amp;scene=0#rd</link>
      <description>
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;选自marutitech&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：赵华龙、黄小天&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote style="color: rgb(62, 62, 62); font-size: 16px; white-space: normal; max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;今年聊天机器人备受瞩目。科技巨头诸如 Facebook 和微软已经大规模发布了 Bot 框架，旨在量产聊天机器人。在 Facebook Messenger 上开发了超过 11,000 个聊天机器人，并且有近 23,000 个开发人员注册了 Facebook 机器人引擎&lt;/span&gt;&lt;span&gt;。此外，大量初创公司拥有自属开发框架和功能性产品。较小的交流平台，如 Telegram 和 Slack，也推出了「机器人商店」（「Bot Stores」），并成立基金吸引开发人员。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;什么是 Bot 框架？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简单地解释，Bot 框架用来制造机器人并定义其行为。作为聊天机器人开发者，开发和定向如此之多的交流平台与聊天机器人开发 SDKs 常会感到无所适从。Bot 开发框架是这样一种软件框架，它能对聊天机器人开发过程中的人工内容做抽象化处理。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，尽管很多 Bot 开发框架宣称「代码一旦写好可部署到任何地方」，你还是很可能为你的每一个目标交流平台开发一个单独的聊天机器人。Bot 开发框架包括机器人制造者 SDK（Bot Builder SDK）、机器人连接器（Bot Connector）、开发者入口（Developer Portal）、机器人目录（Bot Directory）以及一个用来测试已开发机器人的模拟器。此外，Bot 框架并不适合初学者用来学习聊天机器人开发。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;机器人框架与机器人平台的差别？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bot 框架（Bot Framework）有时错误地与 Bot 平台（「Bot Platform」）通用。在开发应用程序时，Bot 平台的作用是提供部署和运行应用程序的，Bot 框架的作用是开发和绑定各种组件到应用程序。Bot 平台是在线生态系统，其中聊天机器人可以被部署并与用户进行交互，代表用户执行操作，包括与其他平台交互。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bot 开发框架是一组预定义函数和开发人员用来加快开发的类，一组可以使你更快更好编码的工具。简单来说，初学者或非技术用户可以用 Bot 平台来开发不需要写代码的机器人，而 Bot 开发框架则被开发人员和码农借助编程语言从头开始构建机器人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;例如 Bot 平台 Motion.ai 可使用户无需编码便能快速创建强大的机器人。原因在于 Motion.ai 提供了一个能创建聊天机器人的工具包，使得机器人可与 APIs 相连并部署到任何一个可用的交流平台。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/ab3aec8562b2838068f525787602d85b3f415ccf"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;一些著名的 Bot 框架&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Facebook bot 引擎&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2016 年 4 月，Facebook 实现了基于 Wit.ai 技术的 Facebook Bot 引擎。Wit.ai 在自己的云服务器运行，Bot 引擎是一个包装器，用于在 Facebook Messenger 平台上部署机器人。Facebook 作为社交巨头的力量在于海量用户，因此他们不需要任何其他的 Bot 开发平台，并且聊天机器人将仅限于 Facebook Messenger（其本身即是一个巨大的空间）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Facebook 正在为 Facebook Bot 引擎采用一种新策略。如果开发人员获得框架，Facebook Messenger 用户将享有各种专业聊天机器人&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Facebook Bot 引擎依赖于机器学习。提供 Bot 框架示例对话之后，它可以处理同一问题的很多不同变体。随着开发人员不断完善聊天机器人，它们的潜力也会越来越巨大。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Wit.ai 提供一些选项：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1.它能提取出一些预定义的实体，比如时间、日期等等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2.提取用户的意图。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3. 提取情绪。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4. 它可进行自我定义和提取。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;微软 Bot 框架&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软几乎与 Facebook 同时宣布了其 Bot 框架。尽管微软的哲学和方法有点不同。就像 Facebook 的产品一样，微软的 SDK 可以被看作是 2 个彼此独立的组件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. Bot 连接器，集成框架&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. LUIS.ai，自然语言理解组件&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软 Bot 框架的集成组件适用于 Slack、Facebook Messenger、Telegram、Webchat、GroupMe、SMS、电子邮件和 Skype，令人印象深刻。此外，Azure 上有一个 PaaS 选项，就是用于 Bots。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软 Bot 框架是一个全面的产品，用于构建和部署高质量的聊天机器人供用户享受最喜欢的对话体验。机器人开发人员都面临着同样的问题：机器人需要基本的输入和输出；它们必须具备语言和会话能力；机器人必须具有高性能，响应性和可扩展性；并且它们必须能够向用户提供理想的对话体验。微软 Bot 框架提供了我们构建，连接，管理和发布智能聊天机器人所需要的一切，无论是通过文字/SMS，还是其他平台诸如 Slack、Skype、 Facebook Messenger、Kik 等，聊天机器人都可以和用户自然地交流。微软 Bot 框架由许多组件组成，包括 Bot 创建者 SDK（Bot Builder SDK）、开发人员门户（Developer Portal）和 Bot 目录（Bot Directory）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;API.ai&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;API.ai 是另一个基于 Web 的 bot 开发框架。API.ai 似乎已经发现了让用户通过输入多个话语来定义实体和意图的缺陷，并因此提供了一个巨大的领域集（a huge set of domains）。API.ai 为 bot 开发提供的一些 SDK 和库，包括 Android、iOS、Webkit HTML5、JavaScript、Node.js、Python 等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;API.ai 建立在如下几个概念上：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1.代理器：代理器对应于应用。一旦我们训练并测试一个代理器，我们就可以把它集成到我们的 app 或设备中去。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2.实体：实体表示那些通常专用于某一领域的概念，作为将 NLP（自然语言处理）短语映射到捕获其含义的批准短语的方式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3.意图：意图表示用户说什么和软件需要采取什么动作之间的映射。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4.动作：动作对应于您的应用在用户的输入触发特定的意图时所采取的步骤。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;5.上下文：上下文是表示用户表达的当前上下文的字符串。这对于区分可能是不明确的并且因取决于前面的话而具有不同含义的短语是有用的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;API.ai 能被集成在很多流行的交流平台、物联网和虚拟个人助理平台。它们中的一些包括 Actions on Google、Slack、Facebook Messenger、Skype、Kik、Line、Telegram、Amazon Alexa、Twilio SMS 和 Twitter 等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Aspect CXP 和 Aspect NLU&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Aspect 客户体验平台（CXP）是设计，实施和部署多渠道客户服务应用程序的平台。Aspect NLU 是一个给出人类语言感觉的组件，其采用的方法与 Wit.ai、API.ai 和微软 Bot 框架完全不同，并能为 Facebook Messenger 上的自助服务对话带来人性化的交谈口吻。这使它能够通过自动化以聊天机器人特有的方式进行扩展。Aspect CXP 使得设计、实现和在多种交流渠道（诸如文本、语音、移动网、社交网络）部署聊天机器人变得容易起来。这很适合那些需要复杂聊天机器人、客服应用和企业软件的地方；不太适合对简单机器人、嵌入式应用和物联网应用的需求。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些是市场上可用的、开发人员构建机器人的 Bot 框架。如果你的组织要花费大量的金钱和时间与客户交流，你可以尝试建立一个机器人来处理这种情况。对话用户界面的时代（The era of Conversational User Interfaces）已经到来，成为掌握趋势的先行者之一吧。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;原文链接：&lt;/span&gt;&lt;/em&gt;&lt;em&gt;&lt;span&gt;http://www.marutitech.com/complete-guide-bot-frameworks/?utm_content=buffer9b406&amp;amp;utm_medium=social&amp;amp;utm_source=twitter.com&amp;amp;utm_campaign=buffer&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&amp;copy;本文为机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Tue, 21 Feb 2017 12:26:02 +0800</pubDate>
    </item>
    <item>
      <title>机器学习算法集锦：从贝叶斯到深度学习及各自优缺点</title>
      <link>http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650723438&amp;idx=1&amp;sn=a778051186c0e1fb3cdb4076868fd54a&amp;chksm=871b1010b06c99063ec5599dcecbed5ce3065e7c2f0ab1cc11a8251f2472838302f89cf51d52&amp;scene=0#rd</link>
      <description>
&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;选自static.coggle.it&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;在我们日常生活中所用到的推荐系统、智能图片美化应用和聊天机器人等应用中，各种各样的机器学习和数据处理算法正尽职尽责地发挥着自己的功效。本文筛选并简单介绍了一些最常见算法类别，还为每一个类别列出了一些实际的算法并简单介绍了它们的优缺点。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;https://static.coggle.it/diagram/WHeBqDIrJRk-kDDY&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/f8a2d14b7a8f1265f9d56ffb3218f442d34043b8"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目录&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;正则化算法（Regularization Algorithms）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;集成算法（Ensemble Algorithms）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;决策树算法（Decision Tree Algorithm）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;回归（Regression）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;人工神经网络（Artificial Neural Network）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;深度学习（Deep Learning）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;支持向量机（Support Vector Machine）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;降维算法（Dimensionality Reduction Algorithms）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;聚类算法（Clustering Algorithms）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;基于实例的算法（Instance-based Algorithms）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;贝叶斯算法（Bayesian Algorithms）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;关联规则学习算法（Association Rule Learning Algorithms）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;图模型（Graphical Models）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;正则化算法（Regularization Algorithms）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/d84f497c3c1356ed625047b3e3494489c4981a67"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;它是另一种方法（通常是回归方法）的拓展，这种方法会基于模型复杂性对其进行惩罚，它喜欢相对简单能够更好的泛化的模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;岭回归（Ridge Regression）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;最小绝对收缩与选择算子（LASSO）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;GLASSO&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;弹性网络（Elastic Net）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;最小角回归（Least-Angle Regression）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;其惩罚会减少过拟合&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;总会有解决方法&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;惩罚会造成欠拟合&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;很难校准&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;集成算法（Ensemble algorithms）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/426f07943f43204e4ee4cf05c5744a854553d2e8"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;集成方法是由多个较弱的模型集成模型组，其中的模型可以单独进行训练，并且它们的预测能以某种方式结合起来去做出一个总体预测。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该算法主要的问题是要找出哪些较弱的模型可以结合起来，以及结合的方法。这是一个非常强大的技术集，因此广受欢迎。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Boosting&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Bootstrapped Aggregation（Bagging）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;AdaBoost&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;层叠泛化（Stacked Generalization）（blending）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;梯度推进机（Gradient Boosting Machines，GBM）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;梯度提升回归树（Gradient Boosted Regression Trees，GBRT）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;随机森林（Random Forest）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;当先最先进的预测几乎都使用了算法集成。它比使用单个模型预测出来的结果要精确的多&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;需要大量的维护工作&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;决策树算法（Decision Tree Algorithm）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/dd5f116079cde79755d2116486f4c83965cf13cf"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;决策树学习使用一个决策树作为一个预测模型，它将对一个 item（表征在分支上）观察所得映射成关于该 item 的目标值的结论（表征在叶子中）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;树模型中的目标是可变的，可以采一组有限值，被称为分类树；在这些树结构中，叶子表示类标签，分支表示表征这些类标签的连接的特征。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;分类和回归树（Classification and Regression Tree，CART）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Iterative Dichotomiser 3（ID3）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;C4.5 和 C5.0（一种强大方法的两个不同版本）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;容易解释&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;非参数型&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;趋向过拟合&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;可能或陷于局部最小值中&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;没有在线学习&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;回归（Regression）算法&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/05255719683d9ebb24e7e05bdeb456a667014355"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;回归是用于估计两种变量之间关系的统计过程。当用于分析因变量和一个 多个自变量之间的关系时，该算法能提供很多建模和分析多个变量的技巧。具体一点说，回归分析可以帮助我们理解当任意一个自变量变化，另一个自变量不变时，因变量变化的典型值。最常见的是，回归分析能在给定自变量的条件下估计出因变量的条件期望。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;回归算法是统计学中的主要算法，它已被纳入统计机器学习。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;普通最小二乘回归（Ordinary Least Squares Regression，OLSR）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;线性回归（Linear Regression）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;逻辑回归（Logistic Regression）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;逐步回归（Stepwise Regression）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;多元自适应回归样条（Multivariate Adaptive Regression Splines，MARS）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;本地散点平滑估计（Locally Estimated Scatterplot Smoothing，LOESS）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;直接、快速&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;知名度高&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;要求严格的假设&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;需要处理异常值&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;人工神经网络&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/d6e5c216ef2c1995991a2407b982ba6ae64cfd50"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人工神经网络是受生物神经网络启发而构建的算法模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;它是一种模式匹配，常被用于回归和分类问题，但拥有庞大的子域，由数百种算法和各类问题的变体组成。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;感知器&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;反向传播&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Hopfield 网络&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;径向基函数网络（Radial Basis Function Network，RBFN）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在语音、语义、视觉、各类游戏（如围棋）的任务中表现极好。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;算法可以快速调整，适应新的问题。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;需要大量数据进行训练&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;训练要求很高的硬件配置&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;模型处于「黑箱状态」，难以理解内部机制&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;元参数（Metaparameter）与网络拓扑选择困难。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;深度学习（Deep Learning）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/ecd4966b8790e2ac1c640b92fbbe34a63d4fa1d4"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度学习是人工神经网络的最新分支，它受益于当代硬件的快速发展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;众多研究者目前的方向主要集中于构建更大、更复杂的神经网络，目前有许多方法正在聚焦半监督学习问题，其中用于训练的大数据集只包含很少的标记。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;深玻耳兹曼机（Deep Boltzmann Machine，DBM）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Deep Belief Networks（DBN）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;卷积神经网络（CNN）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Stacked Auto-Encoders&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点/缺点&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：见神经网络&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;支持向量机（Support Vector Machines）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/88831bc8ad0b0624c1d98295e25f9155887a4cef"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;给定一组训练事例，其中每个事例都属于两个类别中的一个，支持向量机（SVM）训练算法可以在被输入新的事例后将其分类到两个类别中的一个，使自身成为非概率二进制线性分类器。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;SVM 模型将训练事例表示为空间中的点，它们被映射到一幅图中，由一条明确的、尽可能宽的间隔分开以区分两个类别。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随后，新的示例会被映射到同一空间中，并基于它们落在间隔的哪一侧来预测它属于的类别。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在非线性可分问题上表现优秀&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;非常难以训练&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;很难解释&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;降维算法（Dimensionality Reduction Algorithms）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/f3b8236410e4d8d8aa94b51bc68f6845f38eb293"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;和集簇方法类似，降维追求并利用数据的内在结构，目的在于使用较少的信息总结或描述数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这一算法可用于可视化高维数据或简化接下来可用于监督学习中的数据。许多这样的方法可针对分类和回归的使用进行调整。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;主成分分析（Principal Component Analysis (PCA)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;主成分回归（Principal Component Regression (PCR)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;偏最小二乘回归（Partial Least Squares Regression (PLSR)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Sammon 映射（Sammon Mapping）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;多维尺度变换（Multidimensional Scaling (MDS)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;投影寻踪（Projection Pursuit）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;线性判别分析（Linear Discriminant Analysis (LDA)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;混合判别分析（Mixture Discriminant Analysis (MDA)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;二次判别分析（Quadratic Discriminant Analysis (QDA)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;灵活判别分析（Flexible Discriminant Analysis (FDA)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;可处理大规模数据集&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;无需在数据上进行假设&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;难以搞定非线性数据&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;难以理解结果的意义&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;聚类算法（Clustering Algorithms）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/e47270e40957f8c0b48f0f0badb73f2ea9917638"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;聚类算法是指对一组目标进行分类，属于同一组（亦即一个类，cluster）的目标被划分在一组中，与其他组目标相比，同一组目标更加彼此相似（在某种意义上）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;K-均值（k-Means）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;k-Medians 算法&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Expectation Maximi 封层 ation (EM)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;最大期望算法（EM）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;分层集群（Hierarchical Clstering）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;让数据变得有意义&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;结果难以解读，针对不寻常的数据组，结果可能无用。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;基于实例的算法（Instance-based Algorithms）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/25639eb05f7f7fb1dabf2d6522db35fa646e44a7"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基于实例的算法（有时也称为基于记忆的学习）是这样学 习算法，不是明确归纳，而是将新的问题例子与训练过程中见过的例子进行对比，这些见过的例子就在存储器中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;之所以叫基于实例的算法是因为它直接从训练实例中建构出假设。这意味这，假设的复杂度能随着数据的增长而变化：最糟的情况是，假设是一个训练项目列表，分类一个单独新实例计算复杂度为 O（n）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;K 最近邻（k-Nearest Neighbor (kNN)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;学习向量量化（Learning Vector Quantization (LVQ)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;自组织映射（Self-Organizing Map (SOM)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;局部加权学习（Locally Weighted Learning (LWL)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;算法简单、结果易于解读&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;内存使用非常高&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;计算成本高&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;不可能用于高维特征空间&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;贝叶斯算法（Bayesian Algorithms）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/b3d91267f38f52801b65c70ce708b370df32a497"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;贝叶斯方法是指明确应用了贝叶斯定理来解决如分类和回归等问题的方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;朴素贝叶斯（Naive Bayes）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;高斯朴素贝叶斯（Gaussian Naive Bayes）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;多项式朴素贝叶斯（Multinomial Naive Bayes）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;平均一致依赖估计器（Averaged One-Dependence Estimators (AODE)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;贝叶斯信念网络（Bayesian Belief Network (BBN)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;贝叶斯网络（Bayesian Network (BN)）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;快速、易于训练、给出了它们所需的资源能带来良好的表现&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如果输入变量是相关的，则会出现问题&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;关联规则学习算法（Association Rule Learning Algorithms）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/6d720e446249498b469a04fec4cda2d7527c0f17"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;关联规则学习方法能够提取出对数据中的变量之间的关系的最佳解释。比如说一家超市的销售数据中存在规则 {洋葱，土豆}=&amp;gt; {汉堡}，那说明当一位客户同时购买了洋葱和土豆的时候，他很有可能还会购买汉堡肉。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Apriori 算法（Apriori algorithm）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Eclat 算法（Eclat algorithm）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;FP-growth&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图模型（Graphical Models）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/c86b7f4292c3f340016b6d1df3fb45c245b55abb"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;图模型或概率图模型（PGM/probabilistic graphical model）是一种概率模型，一个图（graph）可以通过其表示随机变量之间的条件依赖结构（conditional dependence structure）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;贝叶斯网络（Bayesian network）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;马尔可夫随机域（Markov random field）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;链图（Chain Graphs）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;祖先图（Ancestral graph）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;模型清晰，能被直观地理解&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;确定其依赖的拓扑很困难，有时候也很模糊&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&amp;copy;本文为机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Mon, 20 Feb 2017 12:00:06 +0800</pubDate>
    </item>
    <item>
      <title>独家 | NOR-NET技术详解：AI技术落地移动端新时代即将崛起</title>
      <link>http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650723438&amp;idx=2&amp;sn=959d8ea85deaaf412d36c72f689f3844&amp;chksm=871b1010b06c9906507db3c86efd5fe197b2ca71788e6de12931d85eb044ca021ddbba7cbe75&amp;scene=0#rd</link>
      <description>
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：高静宜&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;技术指导：杨浩进&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote style="color: rgb(62, 62, 62); font-size: 16px; white-space: normal; max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/blockquote&gt;&lt;blockquote style="color: rgb(62, 62, 62); font-size: 16px; white-space: normal; max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;在这个时代，人类生活离不开智能设备。无论是随身携带的手机，还是腕上的智能手表，都与人工智能息息相关。同时，人类的生活方式也在不断化繁为简，现代人出行只需带上一只手机，便可以有效解决社交沟通、交易支付、出行交通等一系列问题。随着社会便携化、智能化的发展需求，在移动端实现人工智能也已经成为大势所趋。然而，人工智能的实现可能不仅需要在硬件配备方面进行大量投入，还需要大型数据中心的支撑。那么，如何在移动端建立可以遍布人类生活的 AI 技术力量呢？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2017 年 2 月 2 日，位于美国西雅图的 AI 创业公司 xnor.ai 宣布获得来自麦德罗纳风险投资集团（Madrona Venture Group）和艾伦人工智能研究所（Allen Institute for Artificial Intelligence）的 260 万美元的种子融资。这个平台致力于开发不依赖于数据中心或互联网连接的，可以直接有效地在移动端或嵌入式设备（例如手机、无人驾驶车辆等）上运行的深度学习模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Xnor.ai 平台的这项针对移动端部署深度学习研发技术，无论是从响应性、速度还是可靠性上来说，都可以达到前所未有的水平。而且，由于数据全部存储于移动端设备上，个人隐私可以得到高水平保障。例如，就物体检测的性能而言，业界完全可以把这项技术应用于手机上，实现物体的实时检测。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;事实上，xnor.ai 团队就曾将 XNOR-Net 部署在价值 5 美元的 Raspberry Pi Zero 上，通过连接一个摄像头实现了实时视频分析，这段网站上的 demo 展示出的实时检测分析效果十分引人注意，给人很强的视觉冲击力。如果在类似于 Raspberry Pi Zero 这样的移动设备上都能进行对枪支和刀具的实时监测并及时报警，那么人们完全可以利用这项技术针对性地开发出更多 AI 安防产品，拓展 AI 安防领域，更不用说这项技术在其他领域中潜在的巨大商业价值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen="" class="video_iframe" data-vidtype="1" frameborder="0" height="417" src="https://v.qq.com/iframe/preview.html?vid=v03750s4kee&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0" width="556"&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;众所周知，深度学习模型大量的矩阵运算使 GPU 加速成了必不可少的硬件支持，这使得深度网络难以在运算资源有限的移动设备上面实现。那么，xnor.ai 又是如何将深度网络部署于移动端的呢？在这里，不得不提到二值神经网络这个概念。&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随着对神经网络研究深度不断推进，学界研究人员发现传统的神经网络对计算成本和内存容量要求较高，而二值化则可以有效地改善这些问题。二值化网络不仅有助于减小模型的存储大小，节省存储容量，而且能加快运算速度，降低计算成本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2015 年 11 月，来自于 Yoshua Bengio 教授带领的加拿大蒙特利尔大学实验室团队的 Matthie Courbariaux 发表了关于二值神经网络 BinaryConnect 的相关论文（BinaryConnect：Training Deep Learing Neural Networks with binary weight during propagations），引起了广泛关注，开启了崭新的二值化网络时代。论文中提出了 BinaryConnect 算法的关键在于仅在前向传播和反向传播中对权重进行二值化 1 或-1，而在参数更新过程保持权重的全精度（即仍为浮点数），这样的做法可以省去接近三分之二的矩阵运算，训练时间和内存空间都得到了大幅度优化，同时，BinaryConnect 在 MNISIST,CIFAR-10 和 SVHN 图像分类数据集上的实验效果可以达到当时世界领先水平。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在此基础上，Matthieu 和 Itay 随后联合发表的论文（Binarized Neural Networks：Training Networks with Weights and Activations Constrained to +1 or -1）提出了更完善的网络模型&amp;mdash;&amp;mdash;BinaryNet，将权值和隐藏层激活值同时进行二值化，并利用 xnorcount 和 popcount 运算操作代替网络中传统的算术运算。这个算法在常用图像数据集上的模型二值化实验也比较成功，可以减少约 60% 的计算复杂度，甚至可以在保证分类准确率的情况下，减少七倍的 GPU 运行时间。同时，实验团队也公开了在 CUDA、Theano 以及 Torch 上的代码，不过很可惜的是，这个算法并没有在如 ImageNet 的大数据集上证明精度是否可以维持。在尽可能减小模型准确率损失的情况下，BinaryNet 的出现通过缩减模型大小，简化运算难度对算法进行加速。这使得深度网络部署于移动端的前景初见曙光。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;值得一提的是，随着神经网络技术的蓬勃发展，许多教授、学者投身工业界，像 Matthie Courbariaux 现已投身 Google，并负责在 TensorFlow 框架中实现对深度模型的量化任务。不同于 Matthie 在论文中的二值化概念 (即不丢失模型准确率，只压缩模型大小), 实际投入应用的量化更适合被理解为离散化。一般来说，在训练神经网络的时候，要对权重做一些微小的调整，而这些微小的调整需要浮点精度才能正常工作，而低精度计算会被网络当做一种噪声。深度网络的一个奇妙之处就在于它可以很好地应对输入噪音，因为网络可以把低精度计算当做一种噪声，这使得量化后的网络在具备较少信息的数值格式下，仍能产生精确的结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量化网络有两个动机，一是缩小尺寸，这是通过存储每层的最大和最小值，然后把每一个浮点值压缩成 8-bit 整数来表示。二是降低资源需求，这需要整个计算都用 8-bit 输入和输出来实现。量化压缩是存在风险的，目前的版本似乎还不是很成熟，Github 上面有很多开发人员认为利用这种方法量化后的模型效率较低。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;2016 年 3 月，Mohammad Rastegari 等人在论文 (XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks) 中首次提出了 XNOR-Net 的概念。这篇论文旨在利用二值化操作寻找到最优的简化网络，并分别介绍了两种有效的网络：Binary-Weight-Networks 和 XNOR-Networks。Binary-Weight-Networks 是对 CNN 中所有的权重做近似二值化，可以节省 32 倍的存储空间。而且，由于权重被二值化，卷积过程只剩加减算法，不再包括乘法运算，可以提高约两倍的运算速度，这促使 CNN 可以在不牺牲准确率的情况下在小存储设备上使用，包括便携式设备。Binary-Weight-Networks 区别于 BinaryNet 的地方在于它进行二值化的方法和网络结构。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;XNOR-Networks 算法则是对 CNN 中所有的权重和输入同时做近似二值化，如果卷积运算中的所有操作数都是二进制的，那么两个二进制向量的点乘就可以等同于同或运算和位运算。作者在这篇文章里主要有两个贡献：一是引入比例因子，大幅度提升精度；二是对典型常规的 CNN 构成进行改动。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;XNOR-Net 算法的基本思路如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Step1：定义一个 L 层的 CNN 结构，使用三个元素 I,W,* 来表示，I 表示卷积输入，W 表示滤波器，*表示卷积算子；利用比例因子&amp;alpha;帮助二值化滤波器去近似全精度滤波器权重，利用比例因子&amp;beta;帮助二值化的输入去近似全精度输入值。这有点类似于批规范化（Batch Normalization）中的仿射参数（Affine Parameters），但不同的是，这里不是通过为网络学习获得的，而是通过计算平均值得到的。在探索计算比例因子&amp;beta;的时候，要对每一次卷积产生的子张量都计算一个&amp;beta;，这一步骤会产生很多冗余的计算。为了降低计算量，作者把输入的所有 channels 计算一个绝对值的平均值矩阵 A，然后通过一个二维的滤波器 k 和 A 的卷积生成 K，那么 K 就包含了针对输入 I 在所有子张量上的比例因子。通过这样一系列数学推导，输入 I 与权重 W 的二值化卷积可以被近似为：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/d2f1f47fa8a6ccd924b0ffd856bc04889f804b52"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;具体过程如下图：&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/59182724bf853619a206eb68a9f7290221dfb5c2"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;图 1：XNOR-Net 近似二值化卷积过程（Mohammad Rastegari et al.）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Step2：一个典型的 CNN 具有卷积、批规范化、激活、池化这样的四层结构，其中，池化层可以对输入运用任何种类的池化方式。但在二值化的输入（-1,1）进入到池化过程时，会产生大量的信息丢失。例如，对二值化输入进行 max-pooling 时，会导致大部分输入只剩+1，使得消息消减，精度降低。为了解决这个问题，作者改善了网络结构，改变这几层的顺序，首先实行批规范化，保证 0 均值，然后进行二值化激活，使数据都是+1 和-1，再做二值化卷积，此时由于比例因子的作用输出的不再是-1 和+1，这会相对减少信息丢失。在这里，作者建议在二值化卷积后加一个非二值化激活步骤（如 ReLU），这可以帮助训练比较复杂的网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;具体过程如下图：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/2be183502f93ea4bc4a403fb273ca69b3647c578"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;图 2：典型的 CNN 与 XNOR-Net 结构（Mohammad Rastegari et al.）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;XNOR-Net 团队在自主搭建的轻型神经网络框架 DarkNet 中实现了在 CPU 上 58 倍速度的提升，这意味着 XNOR-Net 可以在小内存设备上完成实时任务。事实上，在 2016 计算机视觉大会上，XNOR-Net 团队把 yolo object detection 算法的 xnor 版本在 iphone 上面做到了实时探测就成了一大亮点。XNOR-Net 的出现弥补了 BinaryNet 文章的缺失，首次让二值神经网络在 ImageNet 上面完了实验。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有趣的是，XNOR-Net 团队曾在 Github 上公开代码，甚至包括让其名声大噪的 Yolo network，但在公开发表的论文中却并没有公开 C/C++源代码而只是做了 Torch 的版本公开，曾经发布在 Github 的版本也昙花一下，不久便被撤回了。现在看来，这些举动都无疑都是为之后 XNOR-Net 的商业化做准备。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Xnor.ai 的一系列举措不禁让人联想到去年大火的 Prisma APP 离线版的上架以及新一代智能手表操作系统 Andriod Wear 2.0 的发布。这些都是在移动端实现深度学习的经典产品实例，而这些技术成果与研发产品都验证了 Facebook 对未来十年的重点研发战略领域的远见卓识&amp;mdash;&amp;mdash;连接世界、普及网络；人工智能；虚拟现实和增强现实。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除此之外，去年年底 Facebook 发布的 Caffe2Go 也是可以嵌入、部署于移动设备的深度学习框架，具有规模小、训练速度快、对计算机性能要求低等性能。其精华在于 Facebook 硬件优化工程师和算法专家（以贾扬清为代表）做了大量的针对性能上的优化，才使 Caffe2Go 可以顺利部署于手机上。类似的还有 Google 发布于 Github 上的 TensorFlow android camera demo，在这里，Google 将较为复杂的 inception v3 图片分类网络模型进行量化压缩减小 4 倍左右，然后部署于安卓手机上，也可以完成手机端的物体识别、行人检测等任务。虽然这些优化似乎更多是工程意义上的，而不是算法本身具备创新性，但是这些互联网巨头公司的行动无疑会带给我们一些启示：将深度学习框架部署于移动端是未来的一个主流发展趋势。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前，深度学习框架的开发及优化发展迅速，种类也不少，不过，可以支持移动端的框架还是相对少数的，到底哪种框架是部署于移动端的最佳选择，这还有待于考证。相较于 TensorFlow 这种比较复杂的主流深度学习框架，MXNET 作为一种十分灵活、对内存要求较少的深度开源框架也被业界看好，而且它本身就提供了对多种移动端的支持。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;去年 6 月，国内 Face++推出了关于 DoReFa-Net 算法的文章 ( DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients)。DoReLa-Net 对比例因子的设计更为简单，这里并没有针对卷积层输出的每一个过滤映射计算比例因子，而是对卷积层的整体输出计算一个均值常量作为比例因子。这样的做法可以简化反向运算，因为在他们反向计算时也要实现量化。DoReLa-Net 的贡献在于提供了不同量化因子的实验结果，即 2,4,8,16,32 bit 的权重、激活函数量化，同时在后向反馈中也实现了梯度的量化。对于梯度二值化问题，XNOR-Net 中只提出了理论的计算方法，未实现 4~16 bit 的量化实验，也没有在反向梯度计算中使用二值运算。在 SVHN 和 ImageNet 上的实验都可以说明 DoReFa-Net 在有效地应用于 CPU,FPGA,ASIC 和 GPU 上，具有很大的潜力和可行性。但是 DoReLa-Net 并没有使用 xnor 和 popcount 运算，因此实验结果只具备精度参考价值，没有任何加速的效果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人工智能现如今已渗透在医疗、安防、车辆交通、教育等方方面面的领域，将 AI 技术移植到普罗大众的便携式生活中成为必然，未来更多致力于实现神经网络嵌入于移动端的产品将会应运而生，例如，车辆上的导航设施、游戏的手机客户端以及各种各样的手机 APP。这一方面是源于，在移动端实现人工智能十分方便、便携，它可以随时随地满足人们的各种需求；另一方面，在离线的情况下，数据无需上传下传，降低了信息传递时间，同时还能增强用户隐私空间。人们有意愿、有需求直接把 AI 掌控在自己手中，从而达到进一步改善生活品质，甚至于改变生活方式的目的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;同时，从工业发展的角度，人工智能移动端的推行也势在必行。工业机器人、家居机器人等工业化产品也需要依托于具有可移植功能的嵌入式芯片。在硬件条件的发展限制了深度学习运行速度的时候，软件算法技术改进将会不断革新，在这个革新过程中，终端设备智能化已经初见曙光。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，在此过程中，还有一些有待于攻克的瓶颈和存在的问题。比如，如何改善二值神经网络模型在大规模数据库上的表现存在不足的问题；如何对现有的二值化网络算法进行精度和速度上的优化；而进行二值化的网络模型相比于全精度的网络，存在的信息损失这个缺陷，是否可以被三值网络来弥补；还有一个在工业领域十分重要的问题，如何将理论算法高效地落地，甚至是否可以开发出具备落地性的网络模型或是框架。这些问题都将是未来研发人员的关注焦点和研究方向。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;到底 xnor.ai 是否会在人工智能领域掀起一场腥风血雨，我们让时间来解答这个问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&amp;copy;本文为机器之心原创，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Mon, 20 Feb 2017 12:00:06 +0800</pubDate>
    </item>
    <item>
      <title>专栏 | 千人千面智能淘宝店铺背后的算法研究登陆人工智能顶级会议AAAI 2017</title>
      <link>http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650723438&amp;idx=3&amp;sn=ab422ac3df946beef23471ef4b1b4bb9&amp;chksm=871b1010b06c9906e665687db985a38f7439dfb6212b167442cd176c25a148ee30845ab9b6dc&amp;scene=0#rd</link>
      <description>
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;机器之心专栏&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;作者：周畅（钟煌）、刘效飞（翼升）等&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote style="color: rgb(62, 62, 62); font-size: 16px; white-space: normal; max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;千人千面模块上线，每一家淘宝店铺从此都可能有一个隐形智能导购，推荐算法再升级。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;电商时代，消费者对推荐系统已经不再陌生。「蓦然回首」，你发现喜欢的商品就在首页显眼处。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如今，不仅仅是电商网站首页会给你贴心推荐。你逛进一家淘宝商家的店铺，也很有可能享受到推荐算法的服务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是阿里商家事业部推出的智能店铺「千人千面」模块。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;阿里商家事业部相关负责人介绍，单纯通过算法做出的商品推荐，未必符合商家利益。常有商家抱怨，自家想卖的商品得不到推荐，营销被算法牵着鼻子走。而「千人千面」，就是先让商家给出他们想要推送的商品集，算法再从指定候选集中为进入某家商铺的消费者做个性化推荐。如此一来，算法可以为商家的营销服务，为商家既定的 营销计划「锦上添花」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不过要做到这一点并不简单。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;业界推荐系统往往由 Matching 和 Ranking 两部分组成。Matching 部分会根据全网用户的浏览、加购、收藏等行为数据，在一个庞大的商品池中找出较小的候选集。Ranking 则是利用综合用户 Profile，偏好，以及商品特征等信息训练得出的一个打分排序模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是，阿里电商目前拥有百万级别的活跃店铺，单个用户在单个特定店铺内的行为记录非常匮乏，很难按传统方法有效进行 matching。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对此，阿里商家事业部提出一种高可扩展性的 Graph Embedding（图嵌入）方法，并创新性地将它应用到商品的 embedding 中。它能够以非常小的存储空间来计算任意两个商品的相似度。就算你此前从未踏足这家店铺，算法也能根据你此前在别家的浏览记录，从店铺里挑出你可能喜欢的商品，摆在你面前。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;模块投入使用后，商家的商品点击率提升了 30%，成交量提升 60%。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从学术层面来说，该 Graph Embedding 方法可学习到能够描述图中节点间高阶的、非对称相似度的低维 Embedding 向量，并且可以在理论上解释这种基于机器学习的方法和基于预定义的传统节点间相似度的关系，相关论文已被人工智能领域的顶级会议 AAAI'2017 接收。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来是对该论文的中文讲解，完整论文PDF可点击阅读原文下载：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;工业界的推荐系统通常由 Matching 和 Ranking 两个部分组成，Matching 部分会根据全网用户的浏览、加购、收藏等行为数据，利用协同过滤一类的算法（例如基于商品的 ItemCF）在一个庞大的商品池中找出一个足够小的候选集，以缩小后续算法需要评估的范围。Ranking 则是利用综合用户 Profile，偏好，以及商品特征等额外信息训练得出的一个打分排序模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的推荐场景，即对于店铺私域内的千人千面推荐模块来说，其与公网推荐的重要区别在于，推荐的目标仅限于很小的一部分商家指定的商品集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;传统的 Matching 这部分所遇到的难题在于，阿里电商目前拥有百万级别的活跃店铺，这使得单个用户在单个店铺内的行为记录非常稀疏。而在很多情况下，用户在近期首次进入某商铺主页时，由于缺乏店内的行为信息（如足迹商品），很难有效利用店内 ItemCF 来进行推荐。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;ItemCF 的核心问题之一在于如何有效衡量与计算 item 与 item 之间的相似度\parencite{recsurvey05}。对于全网推荐的应用场景，由于商品数量太大，通常我们会离线计算出每个 item 前 k 个相似的 item list\parencite{itemcftopk}，来用于在线打分的推荐方案。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，如果我们直接用全网 topk item 相似度的数据，对于每个商品来说，与他相似的商品数目其实可能很多，但由于 topk 的限制（通常小于 200），只有极少数店铺的商品才能够被召回，即基于全网 top-k 的商品相似度在同店推荐中的召回能力比较有限。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当然，我们可以使用同样的方法，对于每个店铺，仅计算店铺内部的 i2i 数据，来完成推荐。这样做的缺陷在于，完全无法覆盖用户没有店内足迹的情况。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，为了提高相似商品的召回，以覆盖用户没有店内足迹的情况，我们使用了图嵌入算法 APP 来基于用户浏览记录来做商品嵌入&amp;mdash;&amp;mdash;试图将商品嵌入到一个低维空间中，同时保存一些商品之间的结构特征，即商品相似度。这样就可以用稳定、较小的代价在线算出任意两个商品之间的相似度了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「旺铺智能版智能模块」是一款面向中小商家的、商家可运营的个性化商品装修模块。在商家侧算法提供面向场景的选品，同时允许商家对算法商品池进行调整，或者完全手动建立商品池；在消费者端，个性化算法基于商家设置的商品池对访客进行实时投放。产品设计上一定程度上满足了商家确定性需求，在此基础上通过个性化算法提升成交转化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们研究 Graph Embedding 的初衷是为旺铺模块千人千面场景提供覆盖率高的 Match 支持。因为用户在店铺内部的行为稀疏，传统的基于 I2I 的 match 覆盖率较低。而通过 Embedding 可以计算出任意两个商品之间的 Match 分数，极大改善覆盖率问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们提出一种高可扩展性的 Graph Embedding 方法，该方法可学习到能够可描述图中节点间高阶的、非对称相似度的低维 Embedding 向量。同时我们提供理论上的解释，来阐述这种基于机器学习的方法和基于预定义的传统节点 I2I 相似度的关系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1.背景介绍 &amp;amp; 相关工作&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;图是一种抽象程度高、表达能力强的数据结构，它通过对节点和边的定义来描述实体和实体之间的关联关系。常用的图有社交关系网络，通信网络，商品网络，知识图谱等等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而如何衡量图中节点之间的相似度，对于朋友推荐、商品推荐、以及常见的分类聚类问题来说都是一个很重要的前置步骤。Graph Embedding 可以理解成是一种降维技术，它可以将图中的节点映射到一个低维空间里，我们只需要通过计算低维向量之间的关系，就可以得到原来节点之间的关联关系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管传统 Embedding 技术被研究了很久，但他们的复杂度往往都在 N^2 级别以上，难以适应大规模数据。最近的一系列可扩展性较强的 Graph Embedding 工作主要是从 DeepWalk【6】开始，后面有 Line【7】，Node2vec【2】等等。DeepWalk 在原图中做了一些路径采样，然后将路径当作一个句子，路径中的点当作单词，之后就采用 word2vec 中提出的 Skip-Gram with Negative-Sampling【5】方式进行训练，得到每一个节点的 embedding 向量。Line 只针对边进行采样。Node2vec 可以调节参数来进行 BFS 或者 DFS 的抽样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而图中的路径采样在概率上有着非常严重的非对称性，之前的这些方法并没有注意到这件事，也没有从理论上来思考为什么这么干不太科学。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;例如在有向图（图 1）中，对于 A 来说，可能并不关心 C，而对于 C 来说，A 很可能是他的兴趣点。即使在无向图中（图 2），也有同样的现象。这样的节点非对称性关系是由于节点周围的图结构不同造成的。而从 C 出发的路径 C-&amp;gt;B-&amp;gt;A 和从 A 出发的路径 A-&amp;gt;B-&amp;gt;C 有着完全不相同的概率（0.5，0.08）。因此我们不能认为 C-&amp;gt;B-&amp;gt;A 这条路径的产生会带来一个（A-&amp;gt;C）的正样本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/3cdc7e66c9bb3b6e00c609b03c494039e9d42039"/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;图 1 有向图中的非对称性&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/ccf75790e80f02f5e920e4d3fe90167feb451542"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 2 无向图中的非对称性&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.我们的工作&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的工作所做的改进其实非常简单，首先为了有能力表达非对称性相似度，我们为每个节点引入了两种 Embedding 向量，分别是 Source 向量和 Target 向量，如图一所示。我们将对于 A 来说 B 的相似度记为 sim(A，B)，并使用 Source(A) 与 Target(B) 的点积来表示，图一中我们可以从 Embedding 中算出 sim(A，C)&amp;lt;sim(C, A)。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/8543b60289b32da836ff7c5bd90a845e274c58a8"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 3 节点的两种 Embedding 身份&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其次我们遵循了一种标准的、用来估计 Rooted PageRank【3】的蒙特卡洛随机游走的方法【1】【8】来进行正例的采样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;节点 u 对于节点 v 的 Rooted PageRank（PPR）值代表了从 v 出发落在 u 点的概率。我们认为以这种方式生成图中节点对的正样例是更加自然、合理、有说法的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这类游走方法都是基于常见的 Random Ｗalk with Restart，即从一个点出发以（1-alpha）的概率选择邻居进行跳转，另外 alpha 的概率跳转回自己。那么现有的几种方法稍有一些区别：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;例如 Monte Carlo End Point 只保留首次跳转之前的节点，Monte Carlo Full Path 保留路径上的所有节点，将路径的后缀也当作有效的采样【1】。因为这两条路径对于起始点来说可以看作是相互独立的。在最新的工作中也有对前缀路径进行重用的【8】，就不再此展开。值得注意的是，后两种的采样效率相对于 1 来说要更高，尽管这三种方法都在各自的文章中被证明是正确且有 Bound 的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们遵循这类游走方法，企图给图中的节点对创造一些正样本。对于每一个被标记为正例的样本（A, B）我们会根据目标函数更新 A 的 source 向量和 B 的 target 向量。并且随机采样其他的节点作为负样本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们定义给定节点 u，可以预测到节点 v 的概率&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/55c33a01ee6ee09b22e37af7e7a5e0bb3eb9ec83"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;利用 Skip-Gram with Negative-Sampling【5】，近似等价于优化&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/26fa13de0f09c4b5c311bc1a04d0e8811852a2df"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;K 是负采样数，P_D（n）在图中可用均匀分布替代。则总的目标函数如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/e378f320304814d6558d3387e10db5f4bcf6b7e6"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面我们来解释一个有趣的现象，我们非对称的点积最终会是以学习出两点之间的 PPR 的对数为目标。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/5028183426e29bd34e3d6d7f747e1191dddb1cfa"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这里，类似于 Levy【4】的证明，当维数充分大时，可看作互相独立的变量。于是另下式为 0：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/42f0a21deacb30585cc3077f5ae658e5ab0890e0"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;得到：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/f8424293e9519e301909b4dc90690ca4f5135c8c"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于|V|, k 均为常数，我们可以看出 x 只跟 Rooted PageRank 的模拟值 Sim_u(v) 呈对数关系。通过以上证明，论证了该方法可以保持非对称的、高阶相似度的说法，因为 Rooted PageRank 就是一种非对称的、高阶的相似度度量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3.小数据集上的实验&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Link Prediction Task（AUC）：Embedding 方法相对于传统 Pre-defined i2i 指标来说，在 AUC 上很占便宜。因为传统指标大多基于 2 跳以内的关系，包括阿里内部使用的 Swing。这样就有很多正例的结果是 0&amp;mdash;&amp;mdash;完全无法和负例分开，AUC 不高。可以看出我们的方法（APP）在比现有的方法要好一些。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/c130bb6ad2340683fd28780b85172a2cece52f15"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下表是为了体现非对称性的优势，而在负样本中加大了单向边的比例，即 A-&amp;gt;B 有边，B-&amp;gt;A 无边。可以看出我们与之前的方法在 LinkPrediction 任务上有显著提升。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/0d86de282bc9b60bacd5f1559e7cee1a7c7b3968"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Node Recommendation：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/70dc493b9927e1734ecf1c6018399c02689141f4"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;值得注意的是，在寻找 topk 的这个问题当中，我们发现之前的 Embedding 方法似乎并没有传统指标靠谱。但我们的方法可以比较好的反应 Topk 的相似关系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;4.在模块千人千面中的实践&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了缓解用户在店铺内部行为的稀疏性，我们将用户 Session 中的全网点商品击序列转化成一个全网商品点击转换图。之后应用我们的 Graph Embedding 方法得到商品向量。该向量可以用来计算用户点击行为所产生的商品之间的相似度。下图是我们与传统 topk i2i 方法在真实场景中的点击率比较。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/6bc4a1121f0b54245f975d4538186304983ba15f"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的这项工作目前还只是作为 Match 打分的基础算法，我们正在尝试进一步融合一些外部信息，如商品文本属性、类目信息等，提高长尾商品的结构化 Embedding 质量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;1.Fogaras, D.; R&amp;acute;acz, B.; Csalog&amp;acute;any, K.; and Sarl&amp;acute;os, T. 2005. Towards scaling fully personalized pagerank: Algorithms, lower bounds, and experiments. Internet Mathematics 2(3):333&amp;ndash;358.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;2.Grover, A., and Leskovec, J. 2016. node2vec: Scalable feature learning for networks. In International Conference on Knowledge Discovery and Data Mining. ACM.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;3.Haveliwala, T. H. 2002. Topic-sensitive pagerank. In Proceedings of the 11th international conference on World Wide Web, 517&amp;ndash;526. ACM.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;4.Levy, O., and Goldberg, Y. 2014. Neural word embedding as implicit matrix factorization. In Advances in neural information processing systems, 2177&amp;ndash;2185.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;5.Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, 3111&amp;ndash;3119.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;6.Perozzi, B.; Al-Rfou, R.; and Skiena, S. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, 701&amp;ndash;710. ACM.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;7.Tang, J.; Qu, M.;Wang, M.; Zhang, M.; Yan, J.; and Mei, Q. 2015. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, 1067&amp;ndash;1077. ACM.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;8.Liu, Q.; Li, Z.; Lui, J.; and Cheng, J. 2016. Powerwalk: Scalable personalized pagerank via random walks with vertex-centric decomposition. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, 195&amp;ndash;204. ACM.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&amp;copy;本文为机器之心专栏，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号或作者获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Mon, 20 Feb 2017 12:00:06 +0800</pubDate>
    </item>
    <item>
      <title>教程 | 从头开始：用Python实现决策树算法</title>
      <link>http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650723438&amp;idx=4&amp;sn=cf3902a9933afe08ac3c38452044cddd&amp;chksm=871b1010b06c99062809133f3ad6279bccd64768a761a2aa6495367048069bc13788929b276a&amp;scene=0#rd</link>
      <description>
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;选自Machine Learning Mastery&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：&amp;nbsp;Jason Brownlee&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：沈泽江、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;决策树算法是一个强大的预测方法，它非常流行。因为它们的模型能够让新手轻而易举地理解得和专家一样好，所以它们比较流行。同时，最终生成的决策树能够解释做出特定预测的确切原因，这使它们在实际运用中倍受亲睐。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;同时，决策树算法也为更高级的集成模型（如 bagging、随机森林及 gradient boosting）提供了基础。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这篇教程中，你将会从零开始，学习如何用 Python 实现《Classification And Regression Tree algorithm》中所说的内容。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在学完该教程之后，你将会知道：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如何计算并评价数据集中地候选分割点（Candidate Split Point）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如何在决策树结构中排分配这些分割点&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如何在实际问题中应用这些分类和回归算法&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;一、概要&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本节简要介绍了关于分类及回归树（Classification and Regression Trees）算法的一些内容，并给出了将在本教程中使用的钞票数据集（Banknote Dataset）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1.1 分类及回归树&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;分类及回归树（CART）是由 Leo Breiman 提出的一个术语，用来描述一种能被用于分类或者回归预测模型问题的回归树算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们将在本教程中主要讨论 CART 在分类问题上的应用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;二叉树（Binary Tree）是 CART 模型的代表之一。这里所说的二叉树，与数据结构和算法里面所说的二叉树别无二致，没有什么特别之处（每个节点可以有 0、1 或 2 个子节点）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;每个节点代表在节点处有一个输入变量被传入，并根据某些变量被分类（我们假定该变量是数值型的）。树的叶节点（又叫做终端节点，Terminal Node）由输出变量构成，它被用于进行预测。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在树被创建完成之后，每个新的数据样本都将按照每个节点的分割条件，沿着该树从顶部往下，直到输出一个最终决策。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;创建一个二元分类树实际上是一个分割输入空间的过程。递归二元分类（Recursive Binary Splitting）是一个被用于分割空间的贪心算法。这实际上是一个数值过程：当一系列的输入值被排列好后，它将尝试一系列的分割点，测试它们分类完后成本函数（Cost Function）的值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有最优成本函数（通常是最小的成本函数，因为我们往往希望该值最小）的分割点将会被选择。根据贪心法（greedy approach）原则，所有的输入变量和所有可能的分割点都将被测试，并会基于它们成本函数的表现被评估。（译者注：下面简述对回归问题和分类问题常用的成本函数。）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;回归问题&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对落在分割点确定区域内所有的样本取误差平方和（Sum Squared Error）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;分类问题&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：一般采用基尼成本函数（Gini Cost Function），它能够表明被分割之后每个节点的纯净度（Node Purity）如何。其中，节点纯净度是一种表明每个节点分类后训练数据混杂程度的指标。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;分割将一直进行，直到每个节点（分类后）都只含有最小数量的训练样本或者树的深度达到了最大值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1.2 Banknote 数据集&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Banknote 数据集，需要我们根据对纸币照片某些性质的分析，来预测该钞票的真伪。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集中含有 1372 个样本，每个样本由 5 个数值型变量构成。这是一个二元分类问题。如下列举 5 个变量的含义及数据性质：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 图像经小波变换后的方差（Variance）（连续值）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 图像经小波变换后的偏度（Skewness）（连续值）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3. 图像经小波变换后的峰度（Kurtosis）（连续值）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4. 图像的熵（Entropy）（连续值）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;5. 钞票所属类别（整数，离散值）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如下是数据集前五行数据的样本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;code&gt;3.6216,8.6661,-2.8073,-0.44699,0&lt;/code&gt;&lt;br&gt;&lt;code&gt;4.5459,8.1674,-2.4586,-1.4621,0&lt;/code&gt;&lt;br&gt;&lt;code&gt;3.866,-2.6383,1.9242,0.10645,0&lt;/code&gt;&lt;br&gt;&lt;code&gt;3.4566,9.5228,-4.0112,-3.5944,0&lt;/code&gt;&lt;br&gt;&lt;code&gt;0.32924,-4.4552,4.5718,-0.9888,0&lt;/code&gt;&lt;br&gt;&lt;code&gt;4.3684,9.6718,-3.9606,-3.1625,0&lt;/code&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;使用零规则算法（Zero Rule Algorithm）来预测最常出现类别的情况（译者注：也就是找到最常出现的一类样本，然后预测所有的样本都是这个类别），对该问的基准准确大概是 50%。&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你可以在这里下载并了解更多关于这个数据集的内容：UCI Machine Learning Repository。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请下载该数据集，放到你当前的工作目录，并重命名该文件为 data_banknote_authentication.csv。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;二、教程&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本教程分为五大部分：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 对基尼系数（Gini Index）的介绍&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2.（如何）创建分割点&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3.（如何）生成树模型&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4.（如何）利用模型进行预测&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;5. 对钞票数据集的案例研究&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些步骤能帮你打好基础，让你能够从零实现 CART 算法，并能将它应用到你子集的预测模型问题中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.1 基尼系数&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基尼系数是一种评估数据集分割点优劣的成本函数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;数据集的分割点是关于输入中某个属性的分割。对数据集中某个样本而言，分割点会根据某阈值对该样本对应属性的值进行分类。他能根据训练集中出现的模式将数据分为两类。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基尼系数通过计算分割点创建的两个类别中数据类别的混杂程度，来表现分割点的好坏。一个完美的分割点对应的基尼系数为 0（译者注：即在一类中不会出现另一类的数据，每个类都是「纯」的），而最差的分割点的基尼系数则为 1.0（对于二分问题，每一类中出现另一类数据的比例都为 50%，也就是数据完全没能被根据类别不同区分开）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面我们通过一个具体的例子来说明如何计算基尼系数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们有两组数据，每组有两行。第一组数据中所有行都属于类别 0（Class 0），第二组数据中所有的行都属于类别 1（Class 1）。这是一个完美的分割点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先我们要按照下式计算每组数据中各类别数据的比例：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;br&gt;&lt;tbody style="border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent;"&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;proportion = count(class_value) / count(rows)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那么，对本例而言，相应的比例为：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;group_1_class_0 = 2 / 2 = 1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;group_1_class_1 = 0 / 2 = 0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;group_2_class_0 = 0 / 2 = 0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;group_2_class_1 = 2 / 2 = 1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基尼系数按照如下公式计算：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;gini_index = sum(proportion * (1.0 - proportion))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;将本例中所有组、所有类数据的比例带入到上述公式：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;gini_index = (group_1_class_0 * (1.0 - group_1_class_0)) +&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;(group_1_class_1 * (1.0 - group_1_class_1)) +&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;(group_2_class_0 * (1.0 - group_2_class_0)) +&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;(group_2_class_1 * (1.0 - group_2_class_1))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;化简，得：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;gini_index = 0 + 0 + 0 + 0 = 0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如下是一个叫做 gini_index() 的函数，它能够计算给定数据的基尼系数（组、类别都以列表（list）的形式给出）。其中有些算法鲁棒性检测，能够避免对空组除以 0 的情况。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;# Calculate the Gini index for a split dataset&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;def gini_index(groups, class_values):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;gini = 0.0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;for class_value in class_values:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;for group in groups:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;size = len(group)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;if size == 0:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;continue&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;proportion = [row[-1] for row in group].count(class_value) / float(size)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;gini += (proportion * (1.0 - proportion))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;return gini&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以根据上例来测试该函数的运行情况，也可以测试最差分割点的情况。完整的代码如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;# Calculate the Gini index for a split dataset&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;def gini_index(groups, class_values):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;gini = 0.0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;for class_value in class_values:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;for group in groups:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;size = len(group)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;if size == 0:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;continue&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;proportion = [row[-1] for row in group].count(class_value) / float(size)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;gini += (proportion * (1.0 - proportion))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;return gini&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;# test Gini values&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;print(gini_index([[[1, 1], [1, 0]], [[1, 1], [1, 0]]], [0, 1]))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;print(gini_index([[[1, 0], [1, 0]], [[1, 1], [1, 1]]], [0, 1]))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;运行该代码，将会打印两个基尼系数，其中第一个对应的是最差的情况为 1.0，第二个对应的是最好的情况为 0.0。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;1.0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;0.0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.2 创建分割点&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个分割点由数据集中的一个属性和一个阈值构成。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以将其总结为对给定的属性确定一个分割数据的阈值。这是一种行之有效的分类数据的方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;创建分割点包括三个步骤，其中第一步已在计算基尼系数的部分讨论过。余下两部分分别为：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 分割数据集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 评价所有（可行的）分割点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们具体看一下每个步骤。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.2.1 分割数据集&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;分割数据集意味着我们给定数据集某属性（或其位于属性列表中的下表）及相应阈值的情况下，将数据集分为两个部分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一旦数据被分为两部分，我们就可以使用基尼系数来评估该分割的成本函数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;分割数据集需要对每行数据进行迭代，根据每个数据点相应属性的值与阈值的大小情况将该数据点放到相应的部分（对应树结构中的左叉与右叉）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如下是一个名为 test_split() 的函数，它能实现上述功能：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;# Split a dataset based on an attribute and an attribute value&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;def test_split(index, value, dataset):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;left, right = list(), list()&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;for row in dataset:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;if row[index] &amp;lt; value:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;left.append(row)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;else:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;right.append(row)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;return left, right&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;代码还是很简单的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意，在代码中，属性值大于或等于阈值的数据点被分类到了右组中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.2.2 评价所有分割点&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在基尼函数 gini_index() 和分类函数 test_split() 的帮助下，我们可以开始进行评估分割点的流程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对给定的数据集，对每一个属性，我们都要检查所有的可能的阈值使之作为候选分割点。然后，我们将根据这些分割点的成本（cost）对其进行评估，最终挑选出最优的分割点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当最优分割点被找到之后，我们就能用它作为我们决策树中的一个节点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而这也就是所谓的穷举型贪心算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在该例中，我们将使用一个词典来代表决策树中的一个节点，它能够按照变量名储存数据。当选择了最优分割点并使用它作为树的新节点时，我们存下对应属性的下标、对应分割值及根据分割值分割后的两部分数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;分割后地每一组数据都是一个更小规模地数据集（可以继续进行分割操作），它实际上就是原始数据集中地数据按照分割点被分到了左叉或右叉的数据集。你可以想象我们可以进一步将每一组数据再分割，不断循环直到建构出整个决策树。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如下是一个名为 get_split() 的函数，它能实现上述的步骤。你会发现，它遍历了每一个属性（除了类别值）以及属性对应的每一个值，在每次迭代中它都会分割数据并评估该分割点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当所有的检查完成后，最优的分割点将被记录并返回。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;# Select the best split point for a dataset&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;def get_split(dataset):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;class_values = list(set(row[-1] for row in dataset))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;b_index, b_value, b_score, b_groups = 999, 999, 999, None&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;for index in range(len(dataset[0])-1):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;for row in dataset:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;groups = test_split(index, row[index], dataset)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;gini = gini_index(groups, class_values)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;if gini &amp;lt; b_score:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;b_index, b_value, b_score, b_groups = index, row[index], gini, groups&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;return {'index':b_index, 'value':b_value, 'groups':b_groups}&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们能在一个小型合成的数据集上来测试这个函数以及整个数据集分割的过程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X1 X2 Y&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;2.771244718 1.784783929 0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;1.728571309 1.169761413 0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;3.678319846 2.81281357 0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;3.961043357 2.61995032 0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;2.999208922 2.209014212 0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;7.497545867 3.162953546 1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;9.00220326 3.339047188 1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;7.444542326 0.476683375 1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;10.12493903 3.234550982 1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;6.642287351 3.319983761 1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;同时，我们可以使用不同颜色标记不同的类，将该数据集绘制出来。由图可知，我们可以从 X1 轴（即图中的 X 轴）上挑出一个值来分割该数据集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/485be4b866949c75b669170183a2c3bdcd7318b7"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;范例所有的代码整合如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;# Split a dataset based on an attribute and an attribute value&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;def test_split(index, value, dataset):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;left, right = list(), list()&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;for row in dataset:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;if row[index] &amp;lt; value:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;left.append(row)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;else:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;right.append(row)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;return left, right&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;# Calculate the Gini index for a split dataset&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;def gini_index(groups, class_values):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;gini = 0.0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;for class_value in class_values:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;for group in groups:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;size = len(group)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;if size == 0:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;continue&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;proportion = [row[-1] for row in group].count(class_value) / float(size)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;gini += (proportion * (1.0 - proportion))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;return gini&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;# Select the best split point for a dataset&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;def get_split(dataset):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;class_values = list(set(row[-1] for row in dataset))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;b_index, b_value, b_score, b_groups = 999, 999, 999, None&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;for index in range(len(dataset[0])-1):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;for row in dataset:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;groups = test_split(index, row[index], dataset)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;gini = gini_index(groups, class_values)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;print('X%d &amp;lt; %.3f Gini=%.3f' % ((index+1), row[index], gini))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;if gini &amp;lt; b_score:&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;b_index, b_value, b_score, b_groups = index, row[index], gini, groups&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;return {'index':b_index, 'value':b_value, 'groups':b_groups}&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;dataset = [[2.771244718,1.784783929,0],&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;[1.728571309,1.169761413,0],&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;[3.678319846,2.81281357,0],&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;[3.961043357,2.61995032,0],&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;[2.999208922,2.209014212,0],&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;[7.497545867,3.162953546,1],&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;[9.00220326,3.339047188,1],&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;[7.444542326,0.476683375,1],&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;[10.12493903,3.234550982,1],&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;[6.642287351,3.319983761,1]]&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;split = get_split(dataset)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;print('Split: [X%d &amp;lt; %.3f]' % ((split['index']+1), split['value']))&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;优化后的 get_split() 函数能够输出每个分割点及其对应的基尼系数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;运行如上的代码后，它将 print 所有的基尼系数及其选中的最优分割点。在此范例中，它选中了 X1&amp;lt;6.642 作为最终完美分割点（它对应的基尼系数为 0）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X1 &amp;lt; 2.771 Gini=0.494&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X1 &amp;lt; 1.729 Gini=0.500&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X1 &amp;lt; 3.678 Gini=0.408&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X1 &amp;lt; 3.961 Gini=0.278&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X1 &amp;lt; 2.999 Gini=0.469&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X1 &amp;lt; 7.498 Gini=0.408&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X1 &amp;lt; 9.002 Gini=0.469&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X1 &amp;lt; 7.445 Gini=0.278&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X1 &amp;lt; 10.125 Gini=0.494&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X1 &amp;lt; 6.642 Gini=0.000&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X2 &amp;lt; 1.785 Gini=1.000&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X2 &amp;lt; 1.170 Gini=0.494&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X2 &amp;lt; 2.813 Gini=0.640&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X2 &amp;lt; 2.620 Gini=0.819&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X2 &amp;lt; 2.209 Gini=0.934&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X2 &amp;lt; 3.163 Gini=0.278&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X2 &amp;lt; 3.339 Gini=0.494&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X2 &amp;lt; 0.477 Gini=0.500&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X2 &amp;lt; 3.235 Gini=0.408&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;X2 &amp;lt; 3.320 Gini=0.469&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Split: [X1 &amp;lt; 6.642]&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;既然我们现在已经能够找出数据集中最优的分割点，那我们现在就来看看我们能如何应用它来建立一个决策树。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.3 生成树模型&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;创建树的根节点（root node）是比较方便的，可以调用 get_split() 函数并传入整个数据集即可达到此目的。但向树中增加更多的节点则比较有趣。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;建立树结构主要分为三个步骤：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 创建终端节点&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 递归地分割&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3. 建构整棵树&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.3.1 创建终端节点&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们需要决定何时停止树的「增长」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以用两个条件进行控制：树的深度和每个节点分割后的数据点个数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最大树深度：这代表了树中从根结点算起节点数目的上限。一旦树中的节点树达到了这一上界，则算法将会停止分割数据、增加新的节点。更神的树会更为复杂，也更有可能过拟合训练集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最小节点记录数：这是某节点分割数据后分个部分数据个数的最小值。一旦达到或低于该最小值，则算法将会停止分割数据、增加新的节点。将数据集分为只有很少数据点的两个部分的分割节点被认为太具针对性，并很有可能过拟合训练集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这两个方法基于用户给定的参数，参与到树模型的构建过程中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此外，还有一个情况。算法有可能选择一个分割点，分割数据后所有的数据都被分割到同一组内（也就是左叉、右叉只有一个分支上有数据，另一个分支没有）。在这样的情况下，因为在树的另一个分叉没有数据，我们不能继续我们的分割与添加节点的工作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基于上述内容，我们已经有一些停止树「增长」的判别机制。当树在某一结点停止增长的时候，该节点被称为终端节点，并被用来进行最终预测。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;预测的过程是通过选择组表征值进行的。当遍历树进入到最终节点分割后的数据组中，算法将会选择该组中最普遍出现的值作为预测值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如下是一个名为 to_terminal() 的函数，对每一组收据它都能选择一个表征值。他能够返回一系列数据点中最普遍出现的值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;# Create a terminal node value&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;def to_terminal(group):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;outcomes = [row[-1] for row in group]&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;return max(set(outcomes), key=outcomes.count)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.3.2 递归分割&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在了解了如何及何时创建终端节点后，我们现在可以开始建立树模型了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;建立树地模型，需要我们对给定的数据集反复调用如上定义的 get_split() 函数，不断创建树中的节点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在已有节点下加入的新节点叫做子节点。对树中的任意节点而言，它可能没有子节点（则该节点为终端节点）、一个子节点（则该节点能够直接进行预测）或两个子节点。在程序中，在表示某节点的字典中，我们将一棵树的两子节点命名为 left 和 right。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一旦一个节点被创建，我们就可以递归地对在该节点被分割得到的两个子数据集上调用相同的函数，来分割子数据集并创建新的节点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如下是一个实现该递归过程的函数。它的输入参数包括：某一节点（node）、最大树深度（max_depth）、最小节点记录数（min_size）及当前树深度（depth）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;显然，一开始运行该函数时，根节点将被传入，当前深度为 1。函数的功能分为如下几步：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 首先，该节点分割的两部分数据将被提取出来以便使用，同时数据将被在节点中删除（随着分割工作的逐步进行，之前的节点不需要再使用相应的数据）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 然后，我们将会检查该节点的左叉及右叉的数据集是否为空。如果是，则其将会创建一个终端节点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3. 同时，我们会检查是否到达了最大深度。如果是，则其将会创建一个终端节点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4. 接着，我们将对左子节点进一步操作。若该组数据个数小于阈值，则会创建一个终端节点并停止进一步操作。否则它将会以一种深度优先的方式创建并添加节点，直到该分叉达到底部。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;5. 对右子节点同样进行上述操作，不断增加节点直到达到终端节点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/40faf3a05298f0b15a9552867ea437b5c595201f"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.3.3 建构整棵树&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们将所有的内容整合到一起。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;创建一棵树包括创建根节点及递归地调用 split() 函数来不断地分割数据以构建整棵树。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如下是实现上述功能的 bulid_tree() 函数的简化版本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;&lt;em&gt;&lt;code&gt;&lt;span&gt;# Build a decision tree&lt;/span&gt;&lt;/code&gt;&lt;br&gt;&lt;code&gt;&lt;span&gt;def &lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;build_tree&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;(&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;train&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;,&lt;/span&gt;&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&lt;span&gt;max_depth&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;,&lt;/span&gt;&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&lt;span&gt;min_size&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;)&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;:&lt;/span&gt;&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&lt;span&gt;root&lt;/span&gt;&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&lt;span&gt;=&lt;/span&gt;&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&lt;span&gt;get_split&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;(&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;dataset&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;)&lt;/span&gt;&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&lt;span&gt;split&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;(&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;root&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;,&lt;/span&gt;&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&lt;span&gt;max_depth&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;,&lt;/span&gt;&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&lt;span&gt;min_size&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;,&lt;/span&gt;&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&lt;span&gt;1&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;)&lt;/span&gt;&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&lt;span&gt;return&lt;/span&gt;&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&lt;span&gt;root&lt;/span&gt;&lt;/code&gt;&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以在如上所述的合成数据集上测试整个过程。如下是完整的案例。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在其中还包括了一个 print_tree() 函数，它能够递归地一行一个地打印出决策树的节点。经过它打印的不是一个明显的树结构，但它能给我们关于树结构的大致印象，并能帮助决策。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;code&gt;# Split a dataset based on an attribute and an attribute value&lt;/code&gt;&lt;br&gt;&lt;code&gt;def &lt;/code&gt;&lt;code&gt;test_split&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;index&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;value&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;dataset&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;left&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;right&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;list&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;list&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;for&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;row &lt;/code&gt;&lt;code&gt;in&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;dataset&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;if&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;row&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;index&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&amp;lt;&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;value&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;left&lt;/code&gt;&lt;code&gt;.&lt;/code&gt;&lt;code&gt;append&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;row&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;else&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;right&lt;/code&gt;&lt;code&gt;.&lt;/code&gt;&lt;code&gt;append&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;row&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;return&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;left&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;right&lt;/code&gt;&lt;br&gt; &lt;br&gt;&lt;code&gt;# Calculate the Gini index for a split dataset&lt;/code&gt;&lt;br&gt;&lt;code&gt;def &lt;/code&gt;&lt;code&gt;gini_index&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;groups&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;class_values&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;gini&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;0.0&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;for&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;class_value &lt;/code&gt;&lt;code&gt;in&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;class_values&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;for&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;group &lt;/code&gt;&lt;code&gt;in&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;groups&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;size&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;len&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;group&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;if&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;size&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;==&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;0&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;continue&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;proportion&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;row&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;-&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;for&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;row &lt;/code&gt;&lt;code&gt;in&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;group&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;.&lt;/code&gt;&lt;code&gt;count&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;class_value&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;/&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;float&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;size&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;gini&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;+=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;proportion *&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;1.0&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;-&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;proportion&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;return&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;gini&lt;/code&gt;&lt;br&gt; &lt;br&gt;&lt;code&gt;# Select the best split point for a dataset&lt;/code&gt;&lt;br&gt;&lt;code&gt;def &lt;/code&gt;&lt;code&gt;get_split&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;dataset&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;class_values&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;list&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;set&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;row&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;-&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;for&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;row &lt;/code&gt;&lt;code&gt;in&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;dataset&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;b_index&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;b_value&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;b_score&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;b_groups&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;999&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;999&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;999&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;None&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;for&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;index &lt;/code&gt;&lt;code&gt;in&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;range&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;len&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;dataset&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;0&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;-&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;for&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;row &lt;/code&gt;&lt;code&gt;in&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;dataset&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;groups&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;test_split&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;index&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;row&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;index&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;dataset&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;gini&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;gini_index&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;groups&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;class_values&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;if&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;gini&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&amp;lt;&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;b_score&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;b_index&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;b_value&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;b_score&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;b_groups&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;index&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;row&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;index&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;gini&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;groups&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;return&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;{&lt;/code&gt;&lt;code&gt;'index'&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;code&gt;b_index&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;'value'&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;code&gt;b_value&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;'groups'&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;code&gt;b_groups&lt;/code&gt;&lt;code&gt;}&lt;/code&gt;&lt;br&gt; &lt;br&gt;&lt;code&gt;# Create a terminal node value&lt;/code&gt;&lt;br&gt;&lt;code&gt;def &lt;/code&gt;&lt;code&gt;to_terminal&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;group&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;outcomes&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;row&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;-&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;for&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;row &lt;/code&gt;&lt;code&gt;in&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;group&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;return&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;max&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;set&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;outcomes&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;key&lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt;outcomes&lt;/code&gt;&lt;code&gt;.&lt;/code&gt;&lt;code&gt;count&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt; &lt;br&gt;&lt;code&gt;# Create child splits for a node or make terminal&lt;/code&gt;&lt;br&gt;&lt;code&gt;def &lt;/code&gt;&lt;code&gt;split&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;max_depth&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;min_size&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;depth&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;left&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;right&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'groups'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;del&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'groups'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;# check for a no split&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;if&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;not&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;left &lt;/code&gt;&lt;code&gt;or&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;not&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;right&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'left'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'right'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;to_terminal&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;left&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;+&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;right&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;return&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;# check for max depth&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;if&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;depth&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&amp;gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;max_depth&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'left'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'right'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;to_terminal&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;left&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;to_terminal&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;right&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;return&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;# process left child&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;if&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;len&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;left&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&amp;lt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;min_size&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'left'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;to_terminal&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;left&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;else&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'left'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;get_split&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;left&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;split&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'left'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;max_depth&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;min_size&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;depth&lt;/code&gt;&lt;code&gt;+&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;# process right child&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;if&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;len&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;right&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;&amp;lt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;min_size&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'right'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;to_terminal&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;right&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;else&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'right'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;get_split&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;right&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;split&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'right'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;max_depth&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;min_size&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;depth&lt;/code&gt;&lt;code&gt;+&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt; &lt;br&gt;&lt;code&gt;# Build a decision tree&lt;/code&gt;&lt;br&gt;&lt;code&gt;def &lt;/code&gt;&lt;code&gt;build_tree&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;train&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;max_depth&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;min_size&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;root&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;get_split&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;dataset&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;split&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;root&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;max_depth&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;min_size&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;return&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;root&lt;/code&gt;&lt;br&gt; &lt;br&gt;&lt;code&gt;# Print a decision tree&lt;/code&gt;&lt;br&gt;&lt;code&gt;def &lt;/code&gt;&lt;code&gt;print_tree&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;depth&lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt;0&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;if&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;isinstance&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;dict&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;print&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;'%s[X%d &amp;lt; %.3f]'&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;%&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;depth*&lt;/code&gt;&lt;code&gt;' '&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'index'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;+&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'value'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;print_tree&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'left'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;depth&lt;/code&gt;&lt;code&gt;+&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;print_tree&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;'right'&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;depth&lt;/code&gt;&lt;code&gt;+&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;else&lt;/code&gt;&lt;code&gt;:&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;print&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;'%s[%s]'&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;%&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;depth*&lt;/code&gt;&lt;code&gt;' '&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;node&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt; &lt;br&gt;&lt;code&gt;dataset&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;2.771244718&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;1.784783929&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;0&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;1.728571309&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;1.169761413&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;0&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;3.678319846&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;2.81281357&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;0&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;3.961043357&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;2.61995032&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;0&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;2.999208922&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;2.209014212&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;0&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;7.497545867&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;3.162953546&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;9.00220326&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;3.339047188&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;7.444542326&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;0.476683375&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;10.12493903&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;3.234550982&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;br&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;[&lt;/code&gt;&lt;code&gt;6.642287351&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;3.319983761&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;code&gt;]&lt;/code&gt;&lt;br&gt;&lt;code&gt;tree&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;=&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;build_tree&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;dataset&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;,&lt;/code&gt;&lt;code&gt; &lt;/code&gt;&lt;code&gt;1&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;br&gt;&lt;code&gt;print_tree&lt;/code&gt;&lt;code&gt;(&lt;/code&gt;&lt;code&gt;tree&lt;/code&gt;&lt;code&gt;)&lt;/code&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;code&gt;&lt;br&gt;&lt;/code&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在运行过程中，我们能修改树的最大深度，并在打印的树上观察其影响。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当最大深度为 1 时（即调用 build_tree() 函数时第二个参数），我们可以发现该树使用了我们之前发现的完美分割点（作为树的唯一分割点）。该树只有一个节点，也被称为决策树桩。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;[X1 &amp;lt; 6.642]&lt;br&gt; [0]&lt;br&gt; [1]&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当最大深度加到 2 时，我们迫使输算法不需要分割的情况下强行分割。结果是，X1 属性在左右叉上被使用了两次来分割这个本已经完美分割的数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;[X1 &amp;lt; 6.642]&lt;br&gt; [X1 &amp;lt; 2.771]&lt;br&gt; [0]&lt;br&gt; [0]&lt;br&gt; [X1 &amp;lt; 7.498]&lt;br&gt; [1]&lt;br&gt; [1]&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，我们可以试试最大深度为 3 的情况：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;[X1 &amp;lt; 6.642]&lt;br&gt; [X1 &amp;lt; 2.771]&lt;br&gt; [0]&lt;br&gt; [X1 &amp;lt; 2.771]&lt;br&gt; [0]&lt;br&gt; [0]&lt;br&gt; [X1 &amp;lt; 7.498]&lt;br&gt; [X1 &amp;lt; 7.445]&lt;br&gt; [1]&lt;br&gt; [1]&lt;br&gt; [X1 &amp;lt; 7.498]&lt;br&gt; [1]&lt;br&gt; [1]&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些测试表明，我们可以优化代码来避免不必要的分割。请参见延伸章节的相关内容。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在我们已经可以（完整地）创建一棵决策树了，那么我们来看看如何用它来在新数据上做出预测吧。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.4 利用模型进行预测&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;使用决策树模型进行决策，需要我们根据给出的数据遍历整棵决策树。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;与前面相同，我们仍需要使用一个递归函数来实现该过程。其中，基于某分割点对给出数据的影响，相同的预测规则被应用到左子节点或右子节点上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们需要检查对某子节点而言，它是否是一个可以被作为预测结果返回的终端节点，又或是他是否含有下一层的分割节点需要被考虑。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如下是实现上述过程的名为 predict() 函数，你可以看到它是如何处理给定节点的下标与数值的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/ba8ef872c4938030ddc42a109ce3f17d2d6c68d0"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接着，我们使用合成的数据集来测试该函数。如下是一个使用仅有一个节点的硬编码树（即决策树桩）的案例。该案例中对数据集中的每个数据进行了预测。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/5712b635bbbc55843e925f5621b0634331173816"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;运行该例子，它将按照预期打印出每个数据的预测结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Expected=0, Got=0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Expected=0, Got=0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Expected=0, Got=0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Expected=0, Got=0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Expected=0, Got=0&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Expected=1, Got=1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Expected=1, Got=1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Expected=1, Got=1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Expected=1, Got=1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Expected=1, Got=1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在，我们不仅掌握了如何创建一棵决策树，同时还知道如何用它进行预测。那么，我们就来试试在实际数据集上来应用该算法吧。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.5 对钞票数据集的案例研究&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该节描述了在钞票数据集上使用了 CART 算法的流程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第一步是导入数据，并转换载入的数据到数值形式，使得我们能够用它来计算分割点。对此，我们使用了辅助函数 load_csv() 载入数据及 str_column_to_float() 以转换字符串数据到浮点数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们将会使用 5 折交叉验证法（5-fold cross validation）来评估该算法的表现。这也就意味着，对一个记录，将会有 1273/5=274.4 即 270 个数据点。我们将会使用辅助函数 evaluate_algorithm() 来评估算法在交叉验证集上的表现，用 accuracy_metric() 来计算预测的准确率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;完成的代码如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/e3718591bcd4a650bea6a8c99aac09c77ddf86ef"/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/167dce3b1289127e521fa4f23bbb88faccef12c8"/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/97f787cee08888d52ddb9e8338690c106caa37b4"/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/373c180d199f8f7d5aa818dba8b05db2372a7bce"/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/dae518c198905eca6513f16ccc5555eb77248100"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;上述使用的参数包括：max_depth 为 5，min_size 为 10。经过了一些实现后，我们确定了上述 CART 算法的使用的参数，但这不代表所使用的参数就是最优的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;运行该案例，它将会 print 出对每一部分数据的平均分类准确度及对所有部分数据的平均表现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从数据中你可以发现，CART 算法选择的分类设置，达到了大约 83% 的平均分类准确率。其表现远远好于只有约 50% 正确率的零规则算法（Zero Rule algorithm）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Scores: [83.57664233576642, 84.30656934306569, 85.76642335766424, 81.38686131386861, 81.75182481751825]&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Mean Accuracy: 83.358%&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;三、延伸&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本节列出了关于该节的延伸项目，你可以根据此进行探索。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 算法调参（Algorithm Tuning）：在钞票数据集上使用的 CART 算法未被调参。你可以尝试不同的参数数值以获取更好的更优的结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 交叉熵（Cross Entropy）：另一个用来评估分割点的成本函数是交叉熵函数（对数损失）。你能够尝试使用该成本函数作为替代。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3. 剪枝（Tree Pruning）：另一个减少在训练过程中过拟合程度的重要方法是剪枝。你可以研究并尝试实现一些剪枝的方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4. 分类数据集（Categorical Dataset）：在上述例子中，其树模型被设计用于解决数值型或有序数据。你可以尝试修改树模型（主要修改分割的属性，用等式而非排序的形式），使之能够应对分类型的数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;5. 回归问题（Regression）：可以通过使用不同的成本函数及不同的创建终端节点的方法，来让该模型能够解决一个回归问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;6. 更多数据集：你可以尝试将该算法用于 UCI Machine Learning Repository 上其他的数据集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;原文：http://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&amp;copy;本文为机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Mon, 20 Feb 2017 12:00:06 +0800</pubDate>
    </item>
    <item>
      <title>学界 | YodaNN：一个用于超低功耗二值卷积神经网络加速的框架</title>
      <link>http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650723438&amp;idx=5&amp;sn=239640a98506d4c6fdcc85631d7ec511&amp;chksm=871b1010b06c9906f3a8d565c939fe5bcac5451207536cee606ea248687d56334b4a63d0331d&amp;scene=0#rd</link>
      <description>
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;选自arXiv.org&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：晏奇、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;去年，来自瑞士苏黎世联邦理工学院（ETH Zurich）和意大利博洛尼亚大学电气、电子与信息工程系的研究者提出一种用于超低功耗二值卷积神经网络加速的框架 YodaNN。近日，该研究团队对这个框架的论文进行了更新，机器之心在此对其进行了简单的摘要介绍，论文原文请点击文末「阅读原文」查看。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/503888f9c4ff2846381d61a759ff2396476548fb"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;摘要：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在过去几年里，卷积神经网络（CNN）已经为计算机视觉领域带来了革新，推动实现了超越人类准确度的图像分类。但是，我们需要非常高功耗的并行处理器或者通用计算图形处理器（GP-GPU）才能满足运行目前 CNN 的要求。最近在为系统级芯片集成（system-on-chip integration）的 CNN 加速器上的发展已经实现了显著的功耗降低。不幸的是，即便是这些经过高度优化的设备，其包络功率（power envelope）也因超过了移动设备和深层嵌入式应用从而面临因 CNN weight I/O 和 storage 导致的硬性限制。这也阻碍了未来在超低功耗物联网端节点中采用 CNN 来对近传感器（near-sensor）的分析工作。最近在算法和理论中的进展使具有竞争力的分类准确度成为可能&amp;mdash;&amp;mdash;即便当在训练中限制 CNN 使其使用二值权重（+1/-1）来计算也没问题。通过去除对大量乘法运算的需求和减少 I/O 带宽与存储，这些新发现为我们带来了在运算核心中进行重要优化的良机。本文中，我们提出了一种为二值 CNN 优化过的加速器，它在仅 1.33MGE（Million Gate Equivalent，百万级等效门）或 0.19 平方毫米的核心区域上在 1.2 V 的条件下实现了 1510 GOp/s 的速度，而且在 0.6 V 条件下使用 UMC 65 nm 技术时仅有 895 uW 的功率耗散。我们的加速器在能量效率和尺寸效率上的表现都显著超越了当前最佳水平，分别在 0.6 V 和 1.2 V 的条件下实现了 61.2 TOp/s/W 和 1135 GOp/s/MGE 的表现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/1fd3a071de9d7308e0adcab4a16c1d06cd086973"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;算法 1：该伪代码给出了卷积层处理所需的主要步骤的概览&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/6c53664fa856b91649ebb0d04ae86ccfa92af279"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 10：YodaNN 的 floorplan，其带有 9.2 KiB SCM 内存，可并行计算 32 个输出信道&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/812133e9a5cd34f23c727e2b83c786b6e7f55236"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 11：基线架构（定点 Q2.9、SRAM、8&amp;times;8 信道、固定 7&amp;times;7 滤波器）与最终的 YodaNN（二值的、SCM、32&amp;times;32 信道、支持多个滤波器）在内核能效和通量上的比较&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/6e7f19fdd534b0094c6cda8599384a7569f5ece9"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图.12. 固定点和若干二进制架构的核心功率击穿。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/26fcd237f0cd989bded01dbc3268da47d26df2c0"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图.13. 最先进的卷积神经网络加速器的核心区域效率和能源效率比较&amp;nbsp;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&amp;copy;本文为机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Mon, 20 Feb 2017 12:00:06 +0800</pubDate>
    </item>
  </channel>
</rss>
