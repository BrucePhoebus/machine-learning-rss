<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>机器之心</title>
    <link>http://www.iwgc.cn/list/670</link>
    <description>人与科技的美好关系</description>
    <item>
      <title>重磅 | 苹果发布第一篇人工智能研究论文：提出模拟+无监督方法改善合成图像质量</title>
      <link>http://www.iwgc.cn/link/4083671</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自TechCrunch等&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀、蒋思源、朱思颖&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;在 12 月初的时候，&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650721102&amp;amp;idx=4&amp;amp;sn=32e8e8a2f58e31959979c69e5615e712&amp;amp;chksm=871b0f30b06c86268c98d32e1f2fdf455c7a0b78c6d504d297a09326abddfc6e70c1e1fad755&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650721102&amp;amp;idx=4&amp;amp;sn=32e8e8a2f58e31959979c69e5615e712&amp;amp;chksm=871b0f30b06c86268c98d32e1f2fdf455c7a0b78c6d504d297a09326abddfc6e70c1e1fad755&amp;amp;scene=21#wechat_redirect"&gt;苹果正式向外界宣布允许其人工智能和机器学习研究员公开发布和分享他们的最新研究成果&lt;/a&gt;，这一举措稍稍掀开了苹果久负盛名且神秘的创新研究进程的一角。仅在几周之后，他们的人工智能和机器学习研究的第一篇论文发表了，主要聚焦苹果在智能图像识别领域的研究。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;机器学习研究也许会在苹果内部引领新的潮流。该公司最近成立的机器学习小组中六位研究员发表了一篇论文，这篇论文描述了一种用于模拟+无监督学习（simulated + unsupervised learning）的新方法。其目的是提高合成训练图片的质量。这项研究展示了该公司希望在高速增长的人工智能领域中成为领导者的渴望。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌、Facebook、微软还有其他技术类初创公司一直稳步发展他们的机器学习研究小组。这些公司都发表了几百份的学术研究。他们的学术追求都是公开且有据可查，但是苹果公司一直很固执地将研究成果保密。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;变化是从本月初开始的。苹果的 AI 研究部主任 Russ Salakhutdinov 宣布该公司将很快开始发表研究成果。该研究小组的第一次尝试就是很及时很务实的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;近来，使用合成图像和视频训练机器学习模型的频率越来越高了。不使用真实世界的图像是因为其花费的成本和时间很高，而生成图像的成本更少，更容易获取和定制化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在该研究中，苹果指出了与合成图像或计算机图像相比使用真实图像的优缺点，标注必须添加到真正的图像，这是一个「昂贵且耗时的任务」，需要一个人的劳动力单独标记图片中的物体。另一方面，计算机生成的图像能帮助促进这一过程，「因为标注是自动可用的。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管如此，完全换成合成图像可能会导致程序的质量下降的问题。这是因为「合成数据往往不够现实」，往往会产生只对计算机生成的图像的细节才能反应良好的用户体验，而且还不能很好地泛化到它面对的任何真实世界的物体和图像上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这就是这篇论文的初衷所在——在「对抗学习」中综合使用模拟和真实图像，创建出一个领先的人工智能图像程序：&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;在这篇论文中，我们提出了模拟+无监督学习（S+U）学习，其目标是使用未标记的真实数据提升合成图像的真实性。经过提升的真实性能够在没有收集的真实数据或经过人类注释的大型数据集上实现更好机器模型训练。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;我们发现这将实现高质量的真实图像的生成，而且经过了定性研究和用户研究的验证。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;论文剩下的部分介绍了苹果在该主题下的一些研究细节，包括已经开始操作的实验和支持其研究发现的一些数据理论。虽然这篇论文只关注单个图像，但是苹果的该研究团队指出他们最终期望的结果是「探讨精炼的视频」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;苹果提议使用生成式对抗网络（GANs）来提高这些合成图像的质量。生成式对抗网络并不新颖，但苹果正在修改它使其更加符合生成训练图片的目的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;生成式对抗网络很大程度上通过利用竞争性神经网络（competing neural networks）之间的对抗关系来工作。在苹果公司的论文中，模拟器通过精炼机（refiner）进行生成图像，然后将这些精炼过的图像发送到鉴别器（discriminator），鉴别器的任务就是区分真实图像和合成图像。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;论文：通过对抗训练从模拟的和无监督的图像中学习（Learning from Simulated and Unsupervised Images through Adversarial Training）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：随着近年来在图形（graphics）上的进步，在合成的图像上训练模型变得越来越可行了，这也潜在地避免了对昂贵的标注的需求。但是，由于合成图像分布和真实图像分布之间的差别，从合成的图像中学习可能无法得到预期的表现。为了弥合这种差距，我们提出了模拟+无监督学习（Simulated+Unsupervised (S+U) learning），其中的任务是使用无标签的真实数据来提升模拟器输出的真实性，同时也为保留来自该模拟器的标注信息。我们开发了一种用于 S+U 学习的方法，该方法使用了一个类似于生成对抗网络（GAN）的对抗网络，但它的输入是合成图像而非随机向量。我们在标准 GAN 算法的基础上做了一些关键的修改，从而可以保留标注、避免伪像（artifact）和使训练稳定：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;i. 一个「自正则化（self-regularization）」项；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;ii. 一个局部对抗损失（local adversarial loss）；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;iii. 使用精细调节过的图像的历史来更新判别器。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的研究表明这能实现高真实度的图像生成——这在定性评估和用户研究上都得到了证明。我们通过训练用于注视估计和手姿态估计（gaze estimation and hand pose estimation）的模型而对生成的图像进行了量化评估。研究表明我们在使用合成图像上实现了显著的提升，并且在没有任何有标签的真实数据的情况下实现了在 MPIIGaze 数据集上的当前最佳结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9oYx5LnUGMhUE4SPcHDjTlWHD1s8eGFHBSm7oJK4iczZyTfwsNQhMKU5QwOpGG9doQTh1Dq3qXvlA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;算法&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9oYx5LnUGMhUE4SPcHDjTl6JW0lXLaGicczpYKI1HoSCvfrmTiasW2ayicmhjzJlJjggIo0eoEpGpQg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;图 1：模拟+无监督（S+U）学习。其任务是使用无标签的真实数据从模拟器中学习能够提升合成图像的真实度的模型，同时还能保留其标注信息。&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9oYx5LnUGMhUE4SPcHDjTlicm3rBJlYmiblsYj6uufR3HBGsZ1F9c3UI2gQiaLwGX5T24HaozVrAUjw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;图 2：SimGAN 概览。我们使用一个 refiner 神经网络 R 来改善模拟器的输出；该神经网络可以最小化局部对抗损失和一个「自正则化（selfregularization）」项的组合。这个对抗损失会试图欺骗一个判别器网络 D，而 D 则需要试图区分一张图像是否是真实的。上述的「自正则化」项可以最小化合成图像和改善过的图像的之间的图像差异。这保留了标注信息（即注视方向），使得改善过的图像可以用于训练机器学习模型。该 refiner 网络 R 和判别器网络 D 是交替更新的。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9oYx5LnUGMhUE4SPcHDjTlOK4W2WLBtRYDTBoETsw40sXFwic2R7uzcEFrQsTl2TbHhqu2XIpZ1kQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;图 3：局部对抗损失（local adversarial loss）的图示。该判别器网络输出一个 w×h 的概率图。其对抗损失函数是在局部 patch 上的交叉熵损失（cross-entropy losses）的总和。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9oYx5LnUGMhUE4SPcHDjTlCZMKXjQYpjdGmXtG1XHcKM7bHdqPMgfiaweHiaaDzbHlFLibhJb7rVpWw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 4：使用改善过的图像的历史（history of refined images）的图示&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9oYx5LnUGMhUE4SPcHDjTlicB3p6EudGCDDsrQqjB08r8a1Lc5WfTSFDAztEicibKmRVNcWmWqMhL8g/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 9：使用一个改善过的图像的历史来更新判别器。（左图）合成图像；（中图）使用改善过的图像的历史所得到的结果；（右图）没有使用改善过的图像的历史所得到的结果（而仅仅使用了时间最近的改善过的图像）。我们可以观察到明显的没有真实感的伪像，尤其是在眼角附近。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9oYx5LnUGMhUE4SPcHDjTlggGOjRXk3HpeoDh1EquZphN0kpBooRjA80cbMbfRD46GBh36DKmzhg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;图 11：来自 NYU 手势数据集 [35] 的改善过的测试图像样本。（左图）真实图像；（右图）合成图像（上）和对应的 refiner 网络输出的改善过的图像。在真实图像中最大的噪声源是不平滑的深度边界（non-smooth depth boundaries）。该 refiner 网络学习了建模真实图像中的噪声存在，重要的是其不需要任何真实图像的标签。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;点击阅读原文查看论文&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;原文链接：https://techcrunch.com/2016/12/26/apple-leaps-into-ai-research-with-improved-simulated-unsupervised-learning/&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 27 Dec 2016 10:59:58 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 用深度学习对抗癌症：从分子层面研究到大规模人口建模</title>
      <link>http://www.iwgc.cn/link/4083672</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自hpcwire&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Rick Stevens 表示，首个先进癌症计算解决方案的联合设计（Joint Design of Advanced Computing Solutions for Cancer，JDACS4C）「成果」将于 2017 年第二季度的某个时间公开。JDACS4C 一共有三个试点项目，Rick 领导了其中之一，将深度学习（DL）应用到癌症治疗中去。这些项目不仅可以推进癌症研究和治疗，而且可以提高深度学习的能力和基础架构，最终着眼于百亿亿次计算机的研究，DOE 将对这些项目提供部分的资金支持。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;无论以哪种标准，美国对抗癌症（U.S. War on Cancer）和 Precision Medicine Initiative（精准医学计划，PMI）都颇具野心。过去，对抗癌症一直没有很明显的进步，但也不是说没有取得很多成绩。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;只是现在看来前景更为光明。生物医学的进步和下一代领先计算机的兴起（百亿亿次计算机的开发）推动着癌症治疗的发展。深度学习和数据驱动科学的快速发展，使许多人对前景报以乐观的态度，所以 2016 全球超级计算机大会重点关注精准医疗和 HPC 的作用就是偶然了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;三个 JDACS4C 试点项目，包括从分子层面到人口规模方面的诸多研究，以支持 CANcer 分布式学习环境项目（CANcer Distributed Learning Environment project）：这些工作旨在洞察可扩展机器学习工具；通过深度学习、模拟和分析技术，减少治疗时间；为未来计算方案提供信息。也希望能建立「有效利用日益增长的数据和与癌症相关数据的多样性来打造预测性模型，为接下来的癌症研究提供一个新的范式，更好地理解疾病并最终提供指导，支持基于个体预期治疗结果的决策，Rick 说。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些都是远大的目标。因此想要总结出 JDACS4C 的准确谱系，确实有点麻烦，广义上来看，它属于 PMI，美国国家癌症研究所的癌症登月计划，也集中在 美国国家战略计算计划（NSCI）之下。Stevens 指出，早在几年前就开始讨论创建这个大项目框架了，8 月拿到了第一笔资金。以下是三个试点项目的简介：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;RAS 分子项目&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;： 这个项目（(Molecular Level Pilot for RAS Structure and Dynamics in Cellular Membranes）旨在开发新的计算方法，支持 RAS 计划下已经完成的研究，最终完善我们对癌症中的 RAS（基因家族）及其相关信号通路作用的理解，识别 RAS 蛋白膜信号复合物中独有的新治疗靶点。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;临床前筛选&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;： 该项目（Cellular Level Pilot for Predictive Modeling for Pre-clinical Screening）将开发「基于源自人源性肿瘤组织异种移植实验性生物数据的机器学习、大规模数据和预测模型」。旨在创建一个反馈回路，其中，实验模型指导计算模型的方案。这些预测模型可能给癌症治疗的指明了新目标，并帮助确定新的治疗方法。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;人口模型&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这个项目（Population Level Pilot for Population Information Integration, Analysis and Modeling)）旨在建立一个可扩展的框架，能够高效提取、延展、整合及 构建癌症患者的病例信息。这样的一个「引擎」应用在医疗保健的许多方面（转移、成本控制、研究等），将会十分强大。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;显而易见的是，这么复杂的工作需要很多组织的配合。国家癌症研究所的部门包括生物医学信息和信息技术中心（CBIIT），癌症治疗诊断部（DCTD），癌症控制和人口科学部（DCCPS）和弗雷德里克国家癌症研究实验室（Frederick National Laboratory for Cancer Research）。也有四个美国能源部国家实验室被正式分派从事这个项目，这四个实验室分别是阿贡国家实验室（Argonne National Laboratory）、橡树岭国家实验室（Oak Ridge National Laboratory）、劳伦斯利弗莫尔国家实验室（Lawrence Livermore National Laboratory）和洛斯阿拉莫斯国家实验室（Los Alamos National Laboratory）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当所有的试验项目放在一起时，Stevens 指出，我们意识到每个项目都需要深度学习，而且需要它的各种不同用途。因此，我们的想法是，既要构建软件环境和网络拓扑，也要建造这三个项目所需的所有东西，所以我们不会复制。研究人员也定义了关键标准——与我们用来解决不同癌症子问题资源相匹配的、易于处理的深度学习问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9oYx5LnUGMhUE4SPcHDjTl5LUhbKTg4I1VUrYLl9AUKZaS6R3SawTjdZjliaCCMBD591vKsjfU1xA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;早期的第一步是吸引供应商参与，这充分地证明了 Stevens 所说的话，因为几乎所有的主要 HPC 供应商都在积极地加速深度学习路线图。大多数人认为 JDACS4C 试点项目是学习和完善的机会。Stevens 说，JDASC4C 已经与英特尔、Cray、NVIDIA、IBM 等公司达成了合作关系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「所有的实验室都配备了 DGX-1，并且 NVIDIA 已经为不同 GPU、Pascal 等优化了大多数的通用框架。我们在 DGX-1 上运行的任何东西都可以很容易地实现分布式。英特尔有自己的长远计划，并且并不是所有的这些计划都是公开的。我可以表明的是，我们正在与英特尔的所有适合的部门合作。」Stevens 说，他是 ANL 研究员和临床前筛选项目的领导者。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;事实上，英特尔一直很忙，忙于购买 Nervana（一个用于深度学习的完整平台），最近又推出了扩展计划。Stevens 说：「他们谈论到为机器学习而优化的 Knights X 系列的版本。Knights Mil 是他们的线路图的第一个版本，」这个芯片巨头还在 SC16 上推出了深度学习推理加速卡；它是用于神经网络加速的基于现场可编程门阵列（field-programmable gate array (FPGA)）的软硬件解决方案。Stevens 认为英特尔像 NVIDIA 一样，正在制定一个应用战略。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他说：「英特尔非常想尝试确定一种战略，以区分训练和推理平台之间的某些级别。大多数深度学习系统现在在『quasi』上做推理，它比用于训练的平台更小。英特尔希望确保『未来的 IA 架构擅长推理』」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不足为奇的是现在人们花费了大量的精力用于评估来自谷歌、微软、Facebook 等公司的深度学习框架。Stevens 说：「我们也正在评估哪些框架最适合解决我们的问题，我们正在与供应商一起在硬件上优化它们。同时我们也与 Livermore 有合作关系，他们有一个内部的被称为 LBANN 的项目，该项目旨在构建一个可扩展的人工神经网络框架。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「该计划是想以一种独立于框架的方式去开发我们的模型，所以我们可以在不需要重新编码我们的模型的情况下交换框架。这是一个非常常见的深度学习方法，其中有一个脚本层（scripting layer）可以捕获您的模型的表示（用于训练和管理数据的元算法（meta algorithms）等），我们同时与学术界和 NVIDIA 在顶层的工作流引擎上进行合作。因此，我们有一种堆叠式架构（stacked architecture），它与深度学习全景周围的所有不同群体进行合作。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Stevens 说：「有趣的是下一代平台的供应商强烈支持提高机器学习所需的架构理念和功能，以及传统的物理驱动仿真（physics-driven simulation）。」他指出，与传统 HPC 相比，深度学习的快速增长和市场压力正在推动它们朝着这个方向发展。「它也让我们洞察到了 DOE 应用的发展方向：将需要传统的物理驱动的仿真的地方，但通常我们也可以找到一个利用机器学习的方法。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;共享学习是试点项目的重要组成部分。Stevens 说：「我们正在为机器学习社区抽象模型问题，这也是我们正在研究的 seven candle benchmarks 的一种净化版本，」这将包括可分布式的数据、代码，这些内容都 将在 GitHub 上开放。这些元素的第一部分预计会在第二季度发行。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;个别试点小组也正在与学术界开展自己的外联活动。在试点项目计算能力的方面，「我们瞄准了一些平台，特别是 CORAL 平台、Oak Ridge 和 Livermore 的新机器，然后最终选定百亿亿次（exascale）级。这是一个普遍化的概念，所以它不是具体的 GPU 或者具体的多少核。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有趣的是，这三个项目计划会用不同的方式使用深度学习。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9oYx5LnUGMhUE4SPcHDjTlzWEq0uemcV40f5wRMMYfEiaTJhObaoR9hicGg8fRMnQwXFCMjMCQvIBw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为 RAS 是在分子尺度上的项目，所以它在所有项目中拥有最小尺寸规模。你可能听说过 RAS，它是一个著名的癌症基因，其编码会生成嵌入在细胞膜中信号蛋白（signaling protein）。这些蛋白质控制着可以延伸到细胞中并驱动许多不同的细胞过程的信号传导途径。RAS 目前涉及约 30％ 的癌症，包括一些最棘手的癌症，例如胰腺癌。该试点项目将把模拟和湿实验室筛选数据进行结合，以详细阐述 RAS 相关信号级联的细节，并且希望可以找到用于制造能干预这种病症所使用的新药的关键点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;即使一个相对较小的肿瘤也可能有「成千上万个突变，包括驱动突变（driver mutation）和许多偶然突变（passenger mutation），」Stevens 说。这些遗传差异会改变信令网络（signaling network）的重要细节信息。多年来，RAS 本身及其相关信令网已经成为药物靶点，但正如 Stevens 指出的：「这种信令网的行为很不直观。有时如果你击中了其中一个下游组分，它其实会产生负反馈，这实际上增加了你试图去抑制的效果。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 RAS 项目中，仿真基本上是一种在不同粒度（一直延伸到到原子行为，包括量子效应）上进行的分子动力学运动。所需的计算能力（会非常巨大）自然取决于所仿真的粒度水平。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「机器学习被用于跟踪仿真所经历的状态空间（state space），并进行决策——这里是否放大、是否缩小、是否改变我们在集合空间（ensemble space）的不同部分中所观察的参数。它基本上像是该仿真的一个智能监督人那样去更有效地使用它。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「从某种意义上讲，这就像是网络正在观看一部电影并且说道，『好吧，我之前已经看过电影的这个部分了，让我们快进吧，或是哇这真有趣，我之前从来没见过，让我们用慢镜头并放大看。』这种就是机器学习在模拟中所做的事情。在某种意义上，它能够快进并且跳过，「Stevens 说。由 Stevens 领导的这个临床前筛选项目是一个雄心勃勃的尝试，它基本上是从所能得到的尽可能多的临床前及临床癌症数据中进行精筛，并与小鼠模型中产生的新数据结合来建立药物-肿瘤相互作用的预测模型。这是一种生物信息学的和实验性的反馈方法。最终，给定一个特定肿瘤，其分子属性（基因表达、单核苷酸的多态性（Single Nucleotide Polymorphisms/SNP）、蛋白质组学等）已被确定，那么将该数据插到模型中来确定最佳治疗方案就应该是可以实现的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此处的微妙之处在于，这种在单一类肿瘤或相对小门类药物上进行的小规模机器学习工作已做了很多，Stevens 说。「我们正在尝试使用深度学习来整合所有对象（成千上万的细胞系以及从较小数量细胞系中筛选出的化合物）的信息，然后就能将其应用在实验鼠身上。你培养了一群源自该人类肿瘤的实验鼠，而这些小鼠会成为人类临床试验的替代物。因此我可以在肿瘤鼠群体中尝试不同化合物来提供信息——我的肿瘤对给定药物可能会如何反应。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个巨大的挑战来自于是否能够理解所有历史数据，其中大部分数据是非结构化的，且往往是主观的（如病理报告）。「我们所做的第一件事情之一是建立分类器，它可以告诉我们该肿瘤的类型或者是它在身体的哪个部位（根据不同的数据），」他说。数据可疑的情况并不少见。「我们通过我们的分类器来运行它，而如果它是一个新的数据集，那么分类器就可能会说，它真不是来自肝脏，它来自一些其他部位。」通常临床前数据是基于结果的；它不会解释该结果是如何实现的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「现在我们所建立的机器学习模型能够十分精确地预测出一个药物反应或肿瘤类型/结果，但它们不能相当有效地告诉我们个中原因。它们不是解释性的，不是机械论的，」Stevens 说，「我们要做的是以某种方式带来一些机械论的模型或机械论的数据，并将其与机器学习模型混合从而得到两样东西——拥有高精度预测能力的模型以及拥有预测解释能力的模型。因此这种混合方法的思想是一个宽广的开放空间，而我们认为这将会被推广到许多领域。」获得大而高质量的训练模型数据仍然具有挑战性，他说。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第三个项目致力于开发可预测人口规模的模型，Stevens 称之为「病人轨迹（patient trajectories）」，它基本上是在挖掘全国的监控数据。虽然该数据有些分散，但美国国家癌症研究所（National Cancer Institute/NCI）、美国国立卫生研究院（National Institutes of Health/NIH）、美国食品和药物管理局（Food and Drug Administration/FDA）、制药公司和付款人组织（病理报告、疗法、结果、生活方式、人口统计等）所持有的病人数据体量却十分庞大。不幸的是，在很大程度上它像许多生物医学数据一样是非结构化的。「我们不能真正以我们所希望的方式用它进行计算，因此我们正在使用机器学习来将非结构化数据翻译成我们可用于计算的结构化数据，」Stevens 说。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「因此，例如我们想用一台机器来读取所有病理报告并输出生物标记物（biomarkers）、突变状态或药物之类的信息，这样我们才能创建出具有一致性的病例报告。将它看做是一个以人口为基础的模型。在临床前筛选试点项目中，比如我们发现了一些对治疗某一类癌症非常有效的疗法和策略。我们想提取这些信息并将其输入到人口模型中，并说『如果这成为一种常见疗法的话，那么它在全球或全国范围内会对统计数字有多少改变？』或类似的话。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这也是一种连接所有试点项目的方法，Stevens 说。从 RAS 项目中获得的认识以后可能会被用于观察那些或许适用于新疗法的一部分癌症；再反过来把它纳入人口模型项目中以了解可能会产生的影响。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;JDACS4C 试点项目仍处在初期阶段，但希望很高。Stevens 指出，NCI 和 DOE 都获得了它们无法轻易获得的东西。「NCI 没有 DOE 所拥有的众多数学家和计算机科学家。他们也没办法使用最领先的机器。我们（DOE）所获得的是访问所有这些伟大的实验数据、实验设施和公共数据库的权限。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 27 Dec 2016 10:59:58 +0800</pubDate>
    </item>
    <item>
      <title>观点 | 人们都在说人工智能，其实现在我们真正做的是智能增强</title>
      <link>http://www.iwgc.cn/link/4083673</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自CB Insights&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：Rick、林静、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;智能增强技术有助于提高人类的潜能——通过提高工人生产力、减轻一般任务的工作量以及为我们的生活提供更多方便。本文的作者是 Anupam Rastogi，他是 NGP 的成长期技术投资者，专注于企业中的物联网、数据与机器学习的交叉领域。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自 20 世纪 50 年代以来，人工智能（The Artificial Intelligence/AI）与智能增强（Intelligence Augmentation/IA）之间的争论已经持续了半个多世纪。一般来说，智能增强指的是利用信息技术来增强人类能力。这个想法自 1950 年被首次提出后，现在已经变得无处不在。如今人工智能越来越多地被用于广泛描述那些能够模仿人类功能（比如学习和解决问题）的机器，但它最初所建立的前提条件是：人类智能可以被精确描述，且能够用所制作的机器进行模拟。人工通用智能（Artificial General Intelligence/AGI）这个术语通常仅仅表示后者，该定义较前者更严格。当下存在许多人工智能方面前所未有的炒作——其近来令人难以置信的增长曲线、无数的潜在应用、及其潜在的社会威胁。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;更广泛的人工智能定义给一些人造成了困惑，特别是那些或许不太紧跟技术潮流的人。机器学习应用近期所带来的一些十分显著的进步有时会被错误理解和推断，使我们以为人类即将取得 AGI 方面的进展、正在逼近为了社会秩序所需要的一切。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;智能增强与人工通用智能技术之间可能会有一段持续进步的过程。我在本文中谈到，我们所目睹的人工智能领域的快速进展是来自于机器学习对其产生的强大驱动力。然而，满足人工智能——以及人工通用智能——的原始前提条件是大量的、额外的、在近期进展之上的技术突破。智能增强技术有助于提高人类的潜能——通过提高工人生产力、减轻一般任务的工作量以及为我们的生活提供更多方便。我们目前所看到的是机器在任务执行方面的能力提升，在这方面它们几十年前就胜过人类了。而未来十年中，我们会看到机器学习技术进一步渗透众多行业和生活领域，推动这种能力进一步地快速提高。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;旧闻新炒&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如今所使用的许多人工智能和机器学习算法是几十年前发明的。国防机构使用高级机器人、自动驾驶车辆和无人机的时间已将近半个世纪。第一个虚拟现实原型开发于 20 世纪 60 年代。然而截至 2016 年底，没有一份主流出版物不对人工智能即将产生的社会影响发表高论。根据 CB Insights 的数据，对利用人工智能的创业公司的投资资金将于 2016 年达到 42 亿美元，仅仅四年就翻了 8 倍以上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;发生了哪些变化？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;影响因素有很多，但也有这样一个共识：最近的许多事态发展，比如近期谷歌翻译的巨大进展、谷歌 DeepMind 在围棋游戏中的胜利、亚马逊 Alexa 的自然会话接口以及特斯拉的自动驾驶功能，都由机器学习的进步所推动，更确切地说是深度学习神经网络——它是人工智能的一个分支。深度学习理论已经存在了几十年，但是它开始看到了新一轮的焦点，以及自 2010 年左右开始显著加快的进展速度。我们当下所看到的现象是一个雪球效应——深度学习在用例与行业中的影响——的开端。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;影响因素有很多，但也有这样一个共识：最近的许多事态发展，比如近期谷歌翻译的巨大进展、谷歌 DeepMind 在围棋游戏中的胜利、亚马逊 Alexa 的自然会话接口以及特斯拉的自动驾驶功能，都由机器学习的进步所推动，更确切地说是深度学习神经网络——它是人工智能的一个分支。深度学习理论已经存在了几十年，但是它开始看到了新一轮的焦点，以及自 2010 年左右开始显著加快的进展速度。我们当下所看到的现象是一个雪球效应——深度学习在用例与行业中的影响——的开端。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;机器和人类&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在某些类型的任务上，机器的表现长期以来一直优于人类，尤其是那些与计算速度和规模相关的任务。三位学院派经济学家（Ajay A. et al）在最近的一篇论文和哈佛商业评论上一篇文章假定最近机器学习的进展可以归为机器「预测」中的进展一类。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;机器的工作原理是机器使用了之前的苹果图像中的信息来预测当前的图像中是否有苹果。为什么会用『预测』这个词？预测使用的信息是你没有的但必须要生成的信息。机器学习使用的数据是从传感器、图像、视频、输入的注释、或者其他任何能被用比特/二进制（bit）表示的东西。这就是你拥有的信息，机器用这样的信息去填补它缺失的信息来识别物体，并预测下面会发生什么。这是你没有的信息。换句话说，机器学习是一种预测技术。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;完成任何的主要任务都涉及这几个要素：数据收集、预测、判断和行动。人类仍然在基于判断的任务（广义）上远超机器，而且 Ajay 等人假设这些任务的价值会随着机器学习带来的预测成本下降而增加。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;过去几年中，在深度学习的驱动下，虽然已经有了能够展示类似人类软技能的机器，机器在这些领域的能力几乎无法达到「预测」中的水平。下面是一些人类擅长的领域，让机器来模拟这些技能可能需要的新技术突破：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;学会学习&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：最近机器学习使用中一些惊人的成果包括，观察人类在多种实例任务中的行为（这种在手问题输入和输出的大数据集），同时「学习」使用深度神经网络方法。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;常识&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：人类擅长运用「常识」，即用一种不加开放思考或无需大数据集的通用方法来做出判断。在这个领域，除了在使用深度学习处理自然语言任务上有大进展外，机器相对来说还处在初步阶段。研究常识推理的科学家估计机器想要运用常识就需要其他新的技术进展。我们（或者我们的孩子）在和 Alexa 或 Siri 时都要面对这个问题。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;直觉和归零&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：人类大脑擅长直觉和归零，例如从一个非常大的复杂又模糊的选择集合中发现某个事实、想法或者行动过程。学界一直有人在尝试做将直觉带给机器的研究，但是在这个维度上的机器智能还普遍处于初级阶段。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;创造力&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：虽然有很多机器已经能生成一些和人类艺术大师的作品难以区分的作品，但它们在很大程度上还是基于学习这些大师已经创造出的模式。真正的创造力需要为问题生成之前从未见过的全新解决方案或真正创新的艺术成果。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;共情&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：理解情绪、价值系统、设置愿景、领导力和其他仍然还是人类专属的软能力。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;多功能&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：同样一个人可以合理地执行许多人物，比如拿起盒子、驾驶汽车去工作、带小孩和发表演讲。目前的机器和机器人都还是为特定的任务而打造的。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;IA 和 AI&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;根据以上的总结，我们可以得知：机器已经在学习（或者被称为「预测」）的技能方面取得了长足的进展，它们进入了模仿「人类」真正的技能的早期阶段。我们建议的分类方式是：将预测、第一阶段的机器学习以及需要人类参与的自动化功能（human-in-the-loop automation capabilities）归为「IA」技术。这些技术通常是使用机器独有的能力（处理巨大数据集的能力）来有效地增强人类能力，系统最终的输出通常还是由设计和训练它们的人来决定，因为系统设计者会提供一些与机器互补的技能。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从根源上讲，很容易把它与 AGI 弄混淆，所以我们使用了术语 AI 来描述我们在前面提到的机器拥有的那些属于人类的判断、学习和具备常识的能力以及具有先天创造力和同情心的特征。对于强大的 AGI 而言，这也许只是它的一部分，但是要实现复杂的工作流程的全自动化就需要具有大多数这些技能的机器。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;明确了这些概念以后，我们就可以知道如何对当前或者即将出现的那些可能会影响我们日常生活和工作的技术进行分类了：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9oYx5LnUGMhUE4SPcHDjTlnrj7DCiaLLticficpHcia8ToohhhSBice0CpmWJUTBYoyM5VRj2Yclq0Inw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;IA 和 AI 带来的影响&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有一个众所周知的谚语：「我们总是更倾向于在短期内高估技术产生的影响，而在长远上低估它」。这也被称为「阿马拉定律（Amara』s Law）」，人们经常用下图来表示它。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9oYx5LnUGMhUE4SPcHDjTlFy8cMfTy4YVjj2lVKHa90ibXTNXrrJc5aAjKMrkpSC9GHVb2wldDicgQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以在这个图上看到，这条曲线在任何一个轴上都没有刻度。对于曲线开始处的任何一点，我们不能准确地知道它距离拐点有多远。但是这条曲线确实说明了一个很重要的趋势——一项新技术的影响在其初始阶段十分缓慢，然后随着技术的发展和市场的大规模采用，该技术的影响显著变大，最终趋于饱和。人们对于市场预期通常都会忽视这个趋势。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然后，当进入了所谓的著名的「市场炒作周期」，人们对于技术初期影响的预期远远超过了技术的真实影响力，因此，人们就会陷入一种失望的境地。随着技术的影响继续扩大并且达到一种较大的规模之后，该技术会达到生产率的巅峰。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我相信我们今天所看到的有关于术语「AI」的那些重大的发展和认知其实是「IA」曲线的上升阶段，其中使用人工神经网络的深度学习（以及前面提到的一些驱动力如硬件、数据 、云经济学、连接性和其他算法上的进步）正在推动我们走向该曲线上拐点。在许多情况下，相对于回归（regression）和其他统计工具以及基于规则的系统和人工编码实现的逻辑等现有「预测」方法而言，深度学习进行了进一步的提升。机器学习通过提高模型精度，增加处理数据能力以及提高对输入的适应性而推动了发展的速度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于机器智能仍然存在上述限制，所以我认为全自动化技术的发展应该是一条全新的曲线。并且我相信我们还处于这条曲线的早期阶段。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9oYx5LnUGMhUE4SPcHDjTlyZE44mFiaKBBfBf0bOBMpz3cKLUbS3P0em0ibH4Ch7L5hqCdb4pOLhrg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;之前已经有多个和 AI 以及奇点（singularity）相关的并且预测不准确的炒作周期了。许多 AI 先驱在 20 世纪 50 年代的早期认为，具备人类所有能力的机器将在十年或二十年之内就出现。这个目标没有实现的原因不是因为没有足够强大的计算能力，而是在多个新的维度上的科学还没有突破。然而这种根本性突破的时间很难预测以及调整。根据斯蒂芬·霍金所言，截至到 2015 年，「人工智能研究员还不能明确什么时候可以建造出拥有或超过人类的 AI 机器。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可能正处于对于「AI」进行炒作的巅峰阶段。然而，IA（如同上边定义的那样）提供了一个 5 至 10 年的巨大的投资机会。人类和机器正处于一个互补的阶段，他们都有一些不同于对方的卓越的才能。这表明人类能够专注于他们独有的技能同时还可以享受锻炼的乐趣，而机器专注于处理大多数那些不需要人类的判断力，创造力和同情心等能力就能完成的常规任务。目前有很多文章已经写了关于 IA 技术将会引起工作和劳动的性质的变化，并且这些变化并不容易。这篇文章就很好地进行了总结：「你使用机器的能力将决定你未来的薪资。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;即时创新和投资机会在何处？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我相信智能增强技术（运用深度学习以及其他机器学习技术实现人工增强自动化）在中期阶段的影响比多数人认为的要大，而全自动化的影响则远远超出近期相关报道中指出的范围。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本文无意揣测人工通用智能（AGI）是否还远在十年或百年之后，亦非讨论其将成为对社会的威胁与否。我的立足点在于，你是否对正在进行的投资或者即将创立的公司亦或项目有着五至十年的大规模愿景。由机器学习推进的智能增强或人类增强技术具有立竿见影且显著的价值，况且，在这条商业和社会成功之路上鲜有阻力，可谓是一片坦途。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一如传统的 B2B 模式，我们寻找的方案是止痛药而不是维他命，不仅能够做到解决明显的现有痛点，展示其强劲的投资回报率，与现有的工作流程高度合拍，还能与企业中买方、用户和协调人三者利益一致。在这个领域，我保证有人参与的智能技术（智能增强）将有助于提高整体生产力、优化投资成本、提供个性化解决方案、或者助力为客户提供新的产品。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;机器学习技术正被应用于各大垂直产业的许多方面。这是一个关乎整体投资立论的全景话题（至少是独立于其中），但我们可以通过一个简明的列表来窥见一斑，看业内如何通过机器学习这一优势来增强人类自身的能力、提高生产力以及优化资源使用方式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;企业&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;——完成单调重复任务的机器人助手将更具功能性，并将在十年之内更加深入企业各个方面。使用增强技术实现的可穿戴式设备将有助于顺利完成危险或者成本高昂的工作。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;生产制造&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;——合作型的智能机器人可以安全地与人类共事，并且完成那些复杂高危或者重复性的劳动，从而提高生产效率。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;交通和运输&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;——各大科技公司和传统制造商们正在就自动驾驶汽车开发进行一场公开而激烈的角逐。而一般驾驶情况下的减少驾驶员工作量的相关技术在短期内其收效是更可预见的。比如：高速公路，降低因人工驾驶车辆造成的误操作率和交通意外，改善交通车流量及提高燃料使用效率。假以时日，完全自主的生态保障系统将改变都市生活结构，并随之带来更多衍生发展机会。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;医疗保健&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;——机器学习技术基于更为广泛的数据库，从而有助于医疗人员提供高精度，实现个性化诊疗。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;农业&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;——各类农用机器人、作物优化技术、自动灌溉技术以及虫害预警系统将有助于大幅提高农业生产力。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;到了术语更迭的时候了&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于人工智能的范畴已经远远超出了其在科技工业的既有领域而渗入到各大传统行业当中，它开始触及许多并不深谙人工智能科技相关术语的普罗大众。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们最好谨慎地使用「人工智能」这个术语。为避免混淆，减小不利趋势和监管的风险，以及更好地认知即将到来的术语更为丰富的时代，我们应该使用例如「智能增强」（IA）这类的术语来指代近期使用机器学习技术所取得的先进成果。我认为智能增强可以更好地阐释人类与机器的共生关系，而现有技术的影响力正取决于这种关系。之所以这样提，是因为我们不乏先例。随着机器变得越发无所不能，从前被认为需要智能的情形就会从人工智能的定义中清除。比如，光学字符识别（OCR）曾被认为是一种人工智能科技，但它如今已相当普遍，并不在人工智能考虑范围之内了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;还是把人工智能这个词用做描述全自动技术吧，那些我们已经论证过的，那些让我们纠于现状却不甚明朗的技术。而与此同时，我们更应该抓住因为智能增强的高歌猛进而带来的机会。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 27 Dec 2016 10:59:58 +0800</pubDate>
    </item>
    <item>
      <title>学界 | 并行运算，Facebook提出门控卷积神经网络的语言建模</title>
      <link>http://www.iwgc.cn/link/4083674</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自arxiv.org&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李泽南&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9oYx5LnUGMhUE4SPcHDjTlf9C8PqU7VDI62aiblX6mHvTuKZV0BqhCIib7tiafOdZ730VRMQoUb198A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;摘要&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前语言建模的主要方法都是基于循环神经网络的。在本研究中，我们提出了一种使用卷积方式处理的语言建模方式。我们引入了一种新的门控机制，可以缓和梯度传播，它的表现比 LSTM 方式的门控（Oord 等人，2016）更加优秀。我们的方法在 WikiText-103 上创造了新的最高纪录，同时我们也在 Google Billion Word 基准上进行了单 GPU 测试，结果创造了新的最快记录。因为可以并行运算，在对延迟敏感的任务中，我们的模型的速度相较其他模型提升了一个数量级。目前为止，这是第一次出现非训话方式在此类任务中超越了循环方式。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9oYx5LnUGMhUE4SPcHDjTl9flZWtYsia3EHYZGuj9pCZL9Eqb7uKo9l0AvT4yicrFrOcYZIiaBRibEZg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;用于语言建模的门控卷积网络架构&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;引言：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;统计语言模型被用于估算词序列的概率分布。这相当于给定一个词，对下一个词的概率进行建模，例如：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9oYx5LnUGMhUE4SPcHDjTl38d8ekO8agiaywJFicuBE08Peuic7q8hleHGK9OqHTkchEXrzkmN9vFxw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中 wi 是词汇表中的离散字索引。语言模型是语音识别系统（Yu&amp;amp;Deng，2014）以及机器翻译系统的关键组成部分（Koehn，2010）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;近年来，神经网络在此类任务的表现超过了 n 元语法模型（Kneser &amp;amp; Ney，1995；Chen &amp;amp; Goodman，1996）。经典的语言模型面临数据短缺的问题，无法准确表征长段语句，缺乏分析长范围从属关系的能力。神经语言模型通过在应用神经网络的连续空间中嵌入单词来解决这个问题。语言建模的当前技术水平基于长短期记忆网络（LSTM; Hochreiter 等人，1997），理论上可以建模任意长的从属关系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在本文中，我们介绍了门控卷积网络（gated convolutional networks）并将其应用于语言建模。卷积网络可以被堆叠以表示大的上下文尺寸，并且在具有在更大的上下文范围内提取分层更抽象的特征（LeCun＆Bengio，1995）。这种特性允许我们通过在大小 N 和内核宽度 k 的上下文上应用 O（N / k）运算来建模长期从属关系。相反，循环网络将输入视为链结构，因此需要线性数目 O（N）的操作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;输入分层的分析与类似于经典语法形式的构造相似，其构建了间隔增大的句法树结构。例如，由包含复杂内部结构的名词短语和动词短语组成的句子（Manning＆Schutze¨，1999；Steedman，2002）。另外，分层结构也简化了学习，因为相较于链结构，给定上下文大小的非线性的数量减少，从而减轻了消失梯度问题（Glorot＆Bengio，2010）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现代计算机硬件非常适合运行高度并行化的模型。在循环网络中，下一个输出取决于前一个的隐藏状态，它不启用对序列元素的并行化。卷积网络非常适合于此类计算，因为所有输入字的计算可以同时执行。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;门控已经显示出超越循环神经网络最快表现的潜力（Jozefowicz 等人，2016）。我们的门控线性单元通过为梯度提供线性路径，同时保留非线性能力，减少了深层架构的消失梯度问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们在单个 GPU 系统中进行了实验，证明了使用门控卷积网络的语言建模优于其他最近发布的语言模型，如在 Google Billion 上类似设置训练的 LSTM Word 基准（Chelba 等人，2013）。我们还评估了我们的模型分析 WikiText-103 基准中长距离从属关系的能力，其中该模型以整个段落而不是单个句子为条件进行处理，并且我们在此基础上实现了新的最快记录（Merity 等人，2016）。最后，我们展示了门控线性单元可以实现更高的精度和收敛，比 OST 等人的 LSTM 门控更快。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;点击阅读原文，下载此论文。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 27 Dec 2016 10:59:58 +0800</pubDate>
    </item>
    <item>
      <title>机器之心年度盘点 | 从技术角度，回顾2016年语音识别的发展</title>
      <link>http://www.iwgc.cn/link/4067617</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;得益于深度学习与人工神经网络的发展，语音识别在2016年取得了一系列突破性的进展，在产品应用上也越来越成熟。作为语音交互领域中极其重要的一环，语音识别一直是科技巨头研究的重点，国内外也涌现出了一批优秀的创业公司。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgLZmjFhKnsYxCMx43wia6QCE8x2TY4nVc5lghOw2ibtP8oC4hA9xAGQmJx4ibvnWgfjZRXuudpicJA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今年年初，机器之心发布来自 ACM 中文版的文章《&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=401894776&amp;amp;idx=1&amp;amp;sn=e963169d912f307104b97e0d7cd03cb7&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=401894776&amp;amp;idx=1&amp;amp;sn=e963169d912f307104b97e0d7cd03cb7&amp;amp;scene=21#wechat_redirect"&gt;深度 | 四十年的难题与荣耀——从历史视角看语音识别发展&lt;/a&gt;》，文中微软首席语音科学家黄学东为我们深入解读了语音识别的历史以及发展难题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;长久以来，人与机器交谈一直是人机交互领域内的一个梦想。语音识别做为很基础的技术在这一年中再次有了更大的发展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一年中，机器之心拜访过科大讯飞，接触过云知声、思必驰等创业公司，在微软的英语语音识别取得突破后更是深度专访了微软的黄学东、俞栋，不久之前的百度语音开发平台三周年的主题活动上我们也向百度首席科学家吴恩达了解过百度的语音识别发展。我们希望从机器之心文章中梳理出的线索，能为接下来语音识别的发展提供一丝洞见。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这篇文章中，我们会依次梳理 2016 年机器之心关注到的语音识别领域的突破性研究、未来待解决的难题、语音识别发展历史中较为重要的时间点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;一、2016 年语音识别有哪些突破？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这一部分盘点了 2016 年机器之心所关注到的在语音识别准确率上取得的突破，主要涉及的公司包括百度、IBM 和微软等。根据这些突破，我们梳理出了一条语音识别技术发展的线路。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720285&amp;amp;idx=2&amp;amp;sn=3308e3bcea1cdeb2eaee13c241081ad6&amp;amp;chksm=871b0c63b06c85751f06672a9d714f1b414f959e9129120d5d00532b72adeaf7cb9577a0989b&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720285&amp;amp;idx=2&amp;amp;sn=3308e3bcea1cdeb2eaee13c241081ad6&amp;amp;chksm=871b0c63b06c85751f06672a9d714f1b414f959e9129120d5d00532b72adeaf7cb9577a0989b&amp;amp;scene=21#wechat_redirect"&gt;百度 Deep Speech 2 的短语识别的词错率降到了 3.7%&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;发生时间：2016 年 2 月&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Deep Speech 2 于 2015 年 12 月首次发布时，首席科学家吴恩达表示其识别的精度已经超越了 Google Speech API、wit.ai、微软的 Bing Speech 和苹果的 Dictation 至少 10 个百分点。到今年 2 月份时，Deep Speech 2 的短语识别的词错率已经降到了 3.7%&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不久之前，百度又将 Deep CNN 应用于语音识别研究，使用了 VGGNet，以及包含 Residual 连接的深层 CNN 等结构，并将 LSTM 和 CTC 的端对端语音识别技术相结合，使得识别错误率相对下降了 10%（原错误率的 90%）以上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;据百度语音技术部识别技术负责人、Deep Speech 中文研发负责人李先刚博士介绍说，百度正在努力推进 Deep Speech 3，这项研究不排除将会是 Deep Speech 3 的核心组成部分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;技术提升基础：1. 端到端深度学习方法；2. 深层卷积神经网络技术（Deep CNN）应用于语音识别声学建模中，与基于长短时记忆单元（LSTM）和连接时序分类（CTC）的端对端语音识别技术相结合。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715201&amp;amp;idx=4&amp;amp;sn=4555a0f8732224da7055e120a3f9112f&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715201&amp;amp;idx=4&amp;amp;sn=4555a0f8732224da7055e120a3f9112f&amp;amp;scene=21#wechat_redirect"&gt;&lt;span&gt;2.IBM Watson 会话词错率低至 6.9%&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;发生时间：2016 年 5 月&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2015 年，IBM Watson 公布了英语会话语音识别领域的一个重大里程碑：系统在非常流行的评测基准 Switchboard 数据库中取得了 8% 的词错率（WER）。到了今年 5 月份，IBM Watson 团队再次宣布在同样的任务中他们的系统创造了 6.9% 的词错率新纪录。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;技术提升基础：声学和语言建模两方面技术的提高&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3. &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719135&amp;amp;idx=1&amp;amp;sn=012d179f83a6c3b38c6e58b4ac9ba82f&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719135&amp;amp;idx=1&amp;amp;sn=012d179f83a6c3b38c6e58b4ac9ba82f&amp;amp;scene=21#wechat_redirect"&gt;微软新系统英语语音识别词错率低至 6.3%&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;发生时间：2016 年 9 月&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在产业标准 Switchboard 语音识别任务上，微软研究者取得了产业中最低的 6.3% 的词错率（WER）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;技术提升基础：基于神经网络的声学和语言模型的发展，数个声学模型的结合，把 ResNet 用到语音识别。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4. &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719843&amp;amp;idx=1&amp;amp;sn=0c6387d422cf9765b9b10c178d160680&amp;amp;chksm=871b021db06c8b0b0b0447124c5c07818f53a17ba470305049af1d7d49806c87e07307a93811&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719843&amp;amp;idx=1&amp;amp;sn=0c6387d422cf9765b9b10c178d160680&amp;amp;chksm=871b021db06c8b0b0b0447124c5c07818f53a17ba470305049af1d7d49806c87e07307a93811&amp;amp;scene=21#wechat_redirect"&gt;微软英语语音识别词错率达到了 5.9%，媲美人类&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;发生时间：2016 年 10 月&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软人工智能与研究部门的团队报告出他们的语音识别系统实现了和专业速录员相当甚至更低的词错率（WER），达到了 5.9%。5.9% 的词错率已经等同于人速记同样一段对话的水平，而且这是目前行业标准 Switchboard 语音识别任务中的最低记录。这个里程碑意味着，一台计算机在识别对话中的词上第一次能和人类做得一样好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;技术提升基础：系统性地使用了卷积和 LSTM 神经网络，并结合了一个全新的空间平滑方法（spatial smoothing method）和 lattice-free MMI 声学训练。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然在准确率的突破上都给出了数字基准，但百度与微软、IBM（switchboard 上测试）有较大的不同。微软的研究更加学术，是在标准数据库——口语数据库 switchboard 上面完成的，这个数据库只有 2000 小时。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软研究院的研究关注点是基于 switchboard 数据库，语音识别最终能做到什么样的性能。而据百度语音识别技术负责人李先刚介绍，他们的关注点是语音技术能够深入到大家的日常应用中，他们用的数据长达数万小时。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;黄学东在之前接受机器之心专访时也表示他们的这个语音识别系统里面没有 bug，因为要在标准数据上做到这样的水平，实际上体现了工程的完美。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;就各项突破的技术提升基础，我们可以很明晰的梳理出一条线：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 之前 LSTM 这样的模型开始成功应用于语音识别，今年的后续研究不断提升 LSTM 的模型效果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 另外一个比较大的进展是 Deep CNN。Deep CNN 比起双向 LSTM（双向效果比较好）有一个好处——时延。所以在实时系统里会更倾向于用 Deep CNN 而不是双向 LSTM。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3. 端到端学习，这也是百度首席科学家吴恩达在 NIPS 2016 上重点提到的。比如语音识别，输入的是语音，输出的是文本，这是比较纯粹的端对端学习。但是它也有缺点——需要足够大的训练集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgLZmjFhKnsYxCMx43wia6akBDyVmB7nKrUQT75RTHEPiaLYxxh58MwNo1G60Uia9rRcK0ic7uvpQXg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图：吴恩达 NIPS 2016 ppt&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这方面现在的研究工作主要集中在两类模型上。一类就是 CTC 模型，包括 Johns Hopkins 大学的 Dan Povey 博士从 CTC 发展出来的 lattice-free MMI；还有一类是基于注意的序列到序列模型。今天它们的表现也还是比混合模型逊色，训练的稳定性也更差，但是这些模型有比较大的潜力（参考机器之心对俞栋老师的专访）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;国内还有其他几家做语音识别的公司，这里对科大讯飞、搜狗、云知声的语音识别系统做个简单介绍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;去年年底，科大讯飞提出了以前馈型序列记忆网络（FSMN, Feed-forward Sequential Memory Network）为代表的新一代语音识别系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今年，科大讯飞又推出了全新的深度全序列卷积神经网络（Deep Fully Convolutional Neural Network, DFCNN）语音识别框架，使用大量的卷积层直接对整句语音信号进行建模，更好的表达了语音的长时相关性。据介绍，该框架的表现比学术界和工业界最好的双向 RNN 语音识别系统识别率提升了 15% 以上。其结构图如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgLZmjFhKnsYxCMx43wia6mcI197HatX1ZiaFhhPJLXT2qVUS252FDPygYtGAibrIaoSiauw9sibI5sg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;科大讯飞 DFCNN 的结构图&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;同时，我也附上搜狗、云知声提供的各自的语音识别系统的流程，以供大家学习、比较、参考：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgLZmjFhKnsYxCMx43wia6RJnIPibw0pGnl1hVibzx3CqKWpqtKv5RgOOM0tXzrjicvlfTvcFP1dd2Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;语音识别系统流程：语音信号经过前端信号处理、端点检测等处理后，逐帧提取语音特征，传统的特征类型包括 MFCC、PLP、FBANK 等特征，提取好的特征送至解码器，在声学模型、语言模型以及发音词典的共同指导下，找到最为匹配的词序列作为识别结果输出。&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgLZmjFhKnsYxCMx43wia6EkZmUgf7ia0Su3RWibvHRmGYTeCbH9gubU8U6fIbPu7pL5ibYus93AetA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;搜狗 CNN 语音识别系统建模流程&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgLZmjFhKnsYxCMx43wia6Yt9qwlFDRZKbVNhlBMxxhlWm88qxyic39z7atJWXjLdCyaJ6rKrlvcg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;云知声语音识别系统&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;二、难题与前沿方向&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在语音识别高速发展的一年，我们曾专访过黄学东、俞栋等多位领域内的专家，不可避免的探讨了未来语音识别领域所面临的方向、挑战、抑或是难题。现如今整理如下，希望能对大家接下来的语音识别研究有所帮助：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1. 语义理解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;黄学东认为，要做好语音识别需要更好的语义理解，二者相辅相成。「人的鲁棒性非常好，一个新的课题过来，他可以通过会话的方式跟你沟通，也能得到很好的结果。而机器对噪音的抗噪性不够强，对新的课题会话沟通能力比较差。最重要的一点是，语音识别并没有理解你的语义。理解语义是人工智能下一个需要攻克的难题，这也是我们团队花很多时间和精力正在做的事情。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2. 值得关注的四大方向&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在之前机器之心对俞栋的专访中，他为我们指出了语音识别领域的几大前沿：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;在安静环境下并使用近距麦克风的场合，语音识别的识别率已越过了实用的门槛；但是在某些场景下效果还不是那么好，这就是我们这个领域的 frontier。现在大家主攻几点：&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;首先，是不是能够进一步提升在远场识别尤其是有人声干扰情况下的识别率。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;目前一般远场识别的错误率是近场识别错误率的两倍左右，所以在很多情况下语音识别系统还不尽如人意。远场识别至少目前还不能单靠后端的模型加强来解决。现在大家的研究集中在结合多通道信号处理（例如麦克风阵列）和后端处理从拾音源头到识别系统全程优化来增强整个系统的表现。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;另外，大家还在研究更好的识别算法。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;这个「更好」有几个方面：一个方面是能不能更简单。&lt;/span&gt;&lt;/em&gt;&lt;em&gt;&lt;span&gt;现在的模型训练过程还是比较复杂的&lt;/span&gt;&lt;/em&gt;&lt;em&gt;&lt;span&gt;，需要经过很多步骤。如果没有 HTK 和 Kaldi 这样的开源软件和 recipe 的话，很多团队都要用很长时间才能搭建一个还 OK 的系统，即使 DNN 的使用已经大幅降低了门槛。现在因为有了开源软件和 recipe，包括像 CNTK 这样的深度学习工具包，事情已经容易多了，但还有继续简化的空间。这方面有很多的工作正在做，包括如何才能不需要 alignment 、或者不需要 dictionary。现在的研究主要还是基于 end-to-end 的方法，就是把中间的一些以前需要人工做的步骤或者需要预处理的部分去掉。虽然目前效果还不能超越传统的 hybrid system，但是已经接近 hybrid system 的 performance 了。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;另外一个方面，最近的几年里大家已经从一开始使用简单的 DNN 发展到了后来相对复杂的 LSTM 和 Deep CNN 这样的模型。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;但在很多情况下这些模型表现得还不够好。所以一个研究方向是寻找一些特殊的网络结构能够把我们想要 model 的那些东西都放在里面。我们之前做过一些尝试，比如说人在跟另外一个人对话的过程中，他会一直做 prediction，这个 prediction 包括很多东西，不单是包括你下一句想要说什么话，还包括根据你的口音来判断你下面说的话会是怎样等等。我们曾尝试把这些现象建在模型里以期提升识别性能。很多的研究人员也在往这个方向走。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;还有一个方向是快速自适应的方法—就是快速的不需要人工干预的自适应方法（unsupervised adaptation）。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;现在虽然已经有一些自适应的算法了，但是它们相对来说自适应的速度比较慢，或者需要较多的数据。有没有办法做到更快的自适应？就好像第一次跟一个口音很重的人说话的时候，你可能开始听不懂，但两三句话后你就可以听懂了。大家也在寻找像这种非常快还能够保证良好性能的自适应方法。快速自适应从实用的角度来讲还是蛮重要的。因为自适应确实在很多情况下能够提升识别率。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;三、语音识别历史的梳理&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这一部分我简单的梳理了一下语音识别历史上比较关键的一些时间点，至于详细的语音识别技术研究历史可参考之前提到的黄学东老师写的《四十年的难题与荣耀——从历史视角看语音识别发展》。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;1952 年，贝尔实验室 Davis 等人研制了世界上第一个能识别 10 个英文数字发音的实验系统，但只能识别一人的发音。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;1962 年，IBM 展示了 Shoebox。Shoebox 能理解 16 个口语单词以及 0-9 的英文数字。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;1969 年，贝尔实验室的 John Pierce 预言成熟的语音识别在数十年内不会成为现实，因为它需要人工智能。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;1970 年，普林斯顿大学的 Lenny Baum 发明隐马尔可夫模型（Hidden Markov Model)。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;20 世纪 70 年代，卡耐基梅隆大学研发 harpy speech recognition system，能够识别 1011 个单词，相当于 3 岁儿童的词汇量。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;20 世纪 80 年代，语音识别引入了隐马尔可夫模型（Hidden Markov Model)。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;20 世纪 90 年代出现首个消费级产品 DragonDictate，由国际语音识别公司 Nuance 发布。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;2007 年，Dag Kittlaus 和 Adam Cheyer 创立 Siri.Inc。后被苹果收购并于 2011 年首次出现在 iPhone 4s 上。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;2009 年以来，借助机器学习领域深度学习研究的发展以及大数据语料的积累，语音识别技术得到突飞猛进的发展。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;2011 年微软率先取得突破，使用深度神经网络模型之后，语音识别错误率降低 30%。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;2015 年，IBM Watson 公布了英语会话语音识别领域的一个重大里程碑：系统在非常流行的评测基准 Switchboard 数据库中取得了 8% 的词错率（WER）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;语音识别，在这一年有了极大的发展，从算法到模型都有了质的变化，在加上语音领域（语音合成等）的其他研究，语音技术陆续进入工业、家庭机器人、通信、车载导航等各个领域中。当有一天，机器能够真正「理解」人类语言，并作出回应，那时我们必将迎来一个崭新的时代。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;strong&gt;拓展阅读：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=401894776&amp;amp;idx=1&amp;amp;sn=e963169d912f307104b97e0d7cd03cb7&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=401894776&amp;amp;idx=1&amp;amp;sn=e963169d912f307104b97e0d7cd03cb7&amp;amp;scene=21#wechat_redirect" style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;深度 | 四十年的难题与荣耀——从历史视角看语音识别发展&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719843&amp;amp;idx=2&amp;amp;sn=4611f810734df8a72286567e90c65a92&amp;amp;chksm=871b021db06c8b0b283ac93f267d95365392be02b3cde5d2fe6df85b359aa96368f25318e2f5&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719843&amp;amp;idx=2&amp;amp;sn=4611f810734df8a72286567e90c65a92&amp;amp;chksm=871b021db06c8b0b283ac93f267d95365392be02b3cde5d2fe6df85b359aa96368f25318e2f5&amp;amp;scene=21#wechat_redirect" style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;独家 | 专访微软首席语音科学家黄学东： CNTK 是词错率仅 5.9% 背后的「秘密武器」&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720189&amp;amp;idx=1&amp;amp;sn=10a3630e7f65d7b845c1fd032970d46e&amp;amp;chksm=871b03c3b06c8ad527a7dffd215be5b128a7aaf88ec09a131d68a249e9ff77c3edff6204d3d2&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720189&amp;amp;idx=1&amp;amp;sn=10a3630e7f65d7b845c1fd032970d46e&amp;amp;chksm=871b03c3b06c8ad527a7dffd215be5b128a7aaf88ec09a131d68a249e9ff77c3edff6204d3d2&amp;amp;scene=21#wechat_redirect" style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;专访 | 顶级语音专家、MSR首席研究员俞栋：语音识别的四大前沿研究&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720285&amp;amp;idx=2&amp;amp;sn=3308e3bcea1cdeb2eaee13c241081ad6&amp;amp;chksm=871b0c63b06c85751f06672a9d714f1b414f959e9129120d5d00532b72adeaf7cb9577a0989b&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720285&amp;amp;idx=2&amp;amp;sn=3308e3bcea1cdeb2eaee13c241081ad6&amp;amp;chksm=871b0c63b06c85751f06672a9d714f1b414f959e9129120d5d00532b72adeaf7cb9577a0989b&amp;amp;scene=21#wechat_redirect" style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;专访｜百度语音识别技术负责人李先刚：如何利用Deep CNN大幅提升识别准确率？&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719135&amp;amp;idx=1&amp;amp;sn=012d179f83a6c3b38c6e58b4ac9ba82f&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719135&amp;amp;idx=1&amp;amp;sn=012d179f83a6c3b38c6e58b4ac9ba82f&amp;amp;scene=21#wechat_redirect" style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;重磅 | 语音识别新里程碑：微软新系统词错率低至6.3%（附论文）&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719843&amp;amp;idx=1&amp;amp;sn=0c6387d422cf9765b9b10c178d160680&amp;amp;chksm=871b021db06c8b0b0b0447124c5c07818f53a17ba470305049af1d7d49806c87e07307a93811&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719843&amp;amp;idx=1&amp;amp;sn=0c6387d422cf9765b9b10c178d160680&amp;amp;chksm=871b021db06c8b0b0b0447124c5c07818f53a17ba470305049af1d7d49806c87e07307a93811&amp;amp;scene=21#wechat_redirect" style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;重磅 | 微软语音识别实现历史性突破：语音转录达到专业速录员水平（附论文）&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715201&amp;amp;idx=4&amp;amp;sn=4555a0f8732224da7055e120a3f9112f&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715201&amp;amp;idx=4&amp;amp;sn=4555a0f8732224da7055e120a3f9112f&amp;amp;scene=21#wechat_redirect" style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;公司｜IBM Watson 团队取得语音识别新突破，会话词错率低至 6.9%&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720050&amp;amp;idx=1&amp;amp;sn=da2e46b031173adc25d16a68e2fcb54b&amp;amp;chksm=871b034cb06c8a5a1984b1158e83e94095ce12aff6fd0fe42126429e6e942aa786714c904589&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720050&amp;amp;idx=1&amp;amp;sn=da2e46b031173adc25d16a68e2fcb54b&amp;amp;chksm=871b034cb06c8a5a1984b1158e83e94095ce12aff6fd0fe42126429e6e942aa786714c904589&amp;amp;scene=21#wechat_redirect" style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;深度 | 在语音识别这件事上，汉语比英语早一年超越人类水平（附论文）&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718578&amp;amp;idx=4&amp;amp;sn=05badbcd6230247e75b57e3df6a8e131&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718578&amp;amp;idx=4&amp;amp;sn=05badbcd6230247e75b57e3df6a8e131&amp;amp;scene=21#wechat_redirect" style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;业界 | 语音识别软件太傻？斯坦福研究表明语音输入比打字快三倍、准确率更高&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心原创，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 26 Dec 2016 13:09:18 +0800</pubDate>
    </item>
    <item>
      <title>公告 | 对不起，机器之心在连续更新1000天之后……</title>
      <link>http://www.iwgc.cn/link/4054023</link>
      <description>&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8AXdZ3Y3GvExKagAzu3Juibxl4MIXv3L6R8wYrSb79dxoW3J0J0sXTShnJBic9rbRfbfzOnqm5WBoQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8AXdZ3Y3GvExKagAzu3JuibLh15sJ4GZH0kaTkTfDFqia0EMvmFI2chkdnoEVyYHr2y2iaupEHBZicpw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 25 Dec 2016 00:04:33 +0800</pubDate>
    </item>
    <item>
      <title>重磅论文 | 机器学习硬件概览：从算法到架构的挑战与机遇</title>
      <link>http://www.iwgc.cn/link/4048022</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自arXiv&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;近日，MIT 发表一篇论文，从架构（GPU、CPU、FPGA）到算法概述机器学习硬件研究中的机遇与挑战。在人工智能硬件火热的今天，这是一篇不可错过的综述性文章。点击阅读原文可下载此论文。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8AXdZ3Y3GvExKagAzu3Juib8OmczjKp1TY6lGmBgB6m3V9p1qBBNQic7GM6Gbvia803ElrlpqRiaTDaQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：机器学习在从传感器每天收集的大量数据中提取有用信息上发挥着非常重要的作用。在一些应用上，目的是为了分析并理解数据，从而辨清发展趋势（例如，监控、便携式／穿戴式电子设备）。在其他应用中，分析数据的目的是为了能够基于数据快速作出应对（例如，机器人/无人机、自动驾驶汽车、物联网）。对这些应用而言，出于对隐私、安全的考虑，再加上通信带宽的限制，在传感器附近的本地嵌入式处理要比上传到云更好。然而，在传感器端的处理有能耗与成本的限制，还有生产能力与准确率的要求。此外，也需要适应性，以便于传感器适应于不同的应用或环境（例如，在分类器上升级权重与模型）。在许多应用中，机器学习总是涉及到将输入数据转换到更高维度的空间，这伴随着可编程权重、增加数据传输以及最终的能量消耗方面的问题。在此论文中，我们将探讨如何在各种级别的硬件设计上解决这些问题：架构、硬件友好的算法、混合信号线路和高级技术（包括内存与传感器）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;一、 导语&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在是大数据时代。过去两年创造的数据要多于人类历史上所创造的所有数据。这主要是由于传感器（2013 年平均为 100 亿个，预期到 2020 年达到 1 万亿个）和连接设备（2016 年为 64 亿个，预期在 2020 年达到 208 亿个）的使用。这些传感器和设备每年生成数百泽字节（zatabyte）的数据，每秒生成拍字节（petabyte）的数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们需要机器学习从这些数据中提取有用的、可理想地实施的信息。分析数据所需的大量数据分析经常是在云中做的。然而，在数据生成量与生成速度如此大的情况下，再加上通信的高能耗和宽带的限制，在传感器附近本地完成分析的需求越来越大，而非将原始数据发送到云中。在这些边缘地带嵌入机器学习也解决了对隐私、潜在安全性的担忧。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;二、应用&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从多媒体到医疗领域（medical space），许多应用都能从嵌入机器学习中受益。我们会提供几个研究领域的样本；不过，这篇论文主要关注的是计算机视觉，特别是图像分类，作为推进案例。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8AXdZ3Y3GvExKagAzu3Juibz0wSvfKfZUNKicBAWMErZynLuvJEhkfKSl4ZRslvRm8v7gCnun7DY2A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 1：图像分类&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;A.计算机视觉&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;视频可能是最大的大数据。约占今天互联网流量的 70%。比如，全世界每天收集起来、需要审查的视频达 8 亿小时。在许多应用（比如，测量商店、交通模式下的等待时间）中，使用计算机视觉从图像传感器上（而不是云端）的视频中提取有意义的信息是极好的，能减少通信成本。就其他一些应用（比如自动驾驶车辆、无人机导航和机器人技术）来说，会需要本地化处理，因为依赖云端会有很大安全风险，还会有延迟的问题。不过，视频包括大量数据，处理起来，计算会很复杂，因此，分析视频的低成本硬件就成了让这些应用得以实现的关键。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在计算机视觉方面，有许多不同的人工智能任务。本文聚焦图像分类（如表一所示），有图像，任务就是判定图像目标类别。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;B. 语音识别&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;语音识别显著改善了人类与设备的互动，比如智能手机。尽管目前绝大多数应用程序，比如苹果 Siri 和亚马逊的 Alexa 语音服务的处理位于云端，但是，在设备上面执行识别任务更理想，因为可以减少延迟和对连接的依赖，并且能增强隐私。语音识别是实现机器翻译、自然语言处理等很多其他人工智能任务的第一步。人们在研究用于语音识别的低功率硬件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;C. 医学&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;临床医学非常看重对病人的监测，收集长期数据帮助侦测/诊断各种疾病或者监督治疗。比如，持续的 ECG 或 EEG 信号监测将有助于识别心血管疾病，检测癫痫患者的发作。在许多情况下，这些设备要么是穿戴式的，要么是可移植的，因此，能耗必须维持在最低。所以，需要探索使用嵌入机器学习提取有意义的生理信号进行本地化处理的办法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;三、机器学习基础&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个典型机器学习的推论流程可以分为两步（如图 2）：特征提取和分类。有些方法，比如深度神经网络（DNN）就模糊了步骤之间的差别。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8AXdZ3Y3GvExKagAzu3JuibZL256jZDhzMZxYPSX2BlSS7Q00bQ6nOQJ3VO4l2T47HSZp20VsHWqA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 2：推理流程&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;A. 特征提取&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;特征提取是将原始数据转化为对给定任务有意义的输入。传统意义上，特征提取是通过该领域的专家手工进行标注而设计的。例如，计算机视觉中的对象识别就是根据观察到人类视觉是对图像边缘（如梯度）具有敏感性而设计的。因此，许多众所周知的计算机视觉算法 Histogram of Oriented Gradients (HOG) 和 Scale Invariant Feature Transform (SIFT) 使用的就是基于图像梯度的特征。设计这些特征的挑战就是保证他们在过曝或噪点的情况下保持鲁棒性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;B. 分类算法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;特征提取的输出由向量表示，并且用分类器将这个过程映射到一个得分上。根据应用，分类权重可以和阀值比较来决定对象是否是当前的，或者也可以和其他得分相比较决定对象的分类。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;分类算法技术上通常使用支持向量机（SVM）和 Softmax 回归等线性方法，还有核函数支持向量机（kernel-SVM）和 Adaboost 等非线性方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;C. 深度神经网络（DNN）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度神经网络不需要使用手工标注的特征，它能从数据中直接学习到这些特征，这一点和分类器中的权重分配很相似，这样整个系统就是从端到端训练。这种自动学习特征的方式在机器学习中很流行，我们称它为深度神经网络（DNN），也就是常说的深度学习。在很多任务上，深度神经网络通过将输入数据映射到高维空间，这种方式在很多任务上实现的精度要比手工提取的特征精确得多，然而，代价就是高度计算复杂性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现存有许多深度神经网络架构（如卷积神经网络和循环神经网络等）。对于计算机视觉的应用，深度神经网络由多个卷积层（CONV）组成（如图 3）。每层将输入数据抽象到更高一层，称之为特征映射，这种更高层的抽象被提取以保留重要而独特的信息。现代深度神经网络能通过采用非常深的层级来实现优越的性能。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8AXdZ3Y3GvExKagAzu3Juibs4y9UkrpP2xwIel94icPbvMMCvuibwKMnCxnN7nHQxHYmzjSrVLqr9SA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 3：深度神经网络由多个卷积层组成，后面跟着全连接层&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8AXdZ3Y3GvExKagAzu3JuibibpPZN7zWkQwyDYNichx9dI7KH9GlfseJhnbMmByG3rdlXdEYkZgKoTw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;图.4 DNN 中某个卷积的计算&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;表 1 比较了现代深度神经网络和 20 世纪 90 年代流行的神经网络的层级数（深度）、滤波器的权重数量、操作数量（如 MACs）。如今的深度神经网络在计算和储存上都领先几个数量级&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8AXdZ3Y3GvExKagAzu3JuibArRqJajEpKZib5Z5jdfYvweaLNCvQZ6hVogsuc3eyqPK3scxdoRVw8w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;表 1：流行的 CNN 的总结 [21, 22, 25, 27, 28]. 基于 ImageNet top-5error 的精度测量 [19]&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;D. 任务的复杂性 VS 难度&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当比较不同的机器学习方法时，重要的是考虑任务的难度。例如，使用 MNIST 数据集进行手写数字分类的任务就比使用 ImageNet 数据集进行 1000 类物品分类简单得多。所以我们预期分类器或网络的大小（如权重数量）和 MACs 的数量在更难的任务中要多一些，也因此需要更多的能。例如，LeNet-5 被设计用来进行数字分类，而 AlexNet、VGG-16，GoogLeNet 和 ResNet 被设计用于进行 1000 类图像的分类任务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;四、挑战&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;嵌入式机器学习的关键指标是精确度、能耗、吞吐量/延迟性以及成本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;机器学习算法的精确性要在充足的大型数据组上进行测量。有许多广泛使用、公开使用的数据组可供研究人员使用（比如，ImageNet）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;可编程性很重要，因为环境或应用变了后，权重也要更新。在 DNN 的案例中，处理器必须能够支持层数不同、滤波器以及通道大小不一的不同网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;可编程性性的需求会增加数据计算和数据传送。更高的维度会增加生成的数据量，而且可编程性意味着需要读取并保存权重。这就对能效提出了挑战，因为数据传输要比计算更耗费成本。本文中，我们会讨论减少数据传输以最小化能耗的不同方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;吞吐量取决于计算量，它也会随着数据维度的增加而增加。本文中，我们会讨论变换数据以减少所需操作数量的各种办法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;成本在于芯片上所需的存储量。本文中，我们会讨论减少存储成本的各种办法，在芯片面积缩减的同时维持低芯片外存储带宽。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，训练需要大量标签数据（特别是 DNNs）和计算（计算反向传播的多次迭代，判定权重值）。有人正在研究使用 CPU、GPU、 FPGA 和 ASIC 在云端进行训练。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不过，这超出了本文范围。目前，最先进的 DNNs 所耗费的能量比其他形式的嵌入处理（比如视频压缩）要高出几个数量级。我们必须利用多种硬件设计所带来的机遇，解决所有这些问题并减少能耗鸿沟。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;五、 结构中的机遇&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8AXdZ3Y3GvExKagAzu3JuiboVvMNq45jSJMTM8wNXn780hUvUMTib2WGh3oXeHxAdwh5HeRVGMzR4A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 6：高度并行的计算范式&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;A. CPU 和 GPU 平台&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CPU 和 GPU 使用时间架构（比如 SIMD 或 SIMT）来并行执行 MAC。所有的 ALU 都共享同一个控制和存储（寄存器文件）。在这些平台上，所有的分类都由一个矩阵乘法表征。深度神经网络中的卷积层也能够用 Toeplitz 矩阵映射到一个矩阵乘法上。有专为 CPU 和 GPU 设计的软件库能用来优化矩阵乘法。该矩阵乘法按照更高层上的几兆字节的顺序平铺到这些平台的存储层次结构上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;B. 加速器（Accelerators）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;加速器提供了优化数据传输（比如数据流）以最小化来自昂贵的分级存储器体系（如图 7）访问。特别是，对于 DNNs，我们调查了采用了三种数据再使用形式的数据流（卷积、滤波器和图像）。我们采用了一种空间结构（图 6），每个 ALU 处理元素（PE）带有本地存储（大约 0.5-1.0KB) 以及一个共享存储器（全局缓冲器），近 100-500KB。全局缓冲器与芯片外存储器（比如 DRAM）通讯。可以在使用了一个 NoC 的 PEs 之间进行数据传输，以减少对全局缓冲器以及芯片外存储器的访问。三种类型的数据传输包括输入像素、滤波器权重和部分和（即像素和权重的乘积），它们被累积起来用于输出。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8AXdZ3Y3GvExKagAzu3Juibiao2cKRJnD7T5QklCNPMyhuv4JC049XeVnEjD21DvW4g9ibQqEliaa30Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;图 7. 分层级存储器和数据传输能耗&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;近期研究已经提出了一种 DNNs 加速方案，不过，很难直接比较因为实现和设计选择不同所导致的表现上的差异。图 8 可被用于分类现有的基于各自数据处理特征的 DNN 数据流：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8AXdZ3Y3GvExKagAzu3JuibgBCDicxw20ZcsGTNib3iapicGSS4ABxcCfQrNZOGEGYEmLXPYBTOdMhzGg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 8：DNNs 数据流&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;权重固定（Weight stationary，WS）：在该 PE 上，权重存储在 register file 中，并且保持平稳，以尽量减少权重移动成本（图.7(a)）。输入与局部和必须通过空间阵列和全局缓存。可看 [36-41] 中的例子。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;输出固定（Output stationary，OS）：在该 PE 上，输出存储在 register file 中，并且保持平稳，以尽量减少局部和的移动成本（图.7(b)）。输入与权重必须通过空间阵列和全局缓存。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本地不重用（No local reuse,NLR）:从能量（(pJ/bit)）的角度看虽然小的 register file 有效率，但是从区域（(µm2 /bit)）来说，它们的效率就不高了。为了最大化存储功用，同时最小化片外存储器带宽，没有将本地存储分配给 PE，而是将所有区域分配给全局缓冲区以增加其容量（图.7(c)）。代价是会增加空间阵列上的流量及对于所有数据类型的全局缓冲。参见 [45-47] 中的例子。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;行固定（Row stationary，RS）：为了增加所有类型数据（权重、像素、局部和）的 reuse，提出了一个行固定的方法 [35]。一行滤波器卷积保持固定在一个 PE 内，利用 PE 发掘 1-D 的 reuse。多个 1-D 行被结合在空间阵列上去彻底利用卷积 reuse（图.9），这会减少访问全局缓冲区。不同的信道和滤波器中的多个 1-D 的行被映射到每个 PE，以此来减少局部和数据传输并分别的利用过滤器 reuse。最后，跨阵列多通道允许额外的图像和过滤器 reuse 使用全局缓冲。这个数据流展示在 [48] 中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8AXdZ3Y3GvExKagAzu3Juib90SnBxoRfddpDH5VsNjpS3DlBO7GHXibXXyPzlR42glTXgVvQfDEGJw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图.9 行固定数据流&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在具有相同数量的 PE（256）、区域成本（area cost）和深度神经网络（AlexNet）的空间阵列上比较数据流。图 10 展示了每种方法的能耗。行固定法（The row stationary approach）比其他卷积层数据流处理方法要节能 1.4 倍到 2.5 倍，这还是基于所有数据类型都更节能的事实上考虑的。此外，启动芯片和关闭芯片的能量也考虑了进来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8AXdZ3Y3GvExKagAzu3JuibRsO0NhVDxh5eJibiaNgJJgwvPjiblzGCMo7J1wvBybibAd98A2bDgLdNBw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图.10&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;六、算法与硬件联合设计中的机会&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在修改机器学习算法以使它们对硬件更友好同时还维持准确度方面，研究界一直有相关的研究工作；其中尤其值得关注的焦点是在减少计算量、数据传输和存储要求方面。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;A 降低精度&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CPU 和 GPU 等可编程平台的默认大小通常是 32 或 64 位的浮点数表示。尽管这仍然是训练方面的情况，但在推理过程中，使用定点数表示（fixed-point representation）是可能的，这可以减少位宽（bitwidth），从而降低能源消耗和设备尺寸，并且还能增加吞吐量。当把权重和特征推至更低的位宽时，为了保持准确度，通常还需要再训练（retraining）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在人工设计的方法中，位宽可以在不影响准确度的条件下大幅降低到 16 位以下。比如，在使用 HOG 的物体检测中，每 36 维特征向量仅需要每维度 9 位，而 SVM 的每个权重仅使用 4 位 [49]；对于使用可变形组件模型（DPM/deformable parts model）[55] 的物体检测而言，每个特征向量仅需要 11 位，每个 SVM 权重仅需要 5 位 [51]。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;类似地，对于 DNN 推理，加速器支持 16 位定点数是很常见的 [46,48]。在探索位宽对准确度的影响上有一些显著的研究工作 [52]。事实上，有报道称最新一款用于 DNN 的商业硬件支持 8 位整型运算 [53]。因为位宽可能会随层变化，研究者已经在探索使用这种位宽减少来实现硬件优化——相比于一个 16 位的定点的实现，他们得到了 2.56 倍的节能 [54] 或 2.24 倍的吞吐量增长 [55]。通过给网络进行更大的修改，有可能将权重 [56] 或权重及激活（activation）[57,58] 的位宽降低到 1 位，而在准确度上有所损失。硬件上 1 位权重的影响在 [59] 中有所探索。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;B 稀疏性（Sparsity）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于 SVM 分类，其权重可以被投射到一个基础上，从而使得结果得到的权重是稀疏的，乘法数量减少了 2 倍 [51]（图 11）。对于特征提取而言，输入图像可以通过预处理被做得稀疏，可以实现 24% 的功耗减少 [49]。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8AXdZ3Y3GvExKagAzu3Juibk9icsDWAqcxiaTUcL5gDTcuXM4PTbjyKcVjukdiaMf2771icwt3p0YvhkQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 11：基础投射（basis projection）后的稀疏权重 [51]&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于 DNN 而言，MAC 和权重的数量的减少可以通过一个被称为剪枝（pruning）的过程来移除权重而实现。[60] 首先探索了这个方面，其中对输出有最小影响的权重被移除了。在 [61] 中，剪枝被用在了现代 DNN 上来移除小权重。但是，移除权重并不一定会实现更低的功耗。因此，在 [62] 中，权重的移除方式是基于一个能量模型（energy model），从而可以直接最小化能量消耗。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;[48, 51, 63, 64] 提出的专用硬件利用了稀疏权重来提升速度或减少能量消耗。在 Eyeriss [48] 中，处理元件被设计成：当输入为 0 时，直接跳过读取和 MAC，最后实现了 45% 的节能。在 [51] 中，通过使用专用硬件来规避（avoid）稀疏权重，能量和存储成本分别被减少了 43% 和 34%。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;C. 压缩&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;数据的传输和存储在能耗和成本方面都是一个很重要的因素。特征提取可以得到稀疏的数据（比如 HOG 中的梯度和 DNN 中的 ReLU），而在分类中所使用的权重也可以通过剪枝稀疏化。这样所得到的结果是：压缩可以利用数据统计来减少数据传输和存储的成本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;研究者已经探索利用了多种形式的轻量级压缩来减少数据传输。无损压缩可以被用于减少流入和流出芯片的数据传输 [11, 54, 64]。在 [65] 中，简单游程长度编码（simple run-length coding）减少了多达 1.9 倍的带宽，这是在理论上的熵限制的 5%-10%。向量量化（vector quantization）等有损压缩也可被用于特征向量 [51] 和权重 [8, 12, 66]，这样使得它们可以以较低的成本而存储在芯片上。一般而言，压缩/解压的成本是在几千 kgates 的量级上，具有最小的能量开销。在有损压缩的情况下，估计其对表现的准确度的影响是很重要的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;七、混合信号电路中的机会&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大部分数据传输都发生在内存和处理元件（PE/processing element）以及传感器和处理元件之间。在这一章节，我们将讨论这可以如何通过混合设计的电路设计来解决。但是，电路的非理想因素（circuit non-idealities）也应该被考虑到算法设计中；这些电路可以从第六节中讨论的精度减少的算法中受益。除此之外，因为训练通常是通过数字（digital）的方式进行的，ADC 和 DAC 的开销应该在系统评估时被考虑进来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管空间架构的改进让存储和计算的位置更近了（即集成到处理元件中），但要将计算和存储本身整合到一起，还需要一些努力。比如说，在 [67] 中，分类被嵌入到了 SRAM 中。特别地，其字线（WL/wordline）是由一个使用了一个 DAC 的 5 位特征向量驱动的，同时其位单元（bit-cells）存储了二元权重 ±1。位单元的电流实际上是特征向量的值和存储在位单元中的权重值的乘积；其来自列的电流被加到一起以对位线放电（BL 或 BLB/bitline）。然后一个比较器被用于比较结果得到的点积和一个阈值——特别是差分位线的符号阈值（sign thresholding）。因为位单元的变化，这可被认为是一个弱分类器，而且需要 boosting 来将这些弱分类器组合起来形成一个强分类器 [68]。这种方法比从 SRAM 进行 1 位权重读取要节能 12 倍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最近的研究工作也提出了使用混合信号电路来减少 MAC 的计算成本。[69] 表明，使用开关电容器来执行 MAC 可以比数字电路能效更好，不管是在 ADC 还是 DAC 方面。因此，矩阵乘法可以像 [70] 提出的那样被整合到 ADC 中，其中用于 Adaboost 分类的乘法中最显著的部分是使用开关电容器以 8 位逐次逼近格式（8-bit successive approximation format）执行的。这在 [71] 中进行了扩展，使其不仅可以执行乘法，还能在模拟域（analog domain）上进行累加。据估计，3 位和 6 位就足以分别表征权重和输入向量了。这能让计算更接近传感器，并可以将 ADC 转换的数量减少 21 倍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;要进一步减少来自传感器的数据传输，[72] 提出在传感器的模拟域中执行整个卷积层（包括卷积、最大池化和量化）。类似地，在 [73] 中，整个 HOG 特征可以在模拟域中计算，可将传感器带宽减少 96.5%.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;八、高级技术中的机遇&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在此章节，我们将讨论如何使用高级技术取得上一章节所说的数据传输问题。在参考文献 [47] 和 [74] 中分别提到的使用 embedded DRAM (eDRAM) 和 Hyper Memory Cube (HMC) 这样的高级存储技术来减少 DNN 中权重的能量访问成本（energy access cost）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在直接将乘法（multiplication）集成到高级非易失性存储上已经有了大量研究，使用他们作为电阻元件。具体执行乘法时，其中电导设为权重，电压作为输入，电流作为输出。其他工作就是把克希霍夫电流定律的电流值合计起来。在 [75] 中，忆阻器（memristor）被用于一个 16 位的点积运算的计算，其中 8 个忆阻器每个存储 2 位；每个忆阻器执行 1 位 X2 位的乘法计算，那么 16 位的输入需要 16 次循环来完成。在 [76] 中，ReRAM 用于计算 3 位输入和 4 位加权的乘积。与混合信号电路类似，其运算的精确都很有限，同时必须考虑把在 ADC 和 DAC 的转换开销（conversion overhead）计入总成本，特别是当在数字域中训练权重时。可以通过直接在模拟域中训练来避免转换开销，如 [77] 中制造的忆阻器阵列那样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，将计算嵌入传感器本身或许具有可行性。对于从传感器读取数据的带宽占了大部分系统能耗的图像处理来说部分有用。比如，一个 ASP 传感器能被用于计算输入地图，随着压缩而以十倍的比例减少传感器带宽。一个输出梯度带宽也能减少计算和后续处理引擎的能耗。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;九、手工提取的特征 VS 机器学习的特征&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相比于机器学习的特征，如通过 DNN 学习的特征，手工提取特征的方法有更高的能量效率是以牺牲准确率为代价的。对于手工提取的特征来说，其计算量更少并且支持位宽减少。不仅如此，手工提取的方法需要更少的数据传输，因为特征的权重值不是必须的。两种方法的分类权重都是可程控的。图 12 中比较了 HOG 特征提取与 AlexNet 中卷积层、VGG-16 的卷积层的能量消耗，数据来源于在参考文献 [51] 和 [48] 中制作的 65nm 芯片上的性能表现。需要说明的是 HOG 特征提取与视频压缩（实时的高清视频每像素 1 纳焦）的能量消耗差不多，因此 HOG 特征是一个很好的基准来确定近传感器能量消耗的可接受值；但是，DNN 目前需要消耗比 HOG 多几个数量级的能量。我们希望在这篇论文中所强调的一些可以作为设计契机的地方将能够缩小两种方法之间的能量消耗差距。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8AXdZ3Y3GvExKagAzu3JuibwkvEQdia8Aiay2HjiagsNk8C42vOkAVA5xMuNrdB54xtp4ibUibiaZ54Daicg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 12. 能量 VS 准确率：手工提取特征和机器学习特征之间能量对比准确率的权衡比较。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;十、总结&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;机器学习是一个非常重要的研究领域，在各种级别的硬件设计的创新上有许多有潜力的应用与机遇。在设计过程中，权衡准确率、能耗、吞吐量与成本是非常重要的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于数据传送控制着能量消耗，最近的研究主要集中在维持准确率、吞吐量、成本的同时，减少数据传送。这意味着选择带有良好存储层级的架构，比如一个空间阵列（spatial array），以及开发在低成本存储层级上能增加数据重复使用的数据流。对算法与硬件进行联合设计，减少位宽精确度（bitwidth precision），增加稀疏（sparsity）与压缩，这些手段被用于最小化数据传输。有了混合信号线路设计和高级技术，通过将计算嵌入或接近传感器和存储，计算也就更接近数据源了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们也应该考虑在这些不同层级上的交互。例如，通过硬件友好的算法设计而实现的位宽减少可以通过使用混合信号电路和非易失性存储来减少精度上的处理。通过高级技术减少内存访问的成本，这能带来更节能的数据流。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;致谢与参考文献（略）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;点击阅读原文下载本论文&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 24 Dec 2016 13:46:44 +0800</pubDate>
    </item>
    <item>
      <title>前沿 | 量子计算新突破点：电子-光子的「闲聊」</title>
      <link>http://www.iwgc.cn/link/4048023</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自phys&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：朱思颖、杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8AXdZ3Y3GvExKagAzu3JuibicFMXK78Y42OwJjHcSHnZ4fZAhr9djsKyfrZDxpAx4dFNLYic7yiczYXg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了进一步实现基于硅的量子计算机，普林斯顿大学的研究者已经建造了一个可以让单个电子传递其量子信息给光粒子的设备。光粒子也就是光子，在接收信息之后可以充当信使从而把所接收的信息传给其他的电子。由光子所创造的这种连接关系构成了量子计算机的回路。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这项已在 Science 上发表的研究，是普林斯顿大学和加州马利布休斯研究实验室共同指导完成的，展现了他们 5 年多努力的成果。这 5 年多的研究中，他们为单个电子和光子之间的对话来打造强大的能力支撑，据 来自普林斯顿大学的物理教授 Jason Petta 所说。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「就像人类之间的互动一样，一个良好的交流是要以很多事情的铺垫完成为前提的，比如说帮助交流双方讲同一种语言等，」Petta 说。「我们能够使电子态的能量与为光粒子的共振，从而使电子和光子能够互相对话。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这项发现将帮助研究者运用光来联系单独的电子，这些电子充当量子计算机的比特也即是量子计算机里最小的数据单元。一旦实现，量子计算机将是先进的设备，其能够运用微小粒子（如电子）实现高级计算，这些微小粒子遵从量子定律而不是日常生活中的物理定律。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们现在每天所使用的计算机中每一位比特的值要么是 1 要么是 0。量子比特（qubit）可以是 0 与 1 之间的一个值，或同时都为 0、同时都为 1。这种叠加就是为大家所熟知的量子特性，能够让量子计算机处理现在的计算机所不能解决的复杂问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;单个量子计算机已经通过囚禁粒子和超导体建造出来了，但是技术挑战减慢了基于硅的量子设备的研发进度。在构建量子计算机的时候，硅是非常诱人的材料，因为硅的价格不贵而且已广泛用于今天的智能手机和计算机的设备制造中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;研究员们在他们的设备里囚禁了一个电子和一个光子，然后把电子的能量用这样的一个方式转移给光子。通过这样的耦合使得光子把所携带的信息从一个量子位转移到位于一厘米之外的另一个量子位。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量子信息是极度脆弱的，即使是来自所在环境里最轻微的干扰，将会导致其携带信息的全部丢失。光子对抗干扰的能力更加强健，此外光子不仅能够在量子计算机的回路中将携带的量子信息从一个量子比特传递到另一个量子比特，而且可以在量子芯片之间通过电缆传递。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了使这两种不同类型的粒子能够互相「交流」，研究员们还必须建立一个提供适宜「交流」环境的设备。首先，休斯研究实验室（一个由波音公司和通用汽车拥有的研发实验室）的 Peter Deelman 装配出了一个半导体芯片，这个芯片由多层的硅和硅锗构成。这个装配的半导体芯片在其芯片表面之下囚禁了单层的电子。下一步，普林斯顿大学的研究员们在装置的顶部铺设了细微导线，每根导线的宽度只有人头发丝的几分之一。这些直径为毫微米的导线所传导的电压创建了能够囚禁单个电子的能级相图，而这些囚禁的电子将会在限制在硅的被称为双量子点（double quantum dot）的区域里。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;研究员们用这些导线来将囚禁电子的能级调整到与光子所匹配的程度，光子是由芯片顶层超导腔所囚禁。在这项研究发现之前，半导体的量子比特仅能被其相邻的量子比特耦合。通过光来耦合量子比特，将可以在位于芯片相对两端的量子比特之间进行信息传递。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;电子的量子信息无非就是其处于双量子点的 2 个能量穴（energy pocket）其中之一的位置信息。电子能占据两个能量穴之一或同时占据两个能量穴。通过控制用于设备的电压，研究员们就能够控制电子在哪个能量穴中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我们现在有实际转移电子量子态给囚禁在超导腔中的光子的能力了，」普林斯顿的物理系研究生也是论文的第一作者 Xiao Mi 说。「这一过程之前从未在半导体设备中实现过，因为在能够转移电子的量子信息之前其量子态就已丢失。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这个装置中能成功实现量子态的转移，归功于一个新的回路设计，这个回路在设计时能够让铺设的细微导线与量子比特之间的位置更近从而能够减少来自其他电磁放射源的干涉。为了减少这类噪音，研究员们在通向这个装置的导线中放置了能够过滤掉无关信号的滤波器。这些金属导线同时也充当了量子比特的保护盾。通过这些操作保护，相比于之前的实验，他们的量子比特所接受到的噪音量能够有 100 到 1000 倍的减少。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最终，研究人员计划拓展该设备使其能发挥电子的自旋特性。「长期来看，我们想让这个自旋和电荷耦合在一起的系统能自己造出一个可以电控的自旋量子，」Petta 说。「我们发现我们能讲一个电子与光相干耦合在一起，这是实现自旋光耦合的重要一步。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;德国亚琛工业大学量子信息研究所的一位物理学家 David DiVincenzo 虽然没有参与这项研究，但他在 1996 年参与了一篇非常有影响力的论文，该文概述了创建量子计算机所需的五个最低要求。对于普林斯顿 HRL 的工作，DiVincenzo 虽没有参与，但他说：「为了找到合适的条件组合实现单电子量子比特的强耦合条件，已经花了很多精力了。我很高兴地看到有人发现了一个参数空间的区域，在这个区域中，该系统可首次进入强耦合域。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;原文链接：http://phys.org/news/2016-12-electron-photon-small-talk-big-impact-quantum.html&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 24 Dec 2016 13:46:44 +0800</pubDate>
    </item>
    <item>
      <title>前沿 | 日本科学家首次成功演示基于自旋电子学的人工智能</title>
      <link>http://www.iwgc.cn/link/4048024</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自phys&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;日本东北大学的研究者有史以来第一次成功演示了基于自旋电子学的人工智能（spintronics-based artificial intelligence）的基本操作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8AXdZ3Y3GvExKagAzu3JuibotCXgbibbiaqzI4iaxU93ZJbiaOC50tclx0xzkSXGb2sgBVIe4ZVHgldjA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136);"&gt;&lt;span&gt;图 1：(a) 在本演示中用作人工突触的人工制造的自旋电子器件的光学照片。图中也展示了用于电阻切换的测量电路。(b) 该器件的电阻和被施加的电流之间的关系，表现出了类似模拟的电阻变化。(c) 安装在一个陶瓷封装上的自旋器件阵列的照片，这种器件可被用于开发人工神经网络。&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;模拟生物大脑处理信息的方式的人工智能可以快速执行复杂和精细的任务，比如图像识别和天气预测。人工智能近些年来得到的关注越来越大，并且也已经出现了很多有价值的实际应用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当前的人工智能都工作在传统的框架上——即基于半导体的集成电路技术。但是，半导体器件并不具备人脑的紧凑型和低功耗特性。为了解决这一难题，实现用作突触的单个固态器件是非常有前途的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;东北大学的 Hideo Ohno 教授、Shigeo Sato 教授、Yoshihiko Horio 教授、Shunsuke Fukami 副教授和 Hisanao Akima 副教授的所组成的研究团队在他们最近研发出的自旋电子器件（spintronic devices）中开发了一个人工神经网络，其采用了微尺度磁性材料（micro-scale magnetic material），如图 1 所示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;和传统的磁性器件不同，他们所用的自旋电子器件能够以一种模拟的方式记忆 0 到 1 之间的任意值，因此可以执行学习功能，类似于大脑中的突触。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8AXdZ3Y3GvExKagAzu3JuibnbhX1oNLXK5iaqSjBZSnqbLiaqN1AkR7OSxV7W3l5twLK90SgGVvZIgA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 2：开发出的人工神经网络的框图，包含了 PC、FPGA 和自旋电子阵列（SOT/spin-orbit torque，自旋轨道转矩）器件。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;使用这个开发出来的网络（图 2），这些研究者实验了关联记忆操作（associative memory operation）——传统计算机还无法实现这样的运算。经过多次试验之后，研究者确定这种自旋电子器件具备学习能力——他们开发出来的人工神经网络可以像人脑一样成功将它们记忆中的模式和有噪声的输入版本关联起来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个概念证明的演示有望为人工智能技术开辟新的发展空间——在减小紧凑尺寸的同时还能实现高速处理的能力和超低的功耗。这些特性应该有助于将人工智能推广到更广泛的社会应用领域，比如图像/声音识别、可穿戴终端、传感器网络和护理机器人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8AXdZ3Y3GvExKagAzu3JuibXicwMt0LDy71vIwaaSQqqpIicTKiaibwvzzBu0LpULavEYbcSFXuibqWGfg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;图 3. 三类模式，I、C 和 T，展现在 3×3 的框里，被用于关联记忆操作实验。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;原文链接：http://phys.org/news/2016-12-world-spintronics-based-artificial-intelligence.html&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;br/&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;br/&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 24 Dec 2016 13:46:44 +0800</pubDate>
    </item>
    <item>
      <title>圣诞快乐！来听听人工智能为这个圣诞节献上的歌曲</title>
      <link>http://www.iwgc.cn/link/4048025</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;作者：鹿者也&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随着圣诞节的到来，各种换汤不换药的圣诞歌曲又在挑战人们审美疲劳的极限。虽然不知道歌手们自己是如何完成每年的旧瓶装新酒的创作，但今年的圣诞歌曲中确实混入了一朵奇葩，如果这几个由人工智能编写的闹腾旋律姑且也能算是歌曲的话。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;多伦多大学的博士生楚航近日发布了他的新项目，一首完全由人工智能看着一棵圣诞树来编曲，填词创并朗诵的一首神奇的小调，并配有一个火柴人摇摆起舞。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;iframe class="video_iframe" data-vidtype="1" allowfullscreen="" frameborder="0" height="417" width="556" data-src="https://v.qq.com/iframe/preview.html?vid=s03585ecziu&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0"&gt;&lt;/iframe&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;楚航通过建立一个层级递归神经网络（Hierarchy RNN）的模型，然后收录大量音乐数据，从而由人工智能分析大体的音乐结构特点，发现并总结多首相似风格的音乐中存在的类似的特征，再以新颖的构建框架建立多层神经网络模型，最终通过输入一副画面，便能生成相应主题的流行音乐。研究项目中每层网络对应生成不同的音乐成分，并且每一层都是一个「双层 LSTM（double layer LSTM）」且相互关联，让最终输出的音乐具有更高的质量和丰富性。在多层的构筑框架下，还可以在生成音乐之外编出新颖的舞步和歌唱的人声。楚航便是在这样的构建基础下制作了应用 Neural Karaoke，Neural Dancing 以及 Neural Story Singing。&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;课余兴趣的游戏之作&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;研究员楚航是多伦多大学（University of Toronto）的在读博士生，导师为人工智能界大牛 Raquel Urtasun 以及 Sanja Fidler，现主要致力于研究学习 CV（Computer Vision）。此前康奈尔大学（Cornell University）获取硕士学位，于上海交通大学获取本科学位。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;个人主页：&lt;/span&gt;&lt;span&gt;http://chuhang.github.io/&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是作为博士生楚航在多伦多大学的第一年，过往研究主要集中于机器学习（machine learning）以及 2D-3D 转换建模等，现今在跟随导师主要研究 CV 的同时，想要做一些有趣的相关研究项目，能够将以往学习的知识联合运用，由此产生了动力并着手于这项人工智能音乐项目的研究。并在两周的时间里得到了目前的成果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;多层构建 RNN 的方式使得 AI 输出的音乐内容更加丰富&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Neural Karaoke 的模型核心理论名为分层递归神经网络。在当前的人工智能研究中，RNN（时间递归神经网络 recurrent neutral network 和结构递归神经网络 recursive neural network 的统称，但通常意义上单指时间递归神经网络，本文亦是）是一项非常重要和主流的机器学习方法。相较于深入学习强化单个 RNN，楚航在项目中提出了新的神经网络构建框架，即层级递归神经网络。通过在单个 RNN 上再度构建一个新的 RNN，使模型变得多层立体（hierarchical），并让每层神经网络对应生成单独的音乐成分。而每一层所对应的音乐成分并不是固定不变的，例如在楚航的研究中，最底层的（base layer）RNN 负责生成音乐旋律，然后在旋律上关联按键音，接着在其基础上再构建一层 RNN 负责生成和弦，而后构建出第三层 RNN 对应生成鼓点，最终输出一首结构丰富的的音乐。模型的具体结构如图：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8AXdZ3Y3GvExKagAzu3Juibpdsv0Pt8ZLgUkg1laj5X4HbCm99GbuId4QQrrykiayEbukG8Kk3jaIg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在建立多层神经网络的同时，每一层的 RNN 模型中都建立了双层 LSTM（long-short term memory），并让不同层次相互关联，以弥补 RNN 在短期记忆上的匮乏。而对于音乐的组成成分，楚航团队尝试做了一些拓展，除了最基础的旋律+和弦+鼓点外，团队还尝试加入了舞蹈以及歌词，这两项尝试也就对应到了楚航论文中的 Neural Dancing 和 Neural Story Singing。如果能把现有的所有成分融入到一个模型中，理论上可以做到输入一副图片，然后得到一首相应风格的歌曲并伴随着小火柴人的舞步和歌声。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于这项研究的核心价值体现，楚航表示或许应当是提出了一个新的人工智能应用场景，给学界带来一点新的研究方向。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;实验室中的 Neural Karaoke，谷歌 Magenta 与 索尼 Flow Machine&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除却楚航的 Neural Karaoke，谷歌的 magenta system 团队和索尼旗下的 CSL 研究室也各自发布了相似的研究成果。谷歌的 magenta 团队发表的人工智能作曲应用 TensorFlow 的运作方法主要基于深度学习下的增强学习（deep reinforcement learning）和极限类比（maximum likehood）。通过构建一个生成音节的神经网络（Note-RNN）, 然后建立 LSTM 来预测目标音乐规律中的下一个音节，然后通过 RL 法来将其改善。再由音乐理论和奖励基质（reward base）共同构成的奖励方程确定输出音节，而后送入下一个音节网络。Magenta 团队表示，这样结合了 ML 的 RL 调节法不单单可以用在产生美妙的曲调，同时还能够显著的减少神经网络运作中不必要的无用及失败模型。对于 magenta 团队所使用的方法，楚航表示这和他使用的多层神经网络框架并不冲突，两者的应用应该是平行且互补的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而索尼的 CSL 研究室虽然并没有发表相关论文，但也同样发表了他们的应用成果，名为「FlowMachine」的软件，并请专业音乐人 Benoît Carré 填词，最终带来了披头士风格的歌曲「Daddy』s Car」。并且 FlowMachine 和谷歌团队的 TensorFlow 都将在不久后于各自的平台发表。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;让大众以更低廉的成本感受到原创音乐的乐趣&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;楚航表示他今后的研究方向依然是会着重于 CV 和建模方面。而 AI 作曲作为一个课余想到的课题，也确实有打算做下去，将其加以改进。例如融合多层建模和增强学习，或者增加音乐情绪的研究，亦或是输入和输出的双向性和逆转性，都是很有趣研究方向。并且目前正在召集感兴趣的同好加入项目组。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;楚航研究人工智能编曲的初衷就是基于有趣。对于应用的方向，可能会希望建立一个类似社交网站的工具，让大家上传分享各自独特风格的音乐和舞步，带来更多的可玩性。亦或是使用廉价的机器人，节省人们消耗在投资，制作音乐上的金钱，更加容易地听到全新的音乐，享受更简单的原创音乐等等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心原创，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 24 Dec 2016 13:46:44 +0800</pubDate>
    </item>
  </channel>
</rss>
