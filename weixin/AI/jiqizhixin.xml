<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>机器之心</title>
    <link>http://www.iwgc.cn/list/670</link>
    <description>人与科技的美好关系</description>
    <item>
      <title>深度 | 我们真的找对了方法？新研究为欧美脑计划敲响警钟</title>
      <link>http://www.iwgc.cn/link/4435235</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自The Economist&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李泽南&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;UC Berkeley 的最新研究告诉我们，大部分研究者用于分析大脑运行规律的算法也许并不准确。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWic17FA5ZBlbiaEavsI2wmR5XOnbmLiaPdkB4NK4qL5bCmUuaH2kxlkr7x18VMOop6WfrIPB4PJPkmxw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经科学就像其他很多学科一样，对获取数据有着无穷的欲望。在 2013 年，前总统奥巴马宣布的美国脑计划——和同年的欧盟脑计划一样，都致力于探寻人类大脑中数千个神经细胞的交互形式。科学家们希望在数量庞大的信息中找到一些规律，能让我们距离揭开大脑运行机制的面纱更进一步。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是最近在 PLOS 计算生物学上发表的一篇论文对于量变能否产生质变提出了质疑。Eric Jonas 等人使用了神经科学界常用的方法：将人类大脑与计算机类比。就像大脑一样，计算机通过电信号与复杂的环路来处理信息。我们知道计算机的所有运行机制，而面对大脑却只能假设。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Eric Jonas 来自 UC Berkeley，论文的共同作者 Konrad Kording 则来自西北大学，他们都拥有神经科学和电子工程的复合背景，他们都认为计算机是测试现代神经科学分析工具的最佳方法。他们的思路是：将这些分析技术应用到计算机芯片中去，看看分析结果是否和计算机架构相符。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们用于测试的硬件是 MOS Technology 6502 处理器，它是一种 8 位 CPU，于 1975 年面世，曾经在 Atari 游戏机、苹果电脑和 Commodore 上出现。以现在的眼光看来，它显得有点落后——只有 3510 个晶体管。6502 的结构非常简单，这让研究者可以构建一个模型，模拟在运行特定程序时 CPU 中所有晶体管的运行状态，与和这些晶体管相连的数万个通路的所有电压。这一模拟程序每秒钟能够产生 1.5G 的数据，这还处在用于探索大脑运行机制的算法的能力范围内。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;大脑向左，芯片向右&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对比受损和健康大脑的不同特性是一个常见的脑科学研究方式。如果特定部位受损的大脑展现出另一种预测行为，研究人员就可以得知这一部分脑区主要负责处理何种任务。以老鼠为例，如果掐断海马体（两个位于大脑底部，细长形状的小结构）就可以有效地影响它们识别物体的能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当应用于芯片时，这种方法呈现了有趣的假阳性。研究人员发现，禁用一个特定的晶体管组就可以完全阻止「Donkey Kong」程序（第一个出现超级马里奥形象的游戏）启动。「这也许是一个错误，」Jonas 说道。「它们不像是组成基础计算功能循环，实现软件加载的一部分，而更像是其他的什么东西。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一种神经科学研究方法，是寻找神经细胞群的活动与特定行为之间的相关性。在芯片上，研究人员的算法发现其中五个晶体管的活动与屏幕上显示图像的亮度强烈相关。然而再一次，这个发现随即被否定了。Jonas 和 Kording 知道这些晶体管的计算结果并不会直接与屏幕显示的内容相关（在 Atari 游戏机中，显示处理的工作由 Television Interface Adaptor 芯片完成）。这些晶体管只包含一些简单的意义——它们被程序的某些部分使用，最终决定屏幕上会出现什么。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对此，研究人员分析了这种芯片的接线图（在生物学家那里，对应的东西应该是连接组），随后把这张图输入了分析算法。算法生成了很多看起来让人印象深刻的数据，暗示着芯片处理任务时，内部存在着一些复杂的结构。然而仔细检查以后，这些推论很少被证明成立。这些性状非常具有误导性，而且经不起推敲——这倒很符合真正神经科学令人沮丧的研究历史。研究人员有一个完整的蠕虫连接组（Caenorhabditis elegans，来自 1986 年），其中只包含 302 个神经细胞。现在，我们知道自己对于如此简单的「脑结构」也无能为力了（相比之下，英特尔 Skylake K Core i7 约有 17 亿个晶体管——而且我们清楚它的内部机制）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「这其中最基本的问题，」Jonas 说道。「在于现代神经科学的研究方式无法探明我们已知的芯片结构，而这对于我们理解真正发生的事情至关重要。计算机芯片由晶体管组成，它们是微小的电子开关。它们被组成逻辑门，从而实现简单的逻辑运算。这些逻辑门又可以组织成为更复杂的结构，如加法器。算术逻辑单元由一系列加法器组成。再往后更为复杂的结构也是这样层层递进的关系。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是，用神经科学的方式分析这种高级结构是非常困难的，分析算法无法探知芯片内部的电流如何计算出超级马里奥的形象。这不是神经科学才会遇到的问题，Jonas 把他们遇到的麻烦与人类基因组计划进行了比较，人类基因组计划是一项规模庞大的计划，已于 2003 年结束。科学家们希望通过这一计划攻克癌症、衰老这样的难题。然而十几年过去了，事实证明我们不能从浩如烟海的数据中获得太多启示，我们面对无数条四个字母编写的密码一筹莫展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人类脑计划也要面临长达十几年的困局吗？或许不会。在这项研究中，算法还是探测到了主时钟信号，它协调 CPU 中不同的部位的运行。同时，一些神经科学家也批评了论文给出的结论，他们认为计算机芯片和大脑的机制并不能完全类比，同样的分析方式并不能直接移植应用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;法国计算机科学与自动化研究所机器学习专家 Gaël Varoquaux 表示：「6502 与当代 CPU 相比，前者和大脑的区别更明显。这种原始的芯片按照顺序处理输入内容。而大脑（和现代 CPU）是多线程同时计算的。」正如他所说的，神经科学在近年来已经走过了很长的一段路。例如在视觉系统的复杂细节上，人们对于大脑如何区分线和面这样的概念已经有了合乎情理的解释。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Jonas 对这些看法表示部分接受。「我并不否认神经科学近年来的成就，」他辩解道，他继续用人类基因组计划作类比。「这项工程产生的数据，以及其后更先进手段收集得来的数据都为人类的基因研究打下了基础，但由基因组计划引起的对于未来的不实际期待则被打破。收集数据是一回事，想搞清楚数据背后的意义，我们还要等很久。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;论文：Could a Neuroscientist Understand a Microprocessor?&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWic17FA5ZBlbiaEavsI2wmR5Xmh27sEiajNOyke9phT4uiagibVFOHubR5jaG9sEJhUap8kWMY7hWRJRKg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：&lt;/span&gt;&lt;span&gt;在神经科学领域，我们有一个普遍的观念：研究非常依赖于数据，在大型数据集和高级分析算法的帮助下，我们可以探求大脑运行方式。这种量级的数据集尚不存在，而且即使它们存在，我们也无法得知我们算法生成的分析结果是否有效。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了证明这个观点，我们使用了一个具有代表性的微处理器作为生物模型，并进行了一系列实验以检查目前主流的分析方法是否可以理解 CPU 的信息处理方式。微处理器是人造信息处理系统的组成部分，它们形式复杂，同时结构完全已知——从逻辑流程，到逻辑门，再到晶体管的动态。我们的研究展示了现有神经科学分析方法对于 CPU 的运行数据产生了有趣的见解，但这些分析随后被证明对于解释芯片在信息处理时的层级结构毫无意义。这意味着目前神经科学领域的分析方式可能对于我们理解神经系统毫无帮助——无论我们是否拥有足够数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此外，我们的研究结果表明，科学家们需要使用复杂非线性动态系统在已知内容的基础上开展研究，微处理器对于时间序列和结构发现方法而言就是一个可用的平台。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;原文：http://www.economist.com/news/science-and-technology/21714978-cautionary-tale-about-promises-modern-brain-science-testing-methods&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 21 Jan 2017 17:20:27 +0800</pubDate>
    </item>
    <item>
      <title>业界 | FDA批准首个云深度学习临床医疗应用平台Atrerys</title>
      <link>http://www.iwgc.cn/link/4435236</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自福布斯&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德、李泽南&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote style="color: rgb(62, 62, 62); font-size: 16px; white-space: normal; max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;初创公司 Arterys 的产品成为了第一个获得 FDA 批准的机器学习应用，它标志着医疗行业和人工智能领域的一大进步。意味着深度学习和云技术可以真正进入医疗工作流程。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Arterys 的医疗成像平台已被美国食品和药物管理局（FDA）批准投入临床医疗使用，它可以帮助医生诊断病人的心脏问题。这个应用中的神经网络模型从 1000 多个病例中自我学习，并可以在每个新案例中不断获取新知识，提高自己的准确性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在此之前，为获得 FDA 的认证，这一系统通过了严格的临床测试，以证明它至少可以与人类医生达到相同的判断水平。Arterys 与人类的最大不同是他可以在约 15 秒钟内对一个病例作出诊断，而要做同样的事，专业人类分析师需要花上 30 分钟到一个小时。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Arterys 是由 Fabien Beckers，John Axerio-Cilies，Albert Hsiao 和 Shreyas Vasanawala 在斯坦福大学发起的，目前已完成 A 轮融资，公司的创始人们对于机器学习的潜力有着共同的期望。他们目前的工作是帮助医生了解病人心脏的状态，通过准确测量每个心室的体积，人工智能系统可以对病人的健康状况作出评估。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「这是一个令人瞩目的成就，」Beckers 说道。「这是此类医疗成像方法第一次通过批准。这意味着深度学习和云技术可以真正进入医疗工作流程，为医生和病人提供重要帮助。我们的工作开启了一扇门，此类应用从此有了先例。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWic17FA5ZBlbiaEavsI2wmR5Xgro4931ZCbHuib5wkmAI7dMCZ7SfWRMROBlQQN2cX3Nl0ceah56hS2w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;深度学习&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;投入 1000 个病例作为训练数据后，Artery Cardio DL 运行了监督学习算法，并基于它在数据中发现的关联得出大约 1000 万条规则。它的目的是在没有人工干预的情况下做到察觉和识别问题。但是 Becker 肯定道，这个目的并不是要代替医生，而是为他们提供工具，帮助他们更高效地工作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我们正在尝试将它做成定量的和数据驱动的。我们从心脏的应用开始，因为它是最难的器官之一。现在我们知道我们可以做到了，能在其他很多领域用这个 Arterys 了。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「心脏的左心室呈圆形，结构简单，而右心室呈花生状，比较复杂。证明这项技术可以用来分析左右心室的两个图像是一个大成果，因为使用传统方法花的时间太长了。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「这个技术能做到这些证明了它的用处是何其的巨大而深远。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Arterys 是基于云的平台。这一点很重要，因为它能让外科医生在全世界范围内收集数据，Arterys 也将继续学习这些数据。有了时间和足够的数据，终有一天它的精确性不仅能与媲美人类，甚至能超过人类。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当然这也会带来特殊的挑战，因为数据是高度敏感和个人化的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;「&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;医疗成像大约有 30 亿美元的市场，它是基于工作站的——除了医疗行业，还有其他行业使用「工作站」这个词吗？」Beckers 问道。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe class="video_iframe" data-vidtype="5" allowfullscreen="" frameborder="0" height="417" width="556" data-src="https://v.qq.com/iframe/preview.html?vid=z0368bajy6e&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0"&gt;&lt;/iframe&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;云安全&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我们一直在思考为什么云技术不能像进入金融领域里一样进入医疗领域，最后我们发现问题在于数据隐私——在其他行业里，你可以像谷歌、GE 或任何世界级大公司一样行事，但医院不会允许你随意把个人医疗信息（PHI）放到云端处理。」Beckers 解释道。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;解决这个问题的方案是一种被称为 PHI 服务的系统，它允许医院提供的图像数据在被云端收集时将个人识别信息剥离掉。当系统认可的用户和医生登陆时，他/她可以从 Arterys 的云系统中获取成像数据和分析结果，并从医院的安全服务器中获取 PHI，并将个人识别信息与图像数据重新整合在一起。这样，在整个流程中，Arterys 不会接触到任何个人隐私内容信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种由加密和安全传输协议支持的认证系统可能在克服存储和分析个人数据固有的问题方面发挥越来越大的作用。由此观之，FDA 批准 Arterys 解决方案有了更重要的意义。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;学习系统应用于医疗扫描设备生成的大量数字图像数据的巨大潜力已经被讨论一段时间了。现在 FDA 批准了 Arterys，路障已被清除，这一突破性技术将会带出现更多的应用程序。Artery 自己还在生产下一代的技术应用，这一次的目标是癌症。从 FDA 批准释放出的信号，以及政府支持人工智能和机器学习应用的热度，我认为可以期待的更多。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 21 Jan 2017 17:20:27 +0800</pubDate>
    </item>
    <item>
      <title>一周论文 | Image Caption任务综述</title>
      <link>http://www.iwgc.cn/link/4435237</link>
      <description>&lt;h1 style="max-width: 100%; color: rgb(62, 62, 62); font-variant-ligatures: normal; orphans: 2; white-space: normal; widows: 2; background-color: rgb(255, 255, 255); line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;引言&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Image Caption是一个融合计算机视觉、自然语言处理和机器学习的综合问题，它类似于翻译一副图片为一段描述文字。该任务对于人类来说非常容易，但是对于机器却非常具有挑战性，它不仅需要利用模型去理解图片的内容并且还需要用自然语言去表达它们之间的关系。除此之外，模型还需要能够抓住图像的语义信息，并且生成人类可读的句子。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随着机器翻译和大数据的兴起，出现了Image Caption的研究浪潮。当前大多数的Image Caption方法基于encoder-decoder模型。其中encoder一般为卷积神经网络，利用最后全连接层或者卷积层的特征作作为图像的特征，decoder一般为递归神经网络，主要用于图像描述的生成。由于普通RNN存在梯度下降的问题，RNN只能记忆之前有限的时间单元的内容，而LSTM是一种特殊的RNN架构，能够解决梯度消失等问题，并且其具有长期记忆，所以一般在decoder阶段采用LSTM.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); font-variant-ligatures: normal; orphans: 2; white-space: normal; widows: 2; background-color: rgb(255, 255, 255); line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;问题描述&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Image Caption问题可以定义为二元组(I,S)的形式， 其中I表示图，S为目标单词序列，其中S={S1,S2,…}，其中St为来自于数据集提取的单词。训练的目标是使最大似然p(S|I)取得最大值，即使生成的语句和目标语句更加匹配，也可以表达为用尽可能准确的用语句去描述图像。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h1 style="max-width: 100%; color: rgb(62, 62, 62); font-variant-ligatures: normal; orphans: 2; white-space: normal; widows: 2; background-color: rgb(255, 255, 255); line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;数据集&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;论文中常用数据集为Flickr8k,Flick30k,MSCOCO,其中各个数据集的图片数量如下表所示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmrz6yCn8okVud4zBBMTYwTibNliaPcMjcaLjzfnESsHrBS203GpEzibu6GBZicRx0w1jpLjSVIy95OoA/640?wx_fmt=jpeg"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;a title="" class="" rel="gallery0" style="color: rgb(37, 143, 184); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmrz6yCn8okVud4zBBMTYwTAYfULnoC75Dianhxbfjc8XWfE4nSQAnFib0ZWmpeLhOjnChDIqZl0muQ/640?wx_fmt=jpeg"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;数据集图片和描述示例如图&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中每张图像都至少有5张参考描述。为了使每张图像具有多种互相独立的描述，数据集使用了不同的语法去描述同一张图像。如示例图所示，相同图像的不同描述侧重场景的不同方面或者使用不同的语法构成。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h1 style="max-width: 100%; color: rgb(62, 62, 62); font-variant-ligatures: normal; orphans: 2; white-space: normal; widows: 2; background-color: rgb(255, 255, 255); line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本文主要介绍基于神经网络的方法&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); font-variant-ligatures: normal; orphans: 2; white-space: normal; widows: 2; background-color: rgb(255, 255, 255); line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;1 NIC[1]&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Show and Tell: A Neural Image Caption Generator&lt;br/&gt;本文提出了一种encoder-decoder框架，其中通过CNN提取图像特征，然后经过LSTM生成目标语言，其目标函数为最大化目标描述的最大似然估计。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmrz6yCn8okVud4zBBMTYwTj1fOIHvz9no04ldPfg0QcvmkLP1ibodriba6WhfyBkwswKLBU1jN1H4g/640?wx_fmt=jpeg"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该模型主要包括encoder-decoder两个部分。encoder部分为一个用于提取图像特征的卷积神经网络，可以采用VGG16，VGG19, GoogleNet等模型, decoder为经典的LSTM递归神经网络，其中第一步的输入为经过卷积神经网络提取的图像特征，其后时刻输入为每个单词的词向量表达。对于每个单词首先通过one-hot向量进行表示，然后经过词嵌入模型，变成与图像特征相同的维度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); font-variant-ligatures: normal; orphans: 2; white-space: normal; widows: 2; background-color: rgb(255, 255, 255); line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;2 MS Captivator[2]&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;From captions to visual concepts and back&lt;br/&gt;本文首先利用多实例学习，去训练视觉检测器来提取一副图像中所包含的单词，然后学习一个统计模型用于生成描述。对于视觉检测器部分，由于数据集对图像并没有准确的边框标注，并且一些形容词、动词也不能通过图像直接表达，所以本文采用Multiple Instance Learning(MIL)的弱监督方法，用于训练检测器。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmrz6yCn8okVud4zBBMTYwTP2eIetibZ86DM3Wa9nmJiaL0x6yc1660vb6p5ITbYgD8j4hcJdnUv1aQ/640?wx_fmt=jpeg"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); font-variant-ligatures: normal; orphans: 2; white-space: normal; widows: 2; background-color: rgb(255, 255, 255); line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;3 Hard-Attention Soft-Attention[3]&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Show, atten and tell: Neural image caption generation with visual attention&lt;br/&gt;受最近注意机制在机器翻译中发展的启发，作者提出了在图像的卷积特征中结合空间注意机制的方法，然后将上下文信息输入到encoder-decoder框架中。在encoder阶段，与之前直接通过全连接层提取特征不同，作者使用较低层的卷积层作为图像特征，其中卷积层保留了图像空间信息，然后结合注意机制，能够动态的选择图像的空间特征用于decoder阶段。在decoder阶段，输入增加了图像上下文向量，该向量是当前时刻图像的显著区域的特征表达。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmrz6yCn8okVud4zBBMTYwTa9v6dklNu93NNVnDJf1oicYY0iaeEg6MVX83Oze2o8Z1Cjlg6oYCy9yw/640?wx_fmt=jpeg"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); font-variant-ligatures: normal; orphans: 2; white-space: normal; widows: 2; background-color: rgb(255, 255, 255); box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;br/&gt;&lt;/h2&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); font-variant-ligatures: normal; orphans: 2; white-space: normal; widows: 2; background-color: rgb(255, 255, 255); line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;4 gLSTM[4]&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Guiding long-short term memory for image caption generation&lt;br/&gt;使用语义信息来指导LSTM在各个时刻生成描述。由于经典的NIC[1]模型，只是在LSTM模型开始时候输入图像，但是LSTM随着时间的增长，会慢慢缺少图像特征的指导，所以本文采取了三种不同的语义信息，用于指导每个时刻单词的生成，其中guidance分别为Retrieval-based guidance (ret-gLSTM), Semantic embedding guidance(emb-gLSTM) ,Image as guidance (img-gLSTM).&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmrz6yCn8okVud4zBBMTYwTmBLBauze3MmCfoElrekjqoLDL2hUTUYlYPbmmDLpJYETg4ppJPQ1wA/640?wx_fmt=jpeg"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); font-variant-ligatures: normal; orphans: 2; white-space: normal; widows: 2; background-color: rgb(255, 255, 255); line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;5 sentence-condition[5]&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Image Caption Generation with Text-Conditional Semantic Attention&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmrz6yCn8okVud4zBBMTYwT6U7vicPyoygkrQHDHKnHBW5pOedCloTpM1RBOibUL95ic1gT6bEw3jnaQ/640?wx_fmt=jpeg"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该模型首先利用卷积神经网络提取图像特征，然后结合图像特征和词嵌入的文本特征作为gLSTM的输入。由于之前gLSTM的guidance都采用了时间不变的信息，忽略了不同时刻guidance信息的不同，而作者采用了text-conditional的方法，并且和图像特征相结合，最终能够根据图像的特定部分用于当前单词的生成。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); font-variant-ligatures: normal; orphans: 2; white-space: normal; widows: 2; background-color: rgb(255, 255, 255); line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;6 Att-CNN+LSTM [6]&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;What value do explicit high level concepts have in vision to language problems?&lt;br/&gt;如图，作者首先利用VggNet模型在ImageNet数据库进行预训练，然后进行多标签数训练。给一张图片，首先产生多个候选区域，将多个候选区域输入CNN产生多标签预测结果，然后将结果经过max pooling作为图像的高层语义信息，最后输入到LSTM用于描述的生成。该方法相当于保留了图像的高层语义信息，不仅在Image Caption上取得了不错的结果，在VQA问题上，也取得很好的成绩。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmrz6yCn8okVud4zBBMTYwTWNIZdpT0wP6ar72HHzL7CYq57olee7eAzJuZtt1Qq9h6Y1leicZ662Q/640?wx_fmt=jpeg"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); font-variant-ligatures: normal; orphans: 2; white-space: normal; widows: 2; background-color: rgb(255, 255, 255); line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;7 MSM[7]&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;BOOSTING IMAGE CAPTIONING WITH ATTRIBUTES&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmrz6yCn8okVud4zBBMTYwT0rBJa8HnLiczlkkUfSL9DbLlFy96XtWYCELOXzvEfpJb7xYDElSwQsA/640?wx_fmt=jpeg"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该文研究了图像属性特征对于描述结果的影响，其中图像属性特征通过多实例学习[2]的方法进行提取。作者采用了五种不同的组合形式进行对比。其中第3种、第5种，在五种中的表现出了比较好的效果。由于提取属性的模型，之前用于描述图像的单词的生成，所以属性特征能够更加抓住图像的重要特征。而该文中的第3种形式，相当于在NIC模型的基础上，在之前加上了属性作为LSTM的初始输入，增强了模型对于图像属性的理解。第5种，在每个时间节点将属性和文本信息进行结合作为输入，使每一步单词的生成都能够利用图像属性的信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); font-variant-ligatures: normal; orphans: 2; white-space: normal; widows: 2; background-color: rgb(255, 255, 255); line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;8 When to Look[8]&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmrz6yCn8okVud4zBBMTYwTVe1rXQuRibvXVaT6WjrQ2heCut0EXVASLggDVia6wUzLTBzLHTeoLpvw/640?wx_fmt=jpeg"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该文主要提出了何时利用何种特征的概念。由于有些描述单词可能并不直接和图像相关，而是可以从当前生成的描述中推测出来，所以当前单词的生成可能依赖图像，也可能依赖于语言模型。基于以上思想，作者提出了“视觉哨兵”的概念，能够以自适应的方法决定当前生成单词，是利用图像特征还是文本特征。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h1 style="max-width: 100%; color: rgb(62, 62, 62); font-variant-ligatures: normal; orphans: 2; white-space: normal; widows: 2; background-color: rgb(255, 255, 255); line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;结果&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本文列出的模型的在COCO测试集上的结果如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmrz6yCn8okVud4zBBMTYwTdr20NTX1v6arSvalKvaTEiacTmibYNkmeprHTyEcWbGojVBFR8oqjTEQ/640?wx_fmt=jpeg"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以下为online MSCOCO testing server的结果：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmrz6yCn8okVud4zBBMTYwT9ar0JNb8MOic8sIlXzwIPvialjoeQRTWOamASibgKg9ibjke5ibDW1TLkSQ/640?wx_fmt=jpeg"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h1 style="max-width: 100%; color: rgb(62, 62, 62); font-variant-ligatures: normal; orphans: 2; white-space: normal; widows: 2; background-color: rgb(255, 255, 255); line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最近的Image Caption的方法，大多基于encoder-decoder框架，而且随着flickr30,mscoco等大型数据集的出现，为基于深度学习的方法提供了数据的支撑，并且为论文实验结果的比较提供了统一的标准。模型利用之前在机器翻译等任务中流行的Attention方法，来加强对图像有效区域的利用，使在decoder阶段，能够更有效地利用图像特定区域的特征[3]。模型利用图像的语义信息在decoder阶段指导单词序列的生成，避免了之前只在decoder开始阶段利用图像信息，从而导致了图像信息随着时间的增长逐渐丢失的问题[4][5]。模型为了更好的得到图像的高层语义信息，对原有的卷积神经网络进行改进，包括利用多分类和多实例学习的方法，更好的提取图像的高层语义信息，加强encoder阶段图像特征的提取[6][7]。随着增强学习，GAN等模型已经在文本生成等任务中取得了不错的效果，相信也能为Image Caption效果带来提升。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h1 style="max-width: 100%; color: rgb(62, 62, 62); font-variant-ligatures: normal; orphans: 2; white-space: normal; widows: 2; background-color: rgb(255, 255, 255); line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;参考文献&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. Vinyals O, Toshev A, Bengio S, et al. Show and tell: A neural image caption generator[J]. Computer Science, 2015:3156-3164.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2.Fang H, Gupta S, Iandola F, et al. From captions to visual concepts and back[C]// IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2015:1473-1482.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3.Xu K, Ba J, Kiros R, et al. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention[J]. Computer Science, 2016:2048-2057.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4.Jia X, Gavves E, Fernando B, et al. Guiding Long-Short Term Memory for Image Caption Generation[J]. 2015.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;5.Zhou L, Xu C, Koch P, et al. Image Caption Generation with Text-Conditional Semantic Attention[J]. 2016.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;6.Wu Q, Shen C, Liu L, et al. What Value Do Explicit High Level Concepts Have in Vision to Language Problems?[J]. Computer Science, 2016.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;7.Yao T, Pan Y, Li Y, et al. Boosting Image Captioning with Attributes[J]. 2016.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;8.Lu J, Xiong C, Parikh D, et al. Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning[J]. 2016.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h1 style="max-width: 100%; color: rgb(62, 62, 62); font-variant-ligatures: normal; orphans: 2; white-space: normal; widows: 2; background-color: rgb(255, 255, 255); line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;作者&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;朱欣鑫&lt;/strong&gt;，北京邮电大学在读博士，研究方向为视觉语义理解&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;邮箱：&lt;a style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;zhuxinxin@bupt.edu.cn&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;paperweekly最近刚刚成立&lt;span&gt;&lt;strong&gt;多模态&lt;/strong&gt;&lt;/span&gt;组，有对image caption、VQA等多模态任务感兴趣的童鞋可以申请加入！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;关于PaperWeekly&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;PaperWeekly是一个分享知识和交流学问的学术组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;微信公众号：PaperWeekly&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmrz6yCn8okVud4zBBMTYwTbHfzKZtva0qb5msvRByocicTG7tD5pYIdLeJPL0be4Z4kpeeur5cJ2w/640?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微博账号：PaperWeekly（&lt;/span&gt;&lt;a target="_blank" rel="external" style="max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important; text-decoration: underline;"&gt;&lt;span&gt;http://weibo.com/u/2678093863&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&amp;nbsp;）&lt;br/&gt;微信交流群：微信+ zhangjun168305（请备注：加群交流或参与写paper note）&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 21 Jan 2017 17:20:27 +0800</pubDate>
    </item>
    <item>
      <title>招聘 | 加入百度语音，收获在人工智能领域的未来！</title>
      <link>http://www.iwgc.cn/link/4435238</link>
      <description>&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;在上一期的最强大脑比赛结束后，机器之心&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650722318&amp;amp;idx=1&amp;amp;sn=586fc816eb9e318735bc1e4906a44604&amp;amp;chksm=871b1470b06c9d66e4e3bfac05691f7d637291505854be05e791660b50ad3ba8697199349d65&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650722318&amp;amp;idx=1&amp;amp;sn=586fc816eb9e318735bc1e4906a44604&amp;amp;chksm=871b1470b06c9d66e4e3bfac05691f7d637291505854be05e791660b50ad3ba8697199349d65&amp;amp;scene=21#wechat_redirect"&gt;专访了百度首席科学家吴恩达&lt;/a&gt;了解背后的语音技术。看完之后，有没有加入百度语音团队的想法？如今他们急需人才！&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe class="video_iframe" data-vidtype="1" allowfullscreen="" frameborder="0" height="417" width="556" data-src="https://v.qq.com/iframe/preview.html?vid=e0366l4b003&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0"&gt;&lt;/iframe&gt;&lt;em style="color: rgb(136, 136, 136);"&gt;&lt;span&gt;AI Talk：机器之心专访吴恩达视频&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;百度语音技术部定位于面向全行业提供语音及声音信号处理等领域的产品研发、产品交互体验设计，通用平台及工具开发、前瞻技术研究及技术支持。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;百度语音技术部是互联网行业中最早自主研发语音技术的团队，拥有业界领先的语音识别、语音合成等技术，百度深度语音识别系统入选 MIT「2016 十大突破技术」，确立了百度语音技术处在业界领先的地位。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;百度语音在美国硅谷同步设有办公场所，国内外拥有一大批行业顶尖的语音及人工智能人才，加入百度语音，收获你在人工智能领域的未来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWic17FA5ZBlbiaEavsI2wmR5XBwWvkrzqbKiaDwnWA79yzYNDIRY9ymwSYiaMsmGHKRDM10laceBVnBHg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;语音识别高级工程师（工作地点：北京）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;工作职责:&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;负责语音识别技术的研究，&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;负责声学模型、语言模型、解码器三个方向之一的相关工作&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;职责要求:&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;对语音识别的算法细节有深刻的了解&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;深入了解声学建模的过程，以及具体训练算法&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;主导开发过应用级别的语音识别系统&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;具有良好的数学功底&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;具有很强的分析问题和解决问题的能力，对解决具有挑战性问题充满激情&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在 Interspeech，ICASSP 等语音学术会议中有论文发表者优先&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;有强烈的上进心和求知欲，自我管理能力强，有良好的时间意识&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;音频信号处理工程师（工作地点：北京）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;负责语音信号处理相关技术研发&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;负责其他音频相关技术研发&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;负责语音识别和合成等任务的前端信号处理技术研发&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;职位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;精通各种信号处理技术（如语音增强，回声消除，麦克风阵列信号处理等）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;扎实的数学基础、数字信号处理理论与实践经验&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;精通 C/C++、matlab 编程，对数据结构和算法设计有较为深刻的理解，熟悉 Linux 平台&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在 Interspeech，ICASSP 等语音学术会议中有论文发表者优先&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;积极主动的学习能力，能够及时跟进新技术&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;优秀的分析问题和解决问题的能力，对工作充满热情，敢于接受挑战&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;良好的沟通能力和团队合作精神&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;声纹识别算法高级工程师（工作地点：北京）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;负责说话人/语种识别和检测等相关技术的研发&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;负责说话人/语种识别和检测系统开发和线上优化&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;职位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;熟悉说话人/语种识别技术，了解 NIST/国内关于说话人/语种识别的评测&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;熟悉说话人识别、语种识别算法，如 GMM，HMM，SVM，iVector 等算法&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;精通 C/C++编程，熟悉 Linux 平台以及相关 python,shell 等脚本语言&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;具有实际线上运行说话人/语种识别和检测系统开发经验者优先&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;语音合成工程师（工作地点：北京）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;负责语音合成文本语料库的设计开发&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;负责语音合成系统前端文本分析、韵律预测分析和线上问题分析，&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;负责语音合成系统中参数合成系统的研发&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;负责 HTS 语音合成系统引擎和拼接合成系统的研发 C13&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;职位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;精通 C/C++编程，熟悉 Linux 平台，熟悉 python 等脚本开发&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;有扎实的数据结构和算法设计基础，优秀的分析问题和解决问题的能力，对解决具有挑战性问题充满激情&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;具有良好的沟通能力，和良好的团队合作精神&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;有 NLP，机器学习，韵律/句法分析，语音合成声学模型, 拼接合成，声码器（vocoder）和语音引擎等相关经验者优先)&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在国际性程序设计竞赛获奖，数学竞赛获奖者优先&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;语音产品经理（工作地点：北京）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;负责语音产品的产品规划、设计、推进等工作&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;可独立完成产品功能的相关调研、设计、数据分析, 高质量完成项目循环和迭代&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;可对用户需求满足&amp;amp;交互方式提出创新型的想法&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;与技术团队紧密配合, 快速、高效推动产品设计、研发&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;职位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;1 年及以上互联网产品经理从业经验&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;具备互联网产品设计经验，熟悉语音&amp;amp;AI 产品和技术，且有浓厚兴趣&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;有优秀的产品设计、数据分析、用户体验评估能力&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;沟通执行能力强，注重用户体验，有强烈的责任心和团队合作精神&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;思维活跃，有创新精神，能承受工作压力&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;兴趣广泛，热爱生活&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;深度学习算法工程师（工作地点：北京）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;工作职责:&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;利用深度学习算法进行大规模数据的序列模型建模&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;开发 DeepSpeech 训练平台&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;跟进最新的机器学习算法，推进新算法在语音识别领域的应用&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;职责要求:&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;精通 C++编程，熟悉 Linux 平台，在国际性程序设计竞赛获奖者优先&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;对代码的效率优化和数据结构优化有实践经验者优先&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;对语音识别声学模型有研究经验者优先&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;深度学习、自然语言处理、语音识别、图像处理、模式识别相关专业背景，对深度学习的算法有一定的实践&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;优秀的分析问题和解决问题的能力，对解决具有挑战性问题充满激情&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;具有良好的沟通能力，和良好的团队合作精神&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;高性能计算开发工程师（工作地点：北京）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;工作职责:&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;负责语音识别模型训练的高性能计算方面的加速优化&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;负责识别解码器的声学模型计算模块的计算优化&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;负责并行训练平台（GPU 集群）的操作和维护&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;构建各种数据规模下，计算最快的大规模机器学习平台&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;负责实现训练程序自动化训练脚本及流程、效率优化&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;职责要求:&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;计算机相关专业硕士以上学位&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;精通 C/C++编程，熟悉 Linux 平台，有良好的编程功底，对数据结构和算法设计有较为深刻的理解&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;熟悉分布式/高性能计算系统，有 mpi、gpu 等开发经验&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;有丰富的大规模机器学算法习开发和应用经验者优先&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;优秀的分析问题和解决问题的能力，对解决具有挑战性问题充满激情&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;具有良好的沟通能力，和良好的团队合作精神&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;具有语音识别相关经验者优先&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;架构研发工程师（工作地点：北京）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;从事语音及信号处理相关服务软件架构研发&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;研究海量数据的存储、优化分布式计算架构、提升服务吞吐能力，不断提升系统时效性、扩展性、稳定性、性能&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;研究语音及信号技术适应的服务架构，分析和修改架构与服务策略&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;主要方向：离线数据批处理，在线服务系统设计研发&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;职位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;热爱互联网，对语音及信号处理相关技术有浓厚的兴趣，有语音及信号处理背景更佳&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;计算机及计算机相关专业本科或本科以上学历&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;精通 Linux/Unix 平台上的 C/C++编程，有良好的编程习惯&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;精通网络编程、多线程编程&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;对数据结构和算法设计具有深刻的理解，有 1 年以上系统分析和设计的实践经验&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;具备优秀的逻辑思维能力，对解决挑战性问题充满热情，善于解决问题和分析问题&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;有强烈的上进心和求知欲，善于学习新事物&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;良好的团队合作精神，较强的沟通能力和学习能力&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;简历请投递至&lt;/span&gt;&lt;span&gt;:&amp;nbsp;&lt;/span&gt;&lt;span&gt;wuyujing@baidu.com&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 21 Jan 2017 17:20:27 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 不要只看论文，缺乏工程实践才是深度学习研究的瓶颈</title>
      <link>http://www.iwgc.cn/link/4419775</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自dennybritz&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：微胖、杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote style="color: rgb(62, 62, 62); font-size: 16px; white-space: normal; max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;学术界面临的研究瓶颈不仅来自各种技术，还包括社区文化和研究流程等问题。本文提到当下发表的大部分论文中 90% 的内容都是他人的研究成果，剩下的 10% 不过是研究者测试自己的假设，而且这种形式的论文很少会带来惊喜。就连 Andrej Karpathy 都十分赞同的文中一个观点：读论文不会太在意它的结果，只是为了获得灵感。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ZOmknUpmm3m7sKze0ibcvqpuzn1gwV3tkde1S4QOOAeE9rIzWkARR02icytydjicApxhrktPVsfoAA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;读研时，我研究的是 NLP 和信息提取，我几乎将所有时间都花在编码研究思路上。这就是有一个不喜欢接触代码导师（这样的导师估计占所有导师的 95%）的研究生所做的事。当我对问题（problems）表示担心时，总会听到这样的话「这只是一个工程问题；继续。」后来我才意识到这句话的真实含义：「我认为，一篇论文提到了这点就通不过同行评议」。这种心态似乎在学术界普遍存在。但作为一个工程师，我不禁注意到缺乏工程实践如何会让我们停滞不前。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我会拿我熟悉的深度学习社区作为一个例子，但这也可能适用于其他社区。作为研究人员的社区，我们都有一个共同的目标：推动该领域发展。推进当前最先进的技术。有很多方法做到这一点，但是，最常见的是发表论文。绝大多数发表的论文都是渐进式的，我没有贬低的意思。我相信，研究当然是渐进的，也就是说，新成果是建立在别人过去所做的基础之上的。而且这就是它应该的样子。说得具体一点，我读过的大多数论文的内容中有 90% 都是现有的东西，包括数据集、预处理技术、评估标准、基线模型架构等等。作者们通常会再添加一些新的东西，展示一下做了哪些超越现有基线的改进。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前来看，这么做也没什么错。问题不在于这个过程本身，而是如何实施。在我看来，这里面有两个突出的问题，都可以用「工程实践（just engineering）」来解决。1、浪费研究时间；2、缺乏精确性和可重复性。逐一展开。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;浪费研究时间（站在他人肩膀上的不易）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;研究者都是经过高度训练的专业人士。很多人都花了几年到几十年的时间才拿到博士学位，成为各自领域的专家。只有那些人花大部分时间做他们擅长的事——通过提出新的技术来进行创新，才有意义。就像你不想让一个训练有素的外科医生每天花几个小时从纸质表格上输入病人的数据一样。但他们每天做的几乎就是这些事情。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在一个理想状态里，一个有想法的研究者能够轻松以已有成果为基础（亦即上文提到的论文 90% 的内容），剩下 10% 的内容用于测试研究者的假设。（我发现，也有例外，如果你正在做的研究真的很新，但是大部分发表的研究都不属于这个例外）。在实践中，几乎没有什么真正新东西。研究者花上几周的时间重复做数据的预处理和后处理，一遍又一遍地部署，调试基线模型。这些工作通常包括追踪相关论文的作者，弄清楚他们用的到底是什么技巧。论文往往不会提到细节（fine print），因为这会让结果看起来没那么令人印象深刻。在这个过程中，研究者会引入数十个混杂变量（confounding variables），这基本上会让比较变得没什么意义。然而后面还有更多没意义的事情。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我意识到，在他人成果基础上做研究的难易性是决定你正在做的是什么研究的主要因素。大多数研究者都是在自己的研究基础上一遍一遍做研究。当然有人可能会说，这是因为他是某个特定子领域的专家，所以，只有继续关注类似的问题才有意义。虽然不是完全没道理，但我认为，这么做没什么意义（尤其是深度学习领域，里面很多子领域之间联系非常紧密，以至于其间的知识迁移可以做的很好）。我相信，主要的原因是从实验的角度看，在自己工作的基础上做研究是最容易的。它能带来更多的论文发表，并让周转时间变得更快。基线已经用熟悉的代码部署好了，评估已经设置好了，相关工作也写好了，等等。而且这么做，竞争更少——其他人没法能接触到你的实验装置也就没法轻易和你竞争。如果这与在他人成果上做研究一样容易，我们可能会在发表的研究中看到更多的多样性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;并非都是坏消息。当然，有几个趋势正朝着正确的方向发展。发表代码越来越普遍。像 OpenAI 的 gym（以及 Universe）那样的软件包确保了至少评估和数据集能够做到效率化（streamlined)。Tensorflow 等深度学习框通过部署低水平基元（primitives）移除了大量潜在混杂变量。有人说，我们还有很多能做的事情没做到。试想一下，如果我们有标准化的框架、标准的数据库、标准的代码库和编码风格、严格的自动评估框架和在完全相同的数据集上运行的实体，研究效率又会如何。从工程角度来看，所有这些都是简单的事情，但是可能会产生巨大的影响。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我认为，我们低估了这一事实：我们是在和纯软件打交道。听起来似乎显而易见，但是，兹事体大。在诸如医学或心理学领域，设计牢牢加以控制的实验几乎不可能，工作量也相当庞大。而软件领域基本上是自由的。这一领域比我们绝大多数所认为的那样还要独特。但是，我们并没有这么做。我相信，这些变化（以及许多其他变化）还未发生的原因之一在于动机不对称。说实话，几乎所有研究人员更关心论文发表、引证率以及可授予终身教职的聘任制度，而不是真地推进这个领域。他们对有利于自己的现状很满意。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺乏精确性（rigor）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第二问题与第一个问题密切相关。上文也暗示过了。就是缺乏精确性和可重复性。理想状态是，研究人员可以控制住所有无关变量，采用新的技术，然后展示各种基线的改善情况（在显著边际内）。貌似显而易见？好吧，如果你碰巧读了很多深度学习方面的论文，那么，你会觉得这个理想状态就像直接源自科幻电影。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;实践中，当每个人采用不同框架和流程再度实现技术时，比较会变得没有意义。几乎每个深度学习模型在使用过程中都会存在很多会影响结果的「隐藏变量」，包括加进代码中的不明显的模型超参数，data shuffle seeds，变量初始化器以及其他论文通常不会提及的东西，但是，很明显它们会影响最终测量结果。当你用一个不同的框架重新使用你的 LSTM，预处理数据并写下几千行代码，你创造了多少混杂变量？我猜几百甚至几千个吧。如果你可以证明较之基准模型，有 0.5% 的边际改进，你怎么证明它们之间的因果关系？你咋知道这个结果就是结合某些混杂变量的结果？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我本人根本不相信论文结果。我读论文更多是为了获取灵感——关注论文的想法，而不是结果。这不是个应然问题。如果所有的研究人员都发布代码，会怎么样？会解决问题？实际上，并非如此。将 1 万条代码未入文献的代码放到 Github 上，说「在这里，运行这个指令，复制我的结果。」，这和生产人们愿意阅读、理解、证实和以此为基础进行研究的代码不是一回事。这就像望月新一证明 ABC 猜想，除了他，没人看得懂。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;再一次，「不过是个工程问题（just engineering）」有望解决这个难题。解决方案和问题 1（标准代码、数据组、评估实体等）解决方案差不多，但问题也差不多。实际上，发表具有可读性的代码，可能并不最有利于研究人员。如果人们找到 bug 怎么办？需要收回论文吗？除了为你效劳的单位做公关，没有其他清楚的好处，发表代码是在冒险。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;原文：http://blog.dennybritz.com/2017/01/17/engineering-is-the-bottleneck-in-deep-learning-research/&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Fri, 20 Jan 2017 12:01:05 +0800</pubDate>
    </item>
    <item>
      <title>演讲 | BOT大赛计算机视觉赛题经验分享：赛题详解与思路分析</title>
      <link>http://www.iwgc.cn/link/4419776</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;机器之心编辑&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;编辑：朱思颖、蒋思源&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;历时三个月的上海 BOT 大数据应用大赛完满收官，其中「计算机视觉识别」子赛题吸引了来自世界各地的 100 支团队参赛，赛后为了促进计算机视觉技术与创新应用交流与合作，BOT 大赛组委会联合机器之心、清数 D-LAB，邀请大赛优胜团队导师及成员，围绕大赛解题思路和计算机视觉领域前沿技术及创新应用开展分享交流。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;内容目录：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Ⅰ 赛题解读&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Ⅱ 冠军团队指导教授王金桥：大数据时代的视觉智能&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Ⅲ 大唯团队陶进：小样本图像检测深度学习算法研究&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Ⅳ DeeeeeeeeeepNet 团队陈朝才：深度学习在目标检测领域的应用&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Ⅴ 现场精彩问答&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe class="video_iframe" data-vidtype="1" allowfullscreen="" frameborder="0" height="417" width="556" data-src="https://v.qq.com/iframe/preview.html?vid=l0364xxf2kv&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0"&gt;&lt;/iframe&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Ⅰ 赛题解读&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;赛题好变态，这样的赛题是怎么来的？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;BOT 大赛组委会赛题组组长尹相志：计算机视觉识别赛题的设计解读&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本次计算机识别初赛赛题的主题：基于机器视觉的认知情境理解 (CCRCV 2016, Congnitive Context Reasoning for Computer Vision)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;赛题里的每一个子赛题的设计其实都代表了我们在机器视觉领域里的一次尝试或者实验。目前的机器视觉已经进入到深度学习阶段，那么机器视觉在当下人工智能的实现过程中，有哪些需要过的「坎」？大赛组委会特意把这些坎找出来，融入到赛题的设计中。组委会希望通过这样的方式，让参赛的团队联手对付这些难坎并期待有新的突破。即使只有一点小小的推进，对于整个人工智能来说都是很重要的突破。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;具体到我们这次大赛的试题，我们的赛题来源于对现在机器视觉盲点的思考。虽然随着深度学习技术的发展，单纯的图像识别对于计算机来说已不是一件难事，甚至可以做得比人类更好，但前提是必须要给机器足够的训练图片。然而这个训练与人类的认知过程是背道而驰的，人类的小孩不会需要上万张图片才能够理解什么是狗，而且当他看到其他品种的狗，也许初期会叫错，但是慢慢会理解狗这个物种的抽象特征，进而理解狗——这一「概念」。因此未来不管是看到吉娃娃或是藏獒，都能认出这是「狗」。这正是现在机器视觉的盲点，人类可以对图片进行理解，透过小数据去进行泛化的推论，而目前机器视觉则专注于如何从大量数据中抽取图像特征，因此庞大的图片标注以及计算力的考量成为现在计算机视觉识别商业应用落地的最大障碍。本次计算机视觉识别初赛与复赛分别从不同角度来考量这个问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1. 初赛题目设计&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大数据和神经网络之下的机器视觉图像分类，在分类的实现方式上已经有了非常大的改进并且在识别的准确率上有很大的突破，但是我们认为目前的分类做法只是让机器「识别」待分类对象，并不是「懂得」分类对象。即便今天机器认出了长颈鹿，但是长得像长颈鹿的其他对象并不在它的认知范围内。所以对于这个「坎」，我们希望图像分类的做法能够让机器真正懂得分类对象，而不只是识别，比如看到了一个长颈鹿的皮毛或者是斑点就认为这是长颈鹿。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我们在这次大赛的命题上，从一开始就有很清楚的规划，我们希望参赛队能做出的分类是基于认知场景的，是对待分类对象的真正认知和理解，而不只是单纯的识别，这是我们在这次大赛赛题设计中的一个关键考虑点。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如何透过认知解决无法穷举的变体，用一个实体去推演其所有的衍生物。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们今天看到一个「实实在在」的动物，我要知道哪些布娃娃，哪些雕塑，哪些绘画，哪些抽象的形态都是来自于它，这个题目要求参赛选手要建立客观存在的实体和与之相关的抽象形态之的连接，是一个理解并推论的过程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsm56JJiagIibYqeUMrnuuklhPAQALrupMKagGMVV0KiagibcMXNH5bBEuKA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;初赛有 7 个隐藏题目&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;，&lt;strong&gt;这 7 个隐藏题目来自于我们机器视觉的实验&lt;/strong&gt;。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNs2HzptdcBrn5X1K4LiauAfWGoUYiciaN0icNbIThSSXich2NLXfleSzjZicEg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第一个隐藏题目是扮演装扮，在这张图片里大家可以看出来，这是一只扮成长颈鹿的狗。所以机器需要有里跟外的概念，谁是本体，谁是附属，有了这样一个概念才有办法继续进行后面的处理。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsxA5YsgFM7NeBOdZ84wIltRJoj0AatYelKaIicdHXaqxhKfR0TCBmZQA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第二个隐藏题目是图片里有两种动物，我们一开始就没有跟他们说一张图里只有一种动物，我们有的在图片里面放两只，而且有的是抱在怀里，几乎是看不到的，两只的话就可以得到比较高的分数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsibK0iaGtSQPdz54TLesjpJJF2sias2qDf4dw67cPtmq65bxL158fVmBibw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第三个隐藏题目是特殊品种，这个长得像拖把的其实是一只天竺鼠，我们也是后来在收集图片时才知道原来有这种物种。在狗的图片集里我们把全世界最丑的狗也都放进去了，甚至丑的都不具狗形了。其实这些都是个体的变异，怎样去从这些个体的变异中识别出其中的差别是我们这个题目的考察点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsVkQmgf19VXwM2aicWygOvkXPqq7QnLKlm1INgo0gBm9wZIDibS5lpO8A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第四个隐藏题目是微小线索，我个人一直很好奇机器学习到底会怎样去看待那些肉眼几乎找不到动物在哪里的图片。我们人觉察到的微小线索跟机器的认知可能会有所不同，因为人不是从像素层去看动物特征的，而机器可以做到。这其实是一个很难的题目，不知道大家能不能看出，这张图片中有只猫，机器识别出来的概率有 15.7%，稍微差一点。由此看出，当图片的背景变得复杂，卷积层网络从底层抽取特征的时候会提取过度复杂的特征，而真正有用的与动物相关的特征所占有的比例太小，都会使机器降低辨识出动物的可能性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsjBSNUqNZno8ic0NVed86icScgQBVI1VCY2f0libWPQlIP0q6gfkeKQAsg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNs6bMN3RGVhKVCwPpiaMkV4DsCDQvViccv3MnZrusmknpooJ6o4wm6qRXg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大家能看出来这张图片里面有动物什么吗？很奇怪，我拿给十个人看，有九个人看不出来，剩下一个人告诉我说有鳄鱼。这是人跟机器的最大差别，人类的视觉成像会有错觉影响，事实上在这张图片里很难找到好大一只，右边有只长颈鹿，但是人可能就看不到。周围环境的背景色跟它太像了，所以机器学习到的视觉特征很容易与其他树叶交叠所学习到的特征产生误判，这也是比较容出错的地方。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第五个隐藏题目是高度抽象图片的识别，这个题目也是大家觉得很可怕的，因为我们高度抽象的图片选取了几种不同类型的图片，包括非洲十万年前石壁上的原始壁画，这是我们煞费苦心收集的而且还特别对应到本次考题范围的动物中。另外，我们选取的剪纸和对联也是高度抽象化的东西。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNs3fOWfbqibHxZCL9sPghZMDk2OnfWV9FUpPkxbboyOUyaU4g10zaQGWQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第六个是堆叠。我们在设计考题时，想到既然卷积神经网络是从像素层级去提取特征，那如果要给卷积神经网络一个最大的考验该如何下手设计题目呢？我们的设想是，如果有一系列的东西，从像素层级来看都是一模一样的单元构建出来的，那么卷积神经网络在处理时是不是就没有像素层级的特征了？那有这样的物体吗？答案是乐高积木或者是游戏我的世界。一个同样的单元做出来的图片，不会有像素级的特征。我们可以看到，机器在这里面的表现也是稍微差一点点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsicDvreA27moq7iaOQjvLjatvrYRDMNIib9EJkjicgqBOicGmPJXGW6T1NJw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第七个是生成模型的图片识别。第 7 个隐藏赛题的出题概念是源于什么呢？中国有一个很古老的寓言是矛与盾，当一支最强的矛遇上一支最强的盾谁会赢？我们使用了强大的图片生成模型来对抗选手们的图像理解模型。在决赛里我们准备了许多风格迁移的图片。也就是，通过风格迁移的方式，我们将图片中的风格融合到既有的动物图片里去，这种融合可以做到无缝融合，对人来说分辨这样的图片也很勉强，但仍可以看出来原来的痕迹，对机器呢？在这些风格迁移图片里，我们不但做了图片风格不是很强烈的图片的迁移，还做了几个风格极为强烈的日本现代艺术家的作品的风格迁移图片，包括草间弥生和蜷川实花，他们的风格在人看来极为明显。当这样的迁移风格的照片出现的时候，所有的像素细节都被替换掉了，机器能够判断出来的可能性微乎其微，也因此出现了这次大赛第一次发生的状况。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNs8BlOtuTPdWicxqCyCfQkv17563wVPp9nNKrt02TklWsFeiauzlgkk1Aw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNswiaosm0adDKG7tQnpkNsynibOXdIXd5jp9rs7kDoicWuAcggMicV0wVl7w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这张图片答对率是 0，没有任何参赛选手的识别模型能识别出来，这张是融合了草间弥生作品的迁移图片，图中有两种动物，一个是猫，一个是天竺鼠，当草间弥生的风格迁移到这张原图的时候就会生成下图这样的图片，对我们人来说依旧能认得出来，但对机器则是遇到一个难题。我认为对于机器的视觉模型来说，最难的不是其他，就是来自图像生成模型。我们知道现在有很多如 GAN(generative adversarial networks) 这样的生成对抗模型，怎么样通过对抗的方式来让模型不断的进步，这也是未来视觉里很有趣的一个主题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「&lt;strong&gt;初赛的一个整体思路&lt;/strong&gt;，&lt;strong&gt;其实是帮助我们去理解现代的机器视觉能做到什么&lt;/strong&gt;，&lt;strong&gt;不能做到什么&lt;/strong&gt;，&lt;strong&gt;限制在哪里&lt;/strong&gt;。&lt;strong&gt;我们只有认清楚了这些限制&lt;/strong&gt;，&lt;strong&gt;才有可能让大家变得更好&lt;/strong&gt;。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2. 复赛题目设计&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;复赛主题侧重对机器视觉实际应用层面的考察 (初赛主要是基本理论的理解)。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有很多问题是抽象的，没有一个绝对的操作型的定义。也就是说，没有任何人可以告诉你什么是标准答案，这在人的世界里是很常见的，因为很多事情，一百个人有一百种看法，那么在这样的问题里机器如何去得到一个相对精准的判别呢？这是挑战之一，另外一个很重要的挑战是行车记录仪赛题中需要选手们预测行车标志，但其中有几个标志在我们提供的训练集照片里一张都没有出现过。这时候又该怎么办？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;行车纪录器图像评估驾车操作场景&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这个赛题里面，我们希望要看到的是，当你去做端对端的学习时，你需要把整个复杂场景的转换逻辑或者是整个思路全部都依次排列在里面，你的模型得是一个泛用性的模型，否则你没有办法去处理这样的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果能够把问题做一个适度的化简拆解，这个问题就没有那么难。例如让许多选手们觉得困难的如何判断压线与逆向的问题，不直接让机器从整张照片去理解有没有压线，而是先解决车跟线之间的关系，再从车跟线的关系往外延伸。这个赛题里面主要是看大家能不能撇开对深度学习端到端实现的过度追求，有一些追求固然很好。但对于解决真实世界问题的时候，它不应该变成你的枷锁。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来这个是最大的难题，行车标志，我们当初附上了一张行车标志全图给选手，为什么给了一份标志全图？因为在所有图像里行车标志是不会变的，都有固定的外形。唯一有影响的是你的视角，还有就是呈现的远近。在这里可以用两个策略来解决，第一个策略，用深度学习方法来做，应该可以把这些图做一些 3D 的旋转、变形、色调的数据增强，从而来解决这一类的问题。第二可以不用深度学习，用传统的机器视觉中的模版比对可能效果会更好一些。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我一直觉得新旧方法都有它的各自优缺点，怎么把新的去做新的擅长的，旧的去做旧的擅长的，数据量不够的情况下，新的方法可以通过数据增强来解决，或者可以请旧的方法来帮忙。我认为把这些方法做一些融合，这是有助于机器更好的来理解这个任务的一个重要方法。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;卖场货架自动计算产品货架占有率&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;货架这个题目是我们花费最多心思的，除了拍照片之外，我们拍回来的 1600 张照片，将近 12 个人去标了 2 两个多礼拜。这个题目的最大难点，其实在于要在给出的 1000 张训练集照片中，去识别 240 种的商品。我们当时在设计这道题的时候还考虑题目会不会过难，因此给选手许多参考性的材料，包括将多边形转换为 Mask 以及提供裁切过后的商品碎片。当我们给出裁剪干净的商品之后，能够对这些图片做数据增强的方法就有很多了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNssBagxxr8tNEHdaI0ZREn8OLRdiaCehvoVVUkf7VJY4w7zTWNWUlPFEQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我们在这里其实考的是怎么样通过小样本进行学习，因为我们认为现在深度学习最大的问题还是在于样本数。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3. 决赛题目设计&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;中文视觉智能问答&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsLM6YG8cWIjuZPJJckg4juDYre9A9DIibAvVJmEOicyEXm7RAc9caXklQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsjX27b80uThibgbW22ek7vYvxgswCuMXdsQlnY4BUK1fQ2EqZEbV8zDg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;选手在做这道题的时候不可以用电脑屏幕把任何一个照片投出来，一投出来就算丧失资格。因此是在完全不知道图片内容的情况下，去回答这些中文的视觉智能问答，这也是我们希望未来可以继续的一个比较有意思的研究，希望有更多技术强大的团队们可以一起在这个领域钻研。未来的这种智能的对答，以前都是只注重在所谓的语料生成的方式来找到一些似人类的像鹦鹉学舌一样的回答。但是我认为通过图像的理解来产生一个有意义的回答，这是一个更有趣的东西，这是我们的第一次尝试，我也希望可以把它慢慢的扩充，包括一些有场景式的问答，也许下一次就是一个菜单，或者是一张地图。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Ⅱ冠军团队指导教授王金桥：大数据时代的视觉智能&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1. 研究背景：视频语义理解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;预计在 2020 年，互联网数据里大概 90.6% 的数据都是以视频的方式进行呈现。第二就是在物流空间，监控视频每年也以 20% 的速度在增长，在这种海量数据爆炸性增长下进行数据分析，为实现我们的需求带来了很大的挑战。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNs0ib4KHTGTL2NlE1d04iaia6E3d50BVA1aplHAIiagZH1bbvfE6ajkSvt6g/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当前主要经历了三个阶段，从最初城市里任何视频监控都没有到平安城市，安装了一些普通模拟的摄像头，有了数字化的数据，到现在我们基于大量数据以及大量积累的算法，慢慢的可以把这些数据变成知识数据，进入到智慧计算的阶段。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsyccic0rpcNicT825rTrZOcOSjUsVkiaproiakH7b9KdBGU3AHhMOuKticVw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从认知的角度来讲，人工智能可以说是摄像头的大脑。从人工智能发展的角度来讲，经历了从看得见到看得清的过程，我们首先通过安装各种各样的摄像头，现在每个县，每个村都安装了摄像头。另外就是看得清，原来我们在 80 年代的时候，最初来做这种图像识别的时候，当时机器只有 586、486 这种设置，非常复杂，数据非常少，原来的摄像头数据每个图像是 320、240 的分辨率。在这个分辨率下很难提出我们要识别的是内容，以及使用我们人眼的看法。现在随着计算能力的提高以及 GPU 的出现，整个视频发展到了一个大数据深度学习的时代，使大家通过一些专家系统，能够使我们从视频数据中得到更有价值，更加智能的一些分析。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsMS4NVE9cjia3JnnPGfzBR0HtR76qSgDaJKcZv3zQnFiaLl7zticszGwAQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从技术发展的角度来看，当前的发展经历了从规则式到大数据时代的，经过了目标检测、车牌识别、人脸识别、文本识别到视频检索，通过模拟其他的特征，都是通过人工设计的特征，符合边缘的某些特性或者是支持向量阶的方式，来得到一个基本的算法，能满足一些规则的应用。这里面最成功的是车牌识别以及一些文本的检测，手写的识别和人脸检测。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsAjo5kiakIJiatWhmXabcM9H2I7L0tKYadpqPFIicQ6iaxqUT7NxcTsYfibg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;慢慢我们的数据量在不断增加，我们慢慢的支持向量级，整个在小数据的时代，基本上是知识向量统治机器学习的时代。而现在随着 GPU 的出现，数据的海量增加，现在达到了一个全数据驱动与深度架构、智能分析算法三者深度结合的时代，是一个大数据的深度学习。包括现在各种成千上万的分类识别，目前都是基于这种大数据的智能视频分析深度学习框架。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsXibR9SCkOJ1QicjWncDGghhISPVEe1MtWy3VYgSYvzrkVvAIsJadxFkg/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2. 研究项目节选&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;智能物件识别的检测&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个网络设计得非常复杂，包括卷积层、反卷积层、STN 的校正网络、空间变换校正网络、空间上下文描述的学习网络以及空间变换的随机扰动、随机定位，对不同尺度敏感的自动回归以及最后的 NMS 做的这种精细的目标分类和空间关系建模。它是一种由粗到细的一个精细的检测分类融合的框架。目前我们主要是针对生鲜领域，包括苹果、梨、蔬菜，以及各种牛奶，如光明的、蒙牛的。目前已经实现了 50 个大类，200 个子类精细的识别和检测。目前在食物产品中基本上达到 90%，主要目标遮盖小于三分之一，基本上就可以精确检测到。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsWkNia5vKqqYdHLfHibhCpNm7Drx6LAGUqXAUpedTdFnGNRuibj8tKO32Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;图像语义分割&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是一个像素级的语义分割，包括人的场景，主要是做机器人的，就是服务型机器人，常见的床、桌子、椅子、茶杯、手机、水杯、电视、冰箱这种 3C 类的，还有服务类的，比如吃饭，家庭常见的这些目标，让机器人来辅助人完成它所寻找的这个目标的功能。最后是基于服装的解析，就是我们把人分成 22 个部分，包括他的头发长短、性别、年龄，有没有拎包，他拉没拉箱子，长裤短裤，长裙短裙。这是我们当前的一个效果，这是 22 类人的解析，我们可以把人大概在 10 到 20 个小米之内分割成 22 个区域，主要是面向公安的一些拍照购物搜索做一些应用。网络视频的录像当中是没有标签的，我们通过这些录像的区域之间的关系来判断这个视频的镜头和场景的分割，给它做一些视频镜头和场景之间的标签，以及做一些图像的区域和图像集，以及图像整体的一些水平，这个我们主要是用图模型。这是我们从 2004 年做到现在，做了十几年的一个工作，这个工作目前在很多场景下应用。这是基于图的视频图像标注，基本上是我们组一个标志性的工作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsftD3EbVjpRGxJSRGlTo2ZpCx4ibJzCAIHdZfITma96Jj2YDDkfyEslQ/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;哈希的大规模图像检索&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是一个哈希的大规模图像检索，就是把一个图像通过各种变换，我们一个哈希检索做法是在 2014 年发表 fab，影响非常大，我们把这个代码已经开源了，已经被很多公司和企业应用，可以实现这种快速的三维重建以及快速的目标检索，基本上可以实现在一秒内亿级数据的海量查询。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsl1HibyLXIicLjCmMwuKxOjASKZ1nYT9JdiaqR4W0Ed7J3Hw2zYG8Escwg/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3. 研究经验分享&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基于图像视频的处理工作，是一个科学和工程深度结合的一个事情。它的数据就是你必须要对这个方法理解得非常深刻，你才能够针对这个方法去选取适合方法的数据，就是你的方法是解决哪个问题的，你根据这个问题去发现什么样的数据更适合它，你当前的这个方法对那些数据不适合。另外就是数据的清洗非常重要，你在准备数据的时候，这个需要大量经验的积累，因为它也是一个实验性的科学，就是说你在数据标注的时候，这个框是不是要标得特别紧凑，比如说你这个目标遮挡三分之一还是露出来一点，都是需要经验积累的。所以说包括这次 BOT 比赛他们数据标注里不是特别有经验，我们都会对数据进行一个清洗。数据清洗对最后算法性能的影响大概在 10% 到 20%。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从框架选取的角度来说，以不同的应用选取不同的框架。比如说你要用人脸的，ConvNet 的卷积方式更适合，你要想做简单的分类，或者是做一些跨平台的融合，TensorFlow 是大家关注最多的，而且我们现在每个框架基本上都用。另外 MX Net 本身的 Bug 比较多，大多数都是中国人，模型释放的比较少，如果你自己选择转模型就会有大量的损失。所以我建议你以 TensorFlow 为主，或者你做 OCR 或者是语音相关的，你要做时序相关的，一般这种 Touch 的也很多，就看你的应用方向，还有你最后的发布平台。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Ⅲ大唯团队陶进：小样本图像检测深度学习算法研究&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1.&amp;nbsp;图像检测需要做一件什么样的事情&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;图像检测顾名思义，就是模拟人看图片，能够判断图中有什么物体以及这些物体在什么位置。简单来说，给一张有飞机的图片，图像检测需要做的事情是，首先要找出飞机所在的位置，然后对这些所在的位置进行判断里面的物体是什么。传统上做图像检测会把它分三步来做：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第一步是区域选择，这是为了支持目标位置的定位。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第二步是特征提取，当你把目标的位置定位之后，就可以进行图像特征提取，往往这一阶段对整个检测环境起到非常关键的作用。在特征提取时，边界信息的提取很重要，也是容易忽略掉的信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第三步是分类器分类，当我们获取到了整个特征的时候，最后一步会给分类器进行分类的操作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2. 复赛赛题解析&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;超市商品分割分类的问题上，整个过程种最关键的难点是我们的训练样本量非常少，需要分类的种类非常多。我们把下面的数据统计出来，三大类里面，主办方给了我们 363 张图片，其中只有 336 张图片包含了有改进的图片框，后面依此类推。我们要识别 239 个主类，分别对应每个类别我们需要时间的数目。最后是我们做检测的人非常熟悉的一个词，Bounding Box 就是检测框，给了我们 1319 个。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNshb3EBybe4n6JAURSggyic80wC7741aywn5SKnHMzyZhI6DKw02hlvAQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;总而言之，我们怎么在这么少的数据量之下识别这么多的种类？可能很多人一开始就会说，我把所有的检测框架跑一遍，去比对哪个效果最好。可是我想给到大家的一个小小的建议是，在跑框架之前，其实我们是可以对最需要的数据预测一个最直观的分析。我们可以看到，这两张图片上面最大的特征其实是一个集群的物体，就是说某一个泡面旁边种类的泡面和他是一个类型的，我们就通过去找相应的资料处理这个问题。在我们阅读查找文献的时候，SSD 也就是一个检测框架，论文作者提出这个框架的时候，他是基于另一个检测框架选取特征，对小物体和集群物体无法很好识别的问题做了改进。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第二个是我们查到陈新鹏同学在自己的博文上展示了 SSD 模型在面对小规模数据集上的良好效果，他把这个 SSD 模型应用在了自然场景的文本检测上。实际上我们一个单词，或者一个成语也是相当于有点集群的概念在里面。这样的话，就相当于我们可以省掉很多的时间去试一些我们不需要试的框架。但是还有一点是说，数据量实在真的非常少，我们可能还是会想到要做数据增广，依然还是那个建议，你是根据你的数据性质进行适当有效的数据增广。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsN0Y9bYKsuy1sZDc8MWhEKibkLKl1sKYBDMuF5JSxibkrFw5ySF4fhHgA/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们这边看超市物品的时候，我们可以想像，超市的物品出现在不同货架的不同位置上，所以我们从位置的角度去实现增广是一个合理的方式。同时超市倾向于将同一类别的物体摆放在一起供顾客选择。也就是我们左边的图片，后面的会集中摆在一起，泡面会不太常见的出现在一个超市上下货架的两层。所以我们最后依据这样两个假设，我们看到这个其实也验证了我们中国的一个习惯，就是同一类别的物体基本上都放在同一货架上。我们最后采用的增广的方式，就是把一张训练图片进行上下左右三分之一切割，取其中三分之二上的图片，这样一张训练集被变成五张。唯一需要注意的是，分割之后检测目标要进行相应的量化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;ⅣDeeeeeeeeeepNet 团队陈朝才：深度学习在目标检测领域的应用&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1. 检测的应用&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第一个是做人脸检测，这个做完检测之后还会有很多应用，比如人脸识别会先去检测这个人脸，还有人脸的特征点定位。就是检测到人脸之后，再去点击鼻子，做一些美化或者是其他的特效，还有旁边行人的检测。下面是车行的检测，这个在交通当中应用是很多的。比如说首先需要检测到这些车，再去对这些车进行分类，看看这些车是什么车。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsNcHxul4RocGFKGS498zZ0Ifg7H70ceOV69ibp2x4PHdrVXNewKPWUuA/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2. 常用检测框架&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些检测的方法我大体上分为三类：一是 Cascade CNN，这是一个 CNN 的结构；二是 RCNN 相关的，这里面我列了几种，当然还有特别多的优化这里没有放出来。包括 RCNN，然后 FastRCNN、FasterRCNN 和 R-FCN、PVANet。三是 YOLO、SSD。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsVHsSGPXyarVn31gFKiaADe7Ky1ojZpmzGwccH0nIXBgqDG83xkCxtBw/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Cascade CNN&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先说一下 Cascade CNN，我们做目标检测的时候有一种非常暴力的方法就是用一个框去辨别这个图像，做一些分类。比如第一个框我框出来了，用一个分类器去分类，比如我这里检测是人脸，我就去做分类，看看这个框里面是不是人脸，我去扫一遍的话，扫到一个区域是人脸。如果我分类的概率大于某一个阈值的话，输出这个地方是人脸，这是一种非常暴力的方法。针对这个方法我们有很多的改进，比如说这里会有很多框，几万个框计算代价太高了，我会先初步的做一个筛选，比如先用一些大的框去扫，扫到某一个区域可能有人脸，我再对这个区域进行检测。这样的话，我通过一个极连的步骤，可以大大的减少计算的时间。更详细的方法在下面，大家有兴趣可以去看一下，它主要是针对暴力扫的话，这种极连不断的去减少这种计算复杂度的方法。（附该结构的 Github 代码地址 https://github.com/anson0910/CNN_face_detection）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsaKryD3rDKB6UuIibdEhs1SnM9HtYRibU74jsdWZlx2AwKy9YaQILibcXA/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;RCNN&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;RCNN 是目标检测里面的一个开放性的模块，其他的主要思想也和刚才的类似。刚才在扫这个区域的过程当中有三步：第一步去决定一些框，第二步要对这个框进行分类，第三步要根据我刚才得到的这些框好输出最后的结果。这三步 CRNN 里面做了相应的改进。第一步刚才我们的框是暴力的，RCNN 里面会有一个 Region Proposal 的方法，先看哪些地方有物体，先把这些框作为候选的区域，大概只有 2000 个，比刚才要小特别多。得到这些候选框之后，第二步我们要对这个框进行分类，都有一个卷积神经网络，这个卷积神经网络针对这个框，比如我们常用的是 24×24 的，通过卷积神经网络最后的全链接，一般是 4.96，得到一个 4096 位的特征。得到这个特征之后，我们有些常见的特征，对每一类进行训练一个分类器，我们就可以判断出来，比如提取这一项之后进行分类是机器还是人。这个改进了刚才的三步：第一步就是我们用 Region Proposal 的方法替代暴力扫的方法；第二步就是用 CNN 的特征做分类，不是用传统的一些特征；第三步是用一个 SVM 的分类对候选区域进行分类。这里面还有一个改进，就是我这个候选区域框的位置可能不对，这个人就提出来一个框的回归方法，就去纠正这个 Proposal 的位置，这样的话也带来一个检测效果的提升。（附该结构的 Github 代码地址：https://github.com/rbgirshick/rcnn）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNs3ws9pqXJbOo1O4iaNh0KNG03Kd3zhw9S7iaJReWEdNlofxDT5W6zmKyg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;实际上在这里还有一个改进就是，比如说在这个 CNN 的部分，我们一般是用这个模型直接去提这个 4096 位的特征。这样的话就会导致一个问题，就是说你的模型是在这个上面训练的，但是应用在这个检测的里面，可能我的数据跟你原始数据的分布是有一定差异的，它会把这个模型拿过来，在我的检测数据上面进行一些微调。微调的意思就是说，比如这里分类一般是 1000 类，我把这 1000 类，这个检测只有 21 类的话，把这个全链接层换掉，把这个网络拿到我的数据下面训练一下，会带来很大的分类效果的提升。最后综合这三步，RCNN 相对于传统的目标检测的方法效率、速度和精度上都有了特别大的提升。下面的工作基本上都是针对这三步分别进行改进。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;FastRCNN&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;FastRCNN 部分，刚才在目标检测的后两步，要针对每个区域先提取特征，然后再分类，FastRCNN 就是想把这两步融合起来作为一步。这里就提出来 ROI 的方法，其实就是刚才的这个候选时序。第二步就是设计了这一部分，实现了一个 End to End 的训练。这个当中一个核心的 ROI 有一个解释，这张图片通过我的一些卷积层，比如任何一个网络，我会得到一个 Feature Map，这是 N×W，相当于一般的卷积神经网络最后的卷积层。针对我下面这个候选区域，我会把我的候选区域分为一些块，比如说常用的我会分成 7×7 的一共 49 个，分成 49 个之后，我会把这个区域映射到对应的 Feature Map 这个区域当中。这里有一个特殊的方法，一般的神经网络有四个值我取一个最大的。这里是针对这个区域，比如左上角的区域是取一个值，针对大的或者小的输入，都会生成一个特征向量，这样的话就会解决这个问题，虽然我的候选区域大小不同，但是我最后得到的特征维数是一样的，这样的话就可以做一个批处理。最后通过这里有一个分类的损失和合规的损失，就可以分配到这个序列。（附该结构的 Github 代码地址：https://github.com/rbgirshick/fast-rcnn）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsGzDm4ICLUqVVcFg6jeC8B4yaS9sABFWbPLpGfmyOwKLTibXu0YXu7xQ/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;FasterRCNN&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;FastRCNN 解决了刚才这三步当中的一步，还有一个部分没有解决，要去提这个候选框，这个是非常耗时的。FasterRCNN 就解决这个问题，看能不能通过深度学习直接提出这些候选框。这里有一个非常好的设计是 Anchors，比如刚才我通过一张图得到一个 Feature Map 之后，比如这个是 H×W 的话，那么我对于这个 Feature Map 上面，我认为这个点会默认的存在一些光。在 FasterRCNN 当中，我把这个原图做了一个可视化，比如我默认这个点大概存在 9 个不同的框，这个框我设计完之后就可以直接从图片里得到这些框，也会针对每一个框输出。比如你默认设置一个 K 框的话，会默认得到一个分类的框，还有一个是坐标 4。这里只是分类物体，就相当于解决了刚才的提取框，哪些为了可能存在物体。通过 Region Proposal Network，用一个简单的网络可以达到每秒 14 帧的处理速度。当然这里面有很多训练的细节，大家如果感兴趣的话可以看一看。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我这里给了一个 FasterRCNN 一个整体的结构。比如这里是图片的话，这里是我们常用的卷积网络，在这里得到我的 Feature Map，就是这个地方。通过这个 Feature Map 有两个分支，通过这个 Feature Map 可以提刚才的候选区域，通过候选区域又映射回刚才得到的这个特征，Feature Map 就相当于提取了某一个候选区域的特征，这里再加一些全链接层或者卷积层，直接得到它的结构。我们可以看到，整个就是一个端对端的训练。这里是 FasterRCNN 的一个介绍。（附该结构的 Github 代码地址：https://github.com/rbgirshick/py-faster-rcnn）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsOJwGricbxibg6lyJyu5ufBpyF7VQY47U9JoIRHG9gJkpIsicoAdmY4frw/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNs5cyiaEJWia0kB0ISVkyBx0sBSYiaZto1RlGlyFfWhDo3PGfpkRSYTo8kg/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsM0RKGE0eINiapg2ttLfmYdib2jicjshXhrKNUFW1RuZ966NlXrZAEtqJg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;RFCN&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面是 RFCN 的结构。RFCN 解决了什么问题呢？在刚才的这个图当中，如果这里得到了 2000 个的候选区域，每一个区域都要进行分类，这一部分要运算 2000 次，这个特别是在残差网络，在运行过程当中，会把最后的做分类，这样的话预算代价特别高，RFCN 的提出就是为了解决这个问题。这里也是设计了一种特殊的呼应的方法，就是刚才通过这张图得到了我的特征图之后，向大家强制输出跟我的类别相关的一个特征图的数目，比如我一般是 256 个，我把这里强制输出是乘以类别数的 Feature Map。这里有什么好处呢？这个好处就是，我针对每一个类别，比如我输出的是九个 Feature Map，，这里是特征图，这是对应到特征图的区域，把这个分为九块。做铺列的时候，针对第一个 Feature Map 图只举左上的一小块进行铺列，得到第一个图，第二个得到第二个图，九个特征图最后得到九个小块。这个是跟刚才 FasterRCNN 里面是一样的，我这里也给了一个 RFCN 整体的架构，这一部分跟刚才不一样，我如果得到特征图之后，这里是一个新加的卷积层，这里是让大家强制输出成和我类别数相关的一个卷积层，就是 Feature Map 的数目，这边是一个分类，这边是采用刚才这种铺列的方法，最后也得到了检测的结果。这里是一个效果对比图，FasterRCNN 的话速度是 420 毫秒一张图，通过这个简化之后，RFCN 是 170 毫秒一张图，相当于提升了三倍的速度。对应的准确率比 FasterRCNN 要高。（附该结构的 Github 代码地址: https://github.com/daijifeng001/caffe-rfcn）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsB1StndPeibXFJiazdoIrF8E8qy0ztibr8BYHwkWvoicavbaM9ibLEqhgyDQ/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNs54EAaFOQficKffZvqbno2aIibKiayhicuwMpucWylVGyTTyxgPC6F38RpQ/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsfBuGNkSuGI5NaNb2SaicCLsnTgvqicmyFibZy9aFWI4wzz4gobXg9iaHqg/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;PVANET&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这一部分介绍的是 PVANET。我们常用的这一部分，通过图像得到这个特征图的时候，我们一般常用的是这些网络，这个提特征值的时候有没有更好的网络设计？PVANET 对这一部分做了三步改进：第一步使用了 CReLU 的激活方法，一般的卷积层比如有 200 个，会输出 256 个 Feature Map，这相当于有一大部分是冗余的，这个 CReLU 就可以让输出的数量变少，比如 256 个 Feature Map 变成 128 个，这样的话就可以减少预算量。第二步是引入 Inception 网络，Inception 模块是有多个不同大小的卷积组合，得到下一层的 Feature Map。这样有一个好处就是说，相当于做了一个特征的融合，就是说我得到的特征是有不同的特征，不同大小的卷积得到的特征。第三步是把卷积神经网络不同层的输出进行结合，这里有一个什么好处呢？比如说一个网络有五层的话，第五层可能是结构化信息非常高的。这样的话，可能对一些小的物体，这些小的物体到高层的话损失特别大。这里就把低层的 Feature Map 和高层的 Feature Map 进行融合，得到了最后的这个，这一层包含的信息比传统的信息更多。而且更重要的是，这里通过一些设计，让卷积层 Feature Map 的数量变少，达到了很大速度的提升。下面这个图可以看出来，PVANET 可以达到 46 毫秒一张图，比刚才的 RFCN 还要快，刚才的 RFCN 是 133 毫秒，这样就可以达到一个实时的效果，大约 120 帧 1 秒的速度，而且是不同的融合方法，最后检测的准确率是非常高的。（附该结构的 Github 代码地址: https://github.com/lvchigo/PVANet_train）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNs9BNh43hgQ5NfABcpL176fXHpLqKsPA1E1n3XeZ9AuwZBUYyRs7ib5Qw/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;YOLO&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;YOLO 的出发点是什么呢？在刚才这个检测流程当中，我通过这里得到一个 Feature Map 之后，先要在这个 Feature Map 上面得到一些候选区域，又要针对这些候选区域，再从这个 Feature Map 上面提一遍特征。这里会造成对每一个候选区域要做两次预算，这里等于是不需要的。下面的方式就是直接从图片得到最后分类检测的框。YOLO 就是把一张图分为很多的区域，对于一张图有一个狗，怎么对应呢？我的中心框如果对着这里，这里就负责解决这个问题，这里得到最后的结果。这个自行车对应到这个点，轿车对应到这个点，最后直接通过这个网络得到一个输出。这里要注意的是，这个输出可以看到是 7×7×30 的矩阵。首先要判断是什么物体，在这个检测当中是 20 倍，在这里有 20 维去表示是什么物体。还有 8 维，要对这个框的位置进行修正，这 8 维是负责对这个框的区域进行修正。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面是它的整个网络的设计图，这里一共有 24 层的卷积，这里直接通过图片，448×448，最后得到 7×7×30。通过这 30 维，就直接可以算出一个 49 个格子当中，每个格子可能包含什么物体，在什么位置。这样的话，我一次运算就得到了最后的检测结果。这里训练还有一些细节，就比如说这个网络的初始化，会先把一些卷积层在更大的范围内预训练一下，采用 RCNN 的方法，最后再拿到检测训练。这个后面还做一些改进，大家有兴趣的话也可以关注一下。（附该结构的 Github 代码地址: https://github.com/xingwangsfu/caffe-yolo）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsKe9sNIlrXElh0AxSib3RBexMwf1IRo1fH5pvPf7YRNHIqpoljoNxe7Q/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsibauTfyKgdtDIDMv5x6LsYqHxEia9hBZG8iayDYicfCSkqvJDXlbzKzmzA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;SSD&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面是 SSD 的结构，SSD 是这个比赛当中用的很多的一种方法。它解决的是什么问题呢？刚才 YOLO 看上去是非常暴力的，一个图片直接分为 49 个格子。这样的话，如果是这种小的物体的话，可能检测效果会更好。SSD 做了这个改进，比如说看这个网络当中，这里前面是一个 16 层的，到这里如果是 300×300 的话会输出一个 38×38×512，针对这 38× 38 我们会在每一个点上预设一些默认的框，这个地方和前面 FasterRCNN 的 Anchors 有点像，YOLO 只在这个地方输入了一个。SSD 是在网络当中不同的层都有输出。这样的话，其实也相当于一些特征的磨合。当然这个 SSD 在实际的训练当中还有非常多的训练的策略，比如说一个图片是 300×300，之前要做一些图像的增强，可以剪切一些区域去做训练，如果大家对这个感兴趣的话可以看一下这个论文。这里是 SSD 的一个检测效果。我们刚才把这个网络定位一致的话，都用 VGG16 的话 FPS 是 7。这个 SSD 做检测的时候有一个地方，这里是一个 Match 的检测，每次可以处理 8 张图片同时做检测，如果大家去测这个速度的话可能会有损失。（附该结构的 Github 代码地址: https://github.com/zhreshold/mxnet-ssd）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ib7qeAzkJbnskHfbTEctNsNnDx1iax8IvuS4ibJHNxSv3y0ImvNlYhPHMe0baLgYAE6wvANj68DuIw/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3. 所述框架总结&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Deep Learning 的运用 CNN 的方法去提取物体的特征，比传统手动设置的特征更好，这样带来检测的准确度更高。我们引入了回归的方法，对于检测的位置做一些修正，这样的话检测的位置是更准的。刚才我介绍的这几个框架都是端对端的，这样的话，可以让我的检测速度非常快，而且可以用到 GPU 加度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;Ⅴ 现场精彩问答&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1. 视频检测相关的内容问答&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;提问：您刚才提到了很多解决方案，我想问的是解决的过程是什么样子？以及在这个过程当中，比如申请数据集的时候，有训练数据集、开发数据集和测试数据集，在选择数据集的时候需要注意到什么？还有就是在选取框架的时候需要注意什么？以及在迭代的过程当中需要注意什么？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王金桥&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：我觉得整个的基于图像视频的模式，也是一个科学和工程深度结合的一个事情。它的数据就是你必须要对这个方法理解得非常深刻，你才能够针对这个方法去选取适合方法的数据，就是你的方法是解决哪个问题的，你根据这个问题去发现什么样的数据更适合它，你当前的这个方法对那些数据不适合。另外就是数据的清洗非常重要，你在准备数据的时候，这个需要大量经验的积累，因为它也是一个实验性的科学，就是说你在数据标注的时候，这个框是不是要标得特别紧凑，比如说你这个目标遮挡三分之一还是露出来一点，这个都是需要经验积累的。所以说包括这次 BOT 比赛他们数据标注里不是特别有经验，我们都会对数据进行一个清洗。数据清洗对最后算法性能的影响大概在 10% 到 20%。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从框架选取的角度来说，以不同的应用选取不同的框架。比如说你要用人脸的，CoverNet 的卷积方式更适合，你要想做简单的分类，或者是做一些跨平台的融合，Test Flow 应该是大家关注更多的，而且我们现在每个框架基本上都用。另外 MX Net 本身的 Bug 比较多，大多数都是中国人，模型释放的比较少，如果你自己选择转模型就会有大量的损失。所以我建议你以 Test Flow 为主，或者你做 OCR 或者是语音相关的，你要做时序相关的，一般这种 Touch 的也很多，就看你的应用方向，还有你最后的发布平台。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;提问：刚才您有些例子是在手机端实现的，在手机端识别的话都有哪些技术要点？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王金桥&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：手机端识别就是多核多线程要解决的，我们的代码都是自己写的，你要有一个很好的框架来解决这个问题。另外就是说你要把库减少到最小，在 ARM 上做优化。另外就是网络要设计得非常浅，要做一些定点化，目前支持定点化的是 Test Flow，其他的都要你自己去更新，就是你是网络的压缩，要设计一个网络，把计算的样本控制到最小，多线程、多核的调用以及你最后性能的调优，最后代码的移植，目前这些公司应该都在做。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;尹相志：我补充问一下，最近有些论文提出来说，可以把一些很复杂的网络通过一些 Training 的手法做缩减和压缩，你认为这是一个比较重要的方向吗？还是说先从比较浅层的规则来做？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王金桥&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：网络的剪枝、裁减和网络的压缩一般是同时来做的，一般是裁减，我们先用一个非常深的网络，要布这些核心数据，要让它性能最优，我们会减某些层。减了之后，我发现性能损失在 2% 左右，我不能再减了，再减性能损失就下来了。我们要压缩，包括小网络拟合大网络，这种训练的方式的话就要特别依赖你的数据，这种就是进行网络训练不是特别的敏感，目前大部分的方法还是通过剪枝的方式，一般的通过训练的方式得到这种剪枝或者层级量化，不但工作量比较大，而且目前一般的能做 MPGA 或者芯片的，目前倾向于这种方式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;提问：我比较外行，我问一个问题。深度网络输入的应该是一些图片，最后得到的特征，比如说人脸是什么脸形或者什么头发，或者是他背包没有，就是你输出的一些特征。中间层会是一些什么东西呢？是一些小的图片还是一些有意义的标签之类的东西？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王金桥&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：它输入的都是图像，是一个完整的人，他在训练的时候有两种方式，一个是全局的属性，就是这个人的年龄、性别、男女，是整个人体的框直接做一个全局的分类。第二个就是说局部的，是否背包，是否拉着箱子，是什么发形，穿的衣服是什么颜色，基于这些标签，我们输出的是这个序列的标签，和标注是一样的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;提问：中间层是一些什么？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王金桥&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：中间层次就是对这些区域的一些响应，神经网络做一些可视化，通过卷积层，Cover1、Cover2、Cover3 都是和我们的人脸一样，看上去之后都是一些简单的纹理特征，看不出什么差别，他们在 Cover4 和 Cover5 这几层，有结构的特征，比如人脸的话有眼的特征、鼻子，Cover5 就有很明确的特征，比如裙子，这个裙子的轮廓就出来了，响应度很高，别的响应度很低。就是低层到高层的抽象机制，但是这个理解跟我们人并不一致。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2. 深度学习框架的内容问答&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;提问：你刚才介绍了这么多检测的框架，大概会分为 RCN 系列、SSD 和 YOLO。我想问一下，针对不同的问题，我们选择哪一个框架比较好？SSD 和 FasterRCNN 会有什么优点？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈朝才&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：我觉得针对不同的问题需要对应用场景做配置。SSD 这里的设计就是导致你可以一下子可以检测 8 张图，FasterRCNN 一下子只能检测 1 张图片，这样的话会导致会有 8 张图片的延迟。但是我觉得如果你用 FasterRCNN 的话，你可以优化前面的这个，在实际使用的时候可以把这个网络简化一下。YOLO 做了这样一些设计，在做 3×3×256 的运算之前先有一个 1×1×128 的卷积层，这样的话，它会让你下一层的运算量减少一半。就是说前面一层输出的维数变为一半，下一层再做大卷积和的时候，这样的运算更小了。也可以针对这个做一个相同的优化，让这个网络结构变得更小。这样的话，也可以让这个检测速度提升得更快。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;还有另外一点，SSD 的输入是 300×300，其实这个图片的大小和处理速度也是有很大的影响。比如说如果你要检测一个很大的物体的话，我们觉得在计算的时候并不需要像 FasterRCNN 那样有成就感。如果你去看卷积的运算过程的话，这个图片大小是和运算量有一个关系的。就是你实际做检测的话，可以尽量的去压缩输入的大小，得到速度的提升，另外一方面就是这种网络结构。最近谷歌有专门讲检测的速度和精度的平衡。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;提问：如果只考虑精度的话，是不是 FasterRCNN 比 SSD 要好一些？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈朝才&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：我觉得这个问题和场景有关系。因为 SSD 很多训练的策略是非常有效的，因为 FasterRCNN 已经出得非常久了，这个检测效果我觉得可以针对实际场景去看一下这几个框架在做训练的时候采用了一些数据增强的方法，包括一些训练之类的，我觉得这个对于检测效果影响也非常大。像 SSD 最开始其实效果并没有这么好，并没有这么高。但是它通过一些策略，让它训练的精度达到了这个效果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;提问：刚才看到 FasterRCNN 有两个改进，其中一个改进是 Feature Map 的改进，有没有一种方法是这两个改进的融合？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈朝才&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：也可以，这个论文里面针对的，你去做论文的话是针对一个主要的点。我觉得你个人做实验的话，你可以去尝试把这两种方法结合，这个也很方便，因为这两种方法并没有冲突。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;提问：有人做过这两种方法的融合吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈朝才&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：目前没有。但是我个人认为，这两种方法，RFCN 侧重的是用残差网络，后者是得到了检索的，服务的提升也是很大的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;提问：选择这些的依据是什么？是计算量吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈朝才&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这是很基础的，残差网络分类的效果是非常好的，我觉得你在世及应用当中，可能并不会用到残差网络这种特别强大的网络，这种网络还是非常复杂的。在这个论文当中是为了做标准的对比，所以采用的是相同的网络做对比。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;提问：你的意见主要还是从计算量来考虑选择分类器是吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈朝才&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：我觉得如果你追求精度的话，像 SSD 里面做了非常多的数据增强，包括一些针对这种特别小的物体的优化，这都是非常细的，我这里没有涉及到。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;提问：这次竞赛当中好多人都选择 SSD 是为什么？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈朝才&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：因为 SSD 的话，我个人认为其实是 RFCN，因为 SSD 的效果我觉得可能在训练的过程当中根据输入的图片做了一些采样。给出的分辨率都是非常大的，3000×200，他是随机采一些小块做的。我在训练 RFCN 的时候并没有采用这种方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;提问：是不是 SSD 相对来说是做好训练的？其他的这些网络训练难度有什么区别？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈朝才&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这个我觉得区别并不大，是两种不同的思想。如果应用在你的场景当中可以都去看一下，测试一下。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;提问：还有一个问题就是，如果我想检测，比如针对一个现有的，增加一个新的内部检测，从原理上直接是重新训练分类器再训练网络？还是说整个都是你自己写的？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈朝才&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：像一般的标准数据，如果你自己想加一个的话，我觉得可以这样做，先在上面训练一个 20 倍的检测器，针对你的图片，先把 FasterRCNN 的检测结果标在你的数据上面，我就把这个检测出来的框当做我的标准方式去训练，这样相当于做一个检测的迁移。然后你再用这种标出来的结果去训练另外一个 FasterRCNN 的网络，这样的效果是很好的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心编辑，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Fri, 20 Jan 2017 12:01:05 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 跨界人工智能，暮光之城女主角发表学术论文</title>
      <link>http://www.iwgc.cn/link/4419777</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;作者：蒋思源、李泽南&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote style="color: rgb(62, 62, 62); font-size: 16px; white-space: normal; max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;当电影《暮光之城》的女主角 Kristen Stewart 在机器学习论文上作为作者出现时，研究人员都是感到很震惊的。这位 90 后著名影星最近作为联合作者在 arXiv 上（由康奈尔大学运营的在线研究数据库）发表了一篇论文。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ZOmknUpmm3m7sKze0ibcvqNKMwI69VHPVazebppSNicib6ZnEpicHv429bekfAzGzXfjNsypz5tJVQA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Stewart 和其他作者发布的这篇文章主要是涉及「神经风格迁移」，并将其应用到 Stewart 制作并导演的电影短片《Come Swim》中。这部影片是「充满诗意的，心碎人在水下的印象派画像」，它使用风格迁移技术，使电影看上去就好像是画作一样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ZOmknUpmm3m7sKze0ibcvqnOUCpHR5s59yibDcgQVrXiaicp8XxmichtO4DEyYyGGPial2mCHeibV0sMuA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我们通过神经风格迁移（Neural Style Transfer）重新绘制了电影中的关键场景（以手绘风格表现），从而为故事添加上一些艺术手法，这些实现基本上是逐帧转换的，它支撑起了整个电影。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「绘画本身会激起个人在醒来第一时间的冥思（梦中与现实记忆之间的退却），所以将技术应用到关键场景将令我们想唤起的情绪映射到算法中。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ZOmknUpmm3m7sKze0ibcvqVACibl32HBm6S87u1nKAcw9qlUSVLM9b5coSOvXu1PhdJHZUhwB1Nibg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Stewart 希望利用风格转移技术完成一种新颖和优美的影片，她在论文描述了更多的技术细节，还有电影人如何才能够实现这一点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;风格迁移是一种有趣的技术，涉及使用卷积神经网络来实时地改变视频。要做到这一点，基本上你需要做的就是将你的算法应用到图像上（如梵高的画），然后再训练你的系统学习绘画手法并应用到任何常规图像上。该论文简短有力描述如何通过神经风格转化将印象派带入到所制作的作品 Come Swim 中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;早在论文《A Neural Algorithm of Artistic Style》中，研究者 Gatys、Ecker 和 Bethge 介绍了一种使用深度卷积神经网络（CNN）分类器的方法。其 pastiche 图像是通过优化（optimization）找到的：该算法会寻找一张给出了该 CNN 的底层中同种类型激活（activation）的图像，这些底层会获取风格输入（宽笔触和立体美感等等）的整体粗糙美感；该算法还会在更高层产生激活，这是获取能使对象可被识别出来的东西，这接近于那些由内容图像所得出来的东西。从某个起始点（如：随机噪声或内容图像本身）开始，该 pastiche 会逐渐细化，直到这些要求都得到满足。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另外在十月份谷歌《A Learned Representation For Artistic Style》论文中，研究者们探讨了如何构建单个、可延展深度网络，能够贪婪地捕捉不同派别的艺术风格。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;李飞飞十月份也作为《Perceptual Losses for Real-Time Style Transfer and Super-Resolution》的作者讨论过风格迁移。她们的系统在训练之后，前馈网络能以比 Gatys 等人提出的基于优化的方法（optimization-based method）快数百倍的速度为图像改变风格。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;论文：《Bringing Impressionism to Life with Neural Style Transfer in Come Swim》&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ZOmknUpmm3m7sKze0ibcvqeubAgYNRJH17Z9fuUbCMY5RVdIFPt1jKKicZOsmae3go2meqOO9NS4A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：神经风格迁移是近期开发出的先进技术，它使用神经网络艺术性的重绘（redraw) 原图像的风格。本论文探索了在制作环境中的技术应用，使用神经风格迁移用印象派绘画风格重绘电影短片《Come Swim》中的关键场景。我们记录了该技术如何在交互式的创造流程框架中进行部署，从而能获得想要的效果，也提出了一种将广阔的参数空间映射到创造性控制的关键设定的方式。我们希望这种映射方式能为未来的研究提供启发。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心原创，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Fri, 20 Jan 2017 12:01:05 +0800</pubDate>
    </item>
    <item>
      <title>学界 | UC Berkeley最新论文：残差神经网络的可视化</title>
      <link>http://www.iwgc.cn/link/4419778</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自arxiv.org&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：Jane.w、沈泽江、李泽南&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ZOmknUpmm3m7sKze0ibcvqvXibyZEJSysWCNIcQeU8ibW8eibnh2YUfzxQKhFTlnpy2ZzsqbeVE39nA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote style="color: rgb(62, 62, 62); font-size: 16px; white-space: normal; max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;摘要&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当前 ImageNet 计算机视觉识别挑战的最前沿的技术是残差神经网络（residual network）。如利用快捷连接（shortcut connection）的研究方法已经在残差网络和 highway network 的衍生模型中得到大量应用。这些研究潜在地挑战了我们对 CNN 学习浅层（layer）的局部特征（local feature）与深层越来越多的全局特征（global feature）的认识。通过定性可视化和经验性分析（empirical analysis），我们探索了残差跳跃连接（residual skip connection）的意义。正如预期判断，我们的评估显示残差快捷连接能够强制图层来精炼（refine）特征。我们还提供了另一种可视化表达方式，进一步证明了残差网络大体上能学习已知的 CNN 所具有的直观功能。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1.导语&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2015 年，深度残差网络在 ILSVRC 分类比赛中获得了第一名。我们尝试理解启发何恺明等人使用快捷连接和恒等映射（identity mapping）的网络架构的定性特征。为此，我们可视化了 2 幅残差构造块（residual building block）之后的特征图：一幅是最大化地激活了给定通道中的单元（unit）的前 9 个图像组，另一幅是对应的激活单元所用的有导向的反向传播（guided backpropagation）的可视化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从这些可视化可以直观地证明何恺明等人的判断，即从预处理层（preconditioning layer）到恒等映射是有帮助的，并且与恒等映射相关函数更容易学习得到。特别的是，我们观察到相同维度的残差层学习得到的特征更加精炼和锐化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1.1 相关研究&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Zeiler 等人在 AlexNet 特征上进行了相似的可视化，他们的研究引入了去卷积变换（deconvolutional transformation），其中包括了采取激活期望的单位进行可视化并通过一系列去卷积步骤向后反向移动。与从像素空间（pixel space）映射到特征空间（feature space）不同，去卷积变换是从特征空间映射到像素空间。为了通过最大池化（max-pooling）向后反向移动，进行了一个反池化（unpooling）步骤，其中被选择的单元作为正向传递 (forward pass) 中的最大单元被分配了反向传播的值。为了进行去卷积计算，使用与卷积层学习得到的相同的参数来进行转置卷积 (transposed convolution)（也称为分数跨度卷积/fractionally strided convolution）。最后，通过整流（rectification）反向移动，并做反向输入数据的整流。这种方法在像素空间中构建了对给定激活单元的贡献最大的图像部分的可视化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;建立在去卷积方法的基础上，Springenberg 等人开发了有导向的反向传播。有导向的反向传播是去卷积方法的改进，在向前路径中被整流为零的单位（因为它们具有负值）在去卷积通道中也被设置为零。这被证明在视觉上 Springenberg 等人的网络优于基于去卷积的可视化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，Yosinski 等人使用各种方法将 AlexNet 可视化，包括以前的方法和优化来综合地生成最大激活图像&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ZOmknUpmm3m7sKze0ibcvqu1N47M6fEceZYhicibCr5nVkViauNg118tIs2GzukvR0tha2gwKZfkqlA/0?wx_fmt=png"/&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 1：左侧：基本的 shortcut 模块。右侧：投影的 shortcut 模块。&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 实验和架构&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们使用的 CNN 架构是预训练的 50 层残差网络。可以在线查看架构的可视化：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种残差架构由单个卷积层（conv1）、一个最大池化层、一系列残差的快捷构建模块组成。如图 1 所示，有两种残差快捷模块。第一种由 1x1、3x3 和 1x1 的卷积层组成，一个快捷连接将每个输入数据添加到 1x1 卷积以及将输出添加到最终的 1x1 卷积。这决定了堆叠的 3 层网络向特性映射的转变。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第二种称为投影的快捷模块。它由相同的卷积层堆叠组成，现在除了 shortcut 还包含单个 1x1 卷积。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;50 层残差架构由初始预投影块（2a）、两个基本块（2b，2c）组成。下面还含有一个投影块（3a）和一系列基本块（3b，3c）。该架构方式重复两次或更多次（4a，4b，4c，4d，4e，4f。以及：5a，5b，5c）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另外两个差异与投影的快捷模块相关：由于步长（stride）为 2 同时信道（channel）数量增加，空间维度有所减小。这意味着以相同编号命名的构件块（building block）（例如 2a，2b 和 2c）包含相同数量的输出信道。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于我们的可视化，我们使用 Yosinski 等人的代码，并分别针对残差网络和有导向的反向传播修改编程代码，使它应用于有导向的反向传播而不是反卷积。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了获得每个最大激活单元的前 9 个图像，我们使用 ImageNet 的验证集。在所有空间维度以及数据集的所有图像中，具有最高激活值的单位才能被可视化。这意味着给定过滤器的可视化是特定空间单位的可视化（例如，在信道的特征图中的位置）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ZOmknUpmm3m7sKze0ibcvq9gSvticY33R1cpN8eib1cyKBllWo7StHUxRJesojxkiaPwMiaWzqVzfp9w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 2：Res2a 特征的可视化（随机选择）。从左到右，从上到下：信道分别为 12、79、150 和 210。对于每个信道，左边的图是前 9 个图像块。右边的图是相应的有导向的反向传播的可视化。大的灰色边界是由于不同的接受区域（receptive field）大小（根据边距/padding）。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ZOmknUpmm3m7sKze0ibcvqUz8ZqzTj4tAxjXyqt6N18BT8YbnygXLehNdibL8OO5cWCib4Cib5Km5lQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 3：res2b 和 res2c 特征的可视化。与图 2 有相同的过滤器。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了可视化该最大激活单元，我们将图像传送到该单元所在的层，同时将该层中的所有其它激活单元归零，然后使用有导向的反向传播向后传递该层。当可视化前 9 个图像时，我们选择单元的接受区域对应的图像块（patch）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3 结果及分析&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在图 2 及图 3 中，我们对 2a、2b、2c 的构建模块（res2a、res2b、res2c）的通道进行了可视化。滤镜 12 看起来能够识别复杂的线性模型，在 res2b 中的判别力有所提升，在 res2c 中的判别力进一步有轻微增加。滤镜 79 看起来并不能带来相当程度的改变，这与残差模块是特征映射预条件产物的直觉相符。滤镜 150 表现出了优化：在 res2a 中，它能够识别出轻微弯曲的黑色线条，不过在 res2b 及 res2c 中它开始识别环形及更为弯曲的线条。滤镜 120 似乎并不能在 res2a 中识别任何东西，但是突然就能在 res2b 及 res2c 中识别平行的线条。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在图 4 中，我们展示了一个对于特征优化的有趣的案例。在 res3a 中，它识别出了一个单独的光点。在 res3b 中，它识别出了在周边景物下的光点。res3c 中则更进一步。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ZOmknUpmm3m7sKze0ibcvqiazoN0Z074RtOPMY9I0ZtOvK29zlW5MqAw1uFhZOwBErJmeiaYMjeibnw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 4: 通道 400，在 res3a、res3b 及 res3c 中进行了可视化（从左到右，从上到下）。并非随机选择。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ZOmknUpmm3m7sKze0ibcvqc0og6tdEKBwIfZM9PEppWataLwicqN9nakaorx8JX1avKzxfXOroYqg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 5: res5a 可视化（随机的）。从左到右、从上到下分别是：滤镜 7、149、1068 及 1620。放大以看到去卷积的细节。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ZOmknUpmm3m7sKze0ibcvqxp0P5LHxLKvS4q7Xkjb9ZCILUpPuiczicVQmjF457BBicoYw8q0NjYxvg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 6: res5b 及 res5c 的特性。与图 5 中使用了相同的滤镜。放大以看到去卷积的细节。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在图 5 及图 6 中我们随机的选择了四个滤镜进行可视化，它们全都表现出了优化的效果。滤镜 7 似乎能够识别穿着西装的人——因为在初始的 top-9 激活中有两个不是穿着西装的人，但是在 res5c 中全都是。在检查去卷积可视化后，我们证实了，这个特征并非必要地专注于识别婚礼场景，而它似乎专注于西服领子。滤镜 149 能够识别一种马赛克花纹，并具有小而明显的改进。滤镜 1069 似乎能够识别桁架结构，在一开始的时候还有香水瓶在它的 top-9 激活中，不过最后被卫星天线接收盘及吊车主导。滤镜 1620 一开始在带角的动物上激活，在 res5c 中这些激活被优化了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在图 7 中，我们也选择了另一个关于物体识别优化的例子。在 res5a 中，通道 1660 在它的 top-0 激活中有一个双人自行车。在 res5b 中，相同通道被 6 个双人自行车最大化地激活。在 res5c——最后的拥有相同特性大小的残差模块中，所有的 top-9 输入都是双人自行车。我们猜测在分枝中的滤镜（如 res5b_branch2c 及 res5c_branch2c）能专注于特定的、能够改进特征发现的子特性。我们会看到，res5b_branch2c（通道 1660）锁定了有两个座位的自行车，而 res5c_branch2c 则在条幅上被激活。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ZOmknUpmm3m7sKze0ibcvqAwwdnVeLYYSiar3YY2nQrfPPre05wI51jGiaMmhLZ6Nev9KYbmGxpcXQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 7: 通道 1660 的在 res5a、res5b 及 res5c 的 top-9 激活图像（与中间层 res5b_branch2c 及 res5c_branch2c 一起）。在双人自行车上的激活（程度）成功提高。&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;4 额外成果&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了分析 ResNet 带来的好处，我们简单地研究了 AlexNet。AlexNet 并没有那些特征映射，所以我们通过比较最上的 9 个从 AlexNet 的 conv4 到 conv5 激活输入，来发掘在不同层的单元。与超过 1 个输入相匹配的单元很少，并且只有很少像残差网络一样清晰的优化样本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在图 8 中我们也直接对 conv1 中的 kernel 进行了可视化，并发现它们与 AlexNet 十分相像。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ZOmknUpmm3m7sKze0ibcvqia1gWtra4YSC3gibMhfrcynCsBcmDjxjZuib5v5s6WFMPTQ7dQpexPgyg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 8: conv1 kenerl 值的像素图&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;5 后续工作&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在项目的进行过程中，我们一些想法因为时间及计算能力的限制而未能实现。但这些思路为之后的探究指明了方向&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 在每个残差模块（例如在 res2a）内，我们并没有测试 1×1、3×3 加 1×1 的卷积（模式）。这是个特别有趣的尝试，因为首个 1×1 的卷积实际上对通道（内信息）进行了降采样，接着 3×3 的卷积保持了通道的维度，而最后的 1×1 又的通道进行了升采样（如：1024 道 → 256 道 → 256 道 → 1024 道）。对这种情况下的分支的、附加的特性进行可视化应该是有一定价值的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 在跨越 res边界的过程中（如从 res3c → res4c），通道的数目翻倍，并且我们使用了一个映射创建模块（projection building block）（如 res4a）。因为在滤镜之间没有明显的对应关系，我们并没有对这些边界的情况进行可视化。也就是说，在 res4f 中的通道 60 并非映射到 res5a 中的通道 60。对高级概念的演化进行可视化、在其中发现对应（关系）也许是个有趣的过程。在一些初步的探索中我们发现，在前序层（如 res4f）中发现的属性，常常会在下一层中（如 res5f）有不止一个的对应特性（共享多重 top-9 激活模块），并且这些对饮的特性会随后分化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3. 在过去的几个月里，何恺明等人对残差网络的架构进行了多项改进 [4]。我们能够量化地来评价这些改进。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4. 在见识了这些基于残差捷径的改进效果之后，我们也可以尝试使用这种属性来拓展现有的网络（架构）。我们估计，也许可以使用一个预训练过的 7 层 AlexNet 架构，在训练层前面及后面，插入具有相同通道维度的 0-初始化残差模块然后继续训练。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Fri, 20 Jan 2017 12:01:05 +0800</pubDate>
    </item>
    <item>
      <title>重磅 | Torch7团队开源PyTorch：Python优先的深度学习框架</title>
      <link>http://www.iwgc.cn/link/4403735</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自PyTorch.org&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀、李泽南、李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Torch7 团队开源了 PyTorch。据官网介绍，PyTorch 是一个 Python 优先的深度学习框架，能够在强大的 GPU 加速基础上实现张量和动态神经网络。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;官网：http://pytorch.org&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;GitHub：https://github.com/pytorch/pytorch&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;PyTorch 是一个 Python 软件包，其提供了两种高层面的功能：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用强大的 GPU 加速的 Tensor 计算（类似 numpy）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;构建于基于 tape 的 autograd 系统的深度神经网络&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如有需要，你也可以复用你最喜欢的 Python 软件包（如 numpy、scipy 和 Cython）来扩展 PyTorch。目前这个版本是早期的 Beta 版，我们很快就会加入更多的功能。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWib4ZvicIaeEBwxiciaHXZI5ttRvOgwYCXs6wsBU8EWMXU7qiaKxsA5pzeVn41LagLLHhTSLQ5nF81jEww/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;PyTorch 介绍&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在粒度层面（granular level）上，PyTorch 库包含了以下组件：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWib4ZvicIaeEBwxiciaHXZI5ttRuQeicxicSl1eWvnyQXucymZjnu1l0K8fdfOQ8ZOduLscCEww2OuSx6HQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;使用 PyTorch 的原因通常有二：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;作为 numpy 的替代，以便使用强大的 GPU；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;将其作为一个能提供最大的灵活性和速度的深度学习研究平台。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;进一步阐述如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;一个支持 GPU 的 Tensor 库&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你使用 numpy，那么你就使用过 Tensor（即 ndarray）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWib4ZvicIaeEBwxiciaHXZI5ttRC522kNyJlEiaibqY4V9EiakRdSQ5e6MLjgw7WGRBcyf020AARm7DxAGQA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;PyTorch 提供了支持 CPU 和 GPU 的 Tensor，可以极大地加速计算。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们提供了各种各样的用于加速的张量例程（tensor routine），可以满足你的各种科学计算需求，比如 slicing、索引、数学运算、线性代数、reduction。而且它们非常快！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;动态神经网络：基于 tape 的 autograd&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;PyTorch 有一种独特的神经网络构建方法：使用和重放 tape recorder。TensorFlow、Theano、Caffe 和 CNTK 等大部分框架对世界的视角都是静态的，让人们必须先构建一个神经网络，然后一次又一次地使用同样的结构；如果要想改变该网络的行为，就必须完全从头开始。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但使用 PyTorch，通过一种我们称之为「Reverse-mode auto-differentiation（反向模式自动微分）」的技术，你可以零延迟或零成本地任意改变你的网络的行为。我们灵感来自关于这一主题的许多研究论文以及当前和过去的研究成果，比如 autograd、autograd、Chainer 等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;autograd：https://github.com/twitter/torch-autograd&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;autograd：https://github.com/HIPS/autograd&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Chainer：http://chainer.org/&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管这项技术并非 PyTorch 独有，但它仍然是到目前为止最快的实现。你能为你的疯狂研究获得最高的速度和最佳的灵活性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_gif/KmXPKA19gWib4ZvicIaeEBwxiciaHXZI5ttRiaUAxR3xkdBWE0DuU1Fly9lKKSy6BiaxSnLyRVLKQVKkOP7HhvMqpLtA/0?wx_fmt=gif"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Python 优先&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;PyTorch 不是简单地在整体 C++框架上绑定 Python。它深入构建在 Python 之上。你可以像使用 numpy / scipy / scikit-learn 那样轻松地使用 PyTorch。你可以用你喜欢的库和包（如 Cython 和 Numba）在 Python 中编写新的神经网络层。我们的目标是尽量让你不用重新发明轮子。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;命令式体验&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;PyTorch 的设计思路是线性、直观且易于使用。当你需要执行一行代码时，它会忠实执行。PyTorch 没有异步的世界观。当你打开调试器，或接收到错误代码和 stack trace 时，你会发现理解这些信息是非常轻松的。Stack-trace 点将会直接指向代码定义的确切位置。我们不希望你在 debug 时会因为错误的指向或异步和不透明的引擎而浪费时间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;快速精益&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;PyTorch 具有轻巧的框架。我们集成了各种加速库，如 Intel MKL、英伟达的 CuDNN 和 NCCL 来优化速度。在其核心，它的 CPU 和 GPU Tensor 与神经网络后端（TH、THC、THNN、THCUNN）被编写成了独立的库，带有 C99 API。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种配置是成熟的，我们已经使用了多年。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，PyTorch 非常高效——无论你需要运行何种尺寸的神经网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 PyTorch 中，内存的使用效率相比 Torch 或其它方式都更加高效。我们为 GPU 编写了自定义内存分配器，以保证深度学习模型在运行时有最高的内存效率，这意味着在相同硬件的情况下，你可以训练比以前更为复杂的深度学习模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;轻松拓展&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;编写新的神经网络模块，或与 PyTorch 的 Tensor API 相接的设计都是很直接的，不太抽象。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你可以使用 Torch API 或你喜欢的基于 numpy 的库（比如 Scipy）来通过 Python 写新的神经网络层。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你想用 C++ 写网络层，我们提供了基于 cffi（http://cffi.readthedocs.io/en/latest/）的扩展 API，其非常有效且有较少的样板文件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不需要写任何 wrapper code。这里有一个示例：https://github.com/pytorch/extension-ffi&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;安装&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;二进制&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Anaconda&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;conda install&amp;nbsp;pytorch torchvision -c soumith&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;来自源&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Anaconda 环境的说明。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你想要用 CUDA 支持编译、安装：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;NVIDIA CUDA &amp;nbsp;7.5 或之上的版本&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;NVIDIA CuDNN v5.x&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;安装可选依赖包&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;export CMAKE_PREFIX_PATH=[anaconda root directory]&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;conda install numpy mkl setuptools cmake gcc cffi&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;conda install -c soumith magma-cuda75 # or magma-cuda80 if CUDA 8.0&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;安装 PyTorch&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;export MACOSX_DEPLOYMENT_TARGET=10.9 # if OSX&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;pip install -r requirements.txt&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;python setup.py install&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;开始使用&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从以下三点开始学习使用 PyTorch：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;教程：开始了解并使用 PyTorch 的教程（https://github.com/pytorch/tutorials）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;案例：跨所有领域的轻松理解 PyTorch 代码（https://github.com/pytorch/examples）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;API 参考：http://pytorch.org/docs/&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;交流&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论坛：讨论实现、研究等（http://discuss.pytorch.org）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;GitHub 问题反馈：bug 通知、特征要求、安装问题、RFC、想法等。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Slack：通常聊天、在线讨论、合作等（https://pytorch.slack.com/）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;邮件订阅没有骚扰信件、单向邮件推送 PyTorch 的重要通知。订阅：http://eepurl.com/cbG0rv。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;发布和贡献&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;PyTorch 的发布周期（主版本）为 90 天。目前的版本是 v0.1.6 Beta，我们期望在发布前尽量减少 bug。如果你发现了错误，欢迎向我们提交：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;https://github.com/pytorch/pytorch/issues&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们欢迎所有形式的贡献。如果你希望帮助解决 bug，请直接上手，无需多作讨论。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你愿意为 PyTorch 提供新功能、实用函数或核心扩展，请先开一个 issue 与大家讨论一下。请注意：在未经讨论的情况下提交的 PR 可能会导致退回，因为我们可能会采取不同的解决方式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在下一个版本中，我们计划推出三大新功能：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1、分布式 PyTorch&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（这里已经有一个尝试性的实现了：https://github.com/apaszke/pytorch-dist）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2、反反向（Backward of Backward）：在反向传播的过程中进行过程优化。一些过去和最近的研究如 Double Backprop 和 Unrolled GANs 会需要这种特性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3、用于 autograd 的 Lazy Execution Engine：这将允许我们可以通过引入缓存和 JIT 编译器来优化 autograd 代码。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;开发团队&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;PyTorch 是一个社区驱动的项目，由经验丰富的工程师和研究者开发。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前，PyTorch 由 Adam Paszke、Sam Gross 与 Soumith Chintala 牵头开发。其他主要贡献者包括 Sergey Zagoruyko、Adam Lerer、Francisco Massa、Andreas Kopf、James Bradbury、Zeming Lin、田渊栋，Guillaume Lample、Marat Dukhan、Natalia Gimelshein 等人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Note：本项目与 hughperkins/pytorch 有相同的名字，但无关联。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Thu, 19 Jan 2017 11:05:27 +0800</pubDate>
    </item>
    <item>
      <title>独家 | 揭秘美图影像实验室：数据、算法和一件关于美的事</title>
      <link>http://www.iwgc.cn/link/4403736</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：虞喵喵&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「美是无用之用。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;每天清晨，全世界不同时区的女孩儿们依次醒来，洗脸、化妆或是不化妆、吃早饭、上班或是去上学。这些女孩儿中有不少人会用美图推出的不同应用自拍，生成大约 2 亿张照片。这些照片有的去了瑕疵，有的加了滤镜，有的干脆添了妆容。不同的手指按下同一个「发送」键，照片们就出现在不同的社交平台上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你很难说清这些照片有什么用。它们既不像传统的旅行照记录「到此一游」，也不像证件照充满了功利。这些照片更像一种自我暗示和面向广袤世界的宣示——我们存在，我们拥有美。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8mkkOTgrRQux7YyjNTL5mdpCO09ahGPcjwSw63T6N1q4sADlA2kPZh80UiaISlkOLQkpAHPngPHpw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;这是一个属于自拍的时代，没有哪张照片能比这张更能说明我们的处境。&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;饺子是美图影像实验室（MTLAB）北京地区负责人。在这个制造「魔镜」的人看来，「美是一种选择」，既不能强迫他人接受，也不能接受他人的强迫。美是无解的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自成立以来，美图影像实验室的研究成果几乎改变了美图软件、硬件中所有功能。一键美颜、实时美妆，或是时下相当流行的美图秀秀手绘功能，都有这个实验室的功劳。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们一定很爱自拍吧？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;饺子笑了笑，「看到自己的技术用在产品中效果非常好，很有成就感，但可能并不会经常自拍。应用和技术之间，是不同的两回事儿。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如今已有 60 余人的美图影像实验室行事相当低调，在搜索引擎上似乎很难找到相关信息。作为首次采访到美图影像实验室的媒体，希望机器之心能让你对「神秘」的实验室和这份关于美的事业多一分了解。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;民主化的的实验室&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;亚麻、香草金或者冰蓝，只要在屏幕上轻轻一点，所有人都能立即改变自己的发色。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但在实验室的研究员们看来，这项令人惊叹的「染发」功能，纯粹是技术上的分割问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8mkkOTgrRQux7YyjNTL5mdrAeCNiaR9ib3YLR07eaPI8E4JrKYicYLvUUPq2bur0RgEV0EGK33RTE5A/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;左一为原图，可以选择较自然的色彩如亚麻色，也可以选择更夺目的色彩如莓粉色&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「一张照片，头发有的亮有的暗，头发和围巾很像甚至有时候纹理也很像。背景是黑的头发也是黑的，和头发连在一起的眉毛也是黑的，那怎么办呢？我们就需要克服这些问题。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前，负责「克服问题」的美图影像实验室有大约 60 多位研究员，分布在北京、厦门、深圳。位于美图总部的厦门实验室是最老也是最大的团队，主要提供人脸技术、美颜技术、3D 技术和性能优化；北京实验室更偏向计算机视觉，包括视频技术及深度学习；深圳则是与智能硬件相关的影像算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这样的设置自然有吸引人才的考量。深圳有华强北，硬件创业氛围好、相关人才也多，有利于美图手机的迭代研发；北京是全国最大的人才集散地，又有众多高校学府，很适合招揽人才；厦门自 2010 年时就诞生了实验室雏形，有着深厚的技术积累。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8mkkOTgrRQux7YyjNTL5mdNeDsv34OCMMV8wgbicGYltyibnnIcrOpvKXshP6L280y8Nwn7b7X3byw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;美图实验室厦门团队部分成员合影&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;可如此分散的架构，会不会产生协作上的困扰？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「这不单单是美图的问题，很多公司都是这样。」饺子显出某种技术人员独有的认真，讲述起实验室的「解决方案」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;每周，不同地区的实验室都会通过远程会议系统一同召开会议。除了例会，还有定期的论文分享会、主题分享会。论文分享需要每个人介绍一系列文章，讲清内容、问题、自己的理解和未来应用的可能，然后大家共同探讨；主题分享会则由组员讲述近期正在做的、和他人不同的工作，不仅要讲明内容、逻辑清晰，还要应付观众提问，共同提升批判性思维的能力，「和读博士差不多」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;同时，这些内容提前会写成博客——美图在内部建立了一个技术博客，任何人都可以评论和编辑、添加新内容。各次分享会的内容、研究中发现的成果都发布在博客中，即使新人入职也能马上学习新技能，其他人遇到问题也能在博客上寻找解决方案。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「这也是我们实验室民主化的一部分，技术分享同样可以提高实验室的整体效率。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;在前沿研究和真实需求之间的平衡感&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自 2010 年从开发团队中剥离，到 2014 年正式成立，实验室如今已是美图最重要的部门之一。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;比起开发团队，实验室更像研究院，不过与研究院不同的是，研究员们关注的内容既要包括前沿技术，也要顾及实际需求。如果有新人入职，第一件事就是将一个项目从头跟到尾，不仅要清晰流程，还要解决最实际的问题、面对最严格的产品经理和最真实的用户需求。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大头贴是美颜相机中最火的功能之一。这个至今已被使用了 7.9 亿次的功能，是产品经理日本考察时发现的需求。在日本游戏中心相当流行的大头贴机，几乎每个前来游玩的女孩子都想尝试一次。与中国的大头贴机不同，日本的大头贴机不仅自带磨皮美白、放大眼睛的美颜效果，还可以选择不同的美化功能，甚至是拉长双腿。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8mkkOTgrRQux7YyjNTL5md6iblGWv0hr3bscVZ0OEYJk4Fo4j5tia09NeTsp8SibUzPDOutTc9RKByQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;美颜相机中的大头贴功能，有各种卡通模版可以选择&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不过想要在软件中实现这些效果，不少在技术上尚且无法满足。研究员们第一时间告知产品经理能达到的效果，最终折衷成目前已上线的功能。而那些没能被满足的需求，不少已经变成了实验室长期研究的课题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除了对产品和用户更加了解的产品经理，也有不少需求直接来自实验室。钻研技术中碰到哪个点刚好会产生不错的效果、感觉用户可能会喜欢，实验室也会和产品经理讨论是否加入到产品中。前面提到的染发功能，就是其中之一。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个需求需要多长时间才能实现？除了规划的「死线」和工程适配的流程，更具决定性的是「效果」。在采访过程中，饺子反复强调美图对效果的严苛，「如果产品要求很低当然可以做得很快，但这取决于美图价值观，我们要为用户提供最好的效果」。甚至不希望用户注意到他们在背后的努力，「一点，头发就变色了，用户感觉很方便就足够了。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;深度学习与数据&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在内部，美图影像实验室将技术分为 8 类，分别对应基础技术和综合技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作为基础的 MTFaces，包含人脸检测、人脸关键点检测和人脸属性分析，可以了解用户的年龄、性别，为后续的美颜和上妆提供支持；MTSegmentation 是图像分割技术，可以准确找到照片中头发、皮肤、身体的位置；MT3DTech 可以通过一张自拍照重建人脸 3D 模型；MTRestoration 则是画质修复，光线不好、有噪点或者被压缩，都可以一键恢复。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;综合技术是多种技术的融合，比如 MTBeauty 包含图像美化和人像美化；MTStyles 风格化技术则是时下最流行的迁徙技术，能够给图像赋予风格；MTPhotos 可以通过人脸识别和图片标签管理照片；基于人脸检测技术 MTMakeup，通过美图独有人脸网格技术实现实时的视频和图像上妆。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWib4ZvicIaeEBwxiciaHXZI5ttRKUHWFQZMaPTfpyic5uINVVLPkpdYr0YZrhupWR6DcQbCMlNYAKHAPjg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;MTSegmentation 头发部分分割效果图，最右为染色后&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这些被划分的技术背后，深度学习功不可没。运用传统方法进行皮肤分割，准确率往往低于 80%。深度学习则可以让 MTSegmentation 的准确率达到 98.5%，以保证在磨掉小斑点、小细纹的同时，不会将衣服、头发磨平。MTRestoration 也离不开深度学习，机器之心也曾报道过相关的应用案例。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在具体应用上，美图的深度学习有什么特点？在饺子看来，美图影像实验室的优势还是在 to C 的场景和拥有的数据：「有多少层、用了什么样的结构，取决于应用场景。美图软硬件产品为我们提供了很多 to C 的应用场景，这可能是其他平台提供不了的。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除了最为人熟知的美图秀秀，美图的产品线还包括美拍、美颜相机、美妆相机、潮自拍、BeautyPlus 等等。截至 2016 年 10 月 31 日，美图应用的月活跃用户总数约为 4.56 亿，核心影像应用当月产生约 60 亿张照片。据相关调查统计，在中国主流社交网络上传的照片中，有约 53.5% 的照片经过了美图应用的处理。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8mkkOTgrRQux7YyjNTL5mdicRDa8pfWatQj3NrYiaWSicvOXgXtlvBJtQ4f3ia52l5icbOK4e8C4BaVibw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;美图公司在 App Store 上线的产品，共 20 款&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;庞大的数据为美图带来无可比拟的优势，「图像、视频都在向深度学习靠，但深度学习拼到底，很多时候是要靠数据。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;都是自拍照，会不会面临数据结构单一化的问题？美图获得的数据目前的确以自拍照为主，「但食物、风景这些都是有相当比例的，我们的产品也不全是在人脸上做的。美拍上还有视频，这些都是我们技术上独一无二的优势。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;用户是最苛刻的「数据集」&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;美拍是美图「产品矩阵」中不可忽视的一部分，目前其视频观看量已超 79 亿，月点赞数为 46 亿次（2016 年 10 月数据）。在美拍中，「激萌表情」是相当流行的功能，用户可以实时为自己加上兔耳朵、猫鼻子、甚至直接把自己的脸变成小猪的样子。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3D 已经成为时下最重要的技术之一。去年 10 月，Angelababy 曾在微博上发布了一张万圣节上妆照，网友纷纷留言「孕妇不要搞这些花样」、「怀孕还化妆」，事实上这张照片是由美图秀秀「万圣节妆容」功能自动生成的。以假乱真的逼真效果和美图强大的 3D 技术脱不开干系——通过一张 2D 自拍照重建人脸 3D 模型。「万圣节妆」其实是一款 3D 妆容，立体贴合造就了真实的「假象」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8mkkOTgrRQux7YyjNTL5mdfoHp8DNAr47oNhIEUs9icwyh95Q2Hg8ZpAXY64D7aXmg6ibuKQ8CDvicw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Angelababy 万圣节妆容微博，被点赞 71 万次&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;很快美拍还将上线实时 3D 功能，除了在直播的同时自然的放大双眼、添加妆容，还可以添加 3D 兔耳朵、耳机或是卡通魔镜。早在 2015 年 1 月，美图就已战略投资专注 AR 核心技术和产品研发的亮风台，并成立了联合实验室。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8mkkOTgrRQux7YyjNTL5mdVoPZDN9TIX0Hiaxj4iaCOJicw1yNiay0ECjTicuC73o1ZBdiabr79t6nbrBQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;美图影像实验室 2D 转 3D 的效果图&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谁来衡量「用户喜欢」的标准？美图内部设有专门的「设计特效」团队，会对技术结果进行反馈。作为专管效果的「产品经理」，他们更了解美的标准和用户的标准是什么。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;可是作为专注影像技术的实验室，为什么很少在数据集测试中看见他们的身影？「首先，美图没有通过 PR 获得资本市场认可的压力；另外，美图习惯于用产品说话，我们公司很多功能非常实用，好用到理所当然。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于美图来说，爱美的用户们是最苛刻的「数据集」，为用户提供更好的体验就是探索技术的动力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但也并非完全不理会，实验室正考虑在测试数据集和论文身上分散些精力。目前实验室已经尝试了 FDDB 人脸检测数据集，美图的 MT-Face-v3 在大部分阶段表现都是最佳之一。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8mkkOTgrRQux7YyjNTL5mdXyiaj8yxtQr0q7vzNJXGdZAlniasic0vay8DGXWzbAJVFN6T2D1kxWslw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;MT-Face-v3 在 FDDB 上的表现&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来，美图会在技术上增加不少投入。在去年美图赴港 IPO 招股书中提到，未来两年预计投入 3.27 亿港元（人民币 2.90 亿元，占所得款项净额的 6.6%）用于扩大研发能力，包括但不限于招聘工程师、数据科学家、分析师及收购技术相关知识产权。目前美图已注册超过 300 项专利，并持有 94 款软件程序的著作权。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;当技术遭遇美&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自文艺复兴时代起，艺术家们就在想方设法为自己留下自画像。1513 年达芬奇用红色粉笔画下自己晚年的样貌，透过画纸向未来的观众投以深邃目光；伦勃朗通过镜子描绘自己的样貌，右手执笔的他自画像几乎都向左倾；1998 年，梵高的最后一幅自画像《Self-Portrait without beard》拍出 7150 万美元的天价，是历史上第四贵的画作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第一张用相机拍摄的自拍照诞生于 1839 年，Robert Cornelius 打开摄影机，在镜头前坐了足足一分钟。到了 2014 年，Twitter 上转发量最高的的照片，是艾伦和詹妮弗·劳伦斯、布拉德·皮特等明星的自拍照。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8mkkOTgrRQux7YyjNTL5mdwWvJCW4y5amtRBKiaUs99ck6taDicNDEslv3MGrMQbibk9eXAo1FNOHfw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;这张「著名」自拍照被转发超 300 万次&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随着时间流逝，人类的自拍史不断变化，承载自拍的手段和技术也在不断进化。「美本身是没有错的，大家都喜欢，但标准不一样。我作为男生不太喜欢磨皮，拍出来精精神神就好了，更喜欢用潮自拍加滤镜。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作为在这一过程中提供技术基础的美图影像实验室，不会强调「磨皮就是美」、「不磨皮就是美」，而是希望通过人工智能把「工具」做得更好，为每个人提供不同的美的体验。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;就连实验室也不自觉的向这一目标靠拢靠拢，「我们的愿景，是让世界变得更美。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;满足自我欣赏也好，追求他人认可也罢，在弄清「人们为什么会自拍」这个亘古难题之前，不妨把它当作一种人类的本能行为。也许自拍的合理性一直都在，只是我们没有足够的技术去实现。移动设备的出现和类似美图应用的崛起，让自拍已经从一件「奇怪」的事变成新「常态」。当然，每个人都有选择是否自拍的自由。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「美是自己选择的。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;注：封面图为美图影像实验室在去年 12 月的 Siggraph Asia 上，向与会人员展示产品效果。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心原创，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Thu, 19 Jan 2017 11:05:27 +0800</pubDate>
    </item>
  </channel>
</rss>
