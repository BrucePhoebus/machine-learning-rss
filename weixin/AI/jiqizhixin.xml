<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>机器之心</title>
    <link>http://www.iwgc.cn/list/670</link>
    <description>人与科技的美好关系</description>
    <item>
      <title>阿里iDST视觉计算负责人华先胜：算法的红利正在消失</title>
      <link>http://www.iwgc.cn/link/f8b4fef44ba9488a6eff8d7edccfd1505d83f2cb</link>
      <description>
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：刘燕&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote style="color: rgb(62, 62, 62); font-size: 16px; white-space: normal; max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;「我越来越明确自己的兴趣点&amp;mdash;&amp;mdash;把技术研究与现实世界里的问题结合起来，去解决真正的问题、创造价值。」阿里云视觉计算团队负责人华先胜说。华先胜是视觉识别和搜索领域的国际级权威学者，曾被评为 IEEE Fellow、ACM2015 年度杰出科学家、MIT TR 全球 35 位 35 岁以下的杰出青年创新人物，曾担任 ACM Multimedia 等大会程序委员会主席。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;2015 年，华先胜离开职业的起点微软研究院，选择加入阿里巴巴。这在很多人眼里是一个不容易理解的选择，但在华先胜看来，理论研究有价值，但把这些技术放在一个切实的应用场景中让更多人使用同样有意义。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;在阿里巴巴的第一年，华先胜负责电商图片搜索技术的优化，推动了手机淘宝、天猫中的「拍立淘」的技术开发，让用户通过手机拍摄物品照片搜索相同或者相似的商品，这正在成为一种更为高效的商品搜索方式。有数据显示，2015 年双 11 当天，千万消费者使用「拍立淘」功能，达成了超过数千万元的销售额。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;随着云上的视觉计算需求量越来越大，2016 年初，华先胜转入阿里云并创立视觉计算团队，目前该团队隶属于人工智能研究机构 iDST 团队。2 月 27 日，机器之心对华先胜进行了独家访谈，他介绍了视觉计算团队所推动的研究进展与突破，以及他对AI行业发展的看法。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;视觉计算团队实现了哪些突破&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：能否和我们分享一下，您带领的阿里云视觉计算团队的工作有哪些突破性进展？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;华先胜&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：视觉计算团队成立以后就针对几个大的场景，包括监控、交通、安防、人脸、个人图片、医疗等，其中最重要的一部分实际上是城市大脑里面的视频分析，这里面的突破，我觉得可以分为几个方面：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第一是大规模视频分析，我们处理的城市数据量非常大，甚至远远超过电商的数据，这里面涉及到我们要去实时处理分析大规模的视频，所以要依靠阿里云高效力计算平台，构建一套大规模视频数据分析平台；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第二是把电商的图像搜索技术，延伸到城市场景里面来，叫做城市图搜或者叫城市搜索，专业一点的语言甚至可以叫索引整个城市，城市视频数据图像数据那么多，当然会有查找的问题，比如一辆车、一个人、一个物等，这跟电商有相似之处也有不同，从视觉角度来做的，这其实也是非常困难的事；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第三是我们可以对城市里面发生的，交通事故、违章停车、横穿马路等特殊交通事件进行检测和识别。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：视觉计算团队在研究方向上会有明确的侧重点吗？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;华先胜&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：一方面，要确保在云计算上进行大规模的视频分析必须充分可行，必须不断进行算法的优化。另一点就是继续深入行业，在各行各业去挖掘金矿，让算法在里面能够得到优化，把一个个行业吃透，为客户带来真正的价值。当然还有像深度学习本身算法的研究还是有很大的空间，这也是我们接下来要做的事情。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：城市大脑是您所在团队的一个重要项目，除了城市道路的管理、路况预测、交通调度的优化，它还有其他方面的应用方向吗？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;华先胜&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：目前城市大脑以交通方面的应用为主，包括路况检测识别和交通优化等。城市大脑既然是「大脑」，当然应该有更多的功用，包括安防，城市规划，环保，旅游等等。安全防护方面，如上所述，对人、机动车、非机动车等的实时索引，可以提升城市的安全防控能力；还有对一些异常事件，例如塌方、水淹、漏水、交通事故等等，能够快速警报，这个时候，能争取 1 分钟提前警报都会有很大的价值，例如，可能因此而挽救一个人的生命。&lt;/span&gt;&lt;span&gt;当然，很多技术还在研究迭代当中。另一方面，除了城市摄像头的数据，还有卫星数据、无人机数据等等，对城市的规划、环保等等也能起到检测作用。总体来说，就像是整个城市的一个眼睛，其实不是一个眼睛，像是复眼一样，而且不仅仅是看，还要理解识别，要看全、看清、看透，并作出相应决策。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：高效准确地对路况进行仿真预测是破解交通问题的难点，可否具体介绍一下这里应用了怎样的算法去进行实时交通预测？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;华先胜：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我们首先要对整个城市及其交通状况有一个全面、清晰、透彻的了解。要看全，因为城市的摄像头非常多，看全就涉及到刚才讲的大规模视频处理，也包括以前在交通领域里还无法获取的信息，比如行人的信息，过去的交通模型里面其实是没有办法使用的，因为没有办法获取信息，车辆的信息还可以通过其他手段得到一部分，比如说通过地面上的感应线圈也可以得到，当然这个是比较粗一点，车的类型是没办法知道的。第二是通过 GPS 采样信息，但是也不够完整，视频的信息是可以看得非常完整，看到整个车流和人流。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;看得清晰，在技术上来讲，是要看到到底有多少车经过，车的类型是什么，车牌是什么，走到哪里去，左转右转还是直行，速度是多少等，也包括到底多少行人在占用人行横道等，这些对交通的优化都是非常重要的信息，也是过去无法获取的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从看得透彻的角度来讲，实际上是挖掘大量数据之间的关系，从而发现这个规律，或者说发现他们之间的相互制约性，从而得出决策。举个例子，比如说在交通的优化当中，我要优化红绿灯，我不能只看这一个路口的信息，要看很多的路口，因为你如果把这一个路口解决了，有可能反而造成别的路口更加拥堵。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有了这样三个层次的了解之后，才是交通模型。作为我们云计算公司来说，是要在更大规模、更准确的数据状况下，尤其是视觉数据，再加上交通专家的研究成果、交管部门实际经验，我们一起来解决交通的建模和优化问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：在遇到交通事故或是拥堵问题时，利用什么评价指标体系来推演获取最佳的解决策略？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;华先胜：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;通常来说，我们看到车流情况以后，其实就可以对红绿灯进行优化了。做离线的优化，是根据每天的规律，或者每周长时间的规律，对红绿灯做一次性离线的优化，以及配时方案，星期一早上几点到几点是什么样子，中午、晚上是什么样子，星期二是什么样子，每天不一样的方案。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对交通事故的应对需要实时调控，这里也分两类，一种是已经堵起来，还没有堵死的时候已经看到这个趋势，可以对红绿灯进行管控，一个方向时间延长，另外一个方向减少时间等，这是对红绿灯的调控。更聪明的一点做法，我们如果观察到一些事故发生，就对它的规模、可能带来的交通问题做一个大概的估计，提前做出疏导预案，这是可以做到的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：能否为我们详细介绍一下，城市大脑项目中的实时和离线这两个视觉计算平台中的关键技术点和数据规模？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;华先胜&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这是很好的问题。阿里云的计算平台，叫做飞天系统，你可以把它看作是一个超级的计算机。飞天的离线计算和实时计算，这一套系统有 100 万个 CPU 的核，这个是相当大的数量了。有 60 万块硬盘，有一个 EB 的能力，这个 EB 是 1024 个 PB，一个 PB 是 1024 个 TB，一个 TB 是 1024 个 GB，这个量是非常得大的。视频分析背后依靠的就是这样一个大规模的这样一个计算的能力，必须有这样的能力在里面，才能够完得成这些复杂的大量的计算。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于视频而言，当然我们在这里面也会有一点特殊的地方，因为视频处理有它的特点，比如说数据量大、吞吐量大、计算消耗也非常大。我们在这个基础上，跟计算平台一起，让计算平台能够处理这些视频数据。用比喻来说，就是它能吃得进去，消化得了，并把这个营养吸收得了，最终产生结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但这里面的视频处理有特殊性：视频处理有时间上的相关性。比如说我们对某一当前时刻图像进行处理的时候，是依赖于前面的若干时刻图像的，所以在视频里面要很方便地处理这种逻辑。再比如说像交通的场景下，甚至是我当前的视频需要跟别的好几路视频合在一起才能形成一个决策，比如说像红绿灯的管控，我光看一个路口的一路肯定是不行的，甚至光看一个路口的四路也不行的，我要看好几个路口一起来决策，这就是在物理的空间上也是有相关性，我经常把这叫做「时空的相关性」。在这种情况下能够顺利完成计算，从而实时得出决策，这都是通过平台才能达到的。对于算法专家来说，更多的精力是放在算法的研发上，提升算法准确性和本身的计算效率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：在离线和实时处理过程中，如果要达到理想的识别精度，比如道路车辆信息、路况信息等，需要多大规模的训练样本库？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;华先胜&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这个是 case by case 的，对于简单一些的问题，要识别的目标特异性明显，和其他目标和背景的差异性大，就不需要太多的样本。当然，实际应用环境中的情况往往比较复杂，识别模型往往需要到实际应用中迭代优化。离线和实时处理是模型训练好之后的生产环境，不是训练环境。当然，模型的在线更新是和离线、实时处理系统在一起的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：深度学习落地产业应用是近年来的发展趋势，计算速度也是衡量算法能否落地的一个重要性能。我们注意到，这个项目中计算速度的提升效果是非常惊人，单核 CPU 对单帧图片处理速度可以从 998ms 提升至 135ms，可否为我们介绍一下基于 Intel 的 MKL 加速以及在优化深度学习模型方面做了哪些努力吗？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;华先胜：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;其实我们最初的模型在 CPU 上的处理需花费 2600 毫秒，这个其实是相当慢的。后来我们跟英特尔合作，利用英特尔的 CPU 上的优化，在单核上压缩到 900 多毫秒。后来我们再通过算法本身的优化，包括模型的结构优化，参数的优化等等，就降低到 130 多毫秒，这又提升了很多倍，整个提升了十几倍。这十几倍的提升，听起来可能没什么感觉，但对于大量的计算资源来讲是非常重要的。如果你只要一台、两台机器做事情，还不是太大的问题，假如你要 1 万台、2 万台机器同时运行，那就是一个很大的事情了。这个量的相差是非常非常多的。所以大规模计算的效率也是非常重要的方向。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：深度学习计算加速技术的实际应用中，您认为哪一种是更符合工业界需求：GPU (M4) 加速，CPU (Intel MKL) 加速 或者 FPGA 加速？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;华先胜&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：各有千秋吧，当然仅结合 CPU 的特性来优化还是很有挑战的。技术上，FPGA 当然要复杂一些，但成本上应该更优一些。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：人脸技术作为计算机视觉中较为重要的课题，阿里云的人脸识别技术在服务端和手机端分别达到了 99.53%、98.93% 的准确率，能否分享一下这背后的人脸识别技术及算法革新？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;华先胜&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：识别技术上和其他公司并没有关键的区别，但有一些其他方面的创新应用可以讲（例如 3D 试戴、试衣、试妆等），准确率可以说和主流公司提供 comparable，方法上除了流行的方法外，借鉴了拍立淘中电商图像特征学习的经验。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：人脸识别和图像识别技术的应用范围广泛，比如安全金融、智能审核以及图像编辑等，除了支撑阿里巴巴集团内部产品，是否也在推进与其他平台厂商的合作？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;华先胜&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：阿里云的视觉计算技术以对 B 端应用为主，当然也有to C 的。我们更多立足于用视觉智能解决各行各业的问题，过去不能解决或者必须人眼去看才能解决的问题，耗时耗力，变成简单高效。我们还着力打造生态，让第三方算法能够跑在阿里云的视觉计算平台上，为更多的客户、用户带来实在的价值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：现在的人脸识别系统仍然主要依赖有标签数据的训练，但在特定的任务中特定群体（如刑侦或治安监控任务的小孩或青少年）的训练数据量不足导致了应用效果较差，以及图像质量不稳定或者目标有意的伪装都会影响识别。在未来的人脸识别中解决这些问题的方向是什么？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;华先胜&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：在金融场景，可以考虑用眼纹的方法，例如蚂蚁金服收购的 EyeVerify 公司的眼纹技术，进一步增强准确率。但确实很多监控场景中人脸的分辨率都不太高，或者成像质量不好。这种情况可以考虑用一下 context，例如人体特征、步态等。这种场景下，与金融场景中的人脸比对不同，对人或人脸的识别的要求是不一样的，并不要求（也做不到）很高的准确率，而是要很高的召回率，然后通过人工来进一步确认。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：无论是在工业诊断方面还是在医疗图像领域，高精确度都是计算机视觉解决问题的前提条件，目前提升精确度的挑战是什么？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;华先胜&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这种场景和典型的识别场景是不一样的，因为这类场景的目标通常是个小概率事件，正例的目标很少，而且有时正例之间的差异性还很大，甚至无法穷举。在这种情况下，高召回率是主要的目标，准确率是要被牺牲的目标。例如，10000 个样本，如果目标正样本很少，只有 10 个，如果算法测出来有 100 个，只要那是个证样本在这 100 个之内，召回率就是 100%；而这时的准确率只有 10%。然而，这已经是非常不错的结果了，因为我们只需要人工确认这 100 个样本就好了，而不需要看那 10000 个样本，人工省了 99%。所以这种应用，关键是召回，然后一步一步降低虚警，也就是提高准确率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：针对仿真视频图像的生成，阿里云采用了什么样的方法？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;华先胜&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这里有两种生成。一种是三维场景中的物体植入，这种场景是要做三维重建，寻找嵌入位置，然后将三维目标植入场景，随场景一起运动；另一种是平面图形的生成，只要用于生成以假乱真的某个特定类型的图像，方法是自主研发的基于 GAN（生成对抗网络）的方法，目前用于训练数据的大量自动合成。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：简单谈谈阿里云的图像搜索技术有什么特点？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;华先胜&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：阿里的图像搜索技术有深厚的技术和实践积累，在电商中经过多年的精细打磨。目前我们正在将其应用到城市图搜的场景当中。一般而言，图像的索引（indexing）过程是图像搜索的关键，其中又包括了识别、目标检测、特征提取和索引建立，索引建得好不好直接关乎搜索结果排序 (ranking) 的质量（相关性）和搜索效率。识别、目标检测和特征又是索引质量的关键，基本上每一步都是通过深度学习来达成的，一步有问题都不能得到满意的结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：计算机视觉是深度学习中第一个取得突破的领域，前面在静态图片上已经获得很大成功，在您看来，下一步的突破会在哪些方面？还要解决哪些关键性挑战？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;华先胜：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;确实，深度学习是在视觉、语音，包括自动翻译这方面有很好的应用，为什么在文本搜索上可能进展并没有那么明显？当然也有人觉得还没有做到足够深入，也有人讲是因为图像和语音，尤其是图像和语义之间的差距还很大，所以深度学习在里面能够起到很关键的作用。从视觉的角度来讲，我觉得还有很多问题去解决，深度学习本身算法的研究还是有很大的空间，这并不是所有的问题都做得很好了。模型这些年也不断的在演化，训练的策略都在不断的进步。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;还有一个就是人工智能的平台，我觉得也是值得思考的一个方向。就像过去电脑是单机的操作系统，像 Windows，那么在 Windows 这个平台，产生了大量的程序。对于手机也一样，在安卓、在苹果的 iOS 上也产生大量的应用，那么云计算也一样，它也是在云计算的平台上逐渐在形成大量的应用。所以 AI 是不是也会这样？是不是要有一个这样的平台，使得大家去做 AI 应用的开发和研究变得更加容易，就像过去写一个程序一样那么好做，我觉得这可能也是很关键的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从应用的角度来讲，我觉得计算的效率可能也非常重要，尤其是大规模的视觉计算，如果需要大量的数据，计算量非常大，必须是在资源消耗可控的情况下才能完成。如果发现完成这件事情都要破产的话，就没有办法继续做下去了，这里面涉及到系统架构的效率包括算法本身的效率等等之类的各种优化，这个也是很重要的系统问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;算法的红利会逐渐消失&lt;/strong&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：您从业近二十年，经历了人工智能行业的技术变迁，在您看来，哪些因素造就了这一波行业热度？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;华先胜：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我个人认为，技术是其中最重要的原因，应该说是一个根本的推动力。这些年，技术发生了很大变化，首先机器学习的技术，尤其是深度学习的技术，在识别、搜索、生成的方面都比传统方法表现更加优秀。第二是计算能力，特别是云计算使得计算的能力远远的超过以前，而且我们获得大量计算的能力，也变得非常便利，当然，移动设备的发展也是一个重要因素。我记得在上个世纪图像搜索这个事情刚刚开始研究的时候，那时候也很火热，那个时候叫 CBIR，也成就了很多的博士论文。但是当时经常有人提问，你第一张图片到底哪里来呢？到今天今天这根本不再是个问题，因为我们获取数据变得非常容易。还有网络带宽的发展，使得我们在设备端，在互联网上大量的数据得以传播，尤其是视觉的图像识别数据得以传播，这些因素都是促成人工智能火热火爆的场面。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：在您看来，一个成功的商业应用应该具备哪些条件？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;华先胜：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我认为应该具备五个条件：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第一个是算法。你要有好的算法，你的算法要有先进性，你的算法不行一切都没有了基础。（当然你也可以把算法这一个条件看做是科学家，因为人才和算法是紧密相连的）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第二个是要有数据。数据本身就是一个很大的话题，里面有数据的采集、搜集、清洗、有效的标注，甚至包括算法里面数据怎么使用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第三个是用户。你做的这个东西应该有用户的，因为有很多问题是需要用户参与才可以做得越来越好。当然你从商业的角度来讲，没有用户的话也不能够长久。用户本身是数据的消费者，也是数据的提供者，这过去在搜索引擎里面有非常重要的体现，可以说搜索引擎的技术能够做那么好，每个人都有 contribution 的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第四个就是平台。这个就是涉及到你要有强大的计算能力和一套体系架构，能够方便地去研发、部署和生产，这一套是必须要有的。当然现在因为有云计算，所以这部分的瓶颈，对于很多企业来讲已经没有过去那么困难了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第五个就是有好的商业模式。如果没有好的商业模式，就不可能长久。你做一个事情，低频的事情没有多少人用，或者不能给少量用户带来大的价值，最后产生的总体价值不够的话，其实是很难长久的。这几点，我个人觉得其实是都应该具备的。当然了，可能不同的商业应用，应该来说可能有不同的侧重，但是我觉得都应该具备。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：很多大公司押注人工智能，越来越多创业公司也在涌入，公司之间的差距会体现在哪些方面，算法是公司竞争的核心要素吗？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;华先胜&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这是一个很好的问题，也有很多的争论，我说说我自己的观点，我们有很多公司确实是以算法起家的，但是我觉得算法之间的差异，可能会逐渐越来越小。尤其是现在基于深度学习的方法，以及包括很多开源的出现，对于内行人而言，或者叫高手之间，他们之间算法性能的差异其实不会太大。比如说人脸识别，在 AFW 上面，大家测试的差距都在小数点后面一位两位的，没有太大的差距。像 ImageNet 也一样，差不多都是 99.6%、99.7% 这样子，都是不难达到的。那这些对于内行人而言没有太大的差距，但是在真实场景下应用的时候还会有差距，随着时间的推移，大家都在实战当中磨炼的话，都不会差距太大。甚至包括数据的优势，也会减少，很多的公司，不管是大公司还是创业公司，做得稍微早一点，积累了大量的数据，不管是标注的信息还是算法在练习当中搜集的反馈，随着时间的推移算法、数据的红利也都会逐渐减少。&lt;/span&gt;&lt;span&gt;当然，这里是对一个具体的图像识别或搜索或生成算法而言的。在很多行业，数据的获取有barrier, 这时数据本身就是价值。如果不具备或者没有足够量的相关数据，基于数据上的智能和应用就无法完成，这时数据本身就成为了核心竞争力。如果相关数据是容易获取的，就不能成为核心竞争力了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;还有什么是具有竞争力的东西呢？我觉得可能还是要看平台和商业应用。从商业模式上来使得自己的竞争力具有长久性，尤其是在细分的这个行业，你做到非常精深。&lt;/span&gt;&lt;span&gt;因为这部分并不是那么显而易见的，并不是说随便搞搞，我们就都是 90% 几之类的，这个需要你精耕细作的，需要你深入这个行业，结合真实场景数据的一些特点，才能够逐渐把这个行业吃透、打穿，才能够有一席之地、成为高手。那么这个的话，其实是可以有差异化的。因为这个行业非常的多，其实大家不见得一定要挤在一个独木桥上，一定要去刷通用的图像识别这些东西，或者是非常火热的一些领域，其实有很多路可以走的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：现在有不少人工智能威胁论，但反过来看，人们对人工智能整体的发展和展现出来的技术能力，是不是也过于乐观了？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;华先胜&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：我们确实也要冷静看待一些问题，有几个角度来看到。比如说现在的识别就已经做到真的那么好了呢？大家可能有一些体会，这个准确率的数字好像很高，但是在真实场景下，有时候也不那么好。我举个例子，像大家比较公认的 ImageNet 比赛，有数百万张图片，进行 1000 类的分类，我们通常说现在最高的准确率已经做到超过 96% ，错误在3% 左右。那这个其实这里面有很多可以去探讨的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第一点，超过 96% ，是指前五的正确率。也就是说一个图像识别出来 5 个结果，其中有一个对的就算对。如果规定第一个必须对才算达到正确，那可能正确率只有 80% 左右。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第二点，是我们这个世界是很复杂的，远远超过这 1000 类，有很多现实世界当中太多太多不一样的东西都需要去识别。这实际上是一个覆盖率的问题，刚才讲到在标准的测试级上可以到很高，这是一个准确率的问题，准确率当然也是非常关键的，也是推动这个领域发展重要的一个指标，然而真正在现实当中的覆盖也是非常重要的。覆盖直接关系到人的体验，尤其是在识别和搜索这里面。比如说我那一年在做拍立淘的时候也是花很大的心思去解决覆盖的问题，覆盖的意思是就是说你搜什么都能给点相关结果出来。那准确率是说，我搜出来的东西要跟我想象的东西是相关的。这两个都是非常重要的。现在的识别的技术在覆盖上其实是有欠缺的，当然了覆盖的话，也不是说不能解决的。我记得我在前些年也做过一套系统，当时是利用了互联网的数据，使得覆盖能够得到更大的提升，用了互联网的数据自动取挖掘训练数据，使得它可以识别任意的东西，当然这个任意的东西还是有条件的，互联网上可以找得到数据，可以找到足够足量的数据然后可以自动清洗自动建立模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第三点，有没有好的商业应用，有没有真正深入行业产生价值，也是非常关键的。这一部分做不到，就不能长久。我觉得还是应该认真地考虑一下，创业也好，创新也好，基础是不是稳固的？比如说刚才我讲到的我的观点的五个要素是不是都具备了，缺什么，需不需要补，或者我们的优势在哪里？如果我们的优势只在算法上，那么可能还有一点危险，如果我们还有成功的商业模式，有源源不断的商业应用商业价值的产生，那可能就会比较安全一点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心： AI 模型的通用性怎么样？然后为了可用性高，是否最终都需要定制方案，那么开放平台上的 API 还有多大意义？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;华先胜&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这个问题问得比较有深度，前面其实我们也讲过了这也是为什么我讲要深入行业，但是深入行业的话，可能有人会讲了，那你有多少人，你做得过来吗？那这里面的第二个问题就是刚才讲的生态，这个不是一家人能够做得出来的，需要很多人去做，就像操作系统上那么多应用程序，包括手机操作系统上那么多好玩的 APP，各种功能的 APP 那不是苹果一家能做得出来的，所以我们要做成这样的一个生态。就像你搭了一个舞台一样，不是光自己在那里演，有很多人都可以上来演，有很多有创意的人都可以上来演，这个就解决了深入各行各业解决实际应用的，在这里面能够做得更好，在一个行业里面在一个应用领域里面怎么做得更好，这样才能够真正发展起来。现实世界就是这么残酷的，很少有一个模型可以打天下的情况，几乎都是不可能存在的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&amp;copy;本文为机器之心原创，&lt;strong&gt;&lt;span&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Sat, 04 Mar 2017 11:57:36 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 谷歌使用深度学习帮助病理学家检测癌症，算法得分高达89%</title>
      <link>http://www.iwgc.cn/link/12f5c9a989baf4d569427cd83caac9b06da8d280</link>
      <description>
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;选自Google Research&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Martin Stumpe、Lily Peng&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;参与：微胖、李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote style="color: rgb(62, 62, 62); font-size: 16px; white-space: normal; max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;在检查完病人的生物组织样本之后，病理学家的报告通常会成为许多疾病诊断中的黄金标准。特别是在癌症诊断中，病理学家的诊断对病人的治疗有着极大的影响。检查病理切片是一件非常复杂的任务，需要多年的培训从而掌握专业知识、获取经验。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;即使经过了如此密集的培训，不同的病理学家对同一病人可能给出相当不同的诊断，从而导致误诊。例如，诊断同样形式的乳腺癌上的一致率（agreement）只有 48%，几乎和前列腺癌一样低。为了做出准确诊断而需要检查如此大量的信息，缺乏一致率也无可厚非。病理学家要负责检查切片上的所有可见的生物组织。然而，每个病人都会有许多切片，在进行 40 倍放大时每个切片都有 100 多亿的像素（10+gigapixels)。想象一下要浏览 1000 多个百万像素的图片，还要为每个像素负责。不用说，这要覆盖大量的数据，而给的时间往往是有限的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了解决时间有限和诊断不一致的问题，我们研究了如何将深度学习应用到数字化病理学中，通过创造自动检测算法来自然地补充病理学家工作流。我们使用由 Radboud 大学医疗中心提供的图像训练算法，这些训练数据也曾被用于 2016 年 ISBI Camelyon 挑战赛，而该算法经过优化可来确定是扩散到淋巴结的乳腺癌还是扩展到临近乳房的乳腺癌。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;结果如何？标准的像是 Inception（又叫做 GoogLeNet ) 这样的现成深度学习方法对以上两个任务都有效，尽管产生的肿瘤概率预测热点图有点噪音。经过额外定制化之后，包括训练神经网络在不同放大倍数的图像上进行试验（很像病理学家所做的），我们表示是有可能训练一个相当于或超越病理学家（他们试验切片的时间没有限制）表现的模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/e841cfe22c620a3c0ef9d0ea9f60d9e145685dc7"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;左：两个淋巴结活组织检查的图像；中：较早之前我们的深度学习肿瘤检测的结果；右：目前的结果，可看到两者之间噪声（潜在的假正例）的减少&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;事实上，算法产生的预测热点图已经有了很大的改进，算法的 FROC 得分达到了 89%，大大超过了病理学家无时间限制进行诊断的得分（73%）。我们并不是唯一一家看到它的潜力的团队，其他团队在同一数据集上也得到了高达 81% 的结果。更激动人心的是，我们的模型泛化非常好，即使是在不同医院使用不同扫描仪得到的图片上。想要了解更多，可参考我们的论文《Detecting Cancer Metastases on Gigapixel Pathology Images》。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/958f8586df47f0fcae90fa4ca2f5617c44d10e12"/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/4ccb50926f36844162e1062175cf9875c3e9dedf"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;淋巴结活组织检查特写图。该组织包含乳腺癌转移和巨噬细胞，看起来像是肿瘤但却是良性的正确组织。我们的算法成功的识别了肿瘤区域（亮绿），并未被巨噬细胞所迷惑。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然这些结果惊人，但还是要考虑以下重要的提醒：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;像大部分标准一样，FROC 定位得分（FROC localization score）并不完美。在这里，每个切片有一些预定义的平均假正例的情况下，FROC 得分被定义为敏感性（sensitivity)。病理学家做出假正例的情况相当罕见（把正常细胞误诊为肿瘤）。例如，上面提到的 73% 的得分对应 73% 的敏感性，以及 0 个假正例。与之对比，我们算法的敏感性在更多假正例的情况下会上升。每个切片有 8 个假正例的情况下，我们算法的敏感性达到 92%。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;这些算法在之前训练过的任务中，表现良好，不过，尚缺乏人类病理学家的知识广度和经验&amp;mdash;&amp;mdash;比如，识别其他不正常情况的能力，而之前没有详细训练过该模型对这些情况进行分类（比如，发炎过程，自身免疫疾病以及其他类型的癌症。）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;为了确保最佳临床效果，还需要将这些算法吸收到病理学家的工作流程中去，完善这一流程。我们的愿景是，诸如这些算法能提高病理学家的工作效率和诊断的一致性。比如，通过检查排在前面的预测肿瘤区域（包括每个切片有 8 个假正例的区域），病理学家可以降低假率的未检出率（也就是没有被监测到的肿瘤）。另一个例子这些算法能够让病理学家简单而准确地测量肿瘤大小，这一因素与预后 (prognosis) 有关。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;训练模型知识将有趣的研究成果转化为真实产品的第一步。从临床有效到官方批准，还有长的路要走&amp;mdash;&amp;mdash;不过，我们起了一个好头，也希望通过成果共享，加速这一领域的发展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;原文链接：https://research.googleblog.com/2017/03/assisting-pathologists-in-detecting.html&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&amp;copy;本文为机器之心编译，&lt;strong&gt;&lt;span&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Sat, 04 Mar 2017 11:57:36 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 人工智能芯片群雄势力图景：面向应用场景的设计成为趋势</title>
      <link>http://www.iwgc.cn/link/3d180478fece2d05eef8e5956d363269828d9657</link>
      <description>
&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;选自Forbes&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;作者：Karl Freund&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李泽南、微胖&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;毫无疑问，2016 年科技界的关键词是人工智能与机器学习。但直到今天，除了自动驾驶汽车和手机里的智能助手，没有人能列举出几种应用了机器学习的设备。人工智能在哪里？我们不禁产生了怀疑。当我们要求 Siri 播放音乐，或提供明天的天气预报时，那个「她」是在你手机中的意识体，还是苹果服务器的一部分？亚马逊 Alexa 呢？答案是：它们都在云端。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当你正在考虑这些技术上的问题时，投资者和开发者们则更进一步，开始研究 AMD、英特尔、英伟达、高通和 Xilinx 提供的芯片各有哪些优点，哪一家公司的芯片更符合人工智能时代的需要了。本文为你展示了目前芯片行业的图景，针对不同的应用需求（云端、单机和混合场景），对目前市面上的芯片类型进行了分类讨论。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;以行业和部署位置分类&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;根据 Angel List 的数据，目前应用机器学习构建业务的初创公司数量惊人，已达 1700 家，投资机构则超过了 2300 个。下表列出了这些公司触及各领域的大致分类，以计算能力为标准，共分为单机、混合以及云端解决方案三种。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/68559149d7478dc4685dde9619328750b6171532"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器学习应用图景 (Source: Moor Insights &amp;amp; Strategy)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;CPU、GPU、ASIC 还是 FPGA？&lt;/strong&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前机器学习的应用主要是通过大量编制好的数据训练神经网络模型，然后再让训练后的神经网络处理接收需要处理的数据。其中训练神经网络如何「思考」的过程需要耗费大量计算资源，这类任务通常是由数据中心的 GPU 来完成的，而 GPU 的提供商以英伟达为主，在 AMD 等其他公司的竞品性能有限的情况下，目前英伟达的市场地位稳固。但在混合以及单机计算解决方案领域，目前市场上还存在一定的竞争，下图展示了更大方向上机器学习硬件的主要产品。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/4735b98f53bd49ccc76117be3304c6fa496a49d7"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器学习图景中的硬件情况&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在机器学习时代刚刚到来的今天，全球市场并未出现「一家独大」的局面。所有硬件公司都声称自家的计算架构（CPU，GPU，ASIC 和 FPGA）最适用于人工智能和机器学习的计算，并给出了自己的数据。实际上，每一种架构仅会在特定场景、数据、应用或部署方式条件下优于其他架构。数据的复杂性和任务的速度要求决定了计算能力的需求，而应用场景则对延迟和功耗产生不同限制。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CPU，如数据中心常见的英特尔 Xeon 和 Xeon Phi，还有移动设备中采用的高通骁龙，善于处理相对简单的数据，比如文本和 jpeg 图片，一旦要训练神经网络，在处理源自诸如 4K 摄像头或雷达的高速率像素数据时，就会捉襟见肘了。为了解决这一问题，英特尔准备推出新版本的多核 Xeon Phi，代号 Knights Mill，有望今年年底上市。不过，在很多情况下，完成任务需要一个 GPU，一个专用集成电路（ASIC）类似英特尔期望中的 Nervana 引擎或编程 FPGA 以满足低能耗、低延迟的性能水平，这是智能无人机、车辆或导弹方面应用的基本需求。尽管英伟达 GPU 在未来仍会是很多性能测评的胜出者，因为它是最快的解决方案，但是，当加速算法不断演进时，FPGA（通常来自英特尔或 Xilinx）却有能力对机器学习算法进行专门优化，延迟也低。在云端，我们也能发现类似情况，在这个领域里 GPU、FPGA 以及类似谷歌 TPU 的 ASIC 共存，每类都代表一些独特的功能，针对特定数据类型和吞吐要求，各自有其利弊。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一些应用，比如视觉导航自动驾驶系统就需要一种混合硬件的解决方案，以满足应用环境对延迟和数据处理的要求。尽管之前提到的加速善于运行人工智能推理引擎、传感器融合、数据预处理，后评分策略执行也需要很多特定的 I/O，快速传统逻辑最适合 CPU。为了应对这一挑战，英伟达提出了混合硬件平台，英伟达 Jetson TX1 和 DrivePX2 是 ARM 处理器与 GPU 的混和体，甚至还附有存储单元，而英特尔和 Xilinx 的 SoC 中，ARM 处理器和 FPGA 被整合成为一个单独、优雅的低能耗解决方案。所有这些设备都可以在无人机、工厂机器人/合作机器人以及自动车辆等应用场景中找到自己的位置，这些领域要求开发者们必须合理地权衡速度、灵活性以及功耗需要。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/6dd3ae22d956e16325dd13e882b6892b5a8eacec"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;英伟达 Jetson TX1，适用于为嵌入式系统中的机器学习和计算机视觉任务提供计算能力&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而在移动计算领域，高通一直在不断增强自己的骁龙处理器，以囊括各种加速器技术来支持移动端以及其他单机设备的机器学习技术，这些设备是智能物联网的重要组成部分。最新的骁龙 835（8 核处理器，预计由三星 Galaxy S8 在 3 月 29 日首发）已经发展到了 10nm 制程，整合了 4 个主频 2.45GHz 的 Kryo 280，4 个主频 1.9GHz 的 Cortex-A53，Adreno 540 GPU，ISP（图像信号处理器）、DSP（数字信号处理器）等模块，以满足各种编程以及硬件模型加速机器学习算法的需求。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一种强大的架构并不能解决所有问题，不能满足兴起的机器学习应用中所有环境的计算需求。因此，我们必然会看到越来越多的选项，针对不同工程/设计团队，针对具体智能系统、产品服务，量体裁衣。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;原文链接：https://www.forbes.com/sites/moorinsights/2017/03/03/a-machine-learning-landscape-where-amd-intel-nvidia-qualcomm-and-xilinx-ai-engines-live/#742f84ef742f&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&amp;copy;本文为机器之心编译，&lt;strong&gt;&lt;span&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Sat, 04 Mar 2017 11:57:36 +0800</pubDate>
    </item>
    <item>
      <title>学界 | DeepMind 连发两文：从可微分界树构建深度最近邻表征到合成梯度与解耦神经接口的深入研究</title>
      <link>http://www.iwgc.cn/link/6ce7ac554efe08134ff06c7391a3717735c7cd6c</link>
      <description>
&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;选自arXiv.org&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：蒋思源&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;近日 DeepMind 连发两篇论文，其都注重于开发一种更加高效、可解释的模型或算法。其中机器之心重点关注了 DeepMind 使用深度学习构造高效、可解释的最近邻分类树，初步了解了边界树及其变换构造深度最近邻表征。其次 DeepMind 发表了合成梯度 (SG) 与解耦神经接口 (DNI) 深入研究，并表明了 SG 的并入并不会影响神经网络学习系统的表征强度。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：Learning Deep Nearest Neighbor Representations Using Differentiable Boundary Trees&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文地址：https://arxiv.org/abs/1702.08833&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/122ca19315b6b14302db2d0cb78e8af97390f7e1"/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;摘要&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随着计算机硬件水平的进步和算法效率的提升，K 近邻（K-Nearest neighbor/kNN）法近年来已经越来越受到欢迎。如今机器学习模型有很多算法可以选择，每一种都有其自身的长处与短处。其中所有基于 K 近邻（kNN）的方法都要求在样本间有一个优良的表征（representation）和距离度量（distance measure）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们引进了可微分界树（differentiable boundary tree）这一新方法，该方法能学习深度 K 近邻的表征（Deep kNN representations）。我们的方法建立在最近提出来的边界树（boundary tree）算法之上，该算法能进行高效的最近邻分类、回归和检索。通过将树中的遍历建模作为随机事件（stochastic events），我们能构建与树预测（tree's predictions）相关联的可微分成本函数。通过使用深度神经网络转换（transform）数据还有沿树进行反向传播，模型就能学习 K 近邻法的优良表征。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们证明了该方法能学习合适的表征，并通过清晰的可解释架构提供一个非常高效的树。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/bbdad25427d52cd6edfdf0adf02ef920f202073e"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 1：边界树以在线的方式（online manner），一个接一个样本构建。从左到右：给定当前树（左图中描绘）从根节点开始。对于每一个查询（query），我们采用递归的方式遍历整棵树，每一步选择离询问节点（query node）局部最近的节点。一旦遍历停止，我们就会使用最后节点的类做预测（中间那幅图）。如果预测是错误的（如这个案例），我们就会将查询节点作为子节点（child）添加到最终节点中，从而构建一颗新树（最右边那幅），同时丢弃查询结点。因为树的边界根据定义和样本将倾向于靠近分类边界，所以也就有了「边界树（Boundary Tree）」这一名字。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/0e2079e1bd2149330c882e1fd74eb4342de3be06"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 2：方程式 6 中的成本函数在构建中涉及到不同近邻和变换（transitions）的可视化。树在这里是以任意二维空间展示的（为了可视化）。给定询问节点并通过 f_theta 转换所有样本后，我们可以遍历树中节点以下的所有路径（图中标红）。每一个变换（transitions）的概率都进行了计算，直到最后节点的近邻才停止。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这里我们聚集了节点的类标签，并通过它们各自的变换概率（transition probability）加权，从而构建出类的预测作为输出。可以参见图 3，用于计算成本函数的关联神经网络（associated neural net）可视化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/b1c4e985a7979faadfbf2b52339f7e78b5c5ecf8"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 3：需要为每个查询节点（query point）动态地构建神经网络。对于每一个变换（transition），通过树模块输出的转换样本（transformed samples）是共享的。每一个模块都会提取转换样本（transformed samples），并计算它们和转换询问点之间的距离，然后转为对数概率（log probabilities）。数据变换（Transitions）是基于转换样本的。这些转换样本与最终的节点预测相结合以生成类预测，同时损失也通过构建的网络进行传播。可以参见图 2，相似树结构和路径（corresponding tree structure and path）的可视化。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/1f26887dc8132201ae39ee5d06e916c5ca044e7b"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 4：使用 MNIST 手写数字数据库中 1000 个已训练的表征样本所构建的树。样本在这里是以原始像素空间（original pixel space）表达的，但是学到的表征（learned representation）是用来构建树的。注意其简单且可解释结构&amp;mdash;节点是原型样本（prototypical examples）或边界情况（boundary cases）。值得注意的是，这棵树仍然在测试集上获得了 2% 的错误降低率。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：Understanding Synthetic Gradients and Decoupled Neural Interfaces&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文地址：https://arxiv.org/abs/1703.00522&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/070c1b4ec4ba6c20e69a5ad0abff0ff51c2ce03c"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;摘要&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当使用合成梯度（Synthetic Gradients /SG）训练神经网络时，可以在不使用更新锁定（update locking）的情况下训练层级或模块，这样就不需要等待误差真值梯度（true error gradient）沿反向传播，也就导致一种退耦合神经接口（Decoupled Neural Interfaces/DNIs）。这种更新解锁的能力（unlocked ability）可以使用异步的方式更新部分神经网络，并且 Jaderberg 等人（2016）也证明了只有局部信息（local information）能以经验为主地工作（work empirically）。然而，很少有证据表明是什么改变了从函数（functional）、表征（representational）和视角学习动力点（learning dynamics point）实施的 DNI 和 SG。在本论文中，我们通过使用前馈网络上的合成梯度（Synthetic Gradients）来研究 DNI，并期望能更好地理解它们的行为和阐明其对优化算法的影响。我们的研究表明 SG 的并入并不会影响神经网络学习系统的表征强度（representational strength），并证明了线性和深线性（deep linear）模型的学习系统收敛性。在实际操作问题上，我们调查了使用合成梯度估计量逼近损失真值（true loss）的机制，并很惊讶地发现其是如何导致完全不同的层级表征。最后，我们还揭示了合成梯度和其他误差逼近技术（error approximation techniques）的关系，并发现可以使用同一的语言进行讨论和比较。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&amp;copy;本文为机器之心编译，&lt;strong&gt;&lt;span&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Sat, 04 Mar 2017 11:57:36 +0800</pubDate>
    </item>
    <item>
      <title>一周论文 | VAE在自然语言处理中的应用</title>
      <link>http://www.iwgc.cn/link/8aefa28f529b505c51a58c9f7118b5b962e4c24c</link>
      <description>
&lt;p&gt;&lt;span&gt;提及 Generative Models，Variational Autoencoder (VAE) 和 GAN 可以说是两座大山头。二十四期的「&lt;a data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650723149&amp;amp;idx=5&amp;amp;sn=5f8ef659299397737b29c62489f1fabf&amp;amp;chksm=871b1733b06c9e2520bdf64724707e47e139495da9f7a1e268dce0119cd3ab566cd6a6623341&amp;amp;scene=21#wechat_redirect" href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650723149&amp;amp;idx=5&amp;amp;sn=5f8ef659299397737b29c62489f1fabf&amp;amp;chksm=871b1733b06c9e2520bdf64724707e47e139495da9f7a1e268dce0119cd3ab566cd6a6623341&amp;amp;scene=21#wechat_redirect" target="_blank"&gt;GAN for NLP&lt;/a&gt;」一文中对 GAN 在 NLP 中的进展做了详细的介绍，推荐错过的朋友不要再错过。虽然 GAN 在图像生成上效果显著（当然 VAE 也很强），但在 NLP 方面暂时还是 VAE 较为 work。今天的分享作为姊妹篇（捂脸），对 VAE 在 NLP 的应用里最具有代表性的几篇 paper 进行介绍。我会尽量梳理论文之间的联系，希望对大家有所帮助。本期涉及的论文有：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="max-width: 100%; color: rgb(62, 62, 62); font-size: 16px; font-variant-ligatures: normal; orphans: 2; white-space: normal; widows: 2; background-color: rgb(255, 255, 255); box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;《Generating Sentences From a Continuous Spaces》. ICLR 2016&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;《Neural Variational Inference for Text Processing》. ICML 2016&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;《Language as a Latent Variable: Discrete Generative Models for Sentence Compression》. EMNLP 2016&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;《A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues》. AAAI 2017&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;其他&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;a rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;" title=""&gt;&lt;img src="http://img03.iwgc.cn/mpimg/b3a3551f7b0a775729cdac4dd6dde0e5ef747fde"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在展开之前，我先带大家简单回顾一下 VAE 的核心。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1) 如上图所示，VAE 可以看做是 Standard autoencoder 的 regularized version（在 autoencoder 的架构上引入随机 latent variable）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;2) VAE 从 data 学到的是在 latent space 的 region，而不是单个点。换句话说是 encode 学到了一个概率分布 q(z|x)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;3) 引入 KL divergence 让后验 q(z|x)接近先验 p(z)。这里的 motivation 在于如果仅用 reconstruction loss，q(z|x)的 variances 还是会很小（又和原有的单个点差不多了）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;VAE 详细推导这里就不展开，各种 tutorial 也非常多。只要掌握变分推断和理解 reparametrization trick 就基本 ok 了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面进入正题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&amp;mdash; 01 &amp;mdash;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Generating Sentences From a Continuous Spaces&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a rel="external" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;" target="_blank"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;论文链接:&amp;nbsp;&lt;/span&gt;&lt;a rel="external" style="color: rgb(136, 136, 136); text-decoration: underline; max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important;" target="_blank"&gt;&lt;span&gt;https://aclweb.org/anthology/K/K16/K16-1002.pdf&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a rel="external" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;" target="_blank"&gt;&lt;br&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这篇文章对后面很多 paper 影响很大而且我也很喜欢，所以重点介绍一下。paper 最早发表在 ICLR 2016 上，motivation 在于作者为了弥补传统的 RNNLM 结构缺少的一些 global feature（其实可以理解为想要 sentence representation）。其实抛开 generative model，之前也有一些比较成功的 non-generative 的方法，比如 sequence autoencoders[1]，skip-thought[2]和 paragraph vector[3]。但随着 VAE 的加入，generative model 也开始在文本上有更多的可能性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;a rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;" title=""&gt;&lt;img src="http://img03.iwgc.cn/mpimg/b243c62dab7ce00af2fed3e934b39db25aa38765"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Loss 的组成还是和 VAE 一样。具体模型上，encoder 和 decoder 都采用单层的 LSTM，decoder 可以看做是特殊的 RNNLM，其 initial state 是这个 hidden code z（latent variable），z 采样自 Gaussian 分布 G，G 的参数由 encoder 后面加的一层 linear layer 得到。这里的 z 就是作者想要的 global latent sentence representation，被赋予了先验 diagonal Gaussians，同时 G 就是学到的后验。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;模型很简单，但实际训练时有一个很严重的问题：KL 会迅速降到 0，后验失效了。原因在于，由于 RNN-based 的 decoder 有着非常强的 modeling power，直接导致即使依赖很少的 history 信息也可以让&amp;nbsp;reconstruction errors 降得很低，换句话说，decoder 不依赖 encoder 提供的这个 z 了，模型等同于退化成 RNNLM（摊手）。顺便一提，本文最后有一篇 paper 也是为了解决这个问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;先看这篇 paper 提出的解决方法：KL cost annealing 和 Word dropout。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1) KL cost annealing&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;a rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;" title=""&gt;&lt;img src="http://img04.iwgc.cn/mpimg/c974c31e6417374042ac3b577eb5923dc445775b"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;" title=""&gt;&lt;br&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作者引入一个权重 w 来控制这个 KL 项，并让 w 从 0 开始随着训练逐渐慢慢增大。作者的意思是一开始让模型学会 encode 更多信息到 z 里，然后随着 w 增大再 smooth encodings。其实从工程/代码的角度看，因为 KL 这项更容易降低，模型会优先去优化 KL，于是 KL 很快就降成 0。但如果我们乘以一开始很小的 w，模型就会选择忽视 KL（这项整体很小不用降低了），选择优先去降低 reconstruction errors。当 w 慢慢增大，模型也慢慢开始关注降低 KL 这项了。这个技巧在调参中其实也非常实用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2) Word dropout&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;a rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;" title=""&gt;&lt;img src="http://img05.iwgc.cn/mpimg/bb338f76efb43fef7c71d95238b0d7b5f253ff4e"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;既然问题是 RNN-based 的 decoder 能力太强，那我们就来弱化它好了。具体方法是把 input 的词替换成 UNK（我可能是个假的 decoder），模型被迫只能去多多依赖z。当然保留多少 input 也需要尝试，我们把全都不保留的叫做 inputless decoder，实验表明，inputless VAE 比起 inputless RNN language model 不知道好到哪里去了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;受到 GAN 的启发，作者还提出了一个 Adversarial evaluation，用一半真一半假的数据作为样本训练出一个分类器，再对比不同模型生成的句子有多少能骗过这个分类器，这个 evaluation 被用在 Imputing missing words 这个任务上，VAE 的表现同样比 RNNLM 出色。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，作者展示模型的确学到了平滑的 sentence representation。选取两个 sentence 的 code z1 和 z2，z1 和 z2 可以看做向量空间的两个点，这两个点连线之间的点对应的句子也都符合语法且 high-level 的信息也保持局部一致。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;a rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;" title=""&gt;&lt;img src="http://img05.iwgc.cn/mpimg/7f6e2c9534ee901eab1d4980abda91ed0b5e62f7"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&amp;mdash; 02 &amp;mdash;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;Neural Variational Inference for Text Processing&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;论文链接:&amp;nbsp;&lt;/span&gt;&lt;a rel="external" style="color: rgb(136, 136, 136); text-decoration: underline; max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important;" target="_blank"&gt;&lt;span&gt;https://arxiv.org/pdf/1511.06038.pdf&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其实这篇 paper 和第一篇是一起投的 ICLR，后来转投了 ICML 2016，所以时间上其实和第一篇是一样的（两篇文章也有互相引用）。不同于第一篇，作者的出发点是构建一个 generative neural variational framework。为了证明 framework 的优越性，分别在 unsupervised 和 supervised 的任务上提出了两个模型，结果也很令人满意。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;a rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;" title=""&gt;&lt;img src="http://img04.iwgc.cn/mpimg/a65f9620e71e75ef3637c2b5920ca9bd351af0ba"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第一个任务是 unsupervised document modeling，模型叫 Neural Variational Document Model（NVDM）。h 和第一篇的 z 一样，在这里代表&amp;nbsp;latent document semantics，但 document 是以 bag-of-words 的形式（个人以为这里作者主要还是受到 LDA 的影响）。encoder 采用MLP，decoder 是一层 softmax。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第二个任务是 supervised answer selection，模型叫 Neural Answer Selection Model（NASM）。文本的建模方式采用 LSTM（在第二个任务用 LSTM，第一个任务用词袋，可能为了证明普适性）。h 代表 latent question semantics。如上图所示，Zq 和 Za 用来表示 question 和 answer，y 代表 answer 是不是正确答案，用 Zq 和 Za 预测 y。那么 Zq 和 Za 是怎么得到的呢？Zq 延用 LSTM 的 last state，而 Za 则较为复杂，所谓脱离问题谈答案都是耍流氓，所以对 Za 建模时要显式的放入 question 的信息。可这里该怎么表示 question 呢？如果还用 Zq，模型很容易 overfitting。这里我们的 latent h 终于可以出场了，引入 h 不仅起到了 muti-modal&amp;nbsp;的效果，还让模型更 robust，再把基于 attention 的 c(a,h)和 answer&amp;nbsp;的 LSTM last state 组合得到 Za。这种做法对我们在寻找 representation 时有很好的借鉴作用。最后通过推导 variational lower bound 确定 h 的先验是 p(h|q)（第一个任务中先验是 p(h)）, 这里就不赘述了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&amp;mdash; 03 &amp;mdash;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;Language as a Latent Variable: Discrete Generative Models for Sentence Compression&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;论文链接:&amp;nbsp;&lt;/span&gt;&lt;a rel="external" style="color: rgb(136, 136, 136); text-decoration: underline; max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important;" target="_blank"&gt;&lt;span&gt;https://arxiv.org/pdf/1609.07317v1.pdf&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这篇 paper 发表在 EMNLP 2016，同样出自第二篇 paper 的作者。传统的 VAE 是把数据 encode 成 continuous latent variable，这篇 paper 的贡献在于提出了一个 generative model 用来学到 language 的 discrete representation&amp;mdash;一个带有 sequential discrete latent variable 的 VAE。所谓的 discrete latent variable 就是指一个单词，加上 sequential 其实就是一个句子，由于 VAE 本身是压缩数据的，换句话说是用短一点的句子来表示原来的句子，也就是句子压缩。我觉得作者的 intuition 在于每个句子可以有多个缩写，且都可以表示原句，有一点点 distribution 的意思，所以用 latent variable 很合适。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;a rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;" title=""&gt;&lt;img src="http://img03.iwgc.cn/mpimg/e94fa43918a5116faef673a2a576e3b53a939ef1"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;原句和压缩句分别是 s 和 c ，模型整体是 encoder -&amp;gt; compressor -&amp;gt; decoder。我们分解开看，encoder -&amp;gt; compressor 采用 pointer network[4]只从 s 里选取合适的词而不是整个词典，从而大大减少了 search space。compressor -&amp;gt; decoder 是一个带 soft attention 的 seq2seq。这个模型的好处是不需要 label 数据，但是如果我们有足够的 label 数据（真实数据里 c 里的词可不仅仅来自 s），需要额外加个 softmax 从整个词典里选词，同时再定义一个 latent factor 判断是从 s（pointer network）还是从词典里选，更加符合任务需求。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;a rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;" title=""&gt;&lt;img src="http://img04.iwgc.cn/mpimg/38c02097261b7f938c535f9be497824a58a3494c"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;值得一提的是 Variational lower bound 里的 p(c)是 pre-train 好的 language model。因为 Language model 的一个特点是比较喜欢短句子，很适合句子压缩的场景。由于 reparameterisation trick 并不适用 discrete latent variable，作者还采用了 REINFORCE[5]的方法（凡是 discrete 的问题，GAN/VAE 都可以采用 REINFORCE）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&amp;mdash; 04 &amp;mdash;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;论文链接:&amp;nbsp;&lt;/span&gt;&lt;a rel="external" style="color: rgb(136, 136, 136); text-decoration: underline; max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important; font-size: 14px;" target="_blank"&gt;&lt;span&gt;https://arxiv.org/pdf/1605.06069.pdf&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是第一篇把 VAE 的思想引入到 dialogue 的 paper。和普通的 VAE 区别在于 dialogue 的 reconstruction 是生成的下一句 utterance，而不是 input 自身。这篇 paper 的前身是 HRED[6]，HRED 的核心思想是，把&amp;nbsp;dialogue 看做是 two-level：dialogue 是 utterance 的组合，utterance 是 words 的组合。HRED 由 3 个 RNN 组成：encode RNN 把每个&amp;nbsp;utterance 变成 real-valued 的向量 u，context RNN 把每个 turn 里的 u 作为输入变成向量 c，最后把 c 交给 deocde RNN 生成下一个 utterance。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;a rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;" title=""&gt;&lt;img src="http://img04.iwgc.cn/mpimg/bba75480479552821cf74cfcad12220a4ca3a5ab"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;VHRED 在 HRED 的基础上每个 turn 里引入一个 latent variable z，z 由 context RNN 的 c 生成。z 的意义比较笼统，sentiment/topic 怎么解释都行。模型的训练技巧如 KL annealing 等大量借鉴了第一篇 paper 的思想，特别要注意训练时的 z 从后验采样（保证 decode 的正确性），测试时再从先验采样（ KL 已经把分布拉近）。实验表明，latent variable 有助于生成更加 diverse 的回复。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&amp;mdash; 05 &amp;mdash;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;其他&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;论文链接:&amp;nbsp;&lt;/span&gt;&lt;a rel="external" style="color: rgb(136, 136, 136); text-decoration: underline; max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important; font-size: 14px;" target="_blank"&gt;&lt;span&gt;https://arxiv.org/pdf/1605.06069.pdf&lt;/span&gt;&lt;/a&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="max-width: 100%; color: rgb(62, 62, 62); font-size: 16px; font-variant-ligatures: normal; orphans: 2; white-space: normal; widows: 2; background-color: rgb(255, 255, 255); box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;第一次将 VAE 引入机器翻译：&lt;br&gt;《Variational neural machine translation》EMNLP 2016&lt;/span&gt;&lt;br&gt;&lt;span&gt;论文链接:&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;a rel="external" style="color: rgb(136, 136, 136); text-decoration: underline; max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important;" target="_blank"&gt;&lt;span&gt;https://arxiv.org/pdf/1605.07869.pdf&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;为了改进 KL 迅速降到 0，提出 convolutional 和 recurrent 结合的 VAE：&lt;br&gt;《A Hybrid Convolutional Variational Autoencoder for Text Generation》&lt;/span&gt;&lt;br&gt;&lt;span&gt;论文链接:&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;a rel="external" style="color: rgb(136, 136, 136); text-decoration: underline; max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important;" target="_blank"&gt;&lt;span&gt;https://arxiv.org/pdf/1702.02390.pdf&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参考文献&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;[1] Semi-supervised sequence learning&lt;br&gt;[2] Skip-thought vectors&lt;br&gt;[3] Distributed representations of sentences and documents&lt;br&gt;[4] Pointer Networks&lt;br&gt;[5] Recurrent models of visual attention&lt;br&gt;[6] Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;作者&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;br&gt;&lt;/section&gt;&lt;br&gt;&lt;/section&gt;&lt;/section&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;苏辉，中科院软件所在读硕士，研究方向为 dialogue system 和文本生成。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;邮箱: suhui15@mails.ucas.ac.cn&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微信: suhui759596&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Please feel free to contact me if you have similar interests.&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;footer style="max-width: 100%; color: rgb(62, 62, 62); font-size: 16px; font-variant-ligatures: normal; orphans: 2; white-space: normal; widows: 2; background-color: rgb(255, 255, 255); box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/footer&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/f9c45c7b507963e896553eef965b4845a22013d5"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
</description>
      <pubDate>Sat, 04 Mar 2017 11:57:36 +0800</pubDate>
    </item>
    <item>
      <title>独家对话百度副总裁王海峰：NLP 的路还很长</title>
      <link>http://www.iwgc.cn/link/a06d89d191e5dfb3dfac2f25954ee57c7f536177</link>
      <description>
&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：虞喵喵&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;编者按：近日，机器之心独家对话百度副总裁王海峰博士，针对时下的 NLP 热点、百度相关的技术情况及其个人经历展开讨论。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;同时，机器之心与百度联合推出技术专栏，探讨百度在自然语言处理领域的研究成果、实践经验与心得。王海峰博士也表示，「理解语言、拥有智能、改变世界，希望我们的专栏能一起朝这个方向努力」。此篇专访作为合作专栏的开篇，希望读者能从中有所获益。专栏后续内容，请持续关注机器之心。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/db615966e02e82e22159e6c1f9996b7bcbac9d27"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;王海峰博士现任百度副总裁，负责百度搜索引擎、手机百度、百度信息流、百度新闻、百度手机浏览器、百度翻译、自然语言处理、语音搜索、图像搜索、互联网数据挖掘、知识图谱、小度机器人等业务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;学术方面，王海峰博士是 ACL（Association for Computational Linguistics）50 多年历史上唯一出任过主席（President）的华人，也是迄今为止最年轻的 ACL 会士（Fellow）。同时，王海峰博士还在多个国际学术组织、国际会议、国际期刊兼任各类职务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此前，我们曾专访过百度自然语言处理部技术负责人吴华、高级总监吴甜，就百度机器翻译技术展开过详细讨论。想要进一步了解百度机器翻译，可移步《独家对话百度 NLP：先解决语义理解，再谈机器翻译取代人类》（可点击文末阅读原文查看）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因涉及方面较多、篇幅较长，根据专访情况将内容分为上、下两篇。《上篇：产品与技术》，谈百度翻译系统、信息流、知识图谱的特点与技术，以及对数据、知识、记忆等解决 NLP 问题关键点的看法；《下篇：过去与现在》，谈王海峰博士自 1993 年来专注研究机器翻译与自然语言处理的过程，以及发展百度自然语言处理相关技术过程中的经历与思考。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;上篇：产品与技术&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：首先请您介绍一下，目前所负责的研究和关注的重点有哪些？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王海峰：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;从整体上，我在百度负责搜索、信息流、手机百度，百度新闻、百度翻译、手机浏览器、自然语言处理、知识图谱等业务，既包括技术和产品，也包括运营等。我们的很多产品如搜索、信息流等，都是技术驱动的，既有工程上的架构、策略，也有很多人工智能技术，如机器学习、数据挖掘、知识图谱、自然语言处理和语音图像技术等等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自然语言处理一直是这些业务中非常重要的基础技术。从做搜索引擎诞生的第一天开始，最基本的 query 分析，网页内容分析，文本匹配等，都需要自然语言处理。近些年来大家都很关注人工智能，随着深度学习的应用，语音图像很多问题已解决得比较好，但自然语言处理仍然面临很多难题，也是现在人工智能的重点和热点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;十几年来，自然语言处理工作在百度一直很重要，并已有很多积累。2010 年初我加入百度后，建立了独立的自然语言处理部门。既致力于支持百度最核心的搜索和广告等业务，也对自然语言处理技术进行了完整布局。不管是偏基础的分析理解、生成，还是各种应用系统，像机器翻译、问答系统、对话系统都在开展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;百度自然语言处理技术的开展，一方面依托百度强大的数据和计算能力，另一方面将自然语言处理技术实际应用于产品也产生了更多数据。每天有非常多的用户使用搜索，而背后又有万亿量级的网页数据，绝大多数都用语言文字表示，蕴含了非常多可以挖掘的、有价值的信息和知识。这些既为自然语言处理的研究提供了非常好的基础，同时提供了非常重要的应用场景。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：您在 AAAI 上的演讲中提到百度会在 query 中用到 BOW、CNN、RNN 等技术，这些不同的技术在语义理解上有什么样的作用？怎么去应用这些技术？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王海峰&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：Query 理解是一个研究了很多年的方向。Query 理解分很多层，比如最基础的中文 query 理解，要做分词、命名实体识别、短语结构分析等等。在应用深度学习之前百度就达到了很好的效果，在这过程中也积累了非常丰富的用户数据。这些数据的积累又为后来应用深度学习提供了基础。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;百度是世界上最早将深度学习技术应用在搜索引擎中的公司。深度学习本身具有很强的表示能力及大数据学习能力，基于百度积累的海量数据以及强大的计算资源，我们设计研发的针对性的新模型，展现出非常好的学习效果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;学习出来的是什么？更多是语义层面的匹配。用户在 query 中用的是一种表达方式，网页中对应的可能是另外一种。在用户的使用过程中，他的点击数据、行为数据隐藏着不同表达方式之间的关联，机器学习、深度学习就能学到这种关联。本质上，还是更好地利用更多的数据学到了更多东西。BOW（Bag-of-Words，词袋）就是对这些词的语义表示做简单的组合，我们用了更复杂的网络如 CNN、RNN，CNN 能更好自动捕捉一些局部结构信息，RNN及其变体在序列建模中更能体现句篇的长距离依赖特性，它们的表示能力、学习能力就会进一步增强。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经网络不是近几年才出现的。20 多年前我读博士的时候，博士论文也用了 RNN，但那时候的数据量要小很多，计算机的计算能力甚至跟现在的手机都没法比。那时只能用很小的数据去跑模型，能跑出来、也有效果，但远远达不到今天的效果。深度学习很多基础理论也并不是近几年才产生的，但是近几年爆发式的在应用中取得了非常多的成果，大数据和强大的计算能力起到了至关重要的支撑作用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：百度在前几年就上线了机器翻译系统，我们知道机器翻译系统可能用到神经网络、基于规则方法、基于实例的方法，还有基于统计的。这些不同的方法，如何在一个翻译系统中结合？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王海峰&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：我们在世界上最早把深度学习应用到大规模线上翻译系统，2015 年 5 月系统正式上线。但上线的同时，并没有把原来的方法直接替换掉。我们发现多个模型融合使用的效果是最好的，因为深度学习有些问题解决的并不好，每一种方法都有它擅长的地方。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在应用深度学习之前，基于统计的、规则的、实例的方法我们都用了。比如规则方法，擅长抽象语言知识并显式地表示出来，比如语法知识、局部的规则等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从一种语言到另一种语言并不是完全依靠规则的，如果有限的语法能覆盖所有语言现象，翻译这件事就会变得非常简单。现实中语言是非常复杂的，表示很灵活，很多时候并不是从语法演绎出来，而是约定俗成就这么说，这时候基于实例的方法就会效果更好、效率更高。就像我们学英语时，很多时候不需要去分析，一听到中文，相应的英文就会脱口而出。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;统计机器翻译方法和神经网络机器翻译有一些相似的优点，同样可以从非常庞大的语料库中学习。因为它基于参数和模型，鲁棒性也更好。统计方法需要从词，到短语，到句子一层一层去做对齐、抽取、重排序等等；而神经网络翻译模型则可以是端到端的系统，用足够的语料去训练，就可以得到不错的结果。从这个角度看，机器翻译入门的门槛变低了，但想做到特别好仍然非常难。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这几种方法，我们现在更多是在结果级进行融合。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：我们现在的知识图谱包含 3 种：实体图谱、意图图谱、关注点图谱，我们为什么要做这些不同的知识图谱，它们的情况和应用是怎么样的？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王海峰&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：做不同的图谱，其实是应用驱动的。基于实体的知识图谱，就是通常意义上的知识图谱。基本节点是实体，实体的属性、实体和实体之间的关系，一个基本的实体知识图谱就是这样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为什么做关注点图谱？因为我们现在在做信息流，用户关注的不一定是实体。它可以是一个实体或者概念，比如关注人工智能、机器翻译；但也可以是一个事件，比如 AAAI 会议在旧金山召开，这不是实体或概念，而是一个事件，在实体图谱里是没有表示这样的事件的节点的。这时就需要关注点图谱。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;意图图谱我们在内部也称为需求图谱，用户对话的过程中提出了一个需求，下一个需求会是什么？比如「阿拉斯加」，用户关注的是城市还是宠物？如果关注宠物那么接下来关注的是喂养、习性还是其它？这既不是一个实体，也不是一个事件关注点。所以每一种知识图谱都是由不同的应用驱动的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：包括 UC、今日头条等等大家都在做信息流，百度在技术上有哪些不同之处？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王海峰&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：信息流从基本原理上讲，一端是对用户的理解，一端是对内容的理解，然后对它们进行匹配。从这个最基本点看，大家都在做类似的事，但我们可以对内容、对用户理解得更好、更充分。这背后既有数据的优势，也有技术的优势。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;数据方面，通过信息流中的用户行为可以分析用户的一些兴趣点，但不限于此，比如用户搜索的 query，明确表达了用户的需求，而这些需求与用户兴趣爱好或者个体属性等是相关的。再比如用户关注了某个贴吧，这是一个非常强的信号，意味着他对这个东西很感兴趣。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以我们做信息流不是孤立的，而是基于百度整体的各种产品，综合起来会对用户有更好的理解。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一方面是技术。百度在人工智能的方方面面都有着非常深厚的技术积累，我们会综合利用各种技术。刚才谈到不少深度学习技术模型在百度产品中已得到很多应用，而在真正的产品应用中，其它各种机器学习方法，比如 SVM 、CRF、GBDT 等也都会用。技术的选型，是基于对应用需求的充分理解及对数据的深入分析进行的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对内容理解这部分，则更多依赖自然语言理解。在搜索中，虽然也用到大量的自然语言处理技术，例如 query 的理解、改写等，但搜索系统的基础是关键词与文本的匹配，使用的理解技术相对简单。而对于信息流推荐系统，则需要先对一篇完整的文章有深度的分析理解，比如打上合适且丰富的标签，需要的分析理解程度会更深。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：目前我们的信息流里也有机器生成的文章，没有语病、读起来非常通顺，但会缺少所谓的「意图」。对于自动写作的意图和创造这件事，您是怎么看的？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王海峰&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：目前有相对做得比较好的一面，也有局限性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;写稿子、甚至写诗时，机器是在做什么？一方面是基于系统里的结构化数据，把数据组织成语句或者文章。比如我们做篮球解说，首先是拿到比赛赛况的实时数据，基于这些数据模拟解说，学习解说员的常用语言，也做一些简单的推理。再比如写诗也是首先明确诗的主题，比如「桃花」还是「月亮」？然后去规划诗的内容。其背后是基于一个大规模诗集训练得到的生成模型，基于确定好的主题和规划的内容，最后生成的很多诗歌的确看上去让人觉得很惊艳。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;机器能做到上面这些，也并不意味着机器具备了真正的深层次的理解。比如桃花开了，每个人的感受不一样，联想的东西也不一样。机器并没有像人一样真正去具备这些情感，更多的是模仿已有数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;机器相对人来说有很多更擅长的能力，但也有一些远不如人的方面。例如，让搜索匹配到合适的网页，但深层的基于背景知识进行深层次的理解及联想则比较困难。还有，比较个大小长短的，对机器来说易如反掌，但要真正去推理则很困难。再比如，机器可以模仿人来写诗，但让机器真正有感而发去搞艺术创作则很难。总结一下，机器很善于匹配、比较、模仿，但要具备像人一样的理解、推理、创造能力，则还有很长的路要走。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：对于这个问题，常识和记忆是解决的方法吗？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王海峰：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;知识很重要，所以我们现在很重视建设知识图谱。知识图谱的建设已经是非常浩大的工作，而如何利用这些知识进行理解、推理，是更复杂的事。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简单的推理相对容易，比如在搜索里询问名人的年龄，这不是匹配可以得到的，因为答案和当下的时间有关。静态的知识是这个名人的生日，有了生日和当前时间，做个减法就能得到年龄。这是一个简单的推理过程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;再说记忆，首先是记什么，然后是怎么用。机器可以记住网页，可以记用户日志，也可以把经过分析提取后结构化的数据和知识记住。记住了这么多，接下来就是利用这些数据和知识，去分析、去推理、去解决实际问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：大家都在研究用无监督学习或少量数据代替大量的标注数据，来达到同样的训练效果，在 NLP 领域我们有相关的研究或者进程吗？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王海峰：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;具体还是要看问题的目标是什么。如果目标是最终的结果，比如在机器翻译中使用双语语料达到源语言输入、经过翻译之后目标语言输出的目的，就可以用端到端深度学习，训练一个模型找到结果。怎么标注数据，甚至是不是真正有对词、对短语的理解就不那么重要。如果目标是做一个 Parser，得到一棵符合人的认知、人对语法理解的句法树，那就一定需要标注数据，在此基础上加入某些特定的无标注数据也可以进一步提升效果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;关于少还是多的问题，可以首先用较少的数据作为原始标注数据训练一个模型，然后设法全自动或半自动的得到更多数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;再举一个更基本的例子，分词。有些任务涉及到理解就需要分成符合语言学定义的词，有些任务就不太关心片段是不是真正的词。有时候做信息检索是一些片段放在一起，分析 query、分析网页时是同样的片段，两个片段只要能匹配上就可以了。这时候分词的粒度是什么、分出的词是不是符合语言学定义就不那么重要了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：现在生成对抗网络比较热门，在计算机视觉领域得到很多应用。那么生成对抗网络可以在 NLP 中应用吗？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王海峰&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：现在在 NLP 领域是有人在研究，但是还没有特别显著的突破。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不止是生成对抗网络，近年来深度学习在语音图像等领域的应用很成功，在 NLP 领域也出现大量研究成果，但是这些研究成果真正对应用带来质的飞跃还不多。语言的复杂性在于，语言不仅仅是表面的字符串，语言的内涵太丰富了，语言实际上是人对整个客观及主观世界的认知、描述和表达。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：那 NLP 领域，接下来需要着重解决的是哪些问题？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王海峰&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：根本问题还是语言的分析理解，语言的生成，以及知识的掌握和运用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;真正要让先进的 NLP 技术实现大规模应用，我认为更重要的是更好地利用大数据，尤其是实际产品应用中产生的数据。数据是动态增长的，用户会不断产生和反馈新数据。在这个动态过程中，技术会越来越完善。积累到一定程度我相信会带来质变。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;下篇：过去与现在&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/5a422d5c2ac3113ee1f7d8cba6c5e2255622d6e0"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：您 1993 年读大四的时候，为什么选择智能翻译作为本科毕业设计题目？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王海峰&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这其中有我个人兴趣的因素，当时我觉得能让计算机来做翻译很神奇，特别有兴趣。另一方面也有机缘的因素，学校把我分配到了李生老师的课题组做毕业设计。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;（注：李生，哈尔滨工业大学教授，自然语言处理领域专家，ACL 终身成就奖得主）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：当时所谓的「智能翻译」是怎样的状况？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王海峰&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：那时统计机器翻译方法刚刚出现，Peter Brown 那篇最经典的文章就是在 1993 年发表的（注 1）。1993 年初我做毕业设计时，还不知道那篇文章，当时最主流的还是基于规则的方法。我做毕业设计用的是基于规则的方法，这些规则都是人工写的。因为我本科是计算机学科，比较擅长把它们用程序、代码实现出来，当时还有外语系同学和我一起工作，专门负责写语言规则。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;（注 1：Peter Brown et al. The Mathematics of Machine Translation: Parameter Estimation, In Computational Linguistics, 1993.）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：您硕士期间，仅用了一年就开发出了当时 863 测评第一的机器翻译系统，能和我们分享一下这段经历吗？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王海峰&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：刚上硕士时，我用的还是基于规则的方法。当时我写了一个很复杂的规则系统，也有小伙伴一起写语言规则、词典。那时候和现在的互联网方法相似，也是不断快速的迭代。我们会不断进行大量测试，发现翻译得不好的地方，就迅速分析解决。需要改代码，我就马上改代码；需要调规则，外语系的小伙伴就立刻调规则。有时候午饭前发现了一个修改的地方，我就直接不去吃午饭。趁小伙伴们去午饭的时间，我的代码就改好了。等他们回来，就可以继续写规则了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那时非常有干劲儿，几乎每天都是实验楼一开门我就进实验室了，一直到晚上熄灯。当然，现在我也仍然每天很早就到办公室（笑）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：您当年的同学们可能已经转到其他的方向，您为什么 20 多年来一直在坚持机器翻译、NLP 的研究？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王海峰&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：可以说很幸运，这些年一直有需要我的专业能力的工作。但也和个人性格有关，我做事比较坚持，选择了做一件事，就要负责到底，持之以恒不断地做得更好。我已经坚持了 20 多年，相信还会坚持下去，因为自然语言处理的路还很长。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：从您开始研究机器翻译，到现在机器翻译都有哪些比较重要的变化？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王海峰&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：之前说过的四种方法，基于规则的、实例的、统计的、神经网络的，每种方法我都经历过，每个方法都是一个很大的变化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从根本上，我认为还是我们所拥有的基础在变。比如数据的基础，我记得刚来百度的时候，那时候特别开心，因为原来我们用统计方法找一些语料非常困难，几十万句对语料就觉得很好了。然而在百度，通过互联网挖掘到的语料要远远比这个数字大，所以百度翻译效果迅速地就上去了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不只是机器翻译，人工智能这些年很多突破都跟数据有关，语音也是，相比早些年，语音数据获取速度在变快，成本则在降低。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：2010 年时您为什么加入百度？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王海峰：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;这个因素就比较多了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先根本的来说是整体的发展趋势。我毕业时是在外企，那时中国的 IT 公司还比较弱小，也不需要那么多特别深入的技术。随着近些年的发展，像百度这样的公司越来越强大，对 NLP 等技术的需求越来越强。到了 2010 年前后，更多的人都开始选择中国自己的企业。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;比较直接的契机是 2009 年 8 月，Robin 在百度世界大会上发布框计算。我对此很关注，在我看来如果要做框计算，背后需要大量的自然语言处理的技术。所以当时就感觉到，百度要做框计算，那就该有我的用武之地了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;通过与百度人的接触，发现除了业务本身以外，大家的价值观、做事的方式等也特别匹配，所以聊过之后我很快就决定过来了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：NLP 在百度是从您开始建设的，这个过程是怎样的？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王海峰&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：确切地说，自然语言处理部这个部门是我建设的，而百度自然语言处理技术的研发则在我加入百度之前就有了，当时大搜索有一个小组在做这个。我来了以后，从十几个人开始，正式成立了自然语言处理部，致力于直接满足搜索等业务需求的同时，也规划了更完整的布局及长期发展路线图。这个路线图中，既包括技术发展路线，也包括团队成员的个人成长路线。团队和业务都增长得很快，第一年团队规模就翻了好几倍，做的事情也多了很多。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：您现在主管包括搜索、手机百度、信息流等业务，在这些业务之间您如何平衡自己的精力？在学者和管理者之间又该怎样平衡？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王海峰&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：团队不是只有我一个人，很多人都很优秀，大家会各自有分工。这些业务在一起也有非常多的协同。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于我来说，更重要的是把整体的目标和方向定好，并组建最适合达成这些目标的团队，然后就是带领大家高效执行及协同。因为我本人是技术背景，在全面带业务的同时，我的确也会在技术角度投入较多，会看技术发展方向和趋势，也会和大家一起去分析解决具体技术问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于一个大型团队，大到你已经不可能认识每一个人，这时候更重要的是建立机制和形成文化。百度的大搜团队，有着原汁原味的简单可依赖的工程师文化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：您最近比较关注的技术点是哪些?&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王海峰&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：更多是希望能把人工智能的能力在各种业务充分发挥出来，比如搜索、信息流、手机百度等等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果人工智能再向前走，真正做到像人一样思考，除了对语言的理解还要有对知识的掌握和对人的理解。这些都要有一定的应用场景支撑，搜索就是可以支撑这件事的最大平台。到目前为止，搜索引擎拥有最多的数据和知识，它的背后是整个互联网，人类的大量知识都蕴含其中。搜索引擎有条件更快地积累需要的数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;机器之心：在此前的采访中您提到过，「希望 NLP 的技术能更好地触及每一个人」。那接下来 NLP 触及每个人的方式，应用也好、呈现方式也好，具体会是怎样的？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;王海峰&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：事实上 NLP 已经在触达几乎每一个人，因为它用在各种产品里。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;百度绝大多数产品背后都有 NLP，2013 年我们做平台化时，NLP 的平台化也是其中一部分。当时 NLP 做了两个平台，一个是 NLPC（NLP Cloud），另一个是机器学习平台 Malloc。这两个平台当时的应用量都排在前几名，NLPC 平台现在每天调用量已经有上千亿。现在不只是百度，很多公司都很重视 NLP，应该说 NLP 已经在触达每一个人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;说到具体产品，获取信息是人的基本需求之一，在没有计算机的时代，甚至人类还没有文字的时代，始终都需要信息。获取信息最重要方式：一种是有明确需求，输入 query 去找信息；另一种是用户没有主动表达需求，但系统能个性化地猜到用户所需并推荐给用户。这就分别对应着搜索和信息流，一个是人找信息，一个是信息找人。这两种都应用了大量的自然语言处理技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人们每天通过搜索或信息流获取知识的同时，机器也可以不断沉淀数据和知识，不断变得更强。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;面向未来看，自然语言对话会成为未来最自然的人机交互方式，这将会改变每个人使用手机及其它设备的方式，会更加直接地触达每个人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;「百度 NLP」专栏主要关注百度自然语言处理技术发展进程，报道前沿资讯和动态，分享技术专家的行业解读与深度思考。&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/3f2d3515689f0be18f35b09e6f38d1923efda2b9"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&amp;copy;本文为机器之心原创，&lt;strong&gt;&lt;span&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Fri, 03 Mar 2017 12:33:17 +0800</pubDate>
    </item>
    <item>
      <title>重磅 | Science论文详解击败德扑职业玩家的DeepStack，Nature探讨其与Libratus的优劣</title>
      <link>http://www.iwgc.cn/link/b3fc63ffab02d0d47eecdf235fad41b48e33f4eb</link>
      <description>
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;选自Nature、Science&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：微胖、吴攀、蒋思源、曹瑞&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;在顶级职业德州扑克比赛上，人类已经败北，这可算得上今年人工智能领域的第一个大事件，参看机器之心的报道《&lt;a data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650722808&amp;amp;idx=1&amp;amp;sn=4832e766e6c4c7f8d91137bb2515c298&amp;amp;chksm=871b1586b06c9c90f00c61b1ada0908b4b801ae280f191cc7a48146f15634f2905f3adec6330&amp;amp;scene=21#wechat_redirect" href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650722808&amp;amp;idx=1&amp;amp;sn=4832e766e6c4c7f8d91137bb2515c298&amp;amp;chksm=871b1586b06c9c90f00c61b1ada0908b4b801ae280f191cc7a48146f15634f2905f3adec6330&amp;amp;scene=21#wechat_redirect" target="_blank"&gt;重磅 | 德扑人机大战收官，Libratus 击败世界顶尖扑克选手&lt;/a&gt;》和《&lt;a data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650722163&amp;amp;idx=4&amp;amp;sn=762fdec7dc1e618d7a0ad5a8555e052d&amp;amp;chksm=871b0b0db06c821b976cdad504f3ff91bba018a8eb6a7af2a58b899cf17479da40554eb1f136&amp;amp;scene=21#wechat_redirect" href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650722163&amp;amp;idx=4&amp;amp;sn=762fdec7dc1e618d7a0ad5a8555e052d&amp;amp;chksm=871b0b0db06c821b976cdad504f3ff91bba018a8eb6a7af2a58b899cf17479da40554eb1f136&amp;amp;scene=21#wechat_redirect" target="_blank"&gt;学界 | 新论文提出玩扑克人工智能 DeepStack：已达职业玩家水平&lt;/a&gt;》。今天早些时候，关于 DeepStack 的正式论文终于在顶级刊物 Science 的网站上发布，同时也得到了 Nature、科学美国人和 IEEE Spectrum 等众多科学和科技平台的关注和传播。机器之心在此编译了 Nature 的相关介绍文章，并在文后附上了 Science 上最新版论文的摘要介绍。原论文可点击阅读原文查阅。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在无限制德州扑克上，人类顶级职业玩家已被人工智能 bot 击败。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;德州扑克这种复杂的扑克游戏已经被人工智能（AI）掌握。而且这个游戏还不是被征服了一次&amp;mdash;&amp;mdash;两个不同的研究团队所开发的 bot 都在一对一德州扑克比赛上完成了击败人类的壮举。真希望看到它们互相对战一场！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先完成这一胜利的 Bot 是阿尔伯塔大学的计算机科学家开发的 DeepStack，该成果的成功背后还有来自捷克的查尔斯大学和布拉格捷克理工大学的帮助。一个月后，卡内基梅隆大学所开发的 Libratus 又再次在与人类的比赛中取得了胜利。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;过去十年来，这些团队一直在互相激励打造更好的 Bot，现在 DeepStack 背后的团队将其人工智能的细节正式发表到了 Science 上。Nature 在这篇文章中对这两个人工智能的原理进行了介绍，并探讨了这对在线赌博的意义以及人工智能还有什么尚未征服的目标。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;为什么人工智能研究者应该关心扑克？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人工智能已经掌握了好几种棋盘游戏，包括国际象棋以及战略极其复杂的围棋。不过，扑克不同于这类游戏的关键之处在于其增加了复杂性：玩家必须在信息不完全的前提下，算出对手的策略。他们必须考虑对手手中会有什么牌以及对手会如何根据之前下的注猜测自己。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种「不完美信息（imperfect information）」类博弈能反应真实生活我们的问题解决场景，诸如拍卖以及金融谈判，扑克也成为这些场景的人工智能测试平台。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;算法已经破解了更加简单的扑克形式：2015 年，该阿尔伯塔大学团队就已经解决了有限双人扑克难题。DeepStack 和 Libratus 玩的仍然是双人博弈，但却是无限制规则，对于人工智能来说，这个挑战会困难得多。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;人类与人工智能交战情况如何？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;去年 11 月初的四周里，DeepStack 击败了 11 位职业选手中的 10 位，统计上，赢的优势很大，与每位对手玩了 3000 手。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然后，今年 1 月份，Libratus 击败了四个更加优秀的职业选手（专家级扑克玩家），总体交收 12 万多手。计算机最后赢得约为 180 万美元的筹码。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;算法背后的数学原理是什么？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;博弈论（game theory）。不论对手选择哪个策略，这两个人工智能系统都旨在搜寻一个能保证不会产生损失的策略。因为一对一扑克是零和游戏，这也就意味着一个博弈方的损失就是其对手的获利，博弈论证明了这种最优决策是经常存在的。而人类玩家可能会利用弱对手的错误获得更大的收益，但使用这种策略对人工智能不会奏效，它仅仅只是为了胜利而博弈。这也就意味着它并不会被对手故意夸张的行为吓住。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以前的扑克游戏的算法一般都试图提前制定出策略，通过计算大规模的「博弈树」而找到游戏可能展开的不同方式及其所有解决方案。但是这种算法所寻找到的展开可能性数量是十分巨大的，而要将这 10^160 次方可能性进行映射是不可能的。所以研究者决定使用更少的可能性解决问题。在一个博弈中，算法会将现场的情况与先前的计算情况相比较。然后算法会找到最接近的一个并从表中「转换」相应的动作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，现在 DeepStack 和 Libratus 都找到了实时计算解决方案的方法，就如同下象棋和围棋的电脑一般。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;如何比较这两个人工智能？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DeepStack 会在游戏的每一个节点重新计算一小段可能性的树，而不是提前算出整个博弈树。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;开发者利用深度学习创造了这一方法，这种技术利用了一种受到大脑启发的名叫神经网络的架构（正是在这种架构的帮助下，计算机才打败了一位世界上最顶尖的围棋棋手）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自己玩了 1100 万种游戏场景，并且在每一个场景中进行学习，DeepStack 在游戏中已经获得了一种在某个给定点获胜可能性的「直觉」。这让它可以在相对较短的时间内（大约 5 秒）进行更少的可能性计算，并作出实时决策。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Libratus 的团队目前还没有公布它们的方法，所以这一程序是如何运作的还尚不清楚。但我们早知道的是，它使用了预先计算可能性和「转化」的方法，虽然它在游戏出现更多信息的时候会改进策略。但另一方面，随着可能的结果范围变得越来越窄，算法也可以实时计算出解决方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Libratus 也有一个学习元素。其开发者为其加入了一个自我提升的模块，其可以自动分析该 Bot 的玩牌策略，从而可以了解一个对手会如何利用它的缺点。然后它们使用这些信息来永久性地修补这些漏洞。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这两种方法需要明显不同的计算能力：DeepStack 的训练使用了 175 个 core years&amp;mdash;&amp;mdash;相当于运行一个处理单元 150 年或运行几百台计算机几个月。而在比赛过程中，它可以在单一一台笔记本上工作。而 Libratus 则相反，在比赛之前和比赛过程中都使用了一台超级计算机，相当于大约 2900 个 core years。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;它们会 bluff 吗？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;会。人们时常以为唬牌是人类技能，但是，对一台计算机来说，读不读懂对手没啥关系，它们要做的就是处理博弈背后的数学原理。bluff 主要是 一种策略，确保玩家的下注模式不会让对手发现他们手里的牌。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;好吧，哪个结果更亮眼？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;主要看你问谁了。专家可能会在方法的错综复杂之处含糊其辞，但是，总体上这两个人工智能系统都已经玩了足够多的牌，取得了统计学上显著的胜利&amp;mdash;&amp;mdash;而且对手都是职业玩家。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Libratus 玩了更多手，但是，DeepStack 没这个必要，因为它的团队使用了成熟的统计方法，这个方法能够从较少的博弈中证实比赛结果。较之 DeepStack，Libratus 击败了优秀得多的职业选手，不过 平均说来，DeepStack 赢得的优势更大。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;两个人工智能系统会一较高下吗？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;或许吧。比较棘手的一点就是计算能力存在较大差别，因此会影响游戏速度。我们很难找到双方都赞同的游戏规则。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;阿尔伯塔大学计算机科学家 Michael Bowling、DeepStack 的研发者之一说他的团队打算与 Libratus 比赛。不过，Libratus 的研发、 CMU 的 Tuomas Sandholm 说，他们想先看看 DeepStack 击败 Baby Tartanian 8&amp;mdash;&amp;mdash;他们团队较早的人工智能系统，能力也弱一些。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bowling 强调，需要注意的是：胜者或许并不意味着它是更好的机器人程序。虽然大家都在尽力让比赛完美，但是，最接近完美的策略并不总是会在正面交锋中出现。一方可能会偶然击中对方的策略漏洞，但是，这并不意味着整体策略上也有更多或更大的漏洞。除非一个团队以明显优势胜，Bowling 说，「我的感觉是它不会像人类期望的那样博闻强识。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;在线扑克是不是没得玩儿了？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不会。虽然顶级玩家已经开始训练对抗机器，但是，许多在线扑克赌场仍然禁止玩家在比赛中使用机器人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;既然计算机又实现了一个征服人类的里程碑，接下来又该征服啥了？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;还有几座高山等着我们呢。还有许多没被征服的游戏，比如桥牌，它的规则复杂多了，因此目标也不那么明确了。&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来，两个团队自然是要征服多人扑克。这意味着大家几乎要从头开始，因为零和博弈理论并不适用它们：在三人扑克游戏中，对手的一个烂招会间接阻碍另一个玩家，并非总是对对方有利。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是，深度学习的直觉或许能帮助我们找到解决方法，即使在博弈理论并不适用的场景中，Bowling 说。他的团队率先试着将类似的办法应用到三人版的有限德扑中，他介绍说，结果发现，效果好得让人惊讶。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一个挑战是训练人工智能玩游戏，但并不告诉它们游戏规则，而是随着游戏的进行，让系统自己发现规则。这一场景更加真实反映出真实世界的问题解决情况。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;终极测试会是研究出不完全信息算法，使其能利用不完全信息来帮助解决杂乱无章的真实问题难题，比如金融和网络安全。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以下为发表在&lt;span&gt;Science&lt;/span&gt;上的论文的摘要介绍&lt;span&gt;：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/23e337149a7242a2bbd5836202029a577f9e4a9c"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;摘要&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;近些年来，人工智能领域出现了很多突破，其中游戏往往被用作重要的里程碑。过去实现那些成功的游戏的一个常见的特征是它们都具有完美信息（perfect information）的性质。扑克是一个典型的不完美信息（imperfect information）游戏，而且其一直以来都是人工智能领域内的一个难题。在这篇论文中，我们介绍了 DeepStack，这是一种用于扑克这样的不完美信息环境的新算法。它结合了回归推理（recursive reasoning）来处理信息不对称性，还结合了分解（decomposition）来将计算集中到相关的决策上，以及一种形式的直觉（intuition）&amp;mdash;&amp;mdash;该直觉可以使用深度学习进行自我玩牌而自动学习到。在一项涉及到 44000 手扑克的研究中，DeepStack 在一对一无限制德州扑克（heads-up no-limit Texas hold'em）上击败了职业扑克玩家。这种方法在理论上是可靠的，并且在实践中也能得出比之前的方法更难以被利用的策略。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;论文提纲：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DeepStack&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1、持续解决（Continual re-solving）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2、通过直觉实现有限深度的前瞻（Limited depth lookahead via intuition）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3、合理推理（Sound reasoning）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4、解析前瞻树（Sparse lookahead trees）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;5、与完美信息游戏中启发式搜索的关系（Relationship to heuristic search in perfect information games）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;6、与基于抽象的方法的关系（Relationship to abstraction-based approaches）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度反事实价值网络（Deep counterfactual value networks）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1、架构（Architecture）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2、训练（trainning）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;评估 DeepStack&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1、开发度（Exploitability）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;讨论&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DeepStack 是一种可用于一个很大类别的序列不完美信息博弈（sequential imperfect information games）的通用算法。为了明晰这个算法，我们将会在 HUNL 游戏中描述其运算。一个扑克游戏的状态可以被分成玩家的私有信息（两张牌面朝下的手牌）和公共状态（包括牌面朝上的牌和玩家采取的下注动作序列）。游戏中的公开状态的可能序列构成一个公开树（public tree），其中每一个公开状态都有一个相关的公开子树（public subtree）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/ade962ce9e30bc837518ad714ac4ff3411f3de75"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 1：HUNL 中公开树的一部分。红色和天蓝色的边表示玩家动作。绿色边表示公开的公共牌。带有筹码的叶节点表示游戏结束，其中，如果一个玩家根据之前的动作和玩家手牌的联合分布而弃牌或做出决定，那么收益就可能是固定的。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/ff19a9288f9bdd4aa7e9920d3e7ee6ee42f329a4"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 2：DeepStack 架构概览。（A）DeepStack 在公共树（public tree）中的推理，该树总是会为一个公开状态（public state）中其持有的所有牌得出动作概率（action probabilities）。它在玩牌时维持着两个向量：它自己的范围和其对手的反事实价值（counterfactual values）。随着该游戏的进行，它自己的范围会在其采取了一个动作之后使用其所计算出的动作概率来通过贝叶斯规则进行更新。对手反事实价值会如在「Continual re-solving」中所讨论的那样被更新。为了在其必须采取动作时计算出动作概率，它会使用其范围和对手反事实价值来执行一个 re-solve。为了使该 re-solve 可以实现，它限制了玩家的可用动作，且前瞻预测也被限制到了这一轮的结束。在 re-solve 期间，其会使用 DeepStack 所学习到的评估函数来近似用于其前瞻之外的公开状态的反事实价值。（B）该评估函数被表示成了一个神经网络，该网络以当前迭代的公开状态和范围作为输入，然后输出两个玩家的反事实价值。（C）在比赛之前，该神经网络通过生成随机扑克情景（底池大小、台面上的牌和范围）来进行训练，然后解决它们以生成训练样本。完整的伪代码见算法 S1。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/07a63ca55d698275fda381e3f8a8e53668882029"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;算法 S1：Depth-limited continual re-solving&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/95aa4b589f070d96a6bb5cabbb562b626eaf41b3"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 3:深度反事实价值网络（Deep counterfactual value networks）。该网络的输入包括底池大小、公共牌、手牌范围（player ranges），这些首先会被处理成 hand clusters。来自这 7 层全连接隐藏层的输出还要经过后处理（post-processed），从而保证该值（values）满足零和约束（zero-sum constraint），然后这些值又会回过来被映射为 hand counterfactual values。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/b8c904e6a888f5af71015bdca7c74335fa84b6e3"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 4：职业扑克玩家与 DeepStack 对战的表现。以 95% 的置信区间用 AIVAT 估计的表现。下面的柱状图给出了参与者完成的比赛的数量。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;原文链接：http://www.nature.com/news/how-rival-bots-battled-their-way-to-poker-supremacy-1.21580&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&amp;copy;本文为机器之心编译，&lt;strong&gt;&lt;span&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Fri, 03 Mar 2017 12:33:17 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 百度新论文提出Gram-CTC：单系统语音转录达到最高水平</title>
      <link>http://www.iwgc.cn/link/335fb5d2e270a15080376e8d21c8efa39cde9da8</link>
      <description>
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;选自Baidu Research&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李泽南、曹瑞&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;近日，百度硅谷 AI 实验室刘海容、李先刚等人发表论文提出了一种新的语音识别模型 Gram-CTC，将语音识别的速度和准确率大大提高。据研究人员介绍，这一新方法可以显著减少模型训练与推理时间。在相同任务中，新模型的表现在单一模型对比中超过了微软等公司的研究。点击阅读原文下载此论文。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在百度的研究发表之前，微软曾在 2016 年 10 月宣布他们的多系统方法在 2000 小时的口语数据库 switchboard 上测得 5.9% 的误差率。后者被认为是对多系统方法潜力的探索，而百度的此次提出的单系统方法则更易于实用化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CTC 端到端学习使用一个算法将输入和输出联系起来，通常采用深层神经网络。这种方式推崇更少的人工特征设计，更少的中间单元。端到端学习的系统包括：基于 CTC 的语音识别，基于注意机制的机器翻译，目前业界的很多产品中都能找到 CTC 的身影。（参考文章：&lt;a data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720285&amp;amp;idx=2&amp;amp;sn=3308e3bcea1cdeb2eaee13c241081ad6&amp;amp;chksm=871b0c63b06c85751f06672a9d714f1b414f959e9129120d5d00532b72adeaf7cb9577a0989b&amp;amp;scene=21#wechat_redirect" href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720285&amp;amp;idx=2&amp;amp;sn=3308e3bcea1cdeb2eaee13c241081ad6&amp;amp;chksm=871b0c63b06c85751f06672a9d714f1b414f959e9129120d5d00532b72adeaf7cb9577a0989b&amp;amp;scene=21#wechat_redirect" target="_blank"&gt;专访｜百度语音识别技术负责人李先刚：如何利用Deep CNN大幅提升识别准确率？&lt;/a&gt;）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/9017f42e6def526b075300c0e1411db13722c71b"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;在 Fisher-Switchboard 基准测试上，百度的研究者使用域内数据和此前已发表过的结果进行了比较，表中只列出了单一模型的结果。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在多种语言的语音识别中，Deep Speech 利用 CTC 损失呈现出一种端到端的神经架构。百度展示的 Gram CTC 能够扩展 CTC 损失函数，让它自动发现并预测字段，而不是字符。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;使用 Gram CTC 的模型可以用单一模型在 Fisher-Swbd 基准上实现超过以往任何其他模型的表现，这说明使用 Gram-CTC 端到端的学习优于基于上下文和相关音素的系统，使用相同的训练数据也能让训练速度加快两倍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;针对同一段音频，思考下文中可能出现的转录，它们对于语音转录来说都是可行的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;recognize speech using common sense&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;wreck a nice beach you sing calm incense&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CTC 一次只能预测一个字符，假设输入的对象之间相互独立。为了让两种转录相似，CTC 必须要选择两个字符来补全空白，如下图。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/76de87a34ada7d39bc441e41519e32d193b9258b"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;只使用 Option 2 的候选填补空白，我们即可达成第一个目标，即「recognize speech &amp;hellip;」；使用 Option 1 中的候选，我们会得到「wreck a nice beach &amp;hellip;」。另外，从 Option 1 和 2 中共同选择我们会得到很多种无意义的语句。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;字段是介于字符和单词之间的单元，如「ing」，「euax」，「sch」等（包含但不限于词缀），虽然相同的字段可能会因为不同单词或上下文情况出现不同的读音，但字段在英语中通常倾向于同一个发音。在我们的例子中，我们也可以使用字段进行预测：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/36a69651f52cb8342527ce2c106880818c13c713"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;正如上图所示，这种方法可以大量减少无意义的预测组合，此外，预测词缀还具有以下优点：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;更易建模，因为字段比单个字母相对发音更进一步。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;因为字段相对于字母反映了更长一段声音，这种方法可以大大减少算法预测的步数。我们的模型减少了一半的时间步，训练和推理速度大大加快。在同样的硬件环境下，训练 2000 小时数据集的时间从 9 小时缩短至 5 小时。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;该模型可以学会识别相同发音的常见拼写。在上面的例子中，「alm」和「omm」有非常接近的发音。在 CTC 中，这种识别很难；但在 Gram-CTC 中容易很多。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;论文：Gram-CTC：用于序列标注的自动单元选择和目标分解（Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence Labelling）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/2db51e1393ffbfdbadb5308912325dfbf7190f2a"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：大多数已有的序列标注模型（sequence labelling model）都依赖一种目标序列到基本单元序列的固定分解。而这些方法都有两个主要的缺点：1）基本单元的集合是固定的，比如语音识别中的单词、字符与音素集合。2）目标序列的分解是固定的。这些缺点通常会导致建模序列时的次优表现。在本论文中，我们拓展了流行的 CTC 损失标准来减缓这些限制，并提出了一种名为 Gram-CTC 的新型损失函数。在保留 CTC 的优势的同时，Gram-CTC 能自动地学习基础单元（gram）的最佳集合，也能自动学习分解目标序列的最合适的方式。不像 CTC，Gram-CTC 使得该模型能在每个时间步骤上输出字符的变量值，使得模型能捕捉到更长期的依存关系（dependency），并提升计算效率。我们证明此次提出的 Gram-CTC 在多种数据规模的大型词汇语音识别任务上，既提升了 CTC 的表现又改进了 CTC 的效率。而且我们使用 Gram-CTC 也在标准的语音基准上得到了超越当前最佳的结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;原文链接：http://research.baidu.com/gram-ctc-speech-recognition-word-piece-targets/&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&amp;copy;本文为机器之心编译，&lt;strong&gt;&lt;span&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Fri, 03 Mar 2017 12:33:17 +0800</pubDate>
    </item>
    <item>
      <title>开源 | 谷歌开源 Python Fire：可自动生成命令行接口</title>
      <link>http://www.iwgc.cn/link/efff77d4ecf70035c254349e84916161825d863f</link>
      <description>
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;选自Google Open Source&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：黄小天、蒋思源&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今天我们很高兴地宣布 Python Fire 开源。Python Fire 可从任何 Python 代码生成命令行接口（command line interfaces (CLIs)），简单地调用任意 Python 程序中的 Fire 函数以将那个程序自动地转化为 CLI。该库可通过 `pip install fire` 从 pypi 获取，也可参考 Github 上的资源。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;项目地址：https://github.com/google/python-fire&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Python Fire 自动把你的代码转化成一个 CLI，而不需要你做任何额外工作。你不必定义参数、设置帮助信息或写一个主函数定义代码如何运行。相反地，你只需从主模块调用 `Fire` 函数，Python Fire 会接管剩下的一切。它使用检索将任何 Python 对象（无论是类、对象、字典、函数，甚至是整个模块）转化为命令行接口，并输出标注标签和文档，并且指令行界面会随着编码的变化保持实时更新。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了说明这一点，让我们看一个简单的例子。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;#!/usr/bin/env python&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;import fire&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;class Example(object):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp;def hello(self, name='world'):&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp;"""Says hello to the specified name."""&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp;return 'Hello {name}!'.format(name=name)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;def main():&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp;fire.Fire(Example)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;if __name__ == '__main__':&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp;main()&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当 Fire 函数运行时，我们的命令被执行。仅仅通过调用 Fire，现在我们可以把样本类当作命令行工具来使用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;$ ./example.py hello&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Hello world!&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;$ ./example.py hello David&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Hello David!&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;$ ./example.py hello --name=Google&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Hello Google!&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当然你可以继续像使用 Python 普通库那样使用这个模块，从而你可以使用跟 Bash 和 Python 完全一样的的代码。如果你正在写一个 python 库，那么在试验这个模块的时候你就不需要更新你的主要方法（method）或客户端。仅仅只需要以命令行的方式运行一部分你正在试验的库。即使这些库改变了，该命令行工具仍然保持更新。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 Google，工程师们使用 Python Fire 从 python 库生成命令行工具。因为我们有使用 Python 图像库（Python Imaging Library/PIL）和 Fire 建立的图像处理工具。在谷歌大脑，我们使用由 Fire 构建的实验管理工具，该工具能够和 Python 或 Bash 同等程度地管理实验。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;每个 Fire CLI 都带有交互模式。运行 CLI 时使用「-interactive」旗标和命令行以及其他已定义的变量来登录 IPython REPL。请务必查看 Python Fire 的文档，从而了解 Fire 更多实用的特征。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为 Python Fire 十分简单、普遍和强大，我希望能为你的项目提供一个十分有效的库。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;原文链接：http://opensource.googleblog.com/2017/03/python-fire-command-line.html&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&amp;copy;本文为机器之心编译，&lt;strong&gt;&lt;span&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Fri, 03 Mar 2017 12:33:17 +0800</pubDate>
    </item>
    <item>
      <title>学界 | Yoshua Bengio团队连发三篇论文：提出三种生成对抗网络</title>
      <link>http://www.iwgc.cn/link/1e905d85a9e5b406f5bfbf14096e6d6cc90c365b</link>
      <description>
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;机器之心整理&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;毫无疑问，生成对抗网络（GAN）是人工智能研究领域近段时间以来最大的热门之一，前段时间的 WGAN 也引起了研究界的很大关注。著名学者 Yoshua Bengio 所在的团队近日也分享了他们在 GAN 研究上的贡献，并在 2 月下旬一连在 arXiv 上发布了三篇相关论文（其中一篇为论文修正），提出了三种不同的 GAN&amp;mdash;&amp;mdash;边界寻找生成对抗网络（BS-GAN）、最大似然增强的离散生成对抗网络（MaliGAN）和模式正则化的生成对抗网络（Regularized-GAN）。机器之心在这里对这三篇论文进行了摘要介绍，获取论文原文请访问对应链接。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文 1：边界寻找生成对抗网络（Boundary-Seeking Generative Adversarial Networks）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文链接：https://arxiv.org/abs/1702.08431&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/01fbaf2f4edf8691d6e8e1f1cb1d976a9da977fb"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;摘要&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们介绍了一种全新的用于训练生成对抗网络的方法&amp;mdash;&amp;mdash;我们训练一个生成器来匹配一个目标分布，该分布会收敛到处于完美鉴别器的极限的数据分布。这个目标可被视为训练一个生成器来在每次更新的训练中产生在当前鉴别器的决策边界（decision boundary）之上的样本，我们把使用这种算法训练的 GAN 称为边界寻找 GAN（BS-GAN：boundary-seeking GAN）。这种方法可被用于训练带有离散输出的生成器&amp;mdash;&amp;mdash;该生成器可以输出一个参数条件分布（parametric conditional distribution）。我们使用离散图像数据表明了我们提出的算法的有效性。和我们提出的算法相反，我们观察到最近提出的用于重新参数化（re-parametrizing）离散变量的 Gumbel-Softmax 技术不能用于训练带有离散数据的 GAN。最后，我们注意到我们提出的边界寻找算法甚至可以用于连续变量，而且我们通过两个被广泛使用的图像数据集 SVHN 和 CelebA 证明了其有效性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/1c03a33a610b04f08b9064ba482a6574fc50dc7d"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 1：传统的 GAN 和这里提出的 BGAN 在 1 维样本上的定性比较。鉴别器 D 固定于一个带有 0.8 的系数的 logistic 回归。注意传统 GAN 的生成器目标的最大值是正无穷大（红色曲线），而这里提出的 BGAN 的最小值位于决策边界（蓝色曲线）。和这里提出的 BGAN 不同，传统 GAN 的学习梯度（红色虚线）会将生成的样本推到真实样本之外。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/663e197ac557c2fd6d34e704112006d4862c56b1"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 2：左图：用离散 MNIST 数据训练的边界寻找 GAN（BGAN）的生成器所生成的随机样本。这里展示的是该生成器的条件分布的伯努利中心（Bernoulli centers）。样本表现出了很高的多样性，生成的手写数字具有很高的真实感。右图：人工选择过的边界寻找 GAN 的生成器所生成的数字样本。进行人工选择是为了证明这些生成样本的多样性。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/e60d92c6d905c28aa52040e49caeb911836ede8f"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 3：左图：真实的 16 色（4 比特）量化的 CelebA 图像，已被下采样到 32&amp;times;32。右图：由在该量化的 CelebA 上进行了 30 epoch 训练的边界寻找 GAN 的生成器所产生的样本。样本的真实度并不完美，但和原样本具有很好的相似度，我们也观察到了很大的多样性。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/627b989ba9ad74830509a401e3b996458d765937"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 4：当训练一个边界寻找 GAN（BGAN）时，随学习率变化的生成器成本（generator cost）的演变。在所有的学习率上，我们可以观察到生成器成本是一条光滑的曲线，这说明了这里提出的 BGAN 的学习稳定性。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文 2：最大似然增强的离散生成对抗网络（Maximum-Likelihood Augmented Discrete Generative Adversarial Networks）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文链接：https://arxiv.org/abs/1702.07983v1&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img05.iwgc.cn/mpimg/978cf86ebd49598760a29b7b76f86735769e02bc"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;摘要&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管生成对抗网络（GAN）在获取连续分布上已经取得了成功，但其在离散背景（比如自然语言任务）上的应用却相当有限。主要的原因是通过离散变量的反向传播很困难，而且 GAN 训练目标还具有固有的不稳定性。为了解决这些问题，我们提出了最大似然增强的离散生成对抗网络（Maximum-Likelihood Augmented Discrete Generative Adversarial Networks）。我们没有直接优化该 GAN 目标，而是使用遵循对数似然的对应的输出而推导出了一种全新的且低方差的目标。和原来的相比，事实证明这种新的目标在理论上是一致的，且在实践中也是有益的。在多种离散数据集上的实验结果表明了我们提出的方法的有效性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img03.iwgc.cn/mpimg/9b918872fbfc9efd00aee106de762988c6fd1dea"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;算法 1：MaliGAN&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/525bf11d1354ccad5ccf046499edbe577f120bba"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;算法 2：带有混合 MLE 训练的 Sequential MaliGAN&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文 3：模式正则化的生成对抗网络（Mode Regularized Generative Adversarial Networks）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文链接：https://arxiv.org/abs/1612.02136&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（注：本论文为修改版论文，其第一版提交于 2016 年 12 月 7 日，本文为最新的第四版。本论文已被 ICLR 2017 接受。）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/6fdcd533a438cea184a3103520ff8c30085e7682"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;摘要&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管生成对抗网络（GAN）在许多不同的生成任务上都实现了当前最佳的结果，但它们被认为是高度不稳定的且容易出错。我们认为 GAN 的这些糟糕行为是由于在高维空间中训练过的鉴别器的非常特定的函数形状，这可以轻松使得训练陷入困境或将概率质量（probability mass）推向错误的方向，导致集中度（concentration）比其数据生成分布（data generating distribution）更高。我们介绍了几种对其目标进行正则化的方法，它们可以极大地稳定 GAN 模型的训练。我们还表明我们的正则化器（regularizer）可以在训练的早期阶段帮助在数据生成分布的模式上实现公平的概率质量分布，从而能为该模式缺失问题（missing modes problem）提供一种统一的解决方案。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/88fe9137af726c76ea87fb2c1d6141233aa7a076"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 4：左边 1-5 图：用于 MNIST 生成的不同超参数。我们的 Regularized GAN 中的&amp;lambda;1 和 &amp;lambda;2 的值列在对应的样本下方。右边 6-7 图：对 GAN 和 Regularized GAN 进行网格搜索而得到的最佳样本。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/03075e30c8d081ba78b7d449fadad01e284d4fea"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 6：不同的生成模型所生成的样本。对于这里每一个比较的模型，我们直接从它们对应的论文和代码库里面取了 10 个比较好的样本。注意 MDGAN 在全局一致性上更好，而且在局部上也有很好的纹理锐利度。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="http://img04.iwgc.cn/mpimg/596d20ea393c554b4a4ed2e62310ae0b5cc067d0"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 7：Regularized-GAN 和 MDGAN 所生成的侧脸样本&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&amp;copy;本文为机器之心编译，&lt;strong&gt;&lt;span&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@jiqizhixin.com&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
      <pubDate>Fri, 03 Mar 2017 12:33:17 +0800</pubDate>
    </item>
  </channel>
</rss>
