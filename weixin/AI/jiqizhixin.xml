<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>机器之心</title>
    <link>http://www.iwgc.cn/list/670</link>
    <description>人与科技的美好关系</description>
    <item>
      <title>AI Talk | 小度战平人类最强大脑后，我们和吴恩达聊了聊</title>
      <link>http://www.iwgc.cn/link/4347269</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编辑部&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;继上周五百度的小度机器人在《最强大脑》节目中的&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650722118&amp;amp;idx=1&amp;amp;sn=1934dfd96eedb777b506a7f9fca1c44a&amp;amp;chksm=871b0b38b06c822e4419d7263ece28aa7ad683c4478eb908397c2fae2aa7505529f9708c72d4&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650722118&amp;amp;idx=1&amp;amp;sn=1934dfd96eedb777b506a7f9fca1c44a&amp;amp;chksm=871b0b38b06c822e4419d7263ece28aa7ad683c4478eb908397c2fae2aa7505529f9708c72d4&amp;amp;scene=21#wechat_redirect"&gt;跨年龄人脸识别任务&lt;/a&gt;中击败了人类顶级选手后，周五晚上，小度再次在声纹识别任务上迎战了人类最强大脑，并最终以 1:1 的成绩和人类打成了平手。节目之后，机器之心对百度首席科学家吴恩达进行了独家专访，请他谈论了小度在这场比赛中所用到的技术、百度的人工智能研究和团队以及他对中国和世界人工智能研究的思考。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe class="video_iframe" data-vidtype="1" allowfullscreen="" frameborder="0" height="417" width="556" data-src="https://v.qq.com/iframe/preview.html?vid=e0366l4b003&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0"&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作为机器之心新栏目 AI Talk 的一部分，我们对这次视频专访的内容进行了剪辑，完整采访可见下面文字整理版本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;关于小度和声音/语音技术&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;机器之心：简单介绍一下，小度在本期节目中使用到的识别技术及其原理？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;吴恩达&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：在比赛中，小度使用了 2 种前沿的声纹识别算法，为了识别出某个人，会把两种算法的结果结合在一起。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中一种方法是基于卷积神经网络，这是一种端对端的方式。卷积网络把输入切成声音片段，然后尝试识别这些片段是不是来自同一个人。这个神经网络是在 2 万多人的大约 5000 多小时的音频数据上训练出来的。这是一个很大的音频数据集，它使得神经网络变得相当准确。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的第二个系统也有神经网络，但结构不同。第二个系统采用声音片段作为输入，神经网络输出 5000 维表征语音，并基于此进行统计建模。通过统计建模后的结果，抽离出说话人相关的信息，选择出 500 个特征来表征说话人的属性，而不是说话的内容。随后，使用这 500 个特征匹配两个说话人，并判断出是否是同一个说话人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最终，我们用这两个神经网络让它们投票，从而做出最终决策。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;机器之心：看起来语音识别要比语义识别更简单一些，你认为什么时候可以实现人类水平的机器语义识别，从而让人类可以和机器顺畅地交流？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;吴恩达&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：让计算机以人类的水平来完全理解自然语言，这还需要很长的时间，可能需要几年，也可能是几十年，我们难以确定。但我们可以预见在一些非常垂直的应用领域，比如询问天气、叫外卖、拿快递，或者推送今天的新闻这样的基础问题。这些方向非常的垂直，我们看到了自然语言处理在这些方向上的快速发展。以百度的度秘为例，你已经能与这个机器人进行交流，它可以给你合理的答案。在垂直领域它可以做得很好，研究人员有时间考虑到所有的可能性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我认同你所说的语音识别在过去两年已经取得了巨大的发展。事实上，语音识别如今很准确，使得更多用户用它作为文本输入。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;几个月前，斯坦福大学联合百度与华盛顿大学做了一项研究，表明目前的手机端语音输入要比键盘输入快 3 倍还要多。事实上，过去 12 个月里，我们看到所有百度产品上的语音日使用量增加了一倍，也就是语音服务的使用增长了一倍。所以，那些想要更高效、更便利地使用手机的用户更倾向于使用语音输入。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;机器之心：百度语音平台免费提供了一些 API，它能实现什么功能？如何从中受益？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;吴恩达&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：语音已经变成一个非常重要的人机交互方式，在百度大脑，我们正在努力实现越来越多的语音功能并帮助人们用上这种强大的能力。在我们的网站上，最受欢迎的语音功能是语音识别、TTS（尤其是情感 TTS）以及语音唤醒。我们的团队在不断努力将越来越多百度的最好语音技术放到网站上。我们知道，对于第三方公司来说，获取这些技术是非常有用的，但也还需要知道如何有效地使用这些技术。所以百度大脑做的另一件事情是创造能够帮助第三方组织、开发者和公司了解如何最有效地在他们的产品中使用这些技术的材料。所以我们也正在将越来越多这些训练材料放到我们的面向公众的网站上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;机器之心：百度是如何提升语音输入法的识别精度的？其中最困难的部分是什么？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;吴恩达&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：你知道，机器学习系统得到最好表现的一种最可靠的方式是在大量的数据上训练大型模型。如今百度的语音识别系统是建立在 5 万小时的数据上的，这是一个超过 5 年的音频数据。此外，我们在超级计算机上训练模型，它给了我们非常大的计算能力，从而建立足够大的神经网络吸收这些数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除此之外，我们面临的一些挑战是在缺少资源的语言和方言中。我们在尝试让百度语音识别系统覆盖更多的方言。在有很小数据集的方言上，我们尝试了在普通话上学到的东西，并将这些知识用到不同的方言上。所以，百度有很多积极性的研究是关于在没有普通话那样大量数据的情况下，研究在方言上做到最好的算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一个研究挑战是如何让语音识别在没有很多数据的新领域、新应用场景中有好的表现。例如，在不同的语音应用场景，语音片段听起来也各不相同，比如很多口语化的语音的识别问题。在这些小的新垂直应用中，我们没有很多的数据。所以我们也在做大量的研究，想要搞清楚从现有收集到的 5 万小时的数据中能学到什么，然后专门应用到新的垂直领域与新应用中，这些领域中的音频质量或说话方式与我们的训练数据有很大不同。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你知道，语音识别有了极大的发展，在很多领域有很大的应用。但在语音上，仍有许多的研究需要完成。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;机器之心：在 CES 2017 上，百度发布了 DuerOS，你也曾提到今年是对话机器元年，应该如何理解？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;吴恩达&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：我认为我们进入了语音对话接口成为必要事物的计算新时代。20 年前，我们大部分使用台式计算机或笔记本的键盘。大约 10 年前，乔布斯发布了 iPhone，开启了手触屏幕与手机以及其他设备交互的时代。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我认为人机交流的下一个时代是语音交流接口，而且我认为这一趋势如今刚好起飞。因为这一技术刚好到达了这样一个点，你可以坐在家中与对话计算机进行交流，询问航班以及其他信息，而且它们能了解你说的什么，并为你提供有用的信息和服务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以，我对对话计算时代黎明的到来非常乐观。事实上，我们在中国感受到的一件令人激动的事是我们看到了很多的创新，不同的团队建立了不同的很有创意的硬件。比如，小鱼在家、智能音箱、电视盒子等等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有了 DuerOS，我们希望能帮助所有的这些硬件制造者将人工智能、语音对话智能加入到硬件中，从而让更多这样的设备进入家庭。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;机器之心：你认为语音识别技术未来将在哪些领域发挥最大作用？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;吴恩达&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：我认为语音在 4 个类别中将会快速起飞。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第一个是手机。因为在智能手机上语音输入要比键盘输入更快，所以百度在手机的语音识别上增长迅速。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第二个是家居场景。我们看到了智能音响（smart speakers）的崛起，出了智能音响，我认为小鱼在家、电视盒子这样的设备也在增加。我们把这种坐在家的体验叫做背靠式体验（lean back experience），也就是你能背靠沙发发号施令，然后各种家居设备会了解你的需求并作出回应。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第三种是汽车场景。在你驾驶的时候，手放在方向盘上用说的方式与汽车交流，它就知道你想做什么。所以我认为在这个场景中也会发展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后是可穿戴设备。大部分可穿戴设备没有很大的界面，比如智能手表等。所以我认为在这个垂直领域，语音会慢慢发展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以，我认为语音是让你与机器交流如此高效的一个接口，它会在这些垂直领域有很快的发展。可能也有其他领域。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;机器之心：您怎么看语音识别技术的商业前景？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;吴恩达&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：在手机百度、百度地图、百度输入法等许多百度的产品中，我们可以看到过去几年中语音的使用变得越来越频繁了，因为这对用户来说是一个方便得多的文本输入方式。所以有大量第三方硬件制造商、软件开发商和开发者想使用语音来帮助他们的用户与他们的应用或设备进行更加自然和方便的交流；百度大脑项目也是一样，我们通过我们免费的语音识别 API 发布了我们的产品，让第三方也能用上我们的技术。语音识别是最难、门槛最高的技术之一，在百度，我们有幸能够使用足够的资源开发出非常好的语音系统。所以我们希望能够通过我们的技术来帮助许多开发者和企业组织，让他们的用户也能将语音作为一种输入方式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;百度的人工智能研究团队&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;机器之心：百度人工智能团队的日常工作是怎样的？是什么创新机制在支撑团队保持创造力？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;吴恩达&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：现在正是从事人工智能工作的好时候，你能看到有人将人工智能比作是「新型电力（new electricity）」——就像一百年前电力变革了一个又一个的行业一样。我认为人工智能也将类似地给交通和通信等许多行业带来变革。我们很幸运有这么多出色的人才在百度工作，他们不仅在努力使用百度的数据和计算资源来提升这些技术，而且也在寻找新的语音识别和人脸识别等技术并将它们投入到可以真正帮助人们的新场景、新产品和新应用中。每天我到百度工作时，我都为我们有这些能够帮助很多人的技术而感受振奋。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这里我想额外补充一点。我想对所有还在考虑自己的职业生涯的年轻人说一句，我知道当你很年轻的时候，有时候你无法确定该追求怎样的事业。我认为我们现在正生活在一个人工智能领域有无穷机会的时代，如果你还不确定你该做什么，可以考虑加入我们来开发人工智能、研究人工智能，未来几年这一领域将有非常大的机会。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;机器之心：在将人工智能研究成果产品化的过程中，百度人工智能团队是如何与其它业务部门协作的？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;吴恩达&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：将最新的人工智能投入到产品中通常需要多个不同职能的团队的协同。比如说，将语音识别技术集成到手机百度应用中，实际上是有很好的语音识别技术的语音技术团队和有很好的搜索技术的搜索团队的合作成果；正是这种互相理解的合作才让我们的手机百度具备了出色的语音输入能力。再举另一个例子，今天在进行金融交易时，我们会使用人脸识别来确认人们的身份，这也是我们 IDL 的计算机视觉团队与金融服务团队（他们有金融产品和深度的领域知识）合作的成果。所以在百度工作，这方面还是非常好，我们的技术团队可以很容易去创造或发明新技术，并且可以轻松地和其它出色的产品团队合作，将这些新技术快速投入到产品中从而为他们的海量用户提供帮助。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;机器之心：百度在招募人才、组建人工智能团队方面，有哪些经验可以分享？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;吴恩达&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：你知道如今人工智能发展迅速。我认为百度持续在做的一件事是在职员上做投资，扩展我们的团队。据我所知，百度在职员培养、训练上的投资要比其他公司都大，我们进行常规的课程从而让团队了解最新的人工智能技术，所以我们的团队会变得越来越好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在美国和中国，我认为百度正在获得这样的声誉：如果你想要学习人工智能，这里就是你该去的地方。也许很多人不了解，但我认为对全世界的科技巨头而言，李彦宏是第一个意识到深度学习巨大潜力的领导者。我认为李彦宏具有很深的技术背景，同时对人工智能技术有着透彻的理解。我们很幸运在百度成立 17 年的时间里，有他一直在带领着公司建立并且积累基础的人工智能科技。我认为我们现在的这些成果都是建立在李彦宏打下的基础之上的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们都知道 IDL（百度深度学习实验室）主任是林元庆，但很少有人知道其实 IDL 的第一位主任是李彦宏本人。他预见了深度学习的发展趋势，并希望百度首先投入其中。不仅仅是在中国，放眼全世界的科技公司，你很难找出一个和李彦宏相似这样有预见性的领导者了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;对人工智能领域的看法和期望&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;机器之心：小度在《最强大脑》的节目中表现优异，但人工智能技术在实际应用层面还面临诸多挑战，比如无人驾驶汽车的安全性等，您怎么看待这一类难题？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;吴恩达&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：当飞机在大雾或雨天着陆的时候，基本上都是在用基于计算机软件的全自动驾驶。所以，我认为如今已经有了能做出与性命相关决定的软件。我认为，如今有了人工智能，这些重要决定将会更容易作出。无论它是设定在自动驾驶内，还是医疗领域中（比如自动诊断）。当然，我认为人工智能研究者还面临着一个重大责任——就是在各个垂直领域内作出谨慎的评估，这才能够让人们更加相信人工智能。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;说到自动驾驶汽车，这是近年来快速发展的另一领域。它面临的一个重大挑战就是：现有的交通法规大部分是为人类驾驶员所写的。所以自动驾驶在发展中面临的最大挑战就是需要制定既适用于人类又适用于计算机驾驶员的新法规。我认为这是加速全球自动驾驶普及的关键。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;机器之心：很多人说可怕的不是人工智能，而是人工智能落到的坏人手里。您怎么看待这一说法？如何防止出现这样的现象？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;吴恩达&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：之前我们将人工智能比作是新时代的电力——就像是一百年前电力是新出现的超级力量一样，现在的超级力量就是人工智能。在绝大多数情况下，电力都给我们这个世界带来巨大的好处，我们现在几乎不能想象没有电的生活；但不幸的是，电力也被用在一些不好的方面。幸运的是，我认为现在绝大多数人工智能领域内的人都是好人，我们做人工智能是因为我们希望帮助人类。所以我相信总的来说，人工智能将给这个世界带来很大的积极影响。我也认为每一个人工智能工程师和研究者都有个人责任，确保其成果能够有益于这个世界。基于我对全球人工智能业界的了解，我认为现在全球人工智能行业整体上都在做着非常有益于这个世界的事情。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;机器之心：你曾经在《哈佛商业评论》中撰文呼吁各大公司设立首席人工智能官（Chief AI Officer），你认为首席人工智能官需要具备什么样的特质？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;吴恩达&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：百度是世界上最好的人工智能公司之一，我们在公司的每一天都在思考人工智能。我希望能够将我们的一些想法和人工智能社区以及世界上的其他人分享，从而帮助推动全球人工智能的发展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;关于首席人工智能官，我认为目前人工智能所面临的难题之一是：将这种我们已经拥有的技术应用到能够真正有助于我们的业务的使用案例中。所以我认为首席人工智能官应当具备两种关键技能：一是理解这种技术（这很重要却也很难），二是了解自己公司的业务并且搞清楚如何将这些让人惊叹的人工智能技术和你的业务匹配起来，从而让你能够创造出重要的价值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;机器之心：大公司都在重金投资人工智能领域，它们也在数据量上拥有绝对优势，您认为初创型公司还有机会在竞争中占据主导位置吗？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;吴恩达&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：我们最好的语音识别系统大约是在 5 万小时的数据上训练的。我们的语音识别系统，也就是小度这次使用的这套系统是在 2 万说话人数据的基础上训练的。所以如今就有一些问题，如果你想要获得顶级系统，我们就需要大量的数据。所以在一些领域中，小公司使用如今已有的科技建立百度这样有效的系统还是很有挑战的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但我认为在一些垂直领域中，例如，在罕见疾病的医疗成像上，全世界在这些领域可能都没多少图像。所以，我认为在这些垂直领域中，即使少量的数据也可能建立有相当好表现的系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管如此，我也认为百度的数据、资本、超级计算机，再加上我们的人才，确实使得我们能更快地建立最好的人工智能系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;机器之心：人工智能的技术研究在哪些方面改变了百度，又将如何渗透到更多的行业？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;吴恩达&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：人工智能已经完全变革了百度——从网页搜索到我们组织外卖送递的方式，再从我们推荐内容的方式到我们进行人脸识别、身份认证、语音识别的方式等等。所以这些都已经用到了人工智能。我认为除了变革百度的产品之外，我们也很高兴能将人工智能技术提供给第三方，让它们也能使用我们的语音、计算机视觉、NLP 等等各种不同的人工智能技术来变革自己的产品。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我认为，人工智能会在未来改变所有行业的形态。有的时候我的朋友会和我打赌，看看某个行业在最近几年不会被人工智能所改变。你也可以尝试想想看，实际上我们很难想出在未来几年不会受到人工智能影响的行业。我最喜欢的例子是理发师，实际上我发现创造一个能够理发的机器人是很困难的。曾经我在台上演讲也说过类似的话，但我有一位机器人学教授朋友告诉我，她说对于大部分的发型来说确实如此，机器人很难帮他们理发；但她也指出：「至于你的发型嘛，我可以让个机器人剪出来。」所以我觉得实际上我们很难确定一个不会被人工智能改变的领域，我认为不管你的业务是什么，都可以考虑一下利用人工智能来增强你的优势。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;机器之心：你想对中国的人工智能从业者和机器之心说些什么？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;吴恩达&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：我认为中国很幸运有机器之心这样的顶尖媒体将全世界的人工智能进展快速分享给中国的读者。实际上，中国和世界的信息传播有一种奇怪的不对称——全球的人工智能进展可以非常快速地传播到中国，但有时候百度等在中国发布或发表的进展却很少让世界其它地方的人知晓，这可能是因为他们并不阅读中文的媒体。当然我希望这种世界向中国的知识共享能够继续，我也希望我们能做些什么来帮助世界其它地方的人更快地了解中国的人工智能发展和前沿成果，这样我们就能让整个世界的人工智能研究社区都更快速地进步了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOyRulPA7upc5nBHVHQV4nEe1VYKJ0oCNRDHIgfRdEjDQg3atmJKUrSEg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;「AI Talk」&amp;nbsp;是机器之心最新出品的视频访谈栏目，旨在邀请国内外人工智能顶级专家分享对技术和行业的观点，为大家呈现更为直观、丰富的内容。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心原创，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 15 Jan 2017 12:11:12 +0800</pubDate>
    </item>
    <item>
      <title>干货 | Andrej Karpathy CS294课程总结：可视化和理解深度神经网络</title>
      <link>http://www.iwgc.cn/link/4347270</link>
      <description>&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;机器之心&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;I. 介绍&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;这篇文章中，我们将回顾一些目前用来可视化理解深度神经网络的方法。我不会深入探讨这些材料中的细节，而是阐述一些个人观点以及我在学习这些材料时的个人体会。所有原始材料来自 Andrej Karpathy 在伯克利大学的客座讲座，CS294 课程。该讲座的演讲视频可点击文末「阅读原文」查看。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOy8VhBzt7u8O4BAIHRfwZenvgWSJSj2u42Tv5yD8yzEzu8YIgeDkBTiaA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;II. 凸优化（Convex Optimization）vs. 非凸神经网络（Non-Convex Neural Networks）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;凸优化是一个数学上非常严谨的问题，人们对此也一直颇有研究。它的优美之处在于能够推导出易实现的、达到全局最优的下降算法。而非凸优化问题，却很难证明其最优性。也因此，我们会担心针对这些问题提出的优化算法会停滞在局部最小值。然而，这并不是说我们不能证明非凸问题最优解。我已经碰到过一些技术，这些技术使用了区间分析（interval analysis）方法，只要函数在某些阶上（some odrder)Lipschitz 连续，并且它的解（局部最小值）并不会产生组合性爆炸。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经网络本质上是一个非凸问题。因此，对其最优性的形式证明寥寥无几。在过去，担心差在局部最小值是真的，特别是神经网络刚刚开始发展的阶段（上世纪 80 年代）。其中一篇关于探讨这个话题的论文： 多层网络的损失面（The Loss Surfaces of Multilayer Networks by Choromanska etal. 2015），实证表明，随着问题维数增加（可视为隐藏层更多了），你的最终解的损失方差会下降。因此，基本上，最优解和最差解之间的间隔在不断锐减，你的所有的局部最小值会变得相同。因此，非凸优化解决方案不过是走开了，人们并没有真正的解决这个问题，它仅仅是变得不重要了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOyQgRBxj2tNlH6RcSH3pTqsgu8yXGoCwWf5Z1aibLoglAfIXYDgic4BtRg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;III. 层图表达以及 t-SNE 可视化&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（编者注： t-SNE 是 t-distributed stochastic neighbor embedding 的缩写，即 t 分布随机邻域嵌入算法）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;卷积神经网络，简单来说，就是一个多层巨无霸三明治。一种用来视化理解这些网络的方法就是从网络中取出一个单独神经元，观察让这个神经元兴奋的是什么。本质上，我们经验使用这些激活反应来可视化神经元响应的对象。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOy2icgyiawiagVM2Q2nnKZ5mUx1j1cyTiauOtT4KXkhricGiaeroicicDQoX2meA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;图注：可视化激活神经网络的事物&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;br/&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一个技术是可视化网络权重。这需要训练自己的神经网络然后显示它学习到的 Gabor 过滤器。不过这种办法只对卷积神经网络首层有效，因为针对输入的图片所得出的权重在第一层后又会再做卷积操作。当你不断深入网络，我们就不太能解释 这些滤波器的结果了，因为每一层的权重都是在前一层输出结果上进行了卷积操作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOy9f4Nc49sVe3sibJPl5bkEhEfGM5EFXl1h13leib5dvu9WwqdqcCgaUMA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图注：可视化网络权重&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在此，Andrej 给出了一个关于使用 ConvNetJS 来实现可视化技术的链接（https://cs.stanford.edu/people/karpathy/convnetjs/），该项技术能把网络逐层分解，你可以利用这个来观察网络在输出最终分类结果前每一层的梯度、激活函数、 权重等。此外，Andrej 还推荐了一下 TensorFlow Playground：http://playground.tensorflow.org/，以及 Jason Yosinski 的博客：http://yosinski.com/deepvis&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;IV. 不仅仅是单个神经元的可视化&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有一种观察卷积神经网络的方法是看其全局表达，即卷积神经网络对任何一张图片，其顶层的输出结果。我们将一张图片传入卷积神经网络网络去处理。这个网络的每一层将对该图片进行重新的表达，而对于每一层，我们可以学习到原始的图片是如何被整合到这一层中的。因此，为了可视化这一过程，我们希望能将这些表达整合到 2 维空间。这时候，就需要用到一种超炫的技术，叫做 t-SNE 可视化技术（Van der Maaten, Hinton**）。**这种技术将高维的点嵌入到低维空间，而局部的成对距离被保留了（在低维空间相邻的点在高维空间也依然相邻）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOyNI2rmIG2KD0Rt7k8dmD8Up3tThKuOCqHcXzcF6bfbCXPrAqxDic8KAQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;图注：t-SNE 可视化，了解更多可查阅 http://cs.stanford.edu/people/karpathy/cnnembed/&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOyMwQKoVCLFn6ClUQGvK9svpcPeklqEy18sV6PJS6lclFH7CvMGUqCGg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;图注：用强化学习玩 Atari 游戏的可视化&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;V. 遮盖实验&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种用于可视化网络究竟学到了什么的技术是把网络视作黑盒，修改它的输入然后观察输出。假设我们有一张被这个网络能准确地分类成博美犬的图片，现在，我们要做的是将这个图片的某一块「屏蔽（block）」（将这个地方的像素值设置为 0 或 255，或者颜色设置为黑或白即可）。这样，这个网络的输出是对这张被屏蔽的图片的输出。我们可以发现，当我们屏蔽的部位越是重要，比如脸部，那么这个网络做出正确分类可能性就越低。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOydl55lhUUwkvLcR9YdWKBj58FBSFQmIxNZPm4gTkeKWcdU8yuPSL88A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个有趣的发现是，比如，我们有一张照片，其正中部位有一只阿富汗猎犬，旁边是一位男子，网络能正确的将其标记为阿富汗猎犬。但是，如果将男人的脸用像素为 0 的方块遮盖，网络认为是阿富汗猎犬的概率激增。发生这种现象的原因在于，每一张图片只被分配了一个正确的标签，当我们遮盖一些可能会引起网络会做出其他决定的部位，那么，得出这个正确的标签的概率就大大提高了。这也是一种完整性检查，这样网络可以一种通过调整图片可能所属类别标签的概率大小来做出合理判断。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;VI. 去卷积方法（Deconvolution Approaches）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;通常，我们尝试去计算出关于网络权重的损失函数梯度，这样当我们每做一次更新操作，我们就能优化权重。现在，让我们思考一个问题：给出一张图片，怎样才能得出网络中任何一个随机神经元的梯度？有一种可行的方案是：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOyzopePic8E8atv0YLgwOBOnbT1eWic6KSekfNrXmCHFqUoOFiba2e9fbUg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1）将图片输入网络，对于网络深处的某一个神经元，我们将其命名为神经元 a。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2）将它的梯度设置为 1，同层的所有其他神经元的梯度设置为 0。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3）一路将梯度反向传播回图片，得到一张略古怪的噪声图片。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管这张梯度图片很难解释，但至少可以告诉你，如果把这张图片和原始图片叠加，将会提高神经元 a 的激活函数值。而反向传播过程只会改变修正线性单元 ReLU 层。这里的直觉就是：沿着梯度（反向）传递到 ReLU 层的某一个神经元，那么说明这个神经元被激活了。否则，传递就会停止。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一种替代方法是不使用简单的反向传播，而是使用所谓的导向反向传播（guided backpropagation）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOyW24FbfSlbpOUZ2PAzUenJdBgyjvsQegh5R60AMBZIQHwfibiaz8CK0ng/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种技术并不仅能够识别一个修正线性单元 ReLU 是否被激活，而且可以识别所有值为负的梯度。本质上，除了能把所有不被激活的修正线性单元 ReLU 关闭，所有反向传播时遇到的负信号还能被设置到阈值 0。最终，我们只需要反向传递梯度为正的值即可。这样的话，反向传播最后获得的图片就会更加清晰，因为我们去除了所有负梯度对我们所选的神经元造成的影响，只保留了正面的影响（详见： Striving for Simplicity: The all Convolutional Net, Springenberg, Dosovitskiy, et al., 2015 for more information）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOybEMtz4r9wXQNuGqczSpSgjgpjRgOxX2yXchzV40ZB2DeRn0T7eDYSA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图注：使用 guided backpropagation 后噪声明显减少&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;VII. 对图像进行最优化&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;i. 类别可视化&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOyN1SR0SwT4GgMgyNXBHn3EgPwQsrFNdBvw5QnUqVBYeTiboJslI7q4RA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来讲授的技巧涉及在图像上进行最优化操作。考虑如下问题：我们能否找到一个图像，它能够最大化某些类别的分数？为达到该目的，我们希望保持神经网络架构不变，而使用不同损失函数以在图像上进行最优化。这个方法包括如下步骤：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOy3tvUV7zUS53Gwrmp1edlFWmianbIv0FL7ZA4RYSeEibA9RWDlbCadicYA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;(1) 向神经网络中传入一个随机的图像；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;(2) 设定梯度分数向量为 [0,0,…,1,0,...0]（将感兴趣的那一类设为 1，不感兴趣的则为 0），接着对对象进行反向传播；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOyvW28YTasoWC1iavB7UfznibHzzhWMueDrw5LEpLO4DMgV7emMTLiaTOrw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;(3) 进行一个小规模「图像更新」；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;(4) 将更新的图像进行正向传播；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;(5) 重复 (2) 以设定其他的梯度分数向量。我们基于一个随机噪声的图像开始，并对目标类（也就使用了上述的梯度分数向量进行逆传播的那一类）进行梯度上升操作，我们会生成一个图像，它能改进神经网络对目标类的激活状态。从数学上来说，令 I 代表一个图像，y 代表一个目标类，Sy(I) 则是神经网络赋给图像 I 在 y 类上的分数。我们希望解决如下的最优化问题：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOyUYzPog4uDVTiabicWN7YkuiaMmAgGez0eXXQica42EltWAsnSWP99g2iaibg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;也就是说，我们希望找到一个图像 I∗，它能够最大化神经网络赋给 y 类的分数。在如上的等式中，R 是一个正则化项。正则化项改进了输出图像的可视化程度（参见 Simonyan 等人，Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps，ICLR Workshop 2014）。这个内容在去年完成，它论述了类的分数问题。但是，该技巧可以被应用到神经网络中的所有节点上。我们从某个随机图像开始正向传播，直到到达了我们想要研究以及可视化的层为止。对该层中的任一神经节点，重复该步骤（即设定其他梯度分数为 0、目标对象梯度值为 1、在图像上反向传播），以检查哪种图像会最大程度地激活神经网络中的神经节点。注意，这些技巧中都有正则化项，以避免对抗的图像。不同的正则化方案侧重图像的不同方面，以判断我们认为的「正常的」图像。所以，它们会对这些试验的结果有很大影响。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;ii. 特征反演（Feature Inversion）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一个可以探讨的问题是：给定一个卷积神经网络的「编码」（特征代表，可以理解为是神经网络中某一层的输出值），能否根据其重构原来图像？如下的技巧就试图实现这一功能。它分为三步：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;(1) 向网络中传入一些输入图像；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;(2) 忽略输入图像；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;(3) 在某些层对输入进行反向传播，直到在网络中找到这样的层，能够生成与输入图像相同的「编码」（在这一层学到的特征表示）的图像。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从数学上看，令 I 代表一个输入图像，φl(I) 为卷积神经网络 φ 中的激活层 l。我们希望解决如下优化问题：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOygwQiabQ7ZtIbwCXx8icL2L6BOLkDwBEIku50e7CvCXEJtBOGJdPjxjCQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;也就是说，我们希望找到一个图像 I∗，它与图像 I 在神经网络 φ 中的 l 层有相似的特征表示。其中 ||.||2 是代表 L2 范数，R 是正则项（可能是隐式的）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;概括来说，我在这里的想法是，要储存一个图像，只需存储图像的「编码」就行了。我们可以根据上述方法使用这些「编码」来重构图像（尽管有损失）（参见 Yosinski 等人，"Understanding Neural Networks Through DeepVisualization"，ICML 2015 Deep Learning Workshop）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;总的来说，这项技术允许我们看到，图像是如何通过一组特定「编码」（特征代表）在神经网络上被恢复的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;VIII. 卷积网络中的对抗图像&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对抗图像，是对原有图像加入很小的扰动而构成的图像；这些扰动由数据集的数据构成，被特意设定为最坏情况。由此，神经网络会错误地给这个新形成的图像很高的概率。在实践中，我们可以取任一被正确标记的传入神经网络的图像，并基于其它对抗图像对其添加扰动值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOyicdcxTpVpQN9EL6CSwfMeZOASqXnF5yzic14DIoMERalwUXwbFfczhew/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们无意探讨过深的数学细节，但这种情况发生的原因是因为神经网络常有很高的维度，因此，它有着内在的线性本质。直觉上说，我们考虑如下的线性例子：令 x 为一输入图像，w 为该模型的权重值，则获得输出的运算就是 x 和 w 之间的内积，即 wTx。如果我们以 η 来轻微地对输入进行扰动，我们则得到 x¯ = x + η。那么，输出就便成了 x¯ = wT x + wTη。这种对抗扰动导致激活值增长了 wTη。进而，我们可以在某些关于 η 的限制条件下（通常是正则化约束）最大化该项以引起模型中的问题。但是，随着问题维度的不断增加，我们可以在满足正则化条件的情况下，对 η 向量施加很多小的扰动。这些细微的变化加在一起，最终会对输出造成很大的变化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一种考察不同模型间对抗图像的方法，是将他们视为对抗扰动在模型权重向量下高度对齐的结果。这是因为如上解释中所说的内容：这些小的改变迫使该线性模型专注某一个信号，该信号和模型的权重值最相近；即便其它 (从正确图像中得来的) 信号有更大的振幅也是如此。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;IX. Deep Dream 实验&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOyhLF59tZIOVmWictnicoQjUL2k9DZ8yYAtRTaj2IhhLwkz6sxLngvtDXQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图注：Deep Dream GitHub：https://github.com/google/deepdream&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Deep Dream 实验背后的机理实际上是很简单的。基本上我们只要修改一下图像，以增强网络中选定的某层的激活情况。具体地，我们需要：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOySawBpic1kuZJdUuR59theSuZAViarib8t0yATZ98SNjOSM3TibRXUic0F4g/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;(1) 从神经网络中选择某一层；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;(2) 向其中传入某些输入图像，以确定给定层的特征；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;(3) 设定那一层的的梯度值为激活值自身；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;(4) 对该图像进行反向传播。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;X. 神经风格实验&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOyNnVBJdQeKSicpfxMSMrkLwjVpsxcBIHa9bB9W6LjgSXCtECwGSBuZyA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;考虑如下的情景：我们有两个图像，一个为内容图像 Ic, 一个为风格图像 Is，我们想生成第三个图像，使之能够具有 Ic 的内容及 Is 的风格。也就是说，我们要做的是从 Ic 中解析出内容，从 Is 中解析出风格。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOytM9KibrUS7H21ibCtDNyOicv4ibsjkJKH9dZUPOSGrdWVocCMx1D7LBt5Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了解析出内容，我们将 Ic 传入我们的神经网络并储存每一层的激活值。但解析风格的过程却有所不同。我们将 Is 传入神经网络，并计算每一层激活值的 Garmian 矩阵（G=VTV）。从代数的观点来看，Garmian 矩阵 G 仅仅是 V 的列内积值。例如，CONV1 层由 244×244×64 个激活值构成，我们则计算得一 64×64 的 Gram 矩阵，它是由每个区域内配对激活值的协方差求和而成。从图像的角度来说，我们是将一层由三维（244×244×64）矩阵构成的激活值转换到一个二维矩阵（（244×244）×64），并对其取外积以得到该 64×64 的矩阵。对矩阵中的每个项 gi,j，我们都是将输出通道 i 及 j 在那一层上的激活值乘在一起。如果通道 i 及 j 的神经元交结在一起，那么它们会加在一起，我们也会得到一个更大的 gi,j。所以 Gram 矩阵 G 含有在对整个空间位置平均后哪些神经元交结在一起的数据。我们对神经网络中的每一层都计算一个 Gram 矩阵。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOyrJujw2MqB5PiawWplXxqNGticiboW86t6ORzAr3It10IPVGCYZuw1T8jQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最终，我们有了这些信息之后，就能够对整个图像进行最优化以得到： Ic 的内容及 Is 的风格（详见 Leon A. Gatys 等人，《A Neural Algorithm of Artistic Style》，2015）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOy2cONrJ6Mxfsugn0lKjEFk4ticjHnC99QzJYTpP6U5hAumCGjnmqWjeg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;最后，Andrej 还推荐了一个快速神经风格迁移项目，可以实时通过网络摄像头实现风格迁移：https://github.com/jcjohnson/fast-neural-style，参见机器之心文章《&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719733&amp;amp;idx=3&amp;amp;sn=950a7adb4e93bca22e4ef975877482a9&amp;amp;chksm=871b018bb06c889ddc87bccaa0f35de5ca5a35c156cf1164d134d5010065d68dd06ba6e7cdfa&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719733&amp;amp;idx=3&amp;amp;sn=950a7adb4e93bca22e4ef975877482a9&amp;amp;chksm=871b018bb06c889ddc87bccaa0f35de5ca5a35c156cf1164d134d5010065d68dd06ba6e7cdfa&amp;amp;scene=21#wechat_redirect"&gt;开源 | 怎么让你的照片带上艺术大师风格？李飞飞团队开源快速神经网络风格迁移代码&lt;/a&gt;》。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOyyOJBPFfloh0F9nibLIu0C8BVgLFZUPbPdcgTSPZ0eDBgxqsBboibxXGw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;XI. 结语&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这篇回顾中，我们回顾了一些能够用于理解及可视化神经网络的技术。这些技术是从各种资源中搜集来的，其呈现的顺序与重要性无关。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们探讨了如何可视化那些能最大化激活神经元的区块，检查了其权重值及其对激活值的影响（第三节）；我们亦讨论了使用如 t-SNE 这样的技术来可视化全局表达（第四节）；在所讨论的遮盖实验中，我们修改了输入并观察了输出改变情况（第五节）；然后，我们谈到了几种去卷积方法（第六节），接着对图像进行最优化（第七节）以最大化一个类、神经元之间的激活率（firing rates）或是匹配一个特定的编码。此外，我们还基于简化的线性解释，讨论了卷积网络中的对抗输入。最后，我们涉及到了一些关于最优化图像的一些应用（在最后关于 Deep Dream、神经风格的两节中）。这些技术表明，神经网络中的「层」或特性并非仅仅是随机模式，有着能够直觉被理解的特性。我们能够使用这些可视化技巧来发现模型的中的问题，以获得更好的结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，我们想提及一个也许更具价值的观点：如今神经网络给出的解决方案，在神经网络不断增长的情况下的被证明是经验最优的——也就是说，在所谓「好」与「差」的答案之间的鸿沟消失了。所以，困在一个局部最优点，也许不再是一个问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心原创，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 15 Jan 2017 12:11:12 +0800</pubDate>
    </item>
    <item>
      <title>开源 | Udacity发布无人驾驶汽车工程师纳米学位项目：在模拟器中克隆人类行为</title>
      <link>http://www.iwgc.cn/link/4347271</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Github&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杨旋、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Udacity 无人驾驶车工程师纳米学位（Nanodegree）系列计划第三个项目&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;项目地址：&lt;/span&gt;&lt;span&gt;https://github.com/upul/behavioral_cloning&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;行为克隆：模拟（人类）驾驶汽车&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;概述&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本项目的目标是使用一个深度神经网络来模拟人类的驾驶行为。为了达成这个目标，我们会使用一个简单的汽车模拟器。在训练阶段，我们使用键盘在模拟器内对我们的车进行导航。当我们驾驶汽车时，模拟器会记录训练图像和相应的转向角度。然后我们使用这些记录的数据来训练我们的神经网络。训练模型会在两个轨道上进行测试，即训练轨道和验证轨道。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;依赖包&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此项目需要安装 Python 3.5 和以下 Python 库：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Keras：https://keras.io/&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;NumPy：http://www.numpy.org/&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;SciPy：https://www.scipy.org/&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;TensorFlow：http://tensorflow.org/&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Pandas：http://pandas.pydata.org/&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;OpenCV：http://opencv.org/&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Matplotlib (可选)：http://matplotlib.org/&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Jupyter (可选)：http://jupyter.org/&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在终端提示符处运行此命令安装 OpenCV。适用于图像处理：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&amp;nbsp;&lt;span&gt;conda install -c https://conda.anaconda.org/menpo opencv3&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;如何运行模型&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此资源库包含了已经进行训练好的模型，你可以使用以下命令直接测试。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;python drive.py model.json&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实现&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;获取数据&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在训练期间，模拟器以 10Hz 的频率来获取数据。此外，在给定的时间间隔内，它将依次&lt;/span&gt;&lt;span&gt;记录左、中和右部的摄像机拍摄的三个图像。下图显示了我在训练期间收集的示例。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOytX3u34ta13CKiaVpicOrjMNHhro3ql1LK4kZmKcF9KekaPWaus66c5UA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;收集到的数据在传送给深度神经网络之前会被预处理，相应的预处理步骤将在本文的后半部分描述。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;数据集统计&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集包含 24108 张图像（每个方向的摄像机拍摄了 8036 张图像）。训练轨道包含许多缓弯和直线路段。因此，记录中的大多数转向角都是零。所以，对图像和相应的转向角度进行预处理非常必要的，以便将此训练模型推广到那些未经训练的路线（例如我们的验证轨道）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来，我们将解释我们的数据处理流程（data processing pipeline）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;数据处理流程&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下图显示了我们的数据预处理流程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOyHpsbKuu8tRViacM7ObwblgFj4mjvUuDoMU8ca03WcY1qJOlgsoXxdUA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当处于流程中的第一个阶段时，我们会应用随机剪切操作。然而，我们只选择了 90% 的图像来进行随机剪切过程，保留了 10% 的原始图像和转向角，以帮助汽车在训练轨道中导航。下图显示了样本图像剪切操作的结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOyXggRulfIFDj1D9iaxXpxlbHibhKuTBXbEDCwgjia4zVicaBOyriaiccssXgA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;模拟器捕获的图像有很多细节，它们对于模型构建过程没有直接的帮助。这些细节除了会占用的额外空间还需要额外的处理资源。因此，我们从保留的那 10% 的图像的起始处开始删除了 35％ 的原始图像。这个过程在修剪阶段（crop stage）完成。下图显示了图像的裁剪操作的结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOyia71tcLrqz69ibP6IiaN1nRKChicib6icc6JibEgsKce7nZkmttR2QVSGkQQQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;数据处理流程的下一阶段被称为随机反转阶段（random flip stage）。在这个阶段，我们会随机（概率设定为 0.5）翻转图像。这样做的原因是因为是训练轨道中的左弯曲比右弯曲更普遍。因此，为了增加我们的模型的泛化性，我们采取了翻转图像和相应的转向角的方案。下图显示图像的翻转操作的结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOycnIPv5HFibd0FicQVOxCUKlu4mEETmQSHibhTWeTqLBE0icVvxDjEjgSUQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在流程的最后阶段，为了减少训练的时间，我们将图像大小调整为 64×64。样本调整大小的图像如下图所示。进行调整过后的图像传送给神经网络。下图显示了图像的调整大小操作的结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOysF9VMmzyoLoeffxRAvsGzuwPbVY7kOvJwlw2ibUezJwict8sov4OXdew/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来我们将讨论我们的神经网络架构。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;网络架构&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们设计卷积神经网络架构的灵感来自于 NVIDIA 的论文《用于自动驾驶汽车的端到端学习（End to End Learning for Self-Driving Cars）》。我们的模型和 NVIDIA 的模型的主要区别是我们在每个卷积层之后又使用了最大池化（MaxPooling）层来减少训练时间。有关我们的网络架构的更多详细信息，请参见下图。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicS08ZFjdibpLIeOjg73NAOymMl2RdEPFnnVwBeURZx7h8B6FEBxlIZpZhXVvAITlSbLcHalyQYszg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;训练&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;即使在对训练图像进行裁剪和调整（所有图像都进行了增强）之后，训练数据集还是非常大，所以不能将它们完全存于主存。因此，我们使用 Keras 库的 &lt;span&gt;&lt;strong&gt;&lt;em&gt;fit_generator&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt; API 来训练我们的模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们创建了两个生成器（generator），即：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;train_gen = helper.generate_next_batch()&lt;/span&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;validation_gen = helper.generate_next_batch()&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;train_gen&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;span&gt; 和 &lt;strong&gt;&lt;em&gt;&lt;span&gt;validation_gen&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt; 的批量规模是 64。我们在每个训练阶段使用 20032 张图像。应当注意，我们现在所说的图像是经过上文中所提及数据处理管道处理后生成的。除此之外，我们还使用 6400 张图片（同样也是数据处理管道处理过后的）进行验证。我们使用 &lt;strong&gt;&lt;em&gt;&lt;span&gt;Adam&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt; 优化器和 &lt;strong&gt;&lt;em&gt;&lt;span&gt;1e-4&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt; 的学习速率。最后，当涉及到选择训练代数（training epoch）值的时候，我们尝试了几种不同的数据，如 5、8、10、25 和 50。然而，我们发现 8 在训练和验证轨道上的效果都很好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结果&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在项目的初始阶段，我使用了我自己生成的数据集。该数据集很小，并且是使用笔记本电脑键盘在导航汽车时记录的。然而，使用该数据集构建的模型不足以在模拟器中自主导航汽车。后来我使用了 Udacity 发布的数据集。使用该数据集开发的模型（在增强数据的帮助下）在两个轨道上的工作情况都很好，如以下视频所示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;训练轨道&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;iframe class="video_iframe" data-vidtype="1" allowfullscreen="" frameborder="0" height="417" width="556" data-src="https://v.qq.com/iframe/preview.html?vid=a03662bfirm&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0"&gt;&lt;/iframe&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;验证轨道&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;iframe class="video_iframe" data-vidtype="1" allowfullscreen="" frameborder="0" height="417" width="556" data-src="https://v.qq.com/iframe/preview.html?vid=f0366jm8ct1&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0"&gt;&lt;/iframe&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;总结和未来的方向&lt;/strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这个项目中，我们对自动驾驶汽车上下文中的回归问题进行了研究。在初始阶段，我们主要致力于寻找合适的网络架构，并使用我们自己的数据集训练模型。根据均方误差（MSE）显示，我们的模型运行良好。然而，当我们使用模拟器测试模型时，它没有像预期的那样执行。因此，这清楚地表明了，MSE 不是一个评估该项目的性能的优质的指标。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在项目的下一阶段中，我们开始使用一个新的数据集（实际上，它是由 Udacity 发布的数据集）。同时，在构建我们的最终模型时，我们没有完全依赖于 MSE。此外，我们使用相对较少的训练代数（即 8 个阶段）。数据扩增和新的数据集的工作效果令人惊讶，我们的最终模型在了两个轨道上都表现出了出色的性能。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当谈到一些延伸的事情和未来的方向时，我想强调以下几点：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在真实的道路条件下训练模型。为此，我们可能需要找到一个新的模拟器。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;尝试其他可能的数据增强技术（data augmentation technique）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;当我们驾驶汽车时，我们的一系列行为比如改变方向盘的角度和踩刹车不仅仅基于即时的驾驶决策。事实上，驾驶决策是基于当前的交通/道路状况在短暂的几秒钟发生的。因此，使用循环神经网络（RNN）模型（如 LSTM 和 GRU）来执行这个问题会非常有趣。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;最后，训练（深度）强化代理（reinforcement agent）也将是一个有趣的附加项目。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 15 Jan 2017 12:11:12 +0800</pubDate>
    </item>
    <item>
      <title>一周论文 | 多模态机器翻译</title>
      <link>http://www.iwgc.cn/link/4347272</link>
      <description>&lt;h1 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;&lt;strong&gt;引&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;多信息融合是一个重要的研究趋势，尤其是对于训练数据缺乏的任务来说，如何融入其他相关信息来提高本任务的准确率是一个非常值得研究的问题。机器翻译是一个热门的研究领域，随着训练数据规模地增加，各种NN模型的效果也取得了突破的进展，google和百度均已部署上线NMT系统；融合图像、音频、视频、文本等各种模态数据的多模态研究也是一个非常热门的研究方向，本期PaperWeekly将为大家带来NMT和多模态交叉研究的paper解读，共3篇paper：&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;1、Attention-based Multimodal Neural Machine Translation, 2016&lt;br/&gt;2、Multimodal Attention for Neural Machine Translation, 2016&lt;br/&gt;3、Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot, 2016&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h1 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;strong&gt;Attention-based Multimodal Neural Machine Translation&lt;/strong&gt;&lt;/h1&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="作者" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;作者&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Po-Yao Huang, Frederick Liu, Sz-Rung Shiang, Jean Oh, Chris Dyer&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="单位" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;单位&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;CMU&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="关键词" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;关键词&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Visual Features, Attention, Multimodal NMT&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="文章来源" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;文章来源&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;ACL 2016&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="问题" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;多模态神经机器翻译，在传统的seq2seq翻译模型上，利用图像特征信息帮助提高机器翻译的结果&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="模型" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;在WMT16的多模态神经网络机器翻译新任务上的工作。&lt;br/&gt;提出了3种如何将visual feature加入到seq2seq网络中的encoder，从而使得decoder更好的attention到与图像，语义相关部分的模型： global visual feature， regional visual feature，paralle threads.&lt;/p&gt;&lt;p&gt;&lt;a title="global_visua" rel="gallery0" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmm909CYrpybXBB3vCicafPTSngP0DhpPssYJMf5vpzbdZFcxDujcPdGq4q1G4ly3GhwAzO9ZTAWnQ/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;global visual： 直接将VGG中的fc7抽出的feature加入到encoder的first step(head)或者是last step(tail)&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="region_visua" rel="gallery0" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmm909CYrpybXBB3vCicafPTWiaHUMibJ8wwxhftWiaAOplrp23YtQpWUABOQzIn3s2MxsXgibnicGr3IdQ/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;regional visual： 先用R-CNN抽出region box的信息，再用VGG得到fc7的特征，将top4对应的region feature，以及global visual feature分别作为每一个step输入到encoder中&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="parallel_threads" rel="gallery0" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmm909CYrpybXBB3vCicafPT1RNOvtxdNic1TIWJwia2AUAxialQzNTEvpNPAx6QCY0NUlb0bBWFaYvnQ/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;parallel threads: 与regional visual相对应的是，每个thread只利用一个region box的feature，和global visual一样的网络，将top 4对应的4 threads和gloabl thread一起做average pooling，每个therad的参数共享; attention则对应所有threads中的所有hidden states&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;同时本文还提出了三种rescoring translation的结果的方法， 用 1）language model 2）bilingual autoencoder 3）bilingual dictionary分别来挑选translation的句子，发现bilingual dictionary来删选翻译的句子效果最好&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="资源" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;资源&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;数据集： WMT2016 (En-Ge)&lt;br/&gt;图像特征提取： VGG， R-CNN&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="实验结果" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;实验结果&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;在En-Ge的结果如图：&lt;br/&gt;&lt;a title="en-ge" rel="gallery0" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmm909CYrpybXBB3vCicafPTibgeI77NWJf95Ehib43c2ibicZ52tYyhBkowIJelGSAWXu7Wlic5UtwN8bQ/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="相关工作" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;相关工作&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;NMT： Kalchbrenner and Blunsom 2013&lt;br/&gt;Attention NMT： Bahdanau 2014&lt;br/&gt;Joint Space Learning： Zhang 2014，Su 2015，Kiros 2014&lt;br/&gt;多模态上相关工作目前并没有很多，值得快速入手&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="简评" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;简评&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;本文提出了一种针对图像和文本结合的神经网络翻译模型，非常自然的将图像特征加入到seq2seq模型的encoder部分，使decoder不仅能够attention在文本上，同时也能够focus到图像上(global或者region)；并且模型的设计比较简单，没有加入太多复杂的模块。&lt;br/&gt;不过只是简单的将图像的特征作为seq中的一个step，并没有考虑文本和图像之间的相关关系，如joint space，相信加入joint learing会有提升。&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="完成人信息" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;完成人信息&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Lijun Wu from SYSU.&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h1 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;strong&gt;Multimodal Attention for Neural Machine Translation&lt;/strong&gt;&lt;/h1&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="作者" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;作者&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Ozan Caglayan, Loïc Barrault, Fethi Bougares&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="单位" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;单位&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;University of Le Mans, Galatasaray University&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="关键词" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;关键词&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;NMT, Attention&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="文章来源" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;文章来源&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;arXiv 2016.09&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="问题" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;给定图片和源语言描述的情况下，基于attention机制,生成目标语言的图片描述。&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="模型" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;模型有两个encoder，一个是textual encoder,是一个双向GRU，用于获取源语言文本的向量表示$A^{txt} = {a^{txt}_1,a^{txt}_2,…}$，另外一个是visual encoder,使用的是现成由ImageNet数据集训好的ResNet-50网络，用于获取图片的向量表示。$A^{im} = {a^{im}_1,a^{im}_2,…}$. Decoder部分，是两层的stakced GRU,先用attention方式，分别获取文本部分和图像部分的context向量$c^{txt}$和$c^{im}$,然后将两个向量concat在一起，作为新的context 向量$c$。&lt;br/&gt;如图：&lt;/p&gt;&lt;p&gt;&lt;a title="mul_attention" rel="gallery0" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmm909CYrpybXBB3vCicafPTwThgdb7WD1jEFIiahORTOkia0IEZwnCTKOfjfXY60byk60Hlwg4B70Jw/640?wx_fmt=jpeg"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;这样decoder部分的解码翻译的时候，不仅可以考虑到源语言的文本信息，也可以考虑到原始图片的信息。&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="资源" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;资源&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;IAPRTC-12 dataset for English and German&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="相关工作" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;相关工作&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;2014年Bahdanau的Neural Machine Translation by Jointly Learning to Align and Translate，使NMT超过了传统的PBMT，后来的NMT论文基本都是在这个文章基础上进行的改进。&lt;br/&gt;2015年Elliott的工作Multi-language image description with neural sequence models. 也是在给定源语言和图片的情况下，生成目标语言。不过并没有使用attention机制。&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="简评" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;简评&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;该文章的创新之处，在于对图片描述文字进行翻译的时候，考虑到了图片本身的特征信息并引入attention机制。在源语言文本生成出错的情况下，因为有图片信息参考，在一定程度上，可以减轻这种错误带来的影响。不过文章并没有利用外部英德平行语料，这可以考虑作为后面的改进方向。&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="完成人信息" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;完成人信息&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;xiaose@mail.ustc.edu.cn&lt;/p&gt;&lt;p&gt;中国科学技术大学&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h1 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;strong&gt;Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot&lt;/strong&gt;&lt;/h1&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="作者" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;作者&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Hideki Nakayama，Noriki Nishida&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="单位" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;单位&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;The University of Tokyo&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="关键词" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;关键词&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;pivot, multimodal, NMT&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="文章来源" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;文章来源&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;arXiv, 2016.11&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="问题" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;在没有平行语料的情况下，用image当作pivot来实现机器翻译&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="模型" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;整体上讲，模型分成两部分。第一部分是多模态embedding，采用pairwise ranking loss来定义损失函数；第二部分是用RNN来实现的decoder,跟image caption里面的decoder类似。对这个问题来说，我们的训练数据包括$i^{s}$：源端的图片，$d^{s}$：源端图片对应的句子描述；$i^{t}$：目标端的图片，$d^{t}$：目标端图片对应的句子描述，和源端用的不一样的语言。文中提出了2个模型来解决这个问题：&lt;br/&gt;&lt;a title="21-1" rel="gallery0" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmm909CYrpybXBB3vCicafPTkW8rSj5Pvet0ZP9dObewEpbVOtJWYu9GpTDnCRVBzOY9Ig8pW20TlA/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;模型1的多模态端包括了图片的encoder和源句子的encoder。图片encoder可以对源图片和目标图片通用。多模态端用$i^{s}$,$d^{s}$进行训练，损失函数为：&lt;/p&gt;&lt;p&gt;&lt;a title="21-2" rel="gallery0" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmm909CYrpybXBB3vCicafPTcWiaNxmzWTY0Jml3EXcgCjj5iato4wfqYibYBqySk6ujXOKYv5K15axYA/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;$E^{v}$表示图片的encoder(比如用VGG-16提取图片的feature), $E^{s}$表示源句子的encoder(比如用RNN)，$d^{s}_{ng}$表示和源端图片不相关的描述。Decoder端用$i^{t}$,$d^{t}$进行训练，损失函数为标准的 cross-entropy loss（称作图片损失):&lt;/p&gt;&lt;p&gt;&lt;a title="21-3" class="" rel="gallery0" style="color: rgb(37, 143, 184); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmm909CYrpybXBB3vCicafPTJY257PqrjW2OIcRQr3nmGzKs0GYQkIGtRSibIOG9hj4dINfuVfY90FA/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;模型2比模型1更复杂一点。在源端增加了一个目标句子描述的encoder。因此，在多模态embedding的学习中，损失函数增加了目标图片和目标图片描述的pairwise ranking loss.&lt;/p&gt;&lt;p&gt;&lt;a title="21-4" rel="gallery0" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmm909CYrpybXBB3vCicafPTFVmcib5e2mdza7nybQVctzDQIPYCXfv4DmibOoZn5gliboc5WibDO2eyfA/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;在decoder的学习中，模型2除了前面的公式2定义的图片损失外，还增加了目标描述的reconstruction loss，即从多模态端输入目标描述，希望通过embedding和decoder重建这个目标描述。&lt;br/&gt;&lt;a title="21-5" rel="gallery0" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmm909CYrpybXBB3vCicafPTgDT95zsMhXLRMR2hicBibmzq7qmVv1yiabg5y65dKp693uV0RGMyUWhXw/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="资源" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;资源&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;两个Multilingual image-description的数据集：IAPR-TC12（包含2万图片以及英语和德语的描述）和 Multi30K（包含3万图片以及英语和德语的描述)&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="相关工作" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;相关工作&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;对于没有平行语料的机器翻译，多数文章是用某种常见语言作为pivot，比如“Neural Machine Translation with Pivot Languages”, 用英语作为西班牙语法语以及德语法语之间的pivot。缺点是翻译的时候还是要经过pivot那一步。 另外，还要一些工作是用一个模型实现many to many的翻译。在这种情况下，没有平行语料的语言对也能用这个模型进行翻译。不需要经过pivot那个中间层，但是效果一般会差一点。比如“Google’s Multilingual Neural Machine Translation System”这篇文章。&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="简评" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;简评&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;这篇文章的思路很新颖，考虑用图片来作为pivot，实现没有平行语料的语言对之间的翻译。训练完成后可以直接从源语言到目标语言进行翻译，不需要经过图片。但是正如文中提到的，这种方法跟有语料训练出来的翻译效果比起来还是差很多，并且翻译的句子都比较短。另外，对一些图片难以表达的信息很难通过这种方式学到。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;完成人信息&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;yun.chencreek@gmail.com&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;&lt;h1 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="总结" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;span&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;交叉领域的研究总是会带给大家惊喜，交叉领域的交叉领域更是如此，这个领域刚刚开坑，欢迎各位有志之士跳坑。并且在2016年举办了第一届多模态机器翻译（Multimodal Machine Translation）和多语看图说话（Crosslingual Image Description）比赛，比赛主页&lt;a target="_blank" rel="external" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;http://www.statmt.org/wmt16/multimodal-task.html&lt;/a&gt;, 总结性的paper&amp;nbsp;&lt;a target="_blank" rel="external" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;http://anthology.aclweb.org/W/W16/W16-2346.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;扫码下载本期paper&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmm909CYrpybXBB3vCicafPTCCVAYsuNMVd4p8QiaJ3gBZQjeAogHTh88GT7PRe0nDicYoWyonwfWO9w/640?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;相关阅读&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1、&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720765&amp;amp;idx=5&amp;amp;sn=1389fc0efe57457a0e167f7215ffc6d4&amp;amp;chksm=871b0d83b06c8495b2ef85f3746a5518211edd0dc45bb8322bad32fbc197d1bfe639cac390f8&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720765&amp;amp;idx=5&amp;amp;sn=1389fc0efe57457a0e167f7215ffc6d4&amp;amp;chksm=871b0d83b06c8495b2ef85f3746a5518211edd0dc45bb8322bad32fbc197d1bfe639cac390f8&amp;amp;scene=21#wechat_redirect"&gt;PaperWeekly 第十五期---Attention模型在NMT任务中的应用和进展&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2、&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719529&amp;amp;idx=5&amp;amp;sn=c0163856784320377a52fbc18ba5ef7c&amp;amp;chksm=871b0157b06c8841f53629e5fb1e4499703b684862fa619acf2b36ce0c887c2fd1b42e4a60e3&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719529&amp;amp;idx=5&amp;amp;sn=c0163856784320377a52fbc18ba5ef7c&amp;amp;chksm=871b0157b06c8841f53629e5fb1e4499703b684862fa619acf2b36ce0c887c2fd1b42e4a60e3&amp;amp;scene=21#wechat_redirect"&gt;PaperWeekly 第七期&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;关于PaperWeekly&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;PaperWeekly是一个分享知识和交流学问的学术组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;微信公众号：PaperWeekly&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgkSMlEJFo90NY8rCXr3mJMBibduVHxMKSHzQtVkHz8kNwpjCKBiccGuqLE0WpPuAbdtEs6cTF5iabpAQ/640?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;微博账号：PaperWeekly（&lt;a target="_blank" rel="external" style="color: rgb(67, 149, 245); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;http://weibo.com/u/2678093863&lt;/a&gt;&amp;nbsp;）&lt;br/&gt;微信交流群：微信+ zhangjun168305（请备注：加群交流或参与写paper note）&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 15 Jan 2017 12:11:12 +0800</pubDate>
    </item>
    <item>
      <title>重磅 | 微软收购NLP明星公司Maluuba，Bengio将成为微软顾问</title>
      <link>http://www.iwgc.cn/link/4336298</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;机器之心报道&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：微胖、李泽南、朱思颖&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote style="color: rgb(62, 62, 62); font-size: 16px; white-space: normal; max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;2017 年 1 月 13 日，微软宣布收购加拿大初创公司 Maluuba。Maluuba 由加滑铁卢大学毕业生 Kaheer Suleman 和 Sam Pasupalak 所创，关注服务于通用人工智能的自然语言处理研究。在此次收购中，担任 Maluuba 顾问的 Yoshua Bengio 也同时与微软达成了协议，进而成为微软的顾问。目前该交易的金额尚未披露。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vVyKicyjQ1yA6SicZOZvAA1TMa0dZdE0UkFIfibUen3VDKkiaibIchiaBtR3FRTn2svE8aXEDEkP2SOMA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;公司背景&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;加拿大滑铁卢大学（University of Waterloo）以计算机学科闻名，2011 年 8 月 18 日，学校在读研究生 Kaheer Suleman 发明了一款智能程序，取名 Maluuba。同年，他与几位同学创立了 Maluuba 公司，他们最初的想法是做一款智能语音旅行工具，用户可以通过语音搜寻航班。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2012 年 2 月，Maluuba 从三星风投获得 200 万美元种子轮投资。半年后，他们的第一款产品出现在公众视野中，这款程序能把用户的语音请求转化为有用的信息或行动。虽然不少媒体将之称为「Android 平台的 Siri」，但 Maluuba 的初衷却是要挑战 Siri，而后续发展也表明，Maluuba 的确比 Siri 更出色。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Maluuba 通过绑定第三方服务来实现语音助手功能。2012 年 11 月，公司对外发布了自然语音处理 API 接口，移动开发者可以在自己的应用中添加类似 Siri 的语音处理功能。2012 年 12 月份推出了语音购物功能，用户可以通过语音进行购物。在语音助手领域，Maluuba 的步伐比较快。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2013 年，随着越来越多消费类电子产品公司和设备制造商乐于将一些新技术融入自己产品，Maluuba 也加快了与智能手机、电视、自动驾驶汽车等公司的合作。比如，LG 旗舰 G 系列手机的 voicemate 应用就采用了 Maluuba 的技术。2013 年 2 月，Maluuba 正式宣布向 Windows Phone 平台迁移。Maluuba 的 Windows Phone 8 版本拥有 Android 版本的大部分功能，例如可以搜索餐馆、影院、新闻和企业（以及进行语音购物）、设置闹钟、提醒和会议安排、打电话、发短信和邮件、指示方向和天气，甚至还集成了 Outlook 日历。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Maluuba 最初愿景是想让机器拥有人类水平的理解力。人工智能面临的重大挑战之一就是那些缺乏大规模标记数据集的领域，或者难以对相关环境进行较好模拟的领域。语言就是一个很好例子。互联网上包含有无穷无尽的网页，但上面全都是文字，没有一个地方找得到以机器能够理解的形式所书写的关于这些文字意思的内容。因此，机器学会阅读将是人工智能在处理和理解人类语言进程中一个里程碑式事件，也是一个真正人工智能必须达到的标准。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不过，Maluuba 成立之初就认识到，深度强化学习的基础研究和技术成熟尚需时日。2014 年，时机趋于成熟。标志性事件就是 DeepMind 采用了深度学习技术的人工智能程序在无需监督的情况下，就可以掌握多种电子游戏。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2015 年 8 月，Maluuba 融资 900 万加元（A 轮）用于进一步推进深度学习研究。同年 12 月，Maluuba 在深度学习重镇——加拿大蒙特利尔开设了一个新的研发实验室（有 13 名深度学习研究人员，负责人是公司 CTO Kaheer Suleman）。Maluuba 关注机器学习中的两个细分研究领域：对话和机器理解。同时，像 Maluuba 也更关心研发解决通用问题的人工智能，对解决真实世界问题更感兴趣。他们相信自己能找出更好的人机交互方式，并与蒙特利尔大学教授、人工智能专家 Yoshua Bengio 和阿尔伯塔大学教授、强化学习专家 Richard Sutton 等展开合作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前，全世界已有超过 5000 万台移动电子设备（比如，智能手机、自动驾驶汽车等）采用了 Maluuba 的自然语言处理服务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;重要成果：EpiReader&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2016 年 3 月，Maluuba 登上新闻头条。研究人员发布论文（http://arxiv.org/abs/1603.08884v1），介绍了他们最新的进展。论文描述了一个能够阅读几百个童话故事的算法。训练结束后，该算法可以正确地回答算法并不熟悉文本的多选题，准确率超过 70%。研究人员还在《哈利波特和魔法石》上进行测试，该算法能够以近似的准确率回答相关文本问题。这一成绩超过当时最好的神经网络方法 15%，也比当时最好的特征工程解决方案好 2%。Yoshua Bengio 说：「从数字上看，这是一次大的飞跃。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4 月，Maluuba 将一段技术演示视频放上了 YouTube，视频中的人工智能机器人 Marcy 在阅读了第五季《权利的游戏》梗概后，马上领会了故事的复杂情节。好比对这部美剧一无所知的普通人在简单阅读维基百科剧情介绍后，立刻弄懂了整个故事。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;iframe class="video_iframe" data-vidtype="1" allowfullscreen="" frameborder="0" height="417" width="556" data-src="https://v.qq.com/iframe/preview.html?vid=c0317lhzz68&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0"&gt;&lt;/iframe&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;技术演示表明，Maluuba 已经可以处理大批量的文字数据，并且能回答更加复杂困难的开放性问题了。在机器学习和人工智能领域，这是一个巨大的突破。Maluuba 的产品副总裁 Mohamed Musbah 表示：「人们在未来的几个月中会看到一些非常有趣的事情。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2016 年 6 月 7 日，Maluuba 在 arXiv 上发表了一篇论文《Natural Language Comprehension with the EpiReader》（arXiv:1606.02270），介绍了一种全新的机器文本理解模型 EpiReader。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在之前机器之心的专访中，Maluuba 介绍说，EpiReader 采取两个步骤来确定问题答案。第一步 (Extractor), 我们使用了一个双向 GPU 逐字阅读故事和问题，接着采用一种类似 Pointer Network 中的 Attention 机制在故事中挑选出可能作为答案备选的单词。第二步 ( Reasoner )，这些备选答案被插入「完型填空」式的问题中，构成一些「假设」，接着卷积神经网络会将每个假设与故事中的每个句子加以比较，寻找文本蕴涵 ( Textual Entailment ) 关系。简单来说, 蕴涵是指，两个陈述具有很强的相关性。因此，最近似故事假设的蕴涵得分最高。最后，将蕴涵得分与第一步得到的分数相结合，给出每一个备选答案正确的概率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最近，Maluuba 发布了一篇新的技术博客，介绍他们在通用人工智能上的研究（参阅《&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650722208&amp;amp;idx=2&amp;amp;sn=0a123e1f1c922ac6fcdf502c84ceb88d&amp;amp;chksm=871b0bdeb06c82c85dfdd9b0f6a33d46ff10081d4c1f123b390eee8300c4ef95f29a49875dc8&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650722208&amp;amp;idx=2&amp;amp;sn=0a123e1f1c922ac6fcdf502c84ceb88d&amp;amp;chksm=871b0bdeb06c82c85dfdd9b0f6a33d46ff10081d4c1f123b390eee8300c4ef95f29a49875dc8&amp;amp;scene=21#wechat_redirect"&gt;构建好奇的机器，Maluuba 的通用人工智能探索（附论文）&lt;/a&gt;》）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;与微软合作的原因：通用人工智能&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Maluuba 在官方博客上解释道：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;到目前为止，我们关注的领域是机器阅读理解，对话和理解以及通用（人类）智能，比如记忆、常识推理以及资讯搜寻行为。这些领域的早期研究成果加快了我们扩展团队的需求，显然，我们需要用重要资源来支持我们的团队以推进终极目标的实现。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;微软非常契合我们的公司。微软大众化人工智能的雄心让这个星球上每个人和组织与人工智能技术使用方式从根本上保持一致。微软为我们提供了将我们的研究传递给百万个人用户和公司用户的机会，他们可以从真正智能机器的出现中受益良多。另外，微软庞大的技术资源——包括后端基础架构（如微软Azure和其完备的硬件基础设施）以及工程人员将帮助我们加速研究和提供市场解决方案的步伐。简言之，我们的新拍档能让我们更加快速的走向当初的愿景。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软也表示，Maluuba 在深度学习和强化学习方面的专业知识将帮助我们解决问题和决策系统推进我们的人工智能民主化策略，并使其可以为每个人服务——所有消费者，企业和开发者。随着最近微软在语音识别和图像识别上使用深度学习技术的巨大成果，以及今天来自 Maluuba 成员的新力量，公司相信「更好的还在后面，我们将向机器阅读和写作发起新的进攻。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此外，不久之前微软在其官方博客上开放了一个包含 10 万个问题和答案的数据集 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650721463&amp;amp;idx=1&amp;amp;sn=6ff5c103c36e10af4362bda180c834ca&amp;amp;chksm=871b08c9b06c81dfdec9851515c91d2700719c9bdad65a3b7c831e70dd2c0978879a86d4e7f5&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650721463&amp;amp;idx=1&amp;amp;sn=6ff5c103c36e10af4362bda180c834ca&amp;amp;chksm=871b08c9b06c81dfdec9851515c91d2700719c9bdad65a3b7c831e70dd2c0978879a86d4e7f5&amp;amp;scene=21#wechat_redirect"&gt;MS MARCO&lt;/a&gt;。通过将数据集免费开放给领域内更多的研究者，微软希望能够推进机器阅读领域的突破性研究。这个开源数据集的负责人 Rangan Majumder 曾说，「为了实现人工通用智能的目标，我们首先需要机器能够像人类一样阅读和理解文档。这个数据集是向这个方向迈出的一步。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软的长期目标一直是通用人工智能，Maluuba 的研究能够助力微软实现这一宏伟目标。优势互补，微软收购 Maluuba 也就不足为奇了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;蒙特利尔的人工智能领域地位正在获得认可&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这次收购表明，蒙特利尔在人工智能领域的重要地位最近正在逐渐被全球各大公司认可。在微软之前，谷歌曾在 2016 年 11 月宣布在蒙特利尔成立一个新的人工智能研究机构，并对该市的几所大学进行了投资。值得一提的是，谷歌在这一动作中试图拉拢的 Yoshua Bengio——Maluuba 的顾问也随着这次收购与微软产生了联系。在所有大公司都在争抢人工智能领域人才的环境中，微软的努力或许另有深意。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Maluuba 表示，「没有 Yoshua Bengio 教授（深度学习创始人之一）、Richard Sutton（最重要的强化学习先驱）以及蒙特利尔日益壮大的研究生态圈的巨大帮助，我们无法走到今天。特别是，Bengion 教授为我们的研究人员的研究工作提供了非常宝贵的常规咨询和指导。过去几年中，Bengio 教授也因为他的远见——将蒙特利尔打造成人工智能研究的核心而得到了特别认可。通过蒙特利尔大学和麦克吉尔大学的研究，这座城市已经发展成为世界上最大的深度学习学术中心；现在，这个区域大学中有大约 150 深度学习研究人员。加拿大的学院、公司以及创业公司生态系统正为人工智能领域带来巨大创新，证明加拿大，特别是蒙特利尔能够与硅谷试比高下。在这一新的篇章里，我们会继续积极地与蒙特利尔以及发表世界顶级人工智能研究的学术社区合作。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;成功的秘诀&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;前一段时间，在接受机器之心专访时，Maluuba 给其他人工智能创业者和研究人员给出的一些建议或许可以作为公司迈入今天这一新篇章的重要经验之一。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;人工智能方面。我想说，此时创业正当时，也很让人兴奋，因为这里需要解决很多令人激动的问题，这个行业已经到了这样一个阶段：我们正处在解决这些问题的前沿，而且公司非常高兴支持真正的创业者来解决这些问题，无论是资金上还是策略能力上。现在成为这个领域的创业者，很让人激动。&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;br/&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;但是，我想提醒需要注意的几点。&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;br/&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;首先，区分事实和虚构。很多人工智能领域里的信息都过分夸张了，因为对现状缺乏基本了解，而且对人工智能持过于兴奋态度也源于人类本性。区分事实和虚构能帮助你真正理解自己所处的位置，帮助你准确定位所要解决的问题。&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;br/&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;第二点就是挑选别人没有解决过的独特问题，然后试着如何用人工智能加以解决，看看自己解决的情况如何，和别人有什么不同。我认为几年后，这个领域的公司就要比拼：看谁能利用最先进的技术做出没有人想到新产品，解决别人没有解决的问题。&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;援引 T.S. Eliot 一句话作为本文的结束：「In our end is our beginning.」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心原创，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 14 Jan 2017 11:05:11 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 麦肯锡发布自动化未来报告：深度预测其对就业和市场的影响（附报告）</title>
      <link>http://www.iwgc.cn/link/4336299</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自麦肯锡&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀、李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;自动化正在发生，并且必将给全世界的商业和经济带来巨大的好处，但全面的自动化不是朝夕之间就能实现的。近日，麦肯锡全球研究所（McKinsey Global Institute）发布了一份报告《A FUTURE THAT WORKS: AUTOMATION, EMPLOYMENT, AND PRODUCTIVITY》，该报告表示如果要实现自动化的全部潜力，就需要人们和技术携手并进。机器之心在此对这份报告进行了简短的介绍，报告原文请点击文末「阅读原文」下载。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;近来在机器人、人工智能和机器学习领域的进步将我们推到了新的自动化时代的边缘。现在的机器人和计算机不仅能在很多体力工作上比人类做到更好更廉价，而且它们也正越来越胜任需要认知能力的任务，比如进行判断和决策、感知运动、甚至驾驶汽车。自动化将会改变我们每一个人的日常生活，不管你是矿工还是园艺师，还是银行家、时尚设计师、焊工或是 CEO。但自动化还需要多久才会成为工作领域的现实？它们对全球经济中的就业和生产又会有什么影响呢？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;麦肯锡全球研究所（MGI）正在进行一场持续的关于自动化技术及其影响的调研项目，该项目今天发布了一份新报告《Automation, employment, and productivity》，这里简单介绍一下其中的几个关键发现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自动化可以通过减少错误和提升质量与速度来帮助企业提升效益，并且能在一些情况下实现超越人类水平的收益。自动化也有助于提升生产力，正如我们在历史中已经见证过的一样。当生产力增长乏力时，这就能为经济的增长和繁荣提供必需的推动力。它也将有助于抵消许多国家劳动人口比例下降的影响。基于我们的情景建模（scenario modeling），我们估计自动化每年将能给全球生产力带来 0.8% 到 1.4% 的增长。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;分析自动化影响的合适程度是分析某种职业中的单个活动，而不是直接分析整个职业。每一种职业都包含了很多种活动，每一种都有不同的自动化需求。对于目前已有的技术，只有很少的职业（少于 5%）有可能实现完全自动化。但是，几乎每一种职业都有可能实现部分自动化，即其中的一部分活动是可以自动化的。我们估计全世界工作场景中的大约一半的活动都可能通过现在已有的技术实现自动化。这大约相当于 16 万亿美元的工资。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最有可能被自动化的活动是在高结构化和可预测环境中的体力活动，以及数据的收集和处理。在美国，这些活动占据了经济中总活动的 51%，大约相当于 2.7 万亿美元工资。它们在制造业、住宿和食品服务、零售领域最为普遍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而且并不只是低技能、低工资的工作会被自动化；中等技能和高收入、高技能的工作也有一定程度的自动化潜力。随着人们工作流程中的单个活动被自动化所变革，人的工作将会成为机器工作的补充；反之亦然。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管如此，自动化并不是一朝一夕就能实现的。尽管存在这样的技术潜力，但我们估计要让自动化对当前工作活动的影响完全显现，可能还需要很多年的时间。自动化的速度及其对工作者的影响将会因活动、职业、工资和技能水平的差异而有很大的不同。决定自动化的速度和影响程度的因素包括：技术能力、技术成本、与劳动力的竞争（包括技能和供需动态）、效益增益（包括但不限于劳动力成本节省）、社会和监管的接受度。我们的情景模型表明今天我们半数的活动将在 2055 年被自动化，但因为各种影响因素和经济状况的不确定性，这个时间可能会早 20 年，也可能会晚 20 年。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在宏观层面上，自动化的影响可能会很缓慢，比如在整个行业或经济内的影响。但在微观层面上，自动化的影响是相当的快，因为个人工作者的工作可能会被自动化，或者是公司被使用自动化的竞争对手所摧毁。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然目前大部分关于自动化的争论在于它会造成大规模的失业，人们还是会继续与机器一起工作，从而促进人均国内生产总值的增长，这是全球每个国家都在追求的。因此，我们对生产力的评估是假设被自动化取代的人们会找到其他的职位。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大多工人必须要做出改变，而且我们预期业务流程也会发生转变。然而，自动化技术能造成的劳动力的转换（shift）规模在过去几十年中并非史无前例的。它的规模类似于 21 世纪，科技在发展中国家农业劳动力上造成的长期转换的级别。这些转换并没有造成长期的大规模失业，因为它伴随着新类型工作的创造。但我们的分析表明，人类劳动力还是所需要的：我们评估所能获得的全部生产力是只有在人类与机器一起工作的情况下得到的。这反过来将会转变劳动力，需求工作人员与技术有新的配合。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 14 Jan 2017 11:05:11 +0800</pubDate>
    </item>
    <item>
      <title>专栏 | 人工智能硬件不能遗忘的4S机遇</title>
      <link>http://www.iwgc.cn/link/4336300</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;机器之心专栏&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：痴笑&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;2017 年 1 月 11 日至 12 日，首届 AI Frontier 大会在加州硅谷 Santa Clara 召开。主办方 Silicon Valley AI and Big Data Association 邀请 Google Brain 负责人 Jeff Dean，Microsoft 人工智能首席科学家 Li Deng，Amazon Alexa 首席科学家 Nikko Storm，Facebook-Caffe 创始人 Yangqing Jia 等人工智能专家以及多家人工智能初创公司展开热烈的讨论。矽说 携手 机器之心，进驻湾区硅谷核心区域，发回了「独家」场记。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;十年前，初创公司都是 e 开头的&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那是互联网的时代&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;五年前，初创公司都是 i 开头的&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那是移动终端的时代&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今年，初创公司都是 N 开头的&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那是人工智能的时代&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;N 是什么？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Neural Network&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;身处人工智能大潮，矽说在过去的一段时间在人工智能硬件领域，携手机器之心，多次发表评论及《脑芯编》系列。今天，在观摩了 AI frontier 后，矽说小编再次指点江山，人工智能硬件发展过程中容易被忽略的四大决胜关键趋势。（个人观点，仅供参考）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Scalable 规模化扩展&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;几乎每一个讲者都会提及 Scalablity，从框架到实现。无论是 Tensorflow 还是 Caffe，各个框架平台都需要考虑面向不同硬件运行深度学习网络的完整性、一致性。而那些对于需要实现成实际产品的工程，比如自动驾驶（Google Waymo 等）、语音助手（Amazon Alexa 等），高效的选择硬件规模更是一个需要攻关的主要难题。可规模化扩展的能力 (Scalable）成为每个能广为使用的人工智能实现的铲平必须具备的特性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vVyKicyjQ1yA6SicZOZvAA1FmUIibLRb2dcD4QkRVHibz59Tvg829L9X9ia1jE5M1CDglujzmib5NH2mQ/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vVyKicyjQ1yA6SicZOZvAA1crcqibBVXAZ1NxKZjjAEEn5dvPnuXSP2ibjsfk0owtoWNeRpesR2l1rQ/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vVyKicyjQ1yA6SicZOZvAA1FXhfKuVz9G8DRwaibDgBxGQUqesOzmibcR2qG1D2PjcbejpMSDI3v3aA/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vVyKicyjQ1yA6SicZOZvAA13XHXORFVny8Mlqjuc7n5vE3P8pKssgnaNLkRYx1avwyEym6p9zMKHA/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;		&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Reference: Y. Jia, Caffe Talk&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其实，归根结底，Scalabe 是因为硬件总是有限的。过去几年，人工智能网络已经从 AlexNet、VGG-19 的浅层网络的迅速成长成超深层的 ResNet，以及多个网络相辅相成的生成对抗式网络（Generative Adversarial Network）。虽然，以 NVdia 为首的硬件公司也在不断突破，但是其增长速度恐怕无法与目前的神经网络相媲美。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，对于每一个深度学习神经网络的实现，在每一个层次实现时，除了计算的高效性外，将大规模网络裂解（partition）并且映射（mapping）到有限的硬件上就成为了一个踏踏实实的问题。表现在接口设计、数据结构的选择等各个层次。而只有那些可以规模化扩展的人工智能硬件，才能在算法日新月异的今天立于不败之地。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Sensable 传感，也要智能&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你还觉得传感器和人工智能计算是两个完全独立的模块的，恭喜你，你已经被时代抛弃了。大会上，北京文安与 Bosch 都秀出了新一代的图像传感/VR 传感器。毫无例外的，人工智能算法已经与传感器拥抱，同时出现在前端。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vVyKicyjQ1yA6SicZOZvAA1EwW2c6zvjt0NBgtFxHV5xavPQg8OOwSFicXcBHRrv8Fz9O1SrP3406g/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;		&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Reference: R. Liu, Bosch Sensor Talk&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;传感器，作为实时推理（Real-time Inference）的最前端，是模拟信号与数字化的人工智能算法间的桥梁。他可以是摄像头，可以是 lidar/radar，可以是音频，林林总总。当信号越原始，越早地进入神经网络处理，他就越容易带来意想不到的效果。紧密拥抱传感器的 AI 算法已经在 end-to-end Learning 领域迸发出了不一样的火花。未来的人工智能硬件，将不可逆地与传感器结合，成为 IoT 时代的大趋势。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Sythesizable 综和再生，机器变成人&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;每天醒来，你唤的第一个名字叫做 Alexa，每天睡去，最后一声念叨的是 Siri，基于人工智能的语音综合（speech synthesis）已经成为了我们生活中不可缺少的一部分。甚至，未来的骚扰、诈骗电话都会用到 AI Sythesize 技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vVyKicyjQ1yA6SicZOZvAA1cibQntAMGJibicTeBBC4qgQpLwuicL4QWhppfiaDr7elEUjTzsYuseNx0AQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;		&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Reference: L. Deng, Microsoft Chatbot Talk&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;可以被综合的难道仅仅是语音么？Chatbot 们已经学会用自然语言调戏人类，新一代美颜滤镜会造出以一个个印象派的蒙娜丽莎，自动驾驶的车辆们，直接生成方向盘指令。基于 Policy Evaluation 的增强学习（Reinforcement Learning）已经革多少人的命？更勿论，生成对抗学习的大 boss 已经磨刀霍霍向猪羊。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以硬件创业者们，classification/inference 的年代早已随岁月顺水流，未来的硬件的机遇已经超越了理解人类的范畴。这将是一个机器能与人沟通的年代。以综合（synthesis）为目标的人工智能硬件正向我们走来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Savable 从头开始，你就输了&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;总是改变生态的 G 家大神总在你意想不到的给大家送福利。比如正式出版的 Tensorflow 1.0 提供了不少基于图片识别的预训练（pre-trained）网络，基于 Inception 结构。对于很多智能开发者而言，只需要根据实际情况，对最后一到两层的全链接（Fully connected layer）训练下，就能达到玩美智能的效果。高效、省时、省力、省硬件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;同样的故事发生在 Baidu 和 Amazon 的语音识别系统中。大家不约而同的发现，对于不同语言的识别系统，其实只要训练输出层（output layer）就能得到相近的结果。而省去 RNN/CNN 层训练就好像是时间的馈赠，来自上天的礼物。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vVyKicyjQ1yA6SicZOZvAA1Aibdib3P7ggca6ZQ3gPrN1WxDuicItkO7LkWRnthhDMs8icowdgSP3tiaOQ/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vVyKicyjQ1yA6SicZOZvAA1SLJ8MmiaZy2MMj3iaTt2ABtjj5uBumn8nxh822lQI5e4TGCP9MicticDPw/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Reference: N. Storm, Amazon Alexa Talk&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这样的馈赠何尝只是算法层面的专属？首先，可节省（savable）的实现方法让硬件在合理安排存储空间和可配置维度上带来的新的启示。同时，若硬件的可重复使用率提升到了新的高度，那将对云端/终端的 AI 计算资源配置比关系提出一轮新的思考与挑战。其次，面向成熟体系框架 framework 的硬件，对于客户和使用者来说，本身就是一种节省。如果有合适的 scalable 接口和 complier，在 inference 端的映射也将朝向更简约、更低门槛的方式前进。只有充分体会到这一点，完善与框架的接口的设计，才终将改变 AI 硬件的大潮流。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;总结：感谢 AI frontier，让小编能从一个更宏观的角度去仰视人工智能算法的大趋势，也凝练四大智能硬件的重要关键——Scalable，Sensable, Synthesizable and Savable。最后，预祝第二届 AI Frontier 顺利召开。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心专栏文章，&lt;strong&gt;&lt;em style="max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 14 Jan 2017 11:05:11 +0800</pubDate>
    </item>
    <item>
      <title>报名 | 新加坡医疗机器人黑客马拉松</title>
      <link>http://www.iwgc.cn/link/4336301</link>
      <description>&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vVyKicyjQ1yA6SicZOZvAA1CyfUczerYgaMWVicZo5eFbT34rULzgNygoSoYW5fD6fuW8mwNAO75Qg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Calling developers, clinicians, engineers, designers, researchers and business managers to join MIT Hacking Medicine for the first hackathon in Asia focused on social robotics for eldercare.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;On February 10 - 12, 2017, SGInnovate will host a weekend of hacking, brainstorming and building innovative solutions with engineers, clinicians, designers, developers, researchers and business people.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vVyKicyjQ1yA6SicZOZvAA1vHWZk2pb8hboYGJ43OicwiaqmqISscicBDTDT2pgkvNNQzHgg8G8u1TWw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;The MIT Hacking Medicine team will lead participants to build software and hardware applications on the Segway Robot platform to improve the lives of the elderly. During the hack, you’ll be able to create robot assistants capable of understanding and engaging with elderly and patients with conditions such as Alzheimer's and others.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;The Segway Robot was awarded Best Robot Butler at the Consumer Electronics Show and praised as “one of the most appealing robot concepts in the world” by prestigious tech media including Engadget. At the hackathon, you will have the opportunity to try out the Alpha Developer Edition robot, which is currently available to a very selective number of developer partners around the world.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Sound exciting? Sign up to participate!&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Application Closing Date: February 3, 2017, Friday&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Developers, Clinicians, and Entrepreneurs are strongly encouraged to apply soon!&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;The objective of the hackathon is to create and grow a community of innovators around personal robotics for healthcare, mental healthcare and eldercare applications. If you belong to one of the following categories and have a passion for creating impactful solutions for patients and the elderly, then this hackathon is for you.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Healthcare providers: doctors, nurses, allied healthcare providers, etc. – You work with patients on a daily basis and know what’s needed most.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Developers: hardware and software engineers (knowledge and experiences of Android development preferred) – You can improve the lives of patients and the elderly by developing solutions together with clinicians and business managers.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Businesswomen and businessmen – You know what’s needed for a new startup to succeed.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Not a developer? Don’t fret. Each team at the hackathon must be composed of passionate people from the above-mentioned three categories. Teams will form during the hackathon event.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;You will be able to use advances in AI, robotics and gaming to address important issues in healthcare, not at an incremental level, but at a significant impact level. You will use these advances to address significant healthcare problems both in Singapore and the world. These may include teaching autistic children, monitoring and interacting with the elderly in group settings or as a sole companion, evaluating patient’s activities.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;The winning teams will get the opportunity to go on stage at MIT Technology Review’s EmTech Asia - the conference on the emerging technologies that matter - and highlight the most innovative solutions developed during the hackathon.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;On top of contributing towards Singapore’s Smart Nation building efforts, follow on funding for the winning team are up for grabs, as well as access to prototyping facilities labs and the Segway Robot Developer Edition platform available for the winning top three teams.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Robotic platform &amp;amp; tools available at the Hackathon&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vVyKicyjQ1yA6SicZOZvAA1dC5lCotGtXMUhlQXSIANdlowQ6iaFN5MPpAZQLDPOhH2oNRvuHicRXBQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Segway Robot will provide 5 alpha robots for the Hackathon teams to use during the Hackathon with full support during the event. Google Cloud apps will also be available to developers.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Participating Developers, please note:&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;There will be a training session on January 17, 2017 from 9AM-1PM with the Segway Robotics and MIT Hacking Medicine teams @ SGInnovate. This event is strongly recommended for all participating developers. You will get a chance to get familiar with the Segway Robot development package and ask questions to the teams behind the event so that, come February 10, you are ready to hack! Food will be provided at this event.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Find out more about the Hackathon at:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Website: http://hackingmedicine.mit.edu/hacking-robotics-singapore&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;FAQ: http://hackingmedicine.mit.edu/hacking-robotics-singapore/faq/&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Email: hacking.med@sginnovate.com&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vVyKicyjQ1yA6SicZOZvAA1UC5wVibQfCiaWTL6g6yIuU1UUgDs04A78CN85ngBhPflCsaPZ9xnVPBw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vVyKicyjQ1yA6SicZOZvAA1nV6ejyBxawNKIH1UWnrtow1eSVjBlwF51WCXZLIquOkCibVzvNgqrIg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8vVyKicyjQ1yA6SicZOZvAA1HxiaR2kmNL7l38tEYqOichnYCawDjywUvCrxFE3SOyohKkcehD1lNL5g/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 14 Jan 2017 11:05:11 +0800</pubDate>
    </item>
    <item>
      <title>重磅 | Jeff Dean撰文回顾谷歌大脑2016：从研究到应用的重大进展</title>
      <link>http://www.iwgc.cn/link/4321306</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Google Research Blog&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;谷歌大脑负责人 Jeff Dean 在当地时间 1 月 11 日于硅谷人工智能前沿大会 AI Frontiers 现场对谷歌大脑中的深度学习相关工作进行了概述和回顾。会后 Jeff Dean 告诉机器之心，次日谷歌博客将发表一篇与演讲主题类似的总结性的文章，对谷歌大脑过去的工作做出更加系统的回顾。本文后还附有本文涉及的部分资源的汇集以及机器之心的相关报道。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌大脑（Google Brain）团队的长期目标是创造更智能的软件与系统，从而改善人的生活，我们通过在多种不同的领域中纯科研的和应用型的研究追求这一点。鉴于这明显是一个长期的目标，我们想回过头看看过去一年中谷歌大脑团队已经取得的进展，并分享一些我们的 2017 愿景。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;研究成果发表&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;评判研究质量的一种重要途径就是看我们在 ICML、NIPS 和 ICLR 这样的国际顶级机器学习会议上发表的论文。去年，这些大会一共接受了我们团队提交的 27 篇论文，这些论文的涵盖范围非常广，包括程序合成（program synthesis）、网络之间的知识迁移、机器学习模型的分布式训练、语言生成模型、机器人的无监督学习、自动定理证明（automated theorem proving）、对神经网络的更好理解以及改进强化学习的算法等等。另外，我们还有一些论文被其他领域的一些大会所接受，例如自然语言处理领域的 ACL 和 CoNNL 大会、语音领域的 ICASSP 大会、机器人学领域的 ISER 大会，计算机系统领域的 OSDI 大会。我们的团队向即将到来的前沿深度学习研究会议 ICLR 2017 提交了 34 篇论文。你可以通过 https://research.google.com/pubs/BrainTeam.html 了解我们以往发表的论文。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;自然语言理解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让机器能够更好地理解人类语言是我们研究的关键环节。在 2014 年年末，三位谷歌大脑团队的研究者发表了一篇题为《使用神经网络的序列到序列学习》（Sequence to Sequence Learning with Neural Networks）的论文，展示了这种方法在机器翻译中的可用性。2015 年，我们表明这种方法同样还可以应用在生成图像标题、分析句子和解决计算几何问题当中。2016 年，之前的研究（再加上很多的改进）达到了顶峰，谷歌大脑团队的成员们和谷歌翻译团队的成员密切合作，将谷歌翻译的翻译算法用一种完全端对端的学习系统所取代。这个新系统将旧系统和人类高质量翻译对一些语言对的翻译差距缩小了 85％。几个星期后，我们展示了系统如何做「zero-shot 翻译」，学习翻译那些它从未见过的样本句子对的语言。此系统目前已被部署在了谷歌翻译服务中，其所能处理的语言对的数量也越来越多；从而能为我们的用户提供更高质量的翻译，并让人们能够跨越语言障碍更有效地进行沟通。Gideon Lewis-Kraus 在 2016 年 12 月《纽约时报杂志》的《人工智能的觉醒（The Great A.I. Awakening (http://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html)）》一文中记录了谷歌翻译的历程（同时也记录了深度学习和谷歌大脑团队的历史）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;机器人&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;传统机器人的控制算法是精心设计并人工编程而完成的，因此将新的能力嵌入到原有机器人中是一个非常费力的过程。我们相信机器人通过机器学习自动学习获取新技能是一个更好的办法。在 2016 年，我们与 X 的研究团队合作，演示了机械臂是如何学习手眼协调的，我们的机器人在这项研究中大概进行了 80 万次的抓取试验，它们自己汇集经验然后教自己如何更快地完成任务。随后，我们研究挖据了机器人学习新技能的三种方式：通过强化学习（reinforcement learning）、通过它们自己与物体的交互以及通过人类的演示进行学习新技能。我们将在这项工作的基础上继续完成我们的目标，即让机器人能在纷繁复杂的现实世界环境中灵活地学习新任务技能与操作方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;医疗保健&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们对机器学习增强执业医师诊断能力的潜力感到十分兴奋。作为这种潜力的一个示例，我们在美国医疗学会学报（Journal of the American Medical Association /JAMA）上发表了一篇论文《Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs》，并证明了用于图像诊断视网膜糖尿病视网膜病变（diabetic retinopathy）的机器学习系统可以与职业认证的眼科医生达到同等的程度。如果早期糖尿病性视网膜病变没有检查出，现如今将有 4 亿人口存在失明的风险。但是在许多国家，职业眼科医生的数量太少从而不能进行必要的检查，而这个技术将能帮助确保更多的人接受适当的检查。同时，我们还在其他的医疗影像领域内做一些研究，并调查使用机器学习预测其他医疗任务的潜力。我们相信机器学习不论是从质量还是效率都能提升医师和患者的治疗体验，2017 年我们将在这个领域内做得更多。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;音乐与艺术的生成&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;科技一直在帮助人类定义媒体内容是如何创造与传播的，从印刷机、电影到电吉他都是这样。在 16 年，我们开始进行名为 Magenta 的项目，它主要是用来探索艺术和机器智能的交叉点及使用机器学习系统增强人类的创造力。我们从音乐和图像的生成开始，进而进入到了文本生成与虚拟现实等领域。Magenta 正在朝最先进的内容创造生成模型前进。我们为这些话题已经举行过一整天的研讨会，并举办过机器生成艺术的作品展览会。我们探讨了在音乐生成和艺术风格迁移等领域内的主题，并且我们的会话演示获得了 NIPS 2016 最佳演示奖（Best Demo Award at NIPS 2016）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;人工智能的安全和公平性&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当我们开发出强大而复杂的人工智能系统，并将其部署到世界各个角落时，我们总会希望保证这些系统能够安全且公平，我们也希望拥有能够帮助人类了解机器生产方式的工具。在人工智能安全领域，通过与斯坦福大学、伯克利和 OpenAI 等大学和机构的合作，我们共同发表了关于人工智能安全问题论文《Concrete Problems in AI Safety》。该论文概述了一些人工智能安全基础研究应该首先着力的具体领域。目前，我们正在努力的方向是确保训练数据的隐私性，制定不同级别的隐私级别，其中最近的研究是通过知识迁移技术来保证隐私安全。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除了安全问题，我们也正着手让人工智能系统开始进行复杂的决策。我们希望人工智能在处理此类任务时能够保证公平性。在一篇关于监督学习机机会公平性的论文中，我们展示了如何把所有训练的预测因子调至最佳，以防止出现歧视。同时，本文描述了基于 FICO 评分机制的一项案例研究。为了让本研究更加易读，我们为此编写了可视化成果展示，以帮助更多人理解。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;TensorFlow&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2015 年 11 月，我们开源了一个 TensorFlow 的初始版本，从而让机器学习社区的其他人也能从中受益，让我们所有人都可以参与到 TensorFlow 的改进中来。在 2016 年，TensorFlow 已经成长为了 GitHub 上最受欢迎的机器学习项目，拥有超过 570 位贡献者的 10000 次以上的提交。在社区的贡献下，TensorFlow 的模型库也在不断增长，现在光是在 GitHub 上就有超过 5000 个与 TensorFlow 相关的项目了！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此外，TensorFlow 也得到了许多知名的研究团队和大型企业的采纳，其中包括 DeepMind；并且也还在许多非同寻常的应用中得到了使用，比如通过高空图像寻找海牛、帮助日本农夫挑选黄瓜。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们已经为 TensorFlow 带来了大量性能上的改进、增加了分布式训练的支持、将其带入到了 iOS、树莓派和 Windows，并且还将 TensorFlow 和人们广泛使用的大数据框架进行了整合。我们已经扩展了 TensorBoard——TensorFlow 的可视化系统，带有用于可视化计算图和 embedding 的先进工具。我们还让 TensorFlow 可以通过 Go、Rust 和 Haskell 接入，发布了当前表现最佳的图像分类模型，提出了 Wide and Deep 学习，并且回答了 GitHub、StackOverflow 论坛以及 TensorFlow 邮件列表中出现的数以千计的问题。TensorFlow Serving 简化了在生产中应用 TensorFlow 模型的流程；而对于那些在云上使用的模型，Google Cloud Machine Learning 也提供了 TensorFlow 的托管服务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;去年 11 月，我们庆祝了 TensorFlow 开源一周年，并在主要的计算机系统研究会议 OSDI 上提交了一篇关于 TensorFlow 的计算机系统方面的论文《TensorFlow: A System for Large-Scale Machine Learning》。通过与谷歌的编译器团队的同事合作，我们也在致力于研究开发用于 TensorFlow 的后端编译器 XLA；最近我们已经将其一个 alpha 版本添加到了 TensorFlow 开源项目中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;机器学习社区参与&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们也在致力于教育和指导这一领域的人们如何使用机器学习以及开展机器学习研究。去年 1 月，谷歌大脑团队的一位研究负责人 Vincent Vanhoucke 与 Udacity 开放了一个免费的深度学习网络课程。我们还组织了 TensorFlow Playground，这是一个有趣的交互式系统，能够可视化简单的神经网络学习完成任务的方式，帮助人们更好地进行理解。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 6 月份，我们迎来了 Google Brain Residents 项目的第一届 27 位参与者，他们是从 2200 名申请者中筛选出来的，在 7 个月的时间里，他们已经开展了大量的原创研究，帮助完成了 21 篇研究论文。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 8 月份，许多谷歌大脑团队成员都参与了 Reddit r/MachineLearning 上的 AMA（Ask Me Anything）问答，回答了很多关于机器学习社区以及我们团队的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;过去的一年，我们还接待了 46 位实习的学生（大部分是博士生），他们也与我们的团队成员一起开展了一些研究。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;让机器学习遍布整个谷歌&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除了上面提到的公开的活动之外，我们也在谷歌内部不断开展工作，将机器学习专业知识和意识扩展到我们的许多产品团队，并确保谷歌能在整体上充分利用新涌现出的机器学习研究。比如说，我们与平台团队紧密合作，为谷歌的定制机器学习加速器 ASIC——张量处理单元（TPU）——提供了规格和高层面的目标。这种定制芯片能为机器学习负载带来一个数量级的性能提升，并且已经在我们的许多产品中得到了重要的应用，其中包括 RankBrain、最新推出的神经机器翻译系统以及在去年三月对战李世石的 AlphaGo。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;总而言之，对谷歌大脑团队以及谷歌内外许多同事和合作伙伴来说，2016 年是让人兴奋的一年，我们期望我们的机器学习研究能在 2017 年产生显著的影响。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;文中提到的部分资源：&lt;br/&gt;&lt;br/&gt;T&lt;/span&gt;&lt;span&gt;ensorFlow GitHub：https://github.com/tensorflow&lt;br/&gt;TensorFlow Stackoverflow：&lt;/span&gt;&lt;a style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;http://stackoverflow.com/questions/tagged/tensorflow&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;span&gt;TensorFlow 邮件列表：&lt;/span&gt;&lt;a style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;span&gt;谷歌大脑提供的深度学习课程：&lt;/span&gt;&lt;a style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;https://www.udacity.com/course/deep-learning--ud730&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;span&gt;TensorFlow Playground：&lt;/span&gt;&lt;a style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;http://playground.tensorflow.org/&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;span&gt;Google Brain Residents：&lt;/span&gt;&lt;a style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;https://g.co/brainresidency&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;br/&gt;&lt;span&gt;机器之心相关文章列表：&lt;br/&gt;&lt;br/&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718173&amp;amp;idx=1&amp;amp;sn=92253b8f8e7d61f7d280f1fbdae8e4fd&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718173&amp;amp;idx=1&amp;amp;sn=92253b8f8e7d61f7d280f1fbdae8e4fd&amp;amp;scene=21#wechat_redirect"&gt;重磅 | Google Brain团队在线问答两万字全录：改变世界的技术、模型、团队与愿景（附论文）&lt;/a&gt;&lt;br/&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716509&amp;amp;idx=1&amp;amp;sn=6aef1e801fe83b7a038c3b77e83a976b&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716509&amp;amp;idx=1&amp;amp;sn=6aef1e801fe83b7a038c3b77e83a976b&amp;amp;scene=21#wechat_redirect"&gt;深度 | CVPR 2016谷歌论文全收录：直击谷歌计算机视觉研究最新动态（附论文）&lt;/a&gt;&lt;br/&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720407&amp;amp;idx=1&amp;amp;sn=768d7248e0ab5fa469dbae86d11152e1&amp;amp;chksm=871b0ce9b06c85ffefa2e0c8f6fb7ae4cc1c0500cda7bad008fe68ed6b87d7c8765d138e1fd1&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720407&amp;amp;idx=1&amp;amp;sn=768d7248e0ab5fa469dbae86d11152e1&amp;amp;chksm=871b0ce9b06c85ffefa2e0c8f6fb7ae4cc1c0500cda7bad008fe68ed6b87d7c8765d138e1fd1&amp;amp;scene=21#wechat_redirect"&gt;深度 | TensorFlow开源一周年：这可能是一份最完整的盘点&lt;/a&gt;&lt;br/&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650717557&amp;amp;idx=1&amp;amp;sn=6c8239ec01c2a3f0def744063d070eec&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650717557&amp;amp;idx=1&amp;amp;sn=6c8239ec01c2a3f0def744063d070eec&amp;amp;scene=21#wechat_redirect"&gt;深度 | 谷歌官方指南：如何通过玩TensorFlow Playground来理解神经网络&lt;/a&gt;&lt;br/&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650722208&amp;amp;idx=5&amp;amp;sn=392a6aa77f0f5eeb976636f4fe791cc7&amp;amp;chksm=871b0bdeb06c82c84cb811fa6e059b988f56249d027dbdcb5f28fe45a1167f27c1f1e1cd6de2&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650722208&amp;amp;idx=5&amp;amp;sn=392a6aa77f0f5eeb976636f4fe791cc7&amp;amp;chksm=871b0bdeb06c82c84cb811fa6e059b988f56249d027dbdcb5f28fe45a1167f27c1f1e1cd6de2&amp;amp;scene=21#wechat_redirect"&gt;资源 | TensorFlow版本号升至1.0，正式版即将到来&lt;/a&gt;&lt;br/&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718578&amp;amp;idx=3&amp;amp;sn=2c4141258b0134642c5f4312a56301de&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718578&amp;amp;idx=3&amp;amp;sn=2c4141258b0134642c5f4312a56301de&amp;amp;scene=21#wechat_redirect"&gt;业界 | 谷歌开源新的TensorFlow代码，如何进行文本自动摘要&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719099&amp;amp;idx=2&amp;amp;sn=52807674a2235e7ed8065a165427e1d6&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719099&amp;amp;idx=2&amp;amp;sn=52807674a2235e7ed8065a165427e1d6&amp;amp;scene=21#wechat_redirect" style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;业界 | 谷歌 NIPS 2016 提交的8篇论文：从无监督学习到生成模型（附论文下载）&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719470&amp;amp;idx=1&amp;amp;sn=3368dea980517ea7d7942967da2740dc&amp;amp;chksm=871b0090b06c89863620be4e75c757940d03d8a43cd3c1d9a8309b6594c1bccd769cab193177&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719470&amp;amp;idx=1&amp;amp;sn=3368dea980517ea7d7942967da2740dc&amp;amp;chksm=871b0090b06c89863620be4e75c757940d03d8a43cd3c1d9a8309b6594c1bccd769cab193177&amp;amp;scene=21#wechat_redirect" style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;重磅 | 谷歌翻译整合神经网络：机器翻译实现颠覆性突破（附论文）&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720536&amp;amp;idx=2&amp;amp;sn=1617ed96796bba31f4d6c6749b7579db&amp;amp;chksm=871b0d66b06c8470e86bf243fab1b7710dd8b222ecbfa99d5ac61dac07694542e6d4b6846db8&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720536&amp;amp;idx=2&amp;amp;sn=1617ed96796bba31f4d6c6749b7579db&amp;amp;chksm=871b0d66b06c8470e86bf243fab1b7710dd8b222ecbfa99d5ac61dac07694542e6d4b6846db8&amp;amp;scene=21#wechat_redirect" style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;重磅 | 谷歌神经机器翻译再突破：实现高质量多语言翻译和zero-shot翻译（附论文）&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3 style="text-align: justify; line-height: 1.75em;"&gt;&lt;span&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719594&amp;amp;idx=2&amp;amp;sn=b8398c3059b23babb02487baf2cc738f&amp;amp;chksm=871b0114b06c8802ed86e1bc0a17fb33d79500962206eb480bdf023b25c753a7535bb877df2b&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719594&amp;amp;idx=2&amp;amp;sn=b8398c3059b23babb02487baf2cc738f&amp;amp;chksm=871b0114b06c8802ed86e1bc0a17fb33d79500962206eb480bdf023b25c753a7535bb877df2b&amp;amp;scene=21#wechat_redirect"&gt;谷歌深度解读：机器人可以如何通过共享经历学习新技能（附论文）&lt;/a&gt;&lt;/span&gt;&lt;/h3&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719367&amp;amp;idx=2&amp;amp;sn=f55be5c1efdbf75f98ec77d6d94c6b62&amp;amp;chksm=871b00f9b06c89eff2bf15e38deb2d060a5c1f5ffb4e01839b920572f9c5499bf7a0129e25b8&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719367&amp;amp;idx=2&amp;amp;sn=f55be5c1efdbf75f98ec77d6d94c6b62&amp;amp;chksm=871b00f9b06c89eff2bf15e38deb2d060a5c1f5ffb4e01839b920572f9c5499bf7a0129e25b8&amp;amp;scene=21#wechat_redirect" style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;Show and Tell：谷歌在 TensorFlow 上开源图像描述系统&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718578&amp;amp;idx=3&amp;amp;sn=2c4141258b0134642c5f4312a56301de&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718578&amp;amp;idx=3&amp;amp;sn=2c4141258b0134642c5f4312a56301de&amp;amp;scene=21#wechat_redirect" style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;谷歌开源新的 TensorFlow 代码，如何进行文本自动摘要&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718786&amp;amp;idx=4&amp;amp;sn=274bbf2332427a274123f6b9e009487e&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718786&amp;amp;idx=4&amp;amp;sn=274bbf2332427a274123f6b9e009487e&amp;amp;scene=21#wechat_redirect" style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;谷歌开放 TF-Slim：在 TensorFlow 中定义复杂模型的高层库&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716578&amp;amp;idx=1&amp;amp;sn=aae84df4e4e218afcd9f2d7cc88c96eb&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716578&amp;amp;idx=1&amp;amp;sn=aae84df4e4e218afcd9f2d7cc88c96eb&amp;amp;scene=21#wechat_redirect" style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;谷歌新开源「宽度&amp;amp;深度学习」框架：结合记忆和归纳实现更优推荐（附论文）&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720113&amp;amp;idx=1&amp;amp;sn=b45bd28cef19a06c717717d3622d58de&amp;amp;chksm=871b030fb06c8a199334d745ad1be56b6b7aeb364596743be7215cc7c6a15a23053c8b60639f&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720113&amp;amp;idx=1&amp;amp;sn=b45bd28cef19a06c717717d3622d58de&amp;amp;chksm=871b030fb06c8a199334d745ad1be56b6b7aeb364596743be7215cc7c6a15a23053c8b60639f&amp;amp;scene=21#wechat_redirect" style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;谷歌增强型风格迁移新算法：实现基于单个网络的多种风格实时迁移（附论文）&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718798&amp;amp;idx=2&amp;amp;sn=d18431b7d6e352a1001a938d007dec82&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718798&amp;amp;idx=2&amp;amp;sn=d18431b7d6e352a1001a938d007dec82&amp;amp;scene=21#wechat_redirect" style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;谷歌开放Inception-ResNet-v2：一种新的图像分类卷积神经网络模型&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715438&amp;amp;idx=1&amp;amp;sn=3573abf9634a55c9547409a35ca18b38&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715438&amp;amp;idx=1&amp;amp;sn=3573abf9634a55c9547409a35ca18b38&amp;amp;scene=21#wechat_redirect" style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;谷歌开源最精确自然语言解析器SyntaxNet的深度解读：一次关键进步以及一个重要工具&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718527&amp;amp;idx=3&amp;amp;sn=bbb26fade567fe63ba7cd1131fb66e05&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718527&amp;amp;idx=3&amp;amp;sn=bbb26fade567fe63ba7cd1131fb66e05&amp;amp;scene=21#wechat_redirect" style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;业界 | 谷歌公开两个机器人研究数据集：Grasping + Push（附论文）&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720846&amp;amp;idx=1&amp;amp;sn=2f98ec55be6e608ab7b69bae31a8ed23&amp;amp;chksm=871b0e30b06c872680991cbe6441617c36c9e500486decb1603426f918212f7f31fc7ed19737&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720846&amp;amp;idx=1&amp;amp;sn=2f98ec55be6e608ab7b69bae31a8ed23&amp;amp;chksm=871b0e30b06c872680991cbe6441617c36c9e500486decb1603426f918212f7f31fc7ed19737&amp;amp;scene=21#wechat_redirect" style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;重磅 | 谷歌研发人工智能眼科医生：用深度学习诊断预防失明&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719723&amp;amp;idx=2&amp;amp;sn=03925a0eb5a8b78cc2642781b924032c&amp;amp;chksm=871b0195b06c888336f97ac330524580589bf29b073aa76585319a6ad43744a5697d6a424b0f&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719723&amp;amp;idx=2&amp;amp;sn=03925a0eb5a8b78cc2642781b924032c&amp;amp;chksm=871b0195b06c888336f97ac330524580589bf29b073aa76585319a6ad43744a5697d6a424b0f&amp;amp;scene=21#wechat_redirect"&gt;深度 | 人工智能改变MIDI创作：谷歌Magenta项目是如何教神经网络编写音乐的？&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715580&amp;amp;idx=2&amp;amp;sn=564b17f5654b42e492c9e96b8eb8667e&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715580&amp;amp;idx=2&amp;amp;sn=564b17f5654b42e492c9e96b8eb8667e&amp;amp;scene=21#wechat_redirect"&gt;业界 | 谷歌发布TPU只是开始，是时候让英特尔害怕了&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716332&amp;amp;idx=1&amp;amp;sn=f12618a4f044f7e4081da25ad9a05a37&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716332&amp;amp;idx=1&amp;amp;sn=f12618a4f044f7e4081da25ad9a05a37&amp;amp;scene=21#wechat_redirect" style="font-size: 14px;"&gt;研究人工智能安全不再抽象：谷歌、OpenAI 合著论文，提出更加实用的五大研究课题&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Fri, 13 Jan 2017 11:06:47 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 深度学习，推动计算机芯片发展的新动力</title>
      <link>http://www.iwgc.cn/link/4321307</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;em style="box-sizing: border-box;"&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;选自WSJ&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em style=" color: rgb(165, 165, 165); font-size: 12px; background-color: rgb(255, 255, 255); ; ; ; ; ; "&gt;&lt;span&gt;对于半导体行业而言，不论是创业公司，还是已坐拥仿人脑芯片TrueNorth 的 IBM，无不忙着推陈出新，乐此不疲。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9fGRdXNicHNSJy9MqCicPHG1UVN1okYp86IklXucswqDIuwRgdicmPoibbdDySvsljekeBdLhWSX4CgQ/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em style="box-sizing: border-box;"&gt;&lt;span&gt;谷歌去年推出了这款用于深度学习应用的芯片，DON CLARK 摄&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于半导体相关设备的需求减缓，以及更小型化的集成电路带来的性能增益也在下降，半导体行业正处于史上最为荆棘丛生的时期，生产计算机芯片的企业无不艰难度日。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而据该行业的管理人员称，这种重压也重新激发了半导体创新浪潮，越来越多的创业公司正瞄准半导体研发。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在市场规模达3350亿美元的全球半导体行业，大小型公司无不都在竭力设计和研发新型芯片、材料与制造程序。归其缘由，其一就是因得益于新型计算技术的深度学习这一人工智能技术在图像分类、语言翻译和自动驾驶中的广泛应用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中一些研发新科技的企业，径直把目标对准了像是英特尔公司这样的树大根深的巨头们，意欲颠覆后者。作为回应，英特尔&lt;/span&gt;&lt;span&gt;已将一些秉承已久的战略做了调整。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「这是最好的时代，同时也是最坏的时代，」IBM的一位首席科学家 Dharmendra Modha 说，目前他正在带领团队研发一款特殊的类脑芯片。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;创新活动的激增，部分源于智能手机和个人计算机销售量的下降，这也同时导致了史无前例的芯片制造商的大规模合并。据迪罗基（Dealogic，全球金融数据提供商）统计，过去两年中共有707次合并和收购，总市值为2460亿美元。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9fGRdXNicHNSJy9MqCicPHG1xDMrW4RwiaeibfklTedjGIt30n6v02C2qHO9qjFGCY6C6MsVSdBQQm3Q/0?wx_fmt=jpeg"/&gt;&lt;/span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;I&lt;/span&gt;&lt;/em&gt;&lt;span&gt;&lt;span&gt;&lt;/span&gt;&lt;em style="  ;  font-size: 12px; box-sizing: border-box; "&gt;&lt;span&gt;B&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;em style="  ;  font-size: 12px; box-sizing: border-box; "&gt;&lt;span&gt;M 负责脑启发计算的首席科学家 Dharmendra Modha 在加州圣何塞市的IBM 阿尔马登研究中心的芯片测试室中，摄于2015年。《彭博新闻社》 DAVID PAUL MORRIS 摄&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;与此同时，该行业用于提升性能的备用策略——稳步减少每个硅片上微型晶体管的数量——其效益也在遭遇瓶颈。几十年来，芯片制造商都遵从着摩尔定律，该定律以英特尔联合创始人戈登·摩尔的名字命名，认为每过两年左右芯片上可容纳的晶体管数量便会增加一倍。但最近人们发现，相对晶体管增多导致的芯片设计成本的上涨，计算速度和功耗方面的改善不再像以前一样显著。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;半导体行业协会（ SIA ）及其分支研究机构已招募22家企业开启全面的技术研究，或会促使计算技术的提升。他们可能会采取的措施包括，在节省空间的层架中堆叠电路，或用蛋白质等生物材料制作芯片等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度学习方面的研发尤为密集。该技术中的训练系统需要处理海量数据，而研究人员不必通过详尽的指令对其编程。后一方法不仅耗时很长，且计算结果的可信度常常相对较差。采用深度学习技术的互联网企业则尤为关注如何更快地创造出能更快计算出结果的硬件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度学习系统常常结合使用英特尔处理器和英伟达芯片或 AMD 芯片（后者最初的设计目标是渲染视频游戏的图像）。这些芯片上有成千上万的简单处理器同时执行计算，而一款高端英特尔芯片上只有几十个复杂的计算内核。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一些企业甚至表示，更加专业化的硬件也位于需求之列。&amp;nbsp;&lt;/span&gt;&lt;a style=" background-color: transparent; color: rgb(102, 102, 102); text-decoration: underline; ;  box-sizing: border-box; font-size: 14px; "&gt;&lt;span&gt;Alphabet&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&amp;nbsp;旗下的谷歌最近采取了一项不同寻常的举措——为某些深度学习任务从头开始设计芯片。IBM则致力于利用 TrueNorth来进行深度学习研究，TrueNorth 是2014年发布的芯片，由一百万个模仿人类大脑神经元的结构组成。 Modha 先生说，这表明深度学习应用正以惊人的速度增长，2019年之前将会形成「大规模经营」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;风险投资家们也注意到了这一点。芯片产业中的技术挑战和激烈角逐使得大多数的风险投资家都把资金投到了别处。但是一些企业家和投资者则看到了为网络系统等市场研发芯片的新机遇，这里的客户或许更愿意将资源多样化，而非只依赖于一两个主要供应商。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Cerebras Systems 是一家有25位员工的创业公司，预备设计针对深度学习的处理器。创始人 Andrew Feldman 表示，他们惊喜地发现筹措风投基金非常容易。「我们8天之内就筹到了资金」，他说，但他谢绝说明具体数额。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其他为深度学习设计芯片的创业公司有 KnuEdge， Graphcore， Cornami 以及 Wave Computing。Wave Computing 表示，该公司正在研发的由专用处理器驱动的系统能够在6.75秒内完成一次典型的文本分析，而结合使用英特尔 Xeon 和英伟达处理器的系统执行这一任务则需要69分钟。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;英特尔，这一为服务器系统提供约99%的中央处理单元（即 CPU） 的巨头，成为多个研发新型芯片的企业的众矢之的。一些网络巨头们已经开始测试 IBM &amp;nbsp;Power 芯片技术或移动手机中 ARM 的芯片设计，而采用 AMD 的兼容芯片仍不失为另一种选择。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其他芯片购买商则为了某些特定目标，正采用专用芯片增强英特尔的处理器性能。比如，微软最近表示正用现场可编程门阵列芯片即 FPGA 配置 Azure 云服务中的所有服务器。这类芯片出厂后仍可继续配置。微软表示，这一技术通过加速服务器之间的通信，使得微软的 Bing 搜索等具有繁重计算任务的服务也得以提速。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我要做的，比目前英特尔和 AMD 在 CPU 方面做的事情还要多，」微软研发团队中负责芯片创新的领头人、卓越的工程师 Doug Burger 说，「故而我们的新型处理器架构才发生了寒武纪生命大爆发一样的巨变。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;英特尔正不断发起攻势。2015年年底，英特尔用167亿美元买入阿尔特拉（ Altera ），并允许后者可售给微软等企业 FPGA 芯片 。去年，英特尔又收购了 Nervana Systems，还计划在自己的处理器上添加其深度学习技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;英特尔数据中心团队的副总裁和总经理 Jason Waxman 说，Intel 的这些举措为客户免去了编程的麻烦，客户不再需要使用那些可能会随时间增长而需要更新换代的专业芯片。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「看到如此多的创新和创意，我非常激动，」他说，「但是我倾向于认为，大多数新的方向都不会持续长久，我们的产品线不会碎片化。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Fri, 13 Jan 2017 11:06:47 +0800</pubDate>
    </item>
  </channel>
</rss>
