<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>菜鸡的啄米日常 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/chicken-life</link><description>本菜鸡关于机器学习、深度学习的啄米日常</description><lastBuildDate>Wed, 04 Jan 2017 03:17:36 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>【通知】Keras China试运行中</title><link>https://zhuanlan.zhihu.com/p/24215633</link><description>&lt;p&gt;Dear 奥：&lt;/p&gt;&lt;p&gt;       感谢Gladuo同学的辛勤工作，我们最近架设了一个全新网站Keras China，目前网站刚刚弄好，点击&lt;a href="http://keras.net/" class="" data-editable="true" data-title="Keras China"&gt;Keras China&lt;/a&gt;即可访问，前期的想法是首先使用邀请制，多创造一些高质量的内容和比较好的讨论环境，目前尚不开放注册，敬请谅解~&lt;/p&gt;&lt;p&gt;       不开放注册我说个卵？&lt;/p&gt;&lt;p&gt;       当然还是要说的，一方面宣传一下，另一方面提供一些内测资格~请感兴趣的同学填写调查问卷&lt;a href="http://form.mikecrm.com/tAbSej" data-editable="true" data-title="来自 Keras China 的邀请"&gt;来自 Keras China 的邀请&lt;/a&gt;，我们将会根据问卷结果选出&lt;b&gt;300&lt;/b&gt;名左右的同学（别到时候连300人都挑不出来我就打脸了。。。）发放邀请函，我们希望Keras China的首批用户能成为高质量的内容生产者和讨论者，或者能够协助我们改善社区功能，或者能够提出高质量的问题。&lt;/p&gt;&lt;p&gt;       欢迎拍砖，任何意见或建议请留言&lt;/p&gt;&lt;p&gt;                                                                                                                 Yours&lt;/p&gt;&lt;p&gt;                                                                                                              菜鸡Moyan&lt;/p&gt;&lt;p&gt;======================================================================&lt;/p&gt;&lt;p&gt;补充：&lt;/p&gt;&lt;p&gt;（1）如未收到邮件请前往广告邮件/垃圾箱查看，很可能在这里哟&lt;/p&gt;&lt;p&gt;（2）如果真的没收到邀请~表灰心，草创未就很多事情还没搞顺，整理好了俺们再扫榻相迎~&lt;/p&gt;&lt;p&gt;（3）首批用户请做好小白鼠的准备~ &lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24215633&amp;pixel&amp;useReferer"/&gt;</description><author>BigMoyan</author><pubDate>Tue, 06 Dec 2016 19:48:34 GMT</pubDate></item><item><title>【啄米日常】12：keras示例程序解析（3）验证网络siamese</title><link>https://zhuanlan.zhihu.com/p/23956891</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-7fc8351e66f41ceb38cb8dd35dd80c7d_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;你们是懂我的，一般来说，在写完一篇文章后，我会悠然悠然的过个一个月，才会有内心的恐慌感【啊再不更新的话他们一定认为我根本没有学习】来督促我继续更新。&lt;/p&gt;&lt;p&gt;再说了，看着以往文章的点赞量和【赞赏】金额（主要是这个！），我老婆的可乐钱和薯片钱都出不来，凭什么要使劲写！ &lt;/p&gt;&lt;p&gt;但是，今天，我要打破这个传统了，首先，上一篇文字&lt;a href="https://www.zhihu.com/people/41cbe08b267bb0bd3f83caeb13975f11" data-hash="41cbe08b267bb0bd3f83caeb13975f11" class="member_mention" data-title="@爱可可-爱生活" data-editable="true" data-hovercard="p$b$41cbe08b267bb0bd3f83caeb13975f11"&gt;@爱可可-爱生活&lt;/a&gt; 老师翻了牌子，我十分感激，并且诚惶诚恐，这让我深知不好好写文章对不起爱可可老师的厚爱。&lt;/p&gt;&lt;p&gt;当然，这并不是主要的，主要的是上一篇文章有这样一条暖心的评论：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-24cfdfa86fb9cde0461bd17a5621e07f.png" data-rawwidth="721" data-rawheight="124"&gt;啊~就像冬日里的一缕阳光，温暖了我在寒风中踽踽独行的心，还有什么能比被读者所【承认】更令人心满意足呢？比起这来，我对【赞赏】金额的耿耿于怀，显得那么的市侩和俗气（但是并不是说我就不要了啊！！赞赏我才是核心动力！）。&lt;/p&gt;&lt;p&gt;于是我准备加班一篇，以报答这位【没有关注我】也【没有关注我的专栏】的读者，以及更多【点赞完就算】的读者们。&lt;/p&gt;&lt;p&gt;本来这篇准备写一下神经网络可视化的，但是keras的神经网络可视化过于简单，体现不出我的诚意，所以今天学习一个非常实用和比较有代码技巧的一个网络siamese有同学希望我上一些nlp的文章，不要老在cv上打转。说实话nlp我懂得不多，真要写的话对着代码分析也不是不可以，但是终究有点心怯，我还是先把这一亩三分地耕好。&lt;/p&gt;&lt;p&gt;阅读本文你将学到：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Siamese网络用于验证类问题的原理&lt;/li&gt;&lt;li&gt;如何使用泛型模型Model&lt;/li&gt;&lt;li&gt;如何pair-wise的训练一个网络&lt;/li&gt;&lt;li&gt;如何自定义损失函数 &lt;/li&gt;&lt;/ul&gt;&lt;p&gt; 开始咯&lt;/p&gt;&lt;h2&gt;Siamese网络与验证类问题 &lt;/h2&gt;&lt;p&gt;首先，这个英文单词怎么读呢~来跟我读：赛~密~死，中文意思是暹罗，所以这个网络叫暹罗网络。&lt;/p&gt;&lt;p&gt;胡说的，siamese确实是暹罗，但是wiki还给出另一个意思：&lt;/p&gt;&lt;p&gt;&lt;i&gt;Siamese, an informal term for conjoined or fused, including with a two-cylinder motorcycle's exhaust pipes&lt;/i&gt;&lt;/p&gt;&lt;p&gt;也就是很像的容易混淆的， .Siamese网络用于判断两个目标是不是“很像”，也就是常说的验证类问题，比如人脸验证， 指纹验证等。&lt;/p&gt;&lt;p&gt;今天这个例子我们来判断两个数字是否是同一个，相关文献是杨乐坤（LeCun）于06年发表的文章&lt;i&gt;Dimensionality Reduction by Learning an Invariant Mapping。&lt;/i&gt;&lt;/p&gt;&lt;h2&gt;算法描述&lt;/h2&gt;&lt;p&gt;Simese网络长啥样呢？确切的说Simese是一种处理验证类问题的方法，而不是一个具体的网络，它的思路非常直接：&lt;/p&gt;&lt;p&gt;（1）用一个网络提取特征，这个特征要具有足够的鲁棒性和判别性，也就是对一张图片而言，旋转、扭曲、加噪声等变换后，图像出来的特征要相似。对不同图片而言，他们的特征要不相似。这一点跟图片分类的要求是一致的。&lt;/p&gt;&lt;p&gt;（2）通过某个标准来衡量两张图片的相似性，进行是/不是的判决&lt;/p&gt;&lt;p&gt;这里我们用一个简单的MLP作为特征提取的网络，所以整个网络的图是：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-3a4d68e28b4e06097a6a8cc979ca93a3.png" data-rawwidth="693" data-rawheight="244"&gt;&lt;p&gt;显而易见，既然只是对特征进行相似度判断，那么只要用一个网络就可以了。网络的作用就是提取特征。&lt;/p&gt;&lt;p&gt;但是如果用一个网络的话，就要分别调用两次前向运算，然后计算损失，然后再回传回去。整个过程需要人工控制，写起来非常麻烦。&lt;/p&gt;&lt;p&gt;所以我们将网络做成一个多输入的模型，两个输入流经同一个网络获得运算结果。&lt;b&gt;可以看作是共享同一套权重的两个网络。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这是结构，再说训练。&lt;/p&gt;&lt;p&gt;网络的作用是判断两张输入图片是否相似，怎么训练呢？很容易想到，训练样本的正例是一对相同标签的图片，训练样本的负例是一对标签不同的图片。这种用成对成对的样本训练的方式一般称为pair-wise的训练，这与常规的网络训练方式是很不同的（更凶狠的还有3个一组的训练，四个一组的训练……），如何组织训练样本，我们稍后代码上说。&lt;/p&gt;&lt;p&gt;最后一个问题是损失函数，这个其实是Siamese比较关键的地方。感性的想，损失函数应该满足下面两个性质：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;对两张相同标签的图片，相似性越大，损失函数的值就越小&lt;/li&gt;&lt;li&gt;对两张不同标签的图片，相似性越小，损失函数的值就越小&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;不妨设两个图片的输出特征分别为&lt;equation&gt;F_1&lt;/equation&gt;和&lt;equation&gt;F_2&lt;/equation&gt;，它们两个的相似度我们不妨用欧式距离来表示，就记作&lt;equation&gt;D(F_1,F_2)&lt;/equation&gt;吧，那么，我们的损失函数要做到，当&lt;equation&gt;D(F_1,F_2)&lt;/equation&gt;比较小（特征相似），并且即两个图片确实标签相同时，值比较小。&lt;equation&gt;D(F_1,F_2)&lt;/equation&gt;比较小，但两个图片标签却不一样时输出比较大，&lt;equation&gt;D(F_1,F_2)&lt;/equation&gt;大的时候也是同样的道理。&lt;/p&gt;&lt;p&gt;当然，深度学习中的损失函数嘛，一定要可导，这个是必然的。&lt;/p&gt;&lt;p&gt;Siamese网络的损失函数定义如下：&lt;/p&gt;&lt;equation&gt;L(F_1,F_2,Y)=\frac{1}{2} (1-Y)D(F_1,F_2)^2+\frac{1}{2}Ymax\{0,m-D(F_1,F_2)\}^2&lt;/equation&gt;&lt;p&gt;看着麻烦，其实不麻烦。Y是标签，只有两种情况，=1表示两个图片标签不同，=0表示相同。对于Y=0的情况，第二项为0，第一项直接变成两个特征的距离平方，显然是距离越近值越小，距离越远值越大。&lt;/p&gt;&lt;p&gt;对Y=1的情况，第一项为0，第二项是一个hinge loss，多了个平方而已，学过SVM的都见过。当距离小于m的时候，就会获得一个&lt;equation&gt;m-D(F_1,F_2)&lt;/equation&gt;的惩罚，但是当距离大于m的时候，就没有惩罚了。距离越大惩罚越小，刚好对应两张图片类别不同时我们希望的情况。&lt;/p&gt;&lt;p&gt;我们下面要进行Keras代码实现，盘点一下，目前的难点主要有三个：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;如何进行权值共享&lt;/li&gt;&lt;li&gt;如何进行pair-wise的训练&lt;/li&gt;&lt;li&gt;如何定义损失函数&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;Keras实现&lt;/h2&gt;&lt;p&gt;第一步还是准备数据。 &lt;/p&gt;&lt;p&gt;本例所用的数据集是MNIST手写数字数据集，我想不知道这个数据集的应该很少，所以我就不多嘴介绍了。通过keras自带的dataset模块可以轻松导入：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train = X_train.reshape(60000, 784) #将图片向量化
X_test = X_test.reshape(10000, 784)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255 # 归一化
X_test /= 255
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt; 我们这里的网络是一个简单全连接网络，所以需要讲原来的2D图片向量化为一条向量，一共是6W的训练集和1W的测试集。&lt;/p&gt;&lt;p&gt;然后我们搭建网络，如何在Keras中搭建一个权值共享的网络呢？有一个概念我每次写文章都要强调，那就是Keras模型/层是张量到张量的映射，只要保证两个输入经由同样的计算图得到输出，那么显然就可以达到权值共享的作用——实际上它们就是一个两输入一输出的网络。&lt;/p&gt;&lt;p&gt;首先搭建一个简单的全连接网络：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;def create_base_network(input_dim):
    '''Base network to be shared (eq. to feature extraction).
    '''
    seq = Sequential()
    seq.add(Dense(128, input_shape=(input_dim,), activation='relu'))
    seq.add(Dropout(0.1))
    seq.add(Dense(128, activation='relu'))
    seq.add(Dropout(0.1))
    seq.add(Dense(128, activation='relu'))
return seq
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;注意我们将它写成了函数，下面调用这个函数得到一个全连接的模型，我们用这个模型进行特征提取：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;base_network = create_base_network(input_dim)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;多输入的模型需要用功能更强大的Model进行建模，首先声明两个输入张量，然后将它们连接在上面的模型之前，最后搞一个输出出来：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;input_a = Input(shape=(input_dim,))
input_b = Input(shape=(input_dim,))

# because we re-use the same instance `base_network`,
# the weights of the network
# will be shared across the two branches
processed_a = base_network(input_a)
processed_b = base_network(input_b)

distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])

model = Model(input=[input_a, input_b], output=distance)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;等……等等，base_network不是一个模型吗？那base_network(input_a)又是什么鬼？&lt;/p&gt;&lt;p&gt;问这个问题的人，肯定没有看过本专栏的&lt;a href="https://zhuanlan.zhihu.com/p/22135796?refer=chicken-life" data-title="一个不负责任的Keras介绍【下】" class="" data-editable="true"&gt;一个不负责任的Keras介绍【下】&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Keras的Layer也好，model也好，规定的都是输入张量到输出张量的映射，它们都是像函数一样callable的。意思就是，输入张量input_a经过一个网络base_network的作用和，得到了输出张量processed_a。&lt;/p&gt;&lt;p&gt;上面的代码还使用了一个Lambda层来计算两个特征的欧式距离，Lambda层通常用来完成功能比较简单，不含有可训练参数的计算需求。如果Lambda层的输出张量的shape改变了，需要设置输出变量的shape，或传入一个用于计算输出张量的shape的函数。&lt;/p&gt;&lt;p&gt;上面Lambda层所用到的计算欧式距离和计算输出shape的函数定义如下，因为它们是计算图的一部分，显然需要用纯tensor语言编写：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;def euclidean_distance(vects):
    x, y = vects
    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))


def eucl_dist_output_shape(shapes):
    shape1, shape2 = shapes
return (shape1[0], 1)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;最后，调用Model将计算图概括起来，包装为一个真正的Keras model。至此，我们的模型就搭建完毕了。&lt;/p&gt;&lt;p&gt;模型搭建完毕，还需要编写刚才的loss，编写自己的loss是一门技术活，主要是符号式语言编写起来太蛋疼，写完还不怎么拿得准。但这真没办法，loss函数作为计算图的一部分，是必须用这种语言写好的。&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;def contrastive_loss(y_true, y_pred):
    '''Contrastive loss from Hadsell-et-al.'06
    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf
    '''
    margin = 1
return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;仔细看看，return的那个东西就是我们刚才定义的loss function，hinge loss的参数m设置为1.&lt;/p&gt;&lt;p&gt;然后我们需要组织数据，生成一对对的正样本和一对对的负样本以供训练。正样本对从来自同一标签的样本集中抽取，负样本对从不同标签的样本集中抽取。&lt;/p&gt;&lt;p&gt;首先获得各个类别的样本下标，即按照类别来对样本集分组：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;digit_indices = [np.where(y_train == i)[0] for i in range(10)]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;digits_indicse的计算结果应为list of numpy，依次是每个类别的样本的下标。我们根据这些下标来选出正样本和负样本：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;def create_pairs(x, digit_indices):
    '''Positive and negative pair creation.
    Alternates between positive and negative pairs.
    '''
    pairs = [] #一会儿一对对的样本要放在这里
    labels = []
    n = min([len(digit_indices[d]) for d in range(10)]) - 1
    for d in range(10):
        #对第d类抽取正负样本
        for i in range(n):
            # 遍历d类的样本，取临近的两个样本为正样本对
            z1, z2 = digit_indices[d][i], digit_indices[d][i+1]
            pairs += [[x[z1], x[z2]]]
            # randrange会产生1~9之间的随机数，含1和9
            inc = random.randrange(1, 10)
            # (d+inc)%10一定不是d，用来保证负样本对的图片绝不会来自同一个类
            dn = (d + inc) % 10
            # 在d类和dn类中分别取i样本构成负样本对
            z1, z2 = digit_indices[d][i], digit_indices[dn][i]
            pairs += [[x[z1], x[z2]]]
            # 添加正负样本标签
            labels += [1, 0]
return np.array(pairs), np.array(labels)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这部分是纯Python代码，计算过程见注释，比较诡异的是这行代码：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;n = min([len(digit_indices[d]) for d in range(10)]) - 1&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这里n是所有类别的样本数目的最小值再减1，注意到n在循环体中作为循环范围：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;    for d in range(10):
        for i in range(n):
            z1, z2 = digit_indices[d][i], digit_indices[d][i+1]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;将n设置为所有类别样本数目之最小值，可以保证对所有类别而言，生成的正样本数目和负样本数目都是一样的，从而保证整个训练集的类别均衡。-1是因为在循环中需要访问[i+1]，这是为了保证不超出范围。&lt;/p&gt;&lt;p&gt;组织样本的写法有多种多样，生成的样本数目也有所不同。你完全可以编写一种能生成出更多样本的方式，例如正样本不仅仅取相邻的[i]和[i+1]，而是遍历所有的组合可能性，这些完全可以按照实际需求编写。&lt;/p&gt;&lt;p&gt;训练集和测试集的政府样本对均照此生成：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;digit_indices = [np.where(y_train == i)[0] for i in range(10)]
tr_pairs, tr_y = create_pairs(X_train, digit_indices)

digit_indices = [np.where(y_test == i)[0] for i in range(10)]
te_pairs, te_y = create_pairs(X_test, digit_indices)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;代码的最后是对整个网络的训练，相比较而言，这部分已经不是什么值得谈的东西了，简单放在下面就好：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;rms = RMSprop()
model.compile(loss=contrastive_loss, optimizer=rms)
model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,
          validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y),
          batch_size=128,
          nb_epoch=nb_epoch)

# compute final accuracy on training and test sets
pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])
tr_acc = compute_accuracy(pred, tr_y)
pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])
te_acc = compute_accuracy(pred, te_y)

print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))
print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;哦，这里的准确率是自己算的，因为这种网络的准确率keras好像没有定义过，需要一个阈值来判断最终预测结果是正例还是负例，非常简单，贴一下：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;def compute_accuracy(predictions, labels):
    '''Compute classification accuracy with a fixed threshold on distances.
    '''
return labels[predictions.ravel() &amp;lt; 0.5].mean()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;全部代码请参考keras examples里的&lt;a href="https://github.com/fchollet/keras/blob/master/examples/mnist_siamese_graph.py"&gt;mnist_siamese_graph.py&lt;/a&gt;，噫，看后缀有个_graph我怀疑这个例子在上古时代（Keras还有Graph这个类型的时候）就有了，也算缅怀一下那逝去的旧时光吧~&lt;/p&gt;&lt;p&gt;感谢阅读，我真的要过好久才更下一篇啦！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23956891&amp;pixel&amp;useReferer"/&gt;</description><author>BigMoyan</author><pubDate>Sat, 26 Nov 2016 14:34:16 GMT</pubDate></item><item><title>【啄米日常】11：keras示例程序解析（2）：图像风格转移</title><link>https://zhuanlan.zhihu.com/p/23479658</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-2efc3293bb1418ce3c9c8bcb5446aaf4_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;各位看官抱歉……最近比较忙（lan），专栏就忘记更新惹= = ，希望你们还能记得我……&lt;/p&gt;&lt;p&gt;Keras 1.1.1更新了，但这次不准备汇报，因为没什么重要的更新内容——至少从文档看没有比较大的改进，下次有大新闻的时候再出来讲。&lt;/p&gt;&lt;p&gt;继续我们的示例程序解析，这次玩个有意思的~就是July他们前段时间炒的火热的图像风格化，好像人家宣传说的是让电脑学习梵高作画？虽然我觉得这玩意儿真的没啥好炒的，但是效果出来确实很炫酷，所以让我们来学习一下吧~&lt;/p&gt;&lt;p&gt;通过这个示例的解析，你将学习到的是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;如何利用Keras的后端函数进行计算图编译&lt;/li&gt;&lt;li&gt;如何获取反传梯度&lt;/li&gt;&lt;li&gt;如何进行图像风格化&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;另一个副作用是，你将免费获得一次对外行装逼的机会，发朋友圈的时候注意先屏蔽掉那些懂深度学习的人哟~&lt;/p&gt;&lt;h2&gt;什么是图像风格转移&lt;/h2&gt;&lt;p&gt;图像风格转移，就是将一张图片的风格转移到另一张图片上去。确切的说就是：&lt;/p&gt;&lt;p&gt;I have a style image：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-8e0585aa43edc8a39c8173953a14b276.jpg" data-rawwidth="504" data-rawheight="315"&gt;&lt;p&gt;I have a content image：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-e2af25d1ce412ebbe3a8ab9d58c3039c.jpg" data-rawwidth="498" data-rawheight="373"&gt;duang~&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-6cf88b46df47495cccc94c3a5488fa0d.png" data-rawwidth="500" data-rawheight="375"&gt;这个就是图像风格转移，直观来看，就是将一副图片的“风格”转移到另一幅图片，而保持它的内容不变。一般我们将内容保持不变的图称为内容图，content image，把含有我们想要的风格的图片，如梵高的星空，称为风格图，style image。&lt;/p&gt;&lt;p&gt;要厘清这个问题，我们需要回答下面的两个问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;什么是图像风格&lt;/li&gt;&lt;li&gt;如何转移图像风格&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;第一个问题，抱歉，目前还没有定论。我们可以直观的看到什么是风格，却很难定量描述它，印象派还是抽象派，达利还是梵高，即使没有经过专门的美术训练，一般人也能大概搞清楚“这张和这张是一个风格的，这张和那张不是一个风格的”。&lt;/p&gt;&lt;p&gt;但是要进行风格转移，必须对风格有一个定量的刻画，计算机科学嘛，凡事得落在代码上写得出来才算。我们先整一个图像风格的描述再说，是不是最贴切的？不一定，但是凑合能用，效果不赖，就可以了。这个指标我们下面讲。&lt;/p&gt;&lt;p&gt;第二个问题，如何转移风格？转移风格的实质是，使得从内容图像中抽取出来的风格表达G_content要接近于从风格图像中抽取出来的风格表达G_style。在这个过程中还要尽量保持内容图像的内容不要有太大变化，具体怎么做是个技术活，但是本质上，我们的想法是这个意图。&lt;/p&gt;&lt;h2&gt;算法&lt;/h2&gt;&lt;p&gt;CNN做图像分类非常厉害，但是为什么这么厉害呢？现在有一大堆人在研究这个问题，希望unlock the black box，也有一些不错的成果，但是确切的原理还没有完全清楚。虽然具体的事情说不着，但有些大致的认识是没错的。&lt;/p&gt;&lt;p&gt;比如，CNN的本质是对图像特征进行逐层抽象表达，经过一层层卷积层的变换，图像的特征变得越来越高级和稳定，对分类问题而已，我们可以预见网络最后一层的输出是一个具有很强稳定性和语义性的高级特征。靠前层representation倾向于表达图片的具体信息，靠后层的representation倾向于表达图片的高级语义信息（如类别）。&lt;/p&gt;&lt;p&gt;具体到图像风格转移，我们的假设是，如果两张图片有相同的representation，则它们是相似的，两张图片representation的相似性就是我们关于图像内容的衡量指标。两张图片的内容损失就是两张图片在某一层输出的二范数。 &lt;/p&gt;&lt;p&gt; 对于风格，我们用某一层输出的Gram矩阵来进行表达。在Keras中，一张图片在某个卷积层的输出特征是一个形如（batch_size, channels, width, heigt）的四阶张量（th后端），一张图片的话batch_size就是1，抛掉这个维度以后就是一个三阶张量，第一维是通道维，与该层卷积核数目相同，后面两个维度是输出featuremap的大小，画成图是这样：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-e185803a287ebc6377ace52fcea8c32d.png" data-rawwidth="857" data-rawheight="380"&gt;&lt;p&gt;将每个featuremap向量化，我们就得到一个矩阵。图中这个representation得到的是512x196的矩阵。这个矩阵自己跟自己相乘，得到了512x512的矩阵，这个矩阵称为Gram矩阵。显然Gram矩阵表达了一个向量组中两两向量的相关性，它的i行j列元素值就是第i个向量化特征图跟第j个向量化特征图的内积。&lt;/p&gt;&lt;p&gt;两张图的Gram矩阵差的二范数，我们定义为图像的风格表达，我们在最后直观的解释一下为什么Gram矩阵表达图像风格是合理的，这里先承认就好。&lt;/p&gt;&lt;p&gt;有了图像风格和内容的表示，我们就可以进行图像风格转移了。基本思路是通过迭代优化的方式，使一张空白图像在内容上相似于内容图像，而在风格上相似于风格图像，整个系统框图如下：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-3a12a9459d54a510fd35cac1d8805055.png" data-rawwidth="1758" data-rawheight="873"&gt;蓝色箭头为前向运算，红色箭头为反向运算。写了这么多network，实际上只用一个，连接内容图像和风格图像的网络只需要运行一次前向运算获得representation就可以了。我们从一张噪声图片开始，以它作为待优化图片。待优化图片通过网络的前向运算获得特定层的representation，然后通过计算representation与风格和内容的loss获得反传梯度，并修改原图。这就是整个算法的思路&lt;/p&gt;&lt;h2&gt;Keras实现&lt;/h2&gt;&lt;p&gt;必须指出，与Caffe相比，Keras实现具体的前向和反向控制麻烦一点。Caffe有明确的forward和backward，Keras这里只能自己来写。&lt;/p&gt;&lt;p&gt;但，从来就没有万能的好工具，只有会用不会用而已。&lt;/p&gt;&lt;p&gt;整理下思路，实现这个东西我们需要下面几个原料：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一个训练好的神经网络&lt;/li&gt;&lt;li&gt;一张风格图像，用来计算它的风格representation，算完就可以扔了&lt;/li&gt;&lt;li&gt;一张内容图像，用来计算它的内容representation，算完扔&lt;/li&gt;&lt;li&gt;一张噪声图像，用来迭代优化&lt;/li&gt;&lt;li&gt;一个loss函数，用来获得loss&lt;/li&gt;&lt;li&gt;一个求反传梯度的计算图，用来依据loss获得梯度修改图片&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;可以了，开干。&lt;/p&gt;&lt;p&gt;首先是训练好的网络，这里就选VGG网络了，稳定可靠大家都喜欢，Keras的Application模块内置了VGG网络，直接载入就好，因为我们这个实验不需要全连阶层，所以选no_top版本的就可以了，权重只有50多M，爽。&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;from keras.application import vgg16

model = vgg16.VGG16(input_tensor=input_tensor, weights='imagenet', include_top=False)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后准备风格图和内容图，为了符合vgg16的输入要求，我们需要一个函数对图像进行预处理，因为最后的结果也是一张图片，所以有预处理就有后处理。出于篇幅考虑，我们将注释写在代码中。&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;def preprocess_image(image_path):
    #使用Keras内置函数读入图片，由于网络没有全连阶层，target_size可以随便设。
    img = load_img(image_path, target_size=(img_nrows, img_ncols)) 
    #读入的图片用内置函数转换为numpy array格式，这两个函数都在keras.preprocessing.image里
    img = img_to_array(img) 
    #：维度扩展，这步在Keras用于图像处理中也很常见，Keras的彩色图输入shape是四阶张量，第一阶是batch_size。
    #而裸读入的图片是3阶张量。为了匹配，需要通过维度扩展扩充为四阶，第一阶当然就是1了。
    img = np.expand_dims(img, axis=0) #3
    #vgg提供的预处理，主要完成（1）去均值（2）RGB转BGR（3）维度调换三个任务。
    #去均值是vgg网络要求的，RGB转BGR是因为这个权重是在caffe上训练的，caffe的彩色维度顺序是BGR。
    #维度调换是要根据系统设置的维度顺序th/tf将通道维调到正确的位置，如th的通道维应为第二维
    img = vgg16.preprocess_input(img) 
    return img

#可以看到，后处理的567三个步骤主要就是将#4的预处理反过来了，这是为了将处理过后的图片显示出来，resonable。
def deprocess_image(x):
    if K.image_dim_ordering() == 'th':
        x = x.reshape((3, img_nrows, img_ncols))
        x = x.transpose((1, 2, 0))
    else:
        x = x.reshape((img_nrows, img_ncols, 3)) 
   
    x[:, :, 0] += 103.939
    x[:, :, 1] += 116.779
    x[:, :, 2] += 123.68 
    # 'BGR'-&amp;gt;'RGB'
    x = x[:, :, ::-1]
    x = np.clip(x, 0, 255).astype('uint8') 
return x
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;下面，这个例子将内容图、风格图和待优化的图片组成了一个tensor作为网络的输入，按照我们上面说的，风格图和内容图的representation完全可以抽取出来放着，只对待优化的图片进行迭代更新。但这个例子将三张图组合起来。我想这样做是出于只编译一次计算图的考虑，但我个人认为这个例子这里写的很不漂亮，还带来了几个图片必须shape一致的额外约束，后面这个约束是难以容忍的。换我写的话，我宁肯单独做一次特征抽取。&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;#读入内容和风格图，包装为Keras张量，这是一个常数的四阶张量
base_image = K.variable(preprocess_image(base_image_path)) 
style_reference_image = K.variable(preprocess_image(style_reference_image_path)) 

#初始化一个待优化图片的占位符，这个地方待会儿实际跑起来的时候要填一张噪声图片进来。
if K.image_dim_ordering() == 'th':
    combination_image = K.placeholder((1, 3, img_nrows, img_ncols))
else:
    combination_image = K.placeholder((1, img_nrows, img_ncols, 3)) 

#将三个张量串联到一起，形成一个形如（3,3,img_nrows,img_ncols）的张量
input_tensor = K.concatenate([base_image,
                              style_reference_image,
combination_image], axis=0) 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;下面我们要将内容图的内容representation和风格图的风格representation抽取出来，写一个loss函数，这个loss就是我们的优化目标。它由三项构成：（1）风格损失，即Gram矩阵差的二范数（2）内容损失，即representation的差的二范数（3）自然图片正则项，用来使得生成的图片更平滑自然。这三项内容通过适当的加权组合起来。&lt;/p&gt;&lt;p&gt;Keras的损失函数是一个张量的函数，应该由后端或theano/TensorFlow提供的张量函数库完成，损失函数的返回值是一个标量，下面我们定义损失函数：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;#设置Gram矩阵的计算图，首先用batch_flatten将输出的featuremap压扁，然后自己跟自己做乘法，跟我们之前说过的过程一样。注意这里的输入是某一层的representation。
def gram_matrix(x): 
    assert K.ndim(x) == 3
    if K.image_dim_ordering() == 'th':
        features = K.batch_flatten(x)
    else:
        features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))
    gram = K.dot(features, K.transpose(features))
return gram
#设置风格loss计算方式，以风格图片和待优化的图片的representation为输入。
#计算他们的Gram矩阵，然后计算两个Gram矩阵的差的二范数，除以一个归一化值，公式请参考文献[1]
def style_loss(style, combination): #2
    assert K.ndim(style) == 3
    assert K.ndim(combination) == 3
    S = gram_matrix(style)
    C = gram_matrix(combination)
    channels = 3
    size = img_nrows * img_ncols
    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))

#设置内容loss计算方式，以内容图片和待优化的图片的representation为输入，计算他们差的二范数，公式参考文献[1]
def content_loss(base, combination):
    return K.sum(K.square(combination - base))

#施加全变差正则，全变差正则用于使生成的图片更加平滑自然。
def total_variation_loss(x): 
    assert K.ndim(x) == 4
    if K.image_dim_ordering() == 'th':
        a = K.square(x[:, :, :img_nrows-1, :img_ncols-1] - x[:, :, 1:, :img_ncols-1])
        b = K.square(x[:, :, :img_nrows-1, :img_ncols-1] - x[:, :, :img_nrows-1, 1:])
    else:
        a = K.square(x[:, :img_nrows-1, :img_ncols-1, :] - x[:, 1:, :img_ncols-1, :])
        b = K.square(x[:, :img_nrows-1, :img_ncols-1, :] - x[:, :img_nrows-1, 1:, :])
return K.sum(K.pow(a + b, 1.25))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;有同学说你这又是K.batch_flatten又是K.square的我怎么知道怎么用？没办法，符号式计算就是这样，它是一套完整的计算规则，想熟悉这些就要去读backend函数库的文档。Keras，theano，TensorFlow都是如此。大部分你想要的功能在Keras提供的函数库里都有，根据你的需求去搜索然后阅读使用方法即可。&lt;/p&gt;&lt;p&gt;休息一下，盘点一下我们有什么了：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;训练好的网络&lt;/li&gt;&lt;li&gt;损失函数&lt;/li&gt;&lt;li&gt;输入数据的张量占位符&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;还不错，下面我们完成这个代码的核心内容之一，获取反向梯度。时刻牢记在心，Keras的计算图是张量的映射，输入张量我们知道了，就是那三个图组成的tensor，输出张量是什么呢？显然，是损失函数关于输入张量的梯度，损失函数由三部分组成，例子里我们选取conv4_2这层的输出representation作为我们计算内容损失的依据，选取一组卷积层的输出作为风格损失的计算依据——这里没有什么black magic，你完全可以只选一个卷积层输出作为风格损失，或选一组卷积层输出作为内容损失，并没有什么大不了的，而且效果也差不多= =：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;#这是一个张量字典，建立了层名称到层输出张量的映射，通过这个玩意我们可以通过层的名字来获取其输出张量，比较方便。当然不用也行，使用model.get_layer(layer_name).output的效果也是一样的。
outputs_dict = dict([(layer.name, layer.output) for layer in model.layers]) 

#loss的值是一个浮点数，所以我们初始化一个标量张量来保存它
loss = K.variable(0.) 

#layer_features就是图片在模型的block4_conv2这层的输出了，记得我们把输入做成了(3,3,nb_rows,nb_cols)这样的张量，
#0号位置对应内容图像的representation，1号是风格图像的，2号位置是待优化的图像的。计算内容loss取内容图像和待优化图像即可
layer_features = outputs_dict['block4_conv2']
base_image_features = layer_features[0, :, :, :]
combination_features = layer_features[2, :, :, :] 
loss += content_weight * content_loss(base_image_features,
                                      combination_features) 

feature_layers = ['block1_conv1', 'block2_conv1',
                  'block3_conv1', 'block4_conv1',
                  'block5_conv1']
#与上面的过程类似，只是对多个层的输出作用而已，求出各个层的风格loss，相加即可。
for layer_name in feature_layers:
    layer_features = outputs_dict[layer_name]
    style_reference_features = layer_features[1, :, :, :]
    combination_features = layer_features[2, :, :, :]
    sl = style_loss(style_reference_features, combination_features)
    loss += (style_weight / len(feature_layers)) * sl 

#求全变差约束，加入总loss中
loss += total_variation_weight * total_variation_loss(combination_image) 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;刚才说输出张量是损失函数关于输入张量的梯度，现在我们损失函数搞定了，输入张量搞定了，通过调用K.grad获取反传梯度：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;#通过K.grad获取反传梯度
grads = K.gradients(loss, combination_image) 

outputs = [loss]
#我们希望同时得到梯度和损失，所以这两个都应该是计算图的输出
if type(grads) in {list, tuple}:
    outputs += grads
else:
    outputs.append(grads) 
#编译计算图。Amazing！我们写了辣么多辣么多代码，其实都在规定输入输出的计算关系，到这里才将计算图编译了。
#这条语句以后，f_outputs就是一个可用的Keras函数，给定一个输入张量，就能获得其反传梯度了。
f_outputs = K.function([combination_image], outputs)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;值得注意的是，这里编译的计算图，其输入只有待优化的图像，而不是之前定义的“三合一”图片。这个trick可以方便下面的迭代优化，我们只需要输入初始化的噪声图片就可以了。在这里我们可以一窥符号式语言的特点，前面所有的准备只为了最后这一刻的K.function，在这条语句之前，前面所有的语句都是“纸上谈兵”，K.function以后我们定义的这张计算图才具有了生命。&lt;/p&gt;&lt;p&gt;&lt;b&gt;其实到了这一步，整个任务已经完成了，我们只需要写一个迭代优化的代码即可完成任务：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;给定输入图片x，通过f_outputs得到反传梯度&lt;/li&gt;&lt;li&gt; 使用x+学习率*梯度更新x&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;循环上面两步即可。但这里Keras的迭代更新是通过scipy提供的优化模块进行的，所以还是要继续研究下去，这段代码不太好读，我们先看最后的更新过程，然后再回过头来补缺失的部分。图像风格的核心是优化一张噪声图像图像，使得它的内容像内容图像，而且风格像风格图片，这是一个迭代优化的过程，其过程如下：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;# 根据后端初始化一张噪声图片，做去均值
if K.image_dim_ordering() == 'th':
    x = np.random.uniform(0, 255, (1, 3, img_nrows, img_ncols)) - 128.
else:
    x = np.random.uniform(0, 255, (1, img_nrows, img_ncols, 3)) - 128. 

# 迭代10次
for i in range(10):
    print('Start of iteration', i)
    start_time = time.time()
    # 这里用了一个奇怪的函数 fmin_l_bfgs_b更新x，我们一会再看它，这里知道它的作用是更新x就好
    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),
                                     fprime=evaluator.grads, maxfun=20) 
    print('Current loss value:', min_val)
    # save current generated image
    # 每次迭代完成后把输出的图片后处理一下，保存起来
    img = deprocess_image(x.copy()) 
    fname = result_prefix + '_at_iteration_%d.png' % i
    imsave(fname, img) #4
    end_time = time.time()
    print('Image saved as', fname)
print('Iteration %d completed in %ds' % (i, end_time - start_time))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;好，问题的关键就是这个神奇的fmin_l_bfgs_b了，函数名又臭又长，不是搞优化的估计很难get到这个函数名的意义。&lt;/p&gt;&lt;p&gt;这个函数来自scipy.optimize, fmin代表它是一个用来最小化某个函数的优化器，l_bfgs_b代表它用的算法是L-BFGS-B&lt;/p&gt;&lt;p&gt;那么，这是个什么优化算法呢？&lt;/p&gt;&lt;p&gt;呵呵，关你毛事，看函数手册会用就可以了。&lt;/p&gt;&lt;p&gt;好吧我错了，不懂算法的话确实看函数手册也不怎么会用，这狗函数的参数太多了，下面是函数原型，你们感受一下：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;def fmin_l_bfgs_b(func, x0, fprime=None, args=(),
                  approx_grad=0,
                  bounds=None, m=10, factr=1e7, pgtol=1e-5,
                  epsilon=1e-8,
                  iprint=-1, maxfun=15000, maxiter=15000, disp=None,
callback=None, maxls=20):
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;程序中设置了三个主要的参数：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;func：这个是待优化的函数，也就是我们的目标函数&lt;/li&gt;&lt;li&gt;x0：初始值，也就是我们的初始图片&lt;/li&gt;&lt;li&gt;fprime：一个callable的函数，返回梯度，也就是K.grad求出来的玩意&lt;/li&gt;&lt;li&gt;maxfun：最大迭代次数，就是func要被迭代优化多少次&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;如果对这个函数感兴趣的话（呵呵），你可以去研究其他参数都是啥意思，反正，我不感兴趣。&lt;/p&gt;&lt;p&gt;根据函数说明，func和fprime是返回标量值的callable对象(函数)，x0是个1D的nparray，fprime用来更新x，当然其返回值也是一个与x0同shape的1D的nparray&lt;/p&gt;&lt;p&gt;很好，这三个条件我们&lt;b&gt;不&lt;/b&gt;满足两个半。&lt;/p&gt;&lt;p&gt;loss和grad都是计算图中的一部分，数据类型都是Keras tensor。显然不是函数，不callable，这是两个不满足。&lt;/p&gt;&lt;p&gt;grad的返回值和x，是一个2D的矩阵而不是向量，这是半个不满足。&lt;/p&gt;&lt;p&gt;要不怎么说这份代码是没困难创造困难也要上呢，来，下面我们来秀如何解决这两个半不满足。&lt;/p&gt;&lt;p&gt;我们现在手上的计算图，接收一个初始图片的tensor为输入，输出该图片关于损失函数的导数，以及对应的损失函数值，keep it in your mind.&lt;/p&gt;&lt;p&gt;先解决那半个不满足，很简单，将矩阵reshape一下即可，输入函数前reshape成向量，得到输出后reshape成矩阵。我们定义下面的函数来求输入图片关于损失函数的导数，以及对应的损失值：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;# 刚那个优化函数的输出是一个向量
def eval_loss_and_grads(x):
    # 把输入reshape层矩阵
    if K.image_dim_ordering() == 'th':
        x = x.reshape((1, 3, img_nrows, img_ncols))
    else:
        x = x.reshape((1, img_nrows, img_ncols, 3))
    #激动激动，这里调用了我们刚定义的计算图！
    outs = f_outputs([x])
    loss_value = outs[0]
    # outs是一个长为2的tuple，0号位置是loss，1号位置是grad。我们把grad拍扁成矩阵
    if len(outs[1:]) == 1:
        grad_values = outs[1].flatten().astype('float64')
    else:
        grad_values = np.array(outs[1:]).flatten().astype('float64')
return loss_value, grad_values
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这个函数的逻辑很清楚，不再多说。记住刚才的函数要求是，有一个函数func要返回loss值，同时还有一个fprime要返回grad，但这个函数一下返回俩，咋整&lt;/p&gt;&lt;p&gt;一种方法是写两个函数，一个求loss的扔进去当func，另一个求grad的函数送去fprime。但这样的话就得跑两次运算图，不值当，这份代码中我们是这样处理的。定义下面的类：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;class Evaluator(object):
    def __init__(self):
        # 这个类别的事不干，专门保存损失值和梯度值
        self.loss_value = None
        self.grads_values = None

    def loss(self, x):
        # 调用刚才写的那个函数同时得到梯度值和损失值，但只返回损失值，而将梯度值保存在成员变量self.grads_values中，这样这个函数就满足了func要求的条件
        assert self.loss_value is None
        loss_value, grad_values = eval_loss_and_grads(x)
        self.loss_value = loss_value
        self.grad_values = grad_values
        return self.loss_value

    def grads(self, x):
        # 这个函数不用做任何计算，只需要把成员变量self.grads_values的值返回去就行了
        assert self.loss_value is not None
        grad_values = np.copy(self.grad_values)
        self.loss_value = None
        self.grad_values = None
        return grad_values
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后我们实例化一个类对象&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;evaluator = Evaluator()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;再把刚才更新x的部分贴出来看看，是不是就明白多啦：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),
fprime=evaluator.grads, maxfun=20)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这就是这份示例代码的全部内容，全部代码请参考keras examples中的neural_style_transfer.py文件&lt;/p&gt;&lt;h2&gt;那，为什么Gram矩阵能表征图像的风格呢 &lt;/h2&gt;&lt;p&gt;其实具体为什么，我也从理论上说不清，但~还是可以稍微感性的谈一谈，说的对不对您见仁见智。&lt;/p&gt;&lt;p&gt;我们知道CNN某一层的featuremap，一级一级在提取和组合图像的特征，随着层数的增多，提取到的特征越来越抽象和具有语义信息。&lt;/p&gt;&lt;p&gt;而Gram矩阵，说白了就是featuremap两两的相关性。&lt;/p&gt;&lt;p&gt;我们还以梵高的夜空举例：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-1e6e43b4fc5208d29c57c606f9f5dd10.png" data-rawwidth="1494" data-rawheight="752"&gt;&lt;p&gt;譬如说，某一层中有一个滤波器专门检测尖尖的塔顶这样的东西，另一个滤波器专门检测黑色。又有一个滤波器负责检测圆圆的东西，又有一个滤波器用来检测金黄色。&lt;/p&gt;&lt;p&gt;对梵高的原图做Gram矩阵，谁的相关性会比较大呢？如上图所示，“尖尖的”和“黑色”总是一起出现的，它们的相关性比较高。而“圆圆的”和“金黄色”都是一起出现的，他们的相关性比较高。&lt;/p&gt;&lt;p&gt;因此在风格转移的时候，其实也在风景图里去寻找这种“匹配”，将尖尖的渲染为黑色，将圆圆的渲染为金黄色。如果我们承认&lt;b&gt;“图像的艺术风格就是其基本形状与色彩的组合方式” &lt;/b&gt;，这样一个假设，那么Gram矩阵能够表征艺术风格就是理所当然的事情了。&lt;/p&gt;&lt;p&gt;我觉得这个解释是有道理的，事实上好像就是内容相似的图片风格化的效果会好，内容完全不一样的话，风格化起来也比较糟糕。 &lt;/p&gt;&lt;p&gt;当然，一家之言，当不得真。&lt;/p&gt;&lt;p&gt;另外放一个我之前用别的优化方式搞的图片，那会儿根本也不会用scipy的优化器那么高端的东西，直接用梯度的话学习率调不好很容易像素爆炸~无奈之下发明了一种“Accept or reject”的优化方式，就是根据一次迭代后像素值是不是超过合法范围（0~255）来决定接收还是拒绝这次更新，想法当然很粗糙，但是跑出来的图片，私以为还挺好看的~不过风格化的程度没那么高就是了：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-66447f38854ebee0d560d6982dde64fe.png" data-rawwidth="765" data-rawheight="574"&gt;最近比较忙，这篇文章其实上半截老早就写好了，一直懒得继续写……下一篇也不知道是啥时候，你们先关注别的专栏，等宝宝把自己的事儿搞定了再看回来~&lt;/p&gt;&lt;p&gt;鸣谢，鞠躬~现在赶紧去朋友圈发你风格化后的图片来装逼吧！ &lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23479658&amp;pixel&amp;useReferer"/&gt;</description><author>BigMoyan</author><pubDate>Wed, 23 Nov 2016 22:45:32 GMT</pubDate></item><item><title>【啄米日常】10：keras示例程序解析（1）——cifar10_cnn</title><link>https://zhuanlan.zhihu.com/p/22918818</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-0331456b3ba60cd1d1ee386596b8a510_r.jpg"&gt;&lt;/p&gt;经过一段时间（并不）艰苦的找工作历程，本鸡的工作算定了……三方也已经寄出，没有意外的话明年将入职【鹅鹅鹅，曲项向天歌】&lt;p&gt;虽然时间也还很紧张，但总归回归正常节奏了，专栏也继续更新。只汇报每个版本keras变更的话，虽然也很有用，但总是太无聊，所以本鸡准备开一个新坑了~&lt;/p&gt;&lt;p&gt;本鸡准备新开一个系列，用以解析keras的example程序，这些程序很多都可以直接拿来使用，通过学习这些程序也可以快速掌握keras。本鸡将会结合程序进行一步步解释，只求把大致思路和程序中涉及到的知识点说明，完整的程序倒是不一定要跑（你可以自己跑一下嘛）。&lt;/p&gt;&lt;p&gt;作为系列的第一篇，我们先从一个简单的程序开始，开个好头~尽量照顾新入门的同学，多进行背景铺垫。&lt;b&gt;为了行文顺畅，我们一步一步的完成这个程序，但记住程序可不是我写的哟，&lt;/b&gt;好的，开始了哟（咯咯咯…）&lt;/p&gt;&lt;h2&gt;cifar10：小图片分类数据集&lt;/h2&gt;&lt;p&gt;cifar10和cifar100是两个经常使用的benchmark数据集，就是说很多工作的性能都会以这俩数据集的测试表现来进行评估。二者都是&lt;a href="http://groups.csail.mit.edu/vision/TinyImages/" data-editable="true" data-title="80 Million Tiny Images" class=""&gt;80 Million Tiny Images&lt;/a&gt;的子集。cifar10具有10个类别，cifar100具有100个类别，下面是cifar10的一些图片示例：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-9fb731a8b968aaf97652d3d85941b4a7.png" data-rawwidth="476" data-rawheight="364"&gt;可以看到图片都非常小，cifar数据集的图片大小只有32*32，跟你大拇指指甲盖差不多大，今天我们的任务是对它进行分类&lt;h2&gt;载入数据 &lt;/h2&gt;&lt;p&gt;cifar-10是被keras的dataset模块集成的，我们可以轻松的通过dataset的代码下载它，在代码的开始，我们先将数据集下载下来&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;(X_train, y_train), (X_test, y_test) = cifar10.load_data()
print('X_train shape:', X_train.shape)
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;使用上非常简便，只需要一行代码即可，如果你之前没有下载过cifar-10，下载这个数据集还是要花一阵子时间的，如果已经下载过就没什么了。后面的三个print用以输出数据集的一些信息，代码中我们经常会写一些print汇报程序的执行情况，以确保程序正常执行。感兴趣的同学可以跳进dataset的cifar10.py中看看load_data是怎么执行的，这里就先不解释了。&lt;/p&gt;&lt;p&gt;载入完毕数据后，我们将数据类型设为‘float32’，并归一化到0~1区间。讲究的话其实这里还应该减去一下均值。这算一个简单的数据预处理。 &lt;/p&gt;&lt;h2&gt;搭建网络 &lt;/h2&gt;&lt;p&gt;我知道这一步是同学们，尤其是新手最着急做的一步，所以我们先拿出来写，写的过程中自然就知道还要补什么了。作为初次尝试，我们一起来搭建一个普通的卷积网络，在各种花活儿流行的今天，这种【卷积-池化-全连接】形的网络一般被称为“plain”，也就是平的，普通的，一般的网络。&lt;/p&gt;&lt;p&gt;首先我们弄一个model出来，我们知道网络是由一层一层的网络层堆叠起来的，那么model就是层的容器，好比你去超市买大米，第一件事总是去扯一个塑料袋一样。&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;model = Sequential()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Sequential是我们经常说的序贯模型，也就是一条路走到黑，不许跨层连接。另一种更为广义的模型是Model，我们称为泛型模型，这个以后再说。Sequential是Model的特殊情况，你可以把它理解为一个栈。&lt;/p&gt;&lt;p&gt;下面，我们开始搭建模型，通过调用model.add()方法，我们每次能够向模型中添加一个层，这些层是从前向后按顺序排列的：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;model.add(Convolution2D(32, 3, 3, border_mode='same',
                        input_shape=X_train.shape[1:]))  #1
model.add(Activation('relu'))                            #2
model.add(Convolution2D(32, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))                #3
model.add(Dropout(0.25))                                 #4

model.add(Convolution2D(64, 3, 3, border_mode='same'))
model.add(Activation('relu'))
model.add(Convolution2D(64, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())                                     #5
model.add(Dense(512))                                    #6
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes))                           
model.add(Activation('softmax'))                         #7
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;.add()的参数就是一个层对象，我们逐个来看，只看后面标了#的层&lt;/p&gt;&lt;p&gt;#1是一个2D卷积，含有32个卷积核，每个卷积核的尺寸是3*3，border_mode指定了对输出特征图大小的控制，若为‘same’，那么代码内部会通过适当padding保证输出特征图大小与输入特征图大小相同，若为‘valid’，那么就按通常情况处理，在这里通常情况的处理会使得输出特征图丢掉周围一圈的一个像素，因为3*3的卷积核的中心只能从第2行第2列开始，到第n-1行第n-1列结束。&lt;/p&gt;&lt;p&gt;网络的第一层必须指定输入数据的形状，也就是input_shape，这里直接用X_train.shape[1:]表达了。为什么要从第1个数据开始切片呢？因为X_train的shape是【样本数，通道数，图宽度，图高度】这样排列的，而input_shape不需要（也不能）指定样本数。&lt;/p&gt;&lt;p&gt;#2是一个激活函数层，激活函数类型为ReLU，激活层既可以这样单独的写为Activation('relu')，也可以整合到上一个卷积层中，通过一个activation='relu'的关键字参数确定。&lt;/p&gt;&lt;p&gt;#3是一个池化层，池化大小是2x2，方式为最大值池化。也就是对输入featuremap的每个2x2大小的窗，选取其中最大的一个值作为输出。池化可以使特征图的shape迅速缩小，因此对cifar10这种本身图就很小（32x32）的数据，不宜采用很多池化，否则后期图的shape会变成0而导致错误。&lt;/p&gt;&lt;p&gt;#4是Dropout层，用于随机断开一些连接。这种技术非常常见，有点类似于神经网络+集成算法的意思，主要用于防止过拟合。事实上我认为在卷积层处添加Dropout的意义不是很大，但……无所谓了，稍微加一点Dropout也不会怎么样。有同学纠结于Dropout的断开比例选多少合适，一般而言在0.3~0.5之间即可，对于卷积层这种本身连接就不怎么多的层，可以不添加Dropout或选比较少的断开比例。&lt;/p&gt;&lt;p&gt;#5是一个Flatten层，之前说过这个网络是一个plain的【卷积-池化-全连接】模式的网络， 网络到了这里就进入了“bottleneck”，即瓶颈，要开始往全连接转变了。Flatten的输入数据是一堆特征图，而其之后的网络接受的是一维的特征，因此Flatten的作用就是把它的输入数据“压扁”，压为1D的形式（由于batch的存在，实际上是2D，这个先不讨论）。Flatten没有任何参数。&lt;/p&gt;&lt;p&gt;#6特征图经过Flatten后被压为1维向量，从这里就开始进行全连接了。Keras中的全连接层称为Dense，即“稠密的”（话说我也没发现有个层叫“Sparse啊”），大概是要表达这里的连接非常稠密吧……Dense层的一个必填参数就是当前层的神经元个数，这里是512个神经元。&lt;/p&gt;&lt;p&gt;#7经过若干个（通常是1~2个）Dense和Dropout后，网络到达了分类层。在其之前是一个与类别数相同的Dense层，将特征映射为最终的类别，然后通过一个softmax映射为类别概率。我们这里是10分类，因此最后的Dense层神经元数是10。这样，整个网络就搭建完毕。&lt;/p&gt;&lt;h2&gt;配置网络&lt;/h2&gt;&lt;p&gt;完成网络的搭建后，想要run起来我们还需要对网络进行配置，这部分的内容主要通过model的compile函数完成。&lt;/p&gt;&lt;p&gt;这里我们选择SGD+momentum的优化器进行训练，为了能够对学习率等参数进行微调，我们手动生成一个优化器对象：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;lr是学习率，这个数值是我们对优化器调整时经常动的一个值。其他参数不解释了，一般也不用怎么调，记住decay取一个很小的数，momentum取一个很接近1的数就可以了。有了优化器，我们还需要一个损失函数，对多分类问题我们一般取“categorical_crossentropy”作为损失函数，即多类交叉熵。&lt;/p&gt;&lt;p&gt;一般情况下，数据的标签是一个整数。例如“狗”图片是1，“飞机”是2等等，但如果要使用categorical_crossentropy作为损失函数，则必须将整数标签转换为one-hot编码的向量标签，例如狗需要被转换为[0,1,0,0,0,0,0,0,0,0]，只在位置为1的地方有一个热点，其他地方均为0。&lt;/p&gt;&lt;p&gt;因此我们还需要将标签进行转换，在keras中这步是这样做的：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Y_train是y_train的one-hot编码，向量长度为类别数nb_classes，完成这步后，我们使用compile进行模型配置：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;model.compile(loss='categorical_crossentropy',
              optimizer=sgd,
metrics=['accuracy'])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;loss是损失函数，就是刚才说的多类交叉熵，optimizer是我们设置的优化器sgd，metrics用于制定一组指标来进行模型评价，对于分类问题我们一般就制定“准确率”，即accuracy即可&lt;/p&gt;&lt;h2&gt;模型训练与数据提升&lt;/h2&gt;&lt;p&gt;啊……怎么还不开始训练啊……&lt;/p&gt;&lt;p&gt;别着急，训练只需要一行代码，准备充分的一次训练效果要好过盲目的动手训练。在训练之前我们考虑一下数据提升的问题。&lt;/p&gt;&lt;p&gt;cifar10的数据量，没记错的话应该是一共6W条，其中1W作为测试数据，5W作为训练数据，这个数据量对于10分类来说其实不能算小，直接训练也是可以的，假如不使用数据提升，只需要这样即可开始训练：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;if not data_augmentation:
    print('Not using data augmentation.')
    model.fit(X_train, Y_train,
              batch_size=batch_size,
              nb_epoch=nb_epoch,
              validation_data=(X_test, Y_test),
shuffle=True)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;显然这个“data_augmentation”是之前定义的一个用于控制是否进行数据提升的布尔值。如果不进行数据提升，调用模型的fit即可，fit传入训练数据X_train, Y_train，指定每个batch的大小batch_size，指定训练轮数nb_epoch，指定验证集validation_data，指定打乱数据的标记shuffle用于随机打乱数据&lt;/p&gt;&lt;p&gt;如果我们进行数据提升，则可以获取更多的数据，深度学习嘛，跟大数据从来是离不开的，没有大量数据的堆积就没有深度学习。所谓数据提升，就是通过随机翻转，位移，加噪声等方式从已有数据创造新的数据，这样的数据包含了同样的语义信息——一只狗加点噪声，水平翻转，旋转一下还是狗，但这样的样本就增加了网络对加噪声、旋转、翻转等变换的泛化能力。&lt;/p&gt;&lt;p&gt;将图片手工进行变换然后保存起来形成新的数据集是不可取的，（1）数据提升会使图像数据量成倍增加，很难一次载入内存，保存在硬盘上也会浪费很多空间。（2）这样的数据提升随机性不够&lt;/p&gt;&lt;p&gt;Keras采用的是实时数据提升方法，即一边由CPU准备数据，另一边由GPU进行训练。这依赖于生成器。Keras提供了一种ImageGenerator生成器，配合fit函数的另一个版本fit_generator，可以实现实时数据提升：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;else:
    print('Using real-time data augmentation.')

    datagen = ImageDataGenerator(              #1
        featurewise_center=False,  
        samplewise_center=False,  
        featurewise_std_normalization=False,  
        samplewise_std_normalization=False,  
        zca_whitening=False,  
        rotation_range=0, 
        width_shift_range=0.1, 
        height_shift_range=0.1, 
        horizontal_flip=True, 
        vertical_flip=False)

    datagen.fit(X_train)                       #2

    model.fit_generator(datagen.flow(X_train, Y_train,    #3
                        batch_size=batch_size),
                        samples_per_epoch=X_train.shape[0],
                        nb_epoch=nb_epoch,
                        validation_data=(X_test, Y_test)) 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt; 这段代码分为三个阶段&lt;/p&gt;&lt;p&gt;#1 定义了一个数据生成器（严格的说是一个能够生成生成器的类），如果你对生成器的概念不了解，请查阅Python相关内容。大致的说，生成器只在每次调用的时候才返回数据。譬如我们在python中经常写 for i in range(n)这种句子，range(n)生成一个长为n的列表，如果n很大的话需要占用很大的内存空间，而同样功能的for i in xrange(n)就不必如此，这里xrange(n)返回的就是一个生成器，在每次for循环使用的时候才计算下一个数，而不是一次计算完毕。&lt;/p&gt;&lt;p&gt;从ImageDataGenerator的参数名字上我们大致可以推测出这个生成器都做了哪些数据提升，包括去中心化等预处理，旋转，水平位移，垂直位移，水平翻转等。&lt;/p&gt;&lt;p&gt;有些处理，如按特征进行的归一化，白化等需要了解整个样本集的统计信息，#2的代码的作用是获得这些信息&lt;/p&gt;&lt;p&gt;#3在定义完datagen后，调用model的fit_generator进行训练，model有两组训练/测试/评估的函数，不带generator的就是普通版本，如上面用的model.fit，普通版本的数据是直接用numpy array放进去的。带generator的是生成器版本，生成器版本的数据是通过生成器返回的。&lt;/p&gt;&lt;p&gt;git_generator的第一个参数是生成数据的生成器，这里用的是dategen内置的，当然我们也可以自己写，只要返回的数据是形如（X_train, Y_train）或（X_train，Y_train，class_weights）的形式即可。Keras规定这里的生成器是一个无限返回数据的死循环，所以还需要添加samples_per_epoch和nb_epoch两个参数来确定什么时候训练终止，这两个参数指定了每个epoch包含多少个样本以及要训练多少个epoch。samples_per_epoch最好能被batch_size整除。&lt;/p&gt;&lt;h2&gt;自己跑一跑试试吧！&lt;/h2&gt;&lt;p&gt;这份代码的地址是&lt;a href="https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py"&gt;cifar10_cnn&lt;/a&gt;，看完分析就请自己跑一跑试试吧~基本的代码跑完不妨试着自己调调参数，改改网络结构，大家可以把自己跑的结果放在下面，攀比一下哟~&lt;/p&gt;&lt;p&gt;第一篇结束~污码撸~&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22918818&amp;pixel&amp;useReferer"/&gt;</description><author>BigMoyan</author><pubDate>Thu, 13 Oct 2016 13:13:06 GMT</pubDate></item><item><title>【啄米日常】9：keras 1.1.0（抱歉来迟）</title><link>https://zhuanlan.zhihu.com/p/22697113</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-52ff7f13a0f56665505a83ca61d25ddc_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;抱歉了各位看官……最近在忙于找工作，焦头烂额，以至于没有及时更新&lt;/p&gt;&lt;p&gt;现在虽然工作还没定，但估计还能有口饭吃，我就赶紧滚过来更文了= =其实有时候很想写点别的，但是不知道大家感兴趣啥，暂时就先这样吧 &lt;/p&gt;&lt;p&gt;所幸，Keras1.1.0的更新没有啥了不得的东西，简单列出如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;默认后端从theano切换为TensorFlow，这个消息宝宝早就猜到了，以TensorFlow现在的热度绝对是主流，之前就说过theano药丸&lt;/li&gt;&lt;li&gt;增加了一套SpatialDropout，包含2D和3D两个版本，这个东西是Dropout的升级版。Dropout是断开一定比例神经元到神经元的连接，这个SpatialDropout是断开一定比例的featuremap连接，排场大了很多。&lt;/li&gt;&lt;li&gt;增加了AtrousConvolution1D，这是带孔洞的卷积的1D版本，2D版已经加了好久了这次加1D版本也很正常&lt;/li&gt;&lt;li&gt;增加了一套Cropping层，包含1D，2D和3D三个版本，这玩意就是裁剪featuremap，用于把featuremap的周围一圈去掉。&lt;/li&gt;&lt;li&gt;比较重要的是这条，现在model.load_weights()可以通过byname=True参数来将权重匹配到与原来模型结构不同的模型了。啥意思呢？就是说你训练了一个模型，比方说VGG-16，把它的权重保存好了，然后你在原模型里插入一些层，去掉一些层，改变一些连接方式后，还可以将刚才保存的权重再载入回去。Keras会按照层名字对权重进行匹配，并把匹配到的权重载入对应的层。这个选项用于做transfer learning十分方便&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;大致是这样，我还要继续找工作所以就……先这样&lt;/p&gt;&lt;p&gt;然后我觉得中文文档可以更好的利用起来，别人Keras官方讨论的很热烈，我们也可以搞一个小论坛来玩嘛。建站什么的动作太大，你萌可以把问题，经验神马的开issue到&lt;a href="https://github.com/MoyanZitto/keras-cn/issues" data-title="keras-cn issues" class=""&gt;keras-cn issues&lt;/a&gt;这里来，我会（尽量）每天上去看看有没有能帮忙的地方，也会号召群里的大神们多上去帮忙~&lt;/p&gt;&lt;p&gt;high起来啊大家！打比赛，合作论文，问问题，求debug，吹水，找工作，求内推，想跳槽，做好玩的，都可以来keras-cn issues玩耍~&lt;/p&gt;&lt;p&gt;让我萌把github玩起来吧~！ &lt;/p&gt;&lt;p&gt;好像没人好奇为啥没有1.0.9......&lt;/p&gt;&lt;p&gt;反正我挺好奇的……为啥没1.0.9咧？还是曾经有过然后出大bug就又下了？ &lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22697113&amp;pixel&amp;useReferer"/&gt;</description><author>BigMoyan</author><pubDate>Thu, 29 Sep 2016 16:45:34 GMT</pubDate></item><item><title>【啄米日常】8：Keras1.0.8，都有些啥新闻？</title><link>https://zhuanlan.zhihu.com/p/22258509</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/83270094b2c1b18d5b301d121aa5eb9c_r.jpg"&gt;&lt;/p&gt;3天前，Keras放出了1.x时代的第8个release版本1.0.8~还没有更新的你赶快更新哟！现在，本鸡及本鸡的老婆污码撸就一同为大家介绍一下Keras 1.0.8都有啥了不起的大新闻吧！有观众朋友问了，那之前怎么不见你们俩出来介绍版本变化啊？是不是之前的版本更新都不重要啊？其实不是的，之前没有介绍，主要原因是因为没想起来，本文最后会简单的提一下我能记住的一些“大概老用户还不太清楚”的变化。1.0.8的更新如下：&lt;h2&gt;【新增】Application&lt;/h2&gt;之前跟你们讲keras要做keras zoo了，1.0.8新增的模块Application就是一个Keras zoo，除了VGG系列网络，ResNet50以外，这次一同上线的网络还有Inception V3。也就是说，现在你可以直接用import载入这些网络和对应的权重了。值得注意的是，对每个网络，Keras都提供了4种权重，分别是：&lt;ul&gt;&lt;li&gt;tf后端，dim_ordering=tf&lt;/li&gt;&lt;li&gt;tf后端，dim_ordering=th&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;th后端，dim_ordering=tf&lt;/li&gt;&lt;li&gt;th后端，dim_ordering=th&lt;/li&gt;&lt;/ul&gt;后端和dim_ordering搭配起来版本效率较高，而不匹配的版本提供了无缝兼容用户自身代码的可能性，就问你爽不爽？&lt;h2&gt;【新增】LocallyConnected&lt;/h2&gt;1.0.8Keras还新增了局部连接层模块LocallyConnected，包含LocallyConnected1D和LocallyConnected2D两种层。简单的说，局部连接层类似于卷积层，但不进行权值共享，即滤波器只对固定区域起作用而不进行“滑动”&lt;h2&gt;【新增】SeparableConvolution2D，DeConvolution2D&lt;/h2&gt;1.0.8新增了两种卷积，可分离卷积SeparableConvolution2D和反卷积DeConvolution2D，可分离卷积由一个沿深度方向的卷积跟一个深度方向的逐点卷积构成，内部实现使用的是TensorFlow的&lt;a data-editable="true" class="" data-title="tf.nn.separable_conv2d" href="http://wiki.jikexueyuan.com/project/tensorflow-zh/api_docs/python/nn.html#separable_conv2d"&gt;tf.nn.separable_conv2d&lt;/a&gt;，因此目前这个层只能在TensorFlow作为后端时使用。DeConvolution2D是反卷积，当然卷积本身是不可逆的一个运算，反卷积也是通过学习一堆参数来反的。&lt;h2&gt;【新增】GlobalPooling系列&lt;/h2&gt;1.0.8新增加了全局池化GlobalPooling，包括1D和2D的全局最大值池化、平均值池化，全局的池化就是在整个featuremap的范围做池化，4D的输入tensor，它的输出tensor的shape就只是（nb_samples, channels）而已哟&lt;h2&gt;【新增】Bidirectional包装器 &lt;/h2&gt;继（唯一的）TimeDistributed包装器之后，现在Keras提供了一种新的包装器Bidirectional，如名所示，该包装期可以将一个RNN层，如LSTM，包装为双向的RNN，因此BLSTM可借由此包装器实现，关于BLSTM的例子可以参考Keras的一个示例代码imdb_bidirectional_lstm&lt;h2&gt;【新增】model.predict_generator&lt;/h2&gt; 这个预料之中，fit_generator,evaluate_generator都有了，就等老兄predict_generator构成生成器系列三剑客了，没啥可说的&lt;h2&gt; 【新增/修改】一堆example&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;imdb_bidirectional_lstm：双向LSTM，之前提了&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;image_ocr ：这个例子实现了字符的OCR识别，用了一个CTC loss做字符识别——好吧我也不知道这是啥玩意，有意思的是作者十分耿直：“I have no evidence of whether it actually learns general shapes of text, or just is able to recognize all the different fonts thrown at it...the purpose is more to demonstrate CTC inside of Keras”。这位老兄实在是太可爱&lt;/p&gt;&lt;/li&gt;&lt;li&gt; imdb_fasttext：利用fasttext方法进行文本分类&lt;/li&gt;&lt;li&gt;mnist_hierarchical_rnn：用Hirerachical RNN进行mnist分类&lt;/li&gt;&lt;li&gt;mnist_net2net：用net2net的方法进行mnist分类&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;下面说一下我觉得某些朋友还不太清楚的新特性，不一定是1.0.8版本的，蜻蜓点水的说，产个卵就算完&lt;/p&gt;&lt;ul&gt;&lt;li&gt;保存模型：以前是模型和权重分开保存的，十分不方便，现在调用model.save可以将模型和权重一下子保存成一个文件，重建时用load_model重建回来，重建动作包含模型搭建，权重载入和编译等过程。当然原来的分开保存的API也继续可用&lt;/li&gt;&lt;li&gt;Pooling被割开了：之前的Pooling层位于convolutional里面，现在pooling单独成为了一个模块&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;model.pop：在transfer learning时经常需要丢掉一些层，以前大概都用model.layers.pop()，现在model新增了方法.pop()，每次调用都会抛掉模型的最后一层。&lt;/li&gt;&lt;li&gt;AtrousConvolution2D，新加了一种带洞洞的卷积，是我一个朋友提的PR&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;Adamax和Nadam：新增了两种优化器&lt;/li&gt;&lt;li&gt;kullback_leibler_divergence：新增了一种目标函数，也就是KL距离了&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;flow_from_directory：图像预处理模块的ImageDataGenerator新增了这种方法，按照提示调整好文件夹结构，调用此方法能够自动的从你的文件夹中生成图片数据和标签数据，一键搞定哟&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;好像暂时没什么了，我在1.0.7和1.0.8的对比中还看到几个奇怪的layer，但是文档中没看到，暂时先不写，查证了再补。&lt;/p&gt;&lt;p&gt;今天先这样，欢迎关注&lt;a data-title="Keras中文文档" data-editable="true" href="http://keras-cn.readthedocs.io/en/latest/"&gt;Keras中文文档&lt;/a&gt;哟 &lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22258509&amp;pixel&amp;useReferer"/&gt;</description><author>BigMoyan</author><pubDate>Wed, 31 Aug 2016 22:08:09 GMT</pubDate></item><item><title>【啄米日常】7：一个不负责任的Keras介绍（下）</title><link>https://zhuanlan.zhihu.com/p/22135796</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/af958a62aa26b0a502c4501571042e2e_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;本来上篇文末说，（下）可能会推迟一会儿……&lt;/p&gt;&lt;p&gt;但是我控制不住我即己啊！一开始写就想一下子全部写完啊！！&lt;/p&gt;&lt;p&gt;所以我又开始笔耕不缀了，这篇专栏写完，我基本是棵废菇了……&lt;/p&gt;&lt;p&gt;（下）主要说一下Keras的有用特性，以及一些常见问题，如果还有精力的话，补一些使用Keras的陷阱，没精力这部分就留到番外篇了。理解这些特性对深入了解Keras有比较重要的帮助。&lt;/p&gt;&lt;h2&gt;callable，全部Layer都要callable!&lt;/h2&gt;&lt;p&gt;Keras的一大性质是&lt;b&gt;所有的layer对象都是callable的&lt;/b&gt;。所谓callable，就是能当作函数一样来使用，层的这个性质不需要依赖任何模型就能成立。比方说你想算算一个向量x的sigmoid值是多少，如果用keras的话，你可以这样写：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;import keras.backend as K
from keras.layers import Activation
import numpy as np

x = K.placeholder(shape=(3,))
y = Activation('sigmoid')(x)
f = K.function([x],[y])
out = f([np.array([1,2,3])])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;很显然，我绝对不会写这种代码来求sigmoid，这个例子只是说明，可以这么干，而可以这么干的话很多工作就会很方便。比方说有一次我想给目标函数加一项全变差的正则项，而全变差可以用特定的卷积来实现， 那么我的目标函数的全变差正则项完全就可以用一个Convolution2D层来实现。把层和模型当作张量的函数来使用，是需要认真贯彻落实的一个东西。&lt;/p&gt;&lt;p&gt;顺便我们也复习一下上一篇文章说的符号式计算方法。正文第1行先定义了一个“占位符”，它的shape是一个长为3的向量。所谓占位符就是“先占个位置
“的符号，翻译成中文就是”此处应有一个长为3的向量“。注意第2行，这行我们使用了一个激活层，激活层的激活函数形式是sigmoid，在激活层的后面
又有一个括号，括号内是我们的输入张量x，可以看到，层对象‘Activation('sigmoid')’是被当做一个函数来使用的。上篇文章说层就是
张量到张量的运算，那么其输出y自然也是一个张量。&lt;/p&gt;&lt;p&gt;第3行通过调用function函数对计算图进行编译，这个计算图很简单，就是输入张量经过sigmoid作用变成输出向量，计算图的各种优化通过这一步得以完成，现在，f就是一个真正的函数了，就可以按照一般的方法使用了。&lt;/p&gt;&lt;p&gt;之前说了，&lt;b&gt;模型也是张量到张量的映射，所以Layer是Model的父类&lt;/b&gt;，因此，一个模型本身也可以像上面一样使用。总而言之，在Keras中，层对象是callable的。&lt;/p&gt;&lt;h2&gt;Node：Keras的网络层复用 &lt;/h2&gt;&lt;p&gt;Keras的网络层复用是一个很常用的需求，例如当某一层与多个层相连时，实际上这层是将同一种计算方式复用多次。再比如你用一个网络来抽取两条微博的特征，然后在后面用网络来判断二者是否是同一个主题，那么抽取两次微博的特征这一工作就可以复用同一个网络。&lt;/p&gt;&lt;p&gt;Keras的网络复用由一个叫“Node”，或称“计算节点”的东西来实现。笼统地说，每当在某个输入上调用层时，就会为网络层添加一个节点。这个节点将输入张量映射为输出的张量，当你多次调用该层，就会产生多个结点，结点的下标是0,1,2,3...&lt;/p&gt;&lt;p&gt;如果仅仅是这样，这部分的东西你依然不需要了解，问题在于，当一个层有多个计算节点时，它的input，output，input_shape，output_shape等属性可能是ill-defined的，因为不清楚你想要的output或input是哪一个。&lt;/p&gt;&lt;p&gt;此时，需要使用get_output_at()，get_input_at()，get_output_shape_at()等以at为后缀结尾的函数，at的对象就是层的节点编号。例如get_output_shape_at(2)就会返回第3个输出张量的shape。 &lt;/p&gt;&lt;h2&gt;Shape与Shape自动推断&lt;/h2&gt;&lt;p&gt;使用过Keras的都知道，Keras的所有的层有一个“input_shape”的参数，用来指定输入张量的shape。然而这个input_shape，或者有时候是input_dim，只需要在模型的首层加以指定。一旦模型的首层的input_shape指定了，后面的各层就不用再指定，而会根据计算图自动推断。这个功能称为shape的自动推断。&lt;/p&gt;&lt;p&gt;Keras的自动推断依赖于Layer中的get_output_shape_for函数来实现，如果大家还记得上一篇文章的话，在提到如何编写自己的Keras层时，我们提到如果你的网络层改变了输入张量的shape，就应该复写get_output_shape_for这个函数，以使后面的层能知道本层输出的shape。&lt;/p&gt;&lt;p&gt;在所有的Keras中都有这样一个函数，因此后面的层可以通过查看这个函数的返回值获取前层的输入shape，并通过自己的get_output_shape_for将这个信息传递下去。&lt;/p&gt;&lt;p&gt;然而，有时候，这个自动推断会出错。这种情况发生在一个RNN层后面接Flatten然后又接Dense的时候，这个时候Dense的output_shape无法自动推断出。这时需要指定RNN的输入序列长度input_length，或者在网络的第一层通过input_shape就指定。这种情况极少见，大致有个印象即可，遇到的话知道大概是哪里出了问题就好。&lt;/p&gt;&lt;p&gt;一般而言，神经网络的数据是以batch为单位的，但在指明input_shape时不需要说明一个batch的样本数。假如你的输入是一个224*224*3的彩色图片，在内部运行时数据的shape是(None，224，224，3)，这点在你自己编写层是需要注意。&lt;/p&gt;&lt;h2&gt;TH与TF的相爱相杀&lt;/h2&gt;&lt;p&gt;相爱没有，全是相杀&lt;/p&gt;&lt;p&gt;Keras提供了两套后端，Theano和Tensorflow，这是一件幸福的事，手中拿着馒头，想蘸红糖蘸红糖，想蘸白糖蘸白糖th和tf的大部分功能都被backend统一包装起来了，但二者还是存在不小的冲突，有时候你需要特别注意Keras是运行在哪种后端之上，它们的主要冲突有：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;dim_ordering，也就是维度顺序。比方说一张224*224的彩色图片，theano的维度顺序是(3，224，224)，即通道维在前。而tf的维度顺序是(224，224，3)，即通道维在后。dim_ordering&lt;b&gt;不是&lt;/b&gt;一个必须和后端搭配的指标，它只规定了输入图片的维度顺序，只要输入图片按此维度顺序设置即可正确运行。然而，如果dim_ordering与后端搭配的话——我指的是所有层的dim_ordering都与后端搭配，会提高程序的运行效率。否则，数据的shape会在计算过程中不断转来转去，效率会低一些。&lt;/li&gt;&lt;li&gt;卷积层权重的shape：从无到有训练一个网络，不会有任何问题。但是如果你想把一个th训练出来的卷积层权重载入风格为tf的卷积层……说多了都是泪。我一直觉得这个是个bug，数据的dim_ordering有问题就罢了，为啥卷积层权重的shape还需要变换咧？我迟早要提个PR把这个bug修掉！&lt;/li&gt;&lt;li&gt;然后是卷积层kernel的翻转不翻转问题，这个我们说过很多次了，就不再多提。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;总而言之，相爱没有，全部都是相杀。尽管Keras已经在统一theano和tensorflow上走了很多很多步，但还需要走更多的一些步。&lt;/p&gt;&lt;h2&gt;FAQ与学习资料&lt;/h2&gt;&lt;p&gt;FAQ模块请参考这里：&lt;a data-title="FAQ - Keras中文文档" data-editable="true" href="http://keras-cn.readthedocs.io/en/latest/getting_started/FAQ/"&gt;FAQ - Keras中文文档&lt;/a&gt;&lt;/p&gt;&lt;p&gt;另外，在这里有一些使用示范，包括与TensorFlow的联动，分布式训练等：&lt;/p&gt;&lt;p&gt;&lt;a data-title="CNN眼中的世界" data-editable="true" href="http://keras-cn.readthedocs.io/en/latest/blog/cnn_see_world/"&gt;CNN眼中的世界&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a data-title="花式自动编码器" data-editable="true" class="" href="http://keras-cn.readthedocs.io/en/latest/blog/autoencoder/"&gt;花式自动编码器&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a data-title="面向小数据集构建图像分类模型" data-editable="true" class="" href="http://keras-cn.readthedocs.io/en/latest/blog/image_classification_using_very_little_data/"&gt;面向小数据集构建图像分类模型&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a data-title="在Keras模型中使用预训练的词向量" data-editable="true" href="http://keras-cn.readthedocs.io/en/latest/blog/word_embedding/"&gt;在Keras模型中使用预训练的词向量&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a data-title="将Keras作为tensorflow的精简接口" data-editable="true" class="" href="http://keras-cn.readthedocs.io/en/latest/blog/keras_and_tensorflow/"&gt;将Keras作为tensorflow的精简接口&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class="" data-title="Keras/Python深度学习中的网格搜索超参数调优（附源码）" data-editable="true" href="http://geek.csdn.net/news/detail/95494"&gt;Keras/Python深度学习中的网格搜索超参数调优（附源码）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Keras已经开始组建自己的Keras Zoo，也就是预训练的模型库，这个github在：&lt;/p&gt;&lt;p&gt;&lt;a data-title="GitHub - Keras Zoo" data-editable="true" class="" href="https://github.com/fchollet/deep-learning-models"&gt;GitHub - fchollet/fchollet/deep-learning-models&lt;/a&gt;&lt;/p&gt;&lt;p&gt; 关于我们老提的TH和TF的卷积核转换，这里是使用示范：&lt;/p&gt;&lt;p&gt;&lt;a data-title="TF kernel - TH kernel" data-editable="true" class="" href="https://github.com/fchollet/keras/wiki/Converting-convolution-kernels-from-Theano-to-TensorFlow-and-vice-versa"&gt;TF kernel - TH kernel&lt;/a&gt;&lt;/p&gt;&lt;p&gt;学习Keras最快的方法莫过于直接阅读示例代码了，这里是example：&lt;/p&gt;&lt;p&gt;&lt;a data-title="keras/examples at master · fchollet/keras · GitHub" data-editable="true" class="" href="https://github.com/fchollet/keras/tree/master/examples"&gt;keras examples&lt;/a&gt;&lt;/p&gt;&lt;p&gt;这个哥们儿fork的Keras自己搞了一个caffe到keras的模型转换模块，好像不是很完美，不过说不定能凑合用，先放这儿了：&lt;/p&gt;&lt;p&gt;&lt;a data-title="GitHub - MarcBS/keras: Keras' fork with several new functionalities. Caffe2Keras converter, multimodal layers, etc." data-editable="true" class="" href="https://github.com/MarcBS/keras"&gt;GitHub - MarcBS/keras&lt;/a&gt;&lt;/p&gt;&lt;p&gt;如果有什么疑问请留言提问~能回答的我会回答然后贴到这里……回答不来的……我就装没看见了&lt;/p&gt;&lt;p&gt;最近会出一个Keras使用的陷阱集锦，或称防坑指南，如果能做好的话可以作为番外篇~ &lt;/p&gt;&lt;p&gt;好，先这样吧~鞠躬，下台！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22135796&amp;pixel&amp;useReferer"/&gt;</description><author>BigMoyan</author><pubDate>Tue, 23 Aug 2016 21:57:52 GMT</pubDate></item><item><title>【啄米日常】6：一个不负责任的Keras介绍（中）</title><link>https://zhuanlan.zhihu.com/p/22129301</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/b0f672d59ea6c07a2285375fffcd4bfa_r.jpg"&gt;&lt;/p&gt;上回书我们简单的把Keras的框架理了一下，下面我们深入(也不怎么深)具体的模块理一下Keras，主要聊一聊每个模块的具体功能和核心函数&lt;h2&gt;backend：百货商店&lt;/h2&gt;&lt;p&gt;backend这个模块的主要作用，是对tensorflow和theano的底层张量运算进行了包装。用户不用关心具体执行张量运算的是theano还是tensorflow，就可以编写出能在两个框架下可以无缝对接的程序。backend中的函数要比文档里给出的多得多，完全就是一家百货商店。但一般情况下，文档里给出的那些就已经足够你完成大部分工作了，事实上就连文档里给出的函数大部分情况也不会用，这里提几个比较有用的函数——当然是对我来说比较有用，毕竟这是一份不怎么负责任的介绍，如果你想找对你有用的函数，就去backend淘一淘吧~：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; function：毫无疑问这估计是最有用的一个函数了，function用于将一个计算图（计算关系）编译为具体的函数。典型的使用场景是输出网络的中间层结果。&lt;/li&gt;&lt;li&gt;image_ordering和set_image_ordering：这组函数用于返回/设置图片的维度顺序，由于Theano和Tensorflow的图片维度顺序不一样，所以有时候需要获取/指定。典型应用是当希望网络自适应的根据使用的后端调整图片维度顺序时。&lt;/li&gt;&lt;li&gt;learning_phase：这个函数的主要作用是返回网络的运行状态，0代表测试，1代表训练。当你需要便携一个在训练和测试是行为不同的层（如Dropout）时，它会很有用。&lt;/li&gt;&lt;li&gt;int_shape：这是我最常用的一个函数，用于以整数tuple的形式返回张量的shape。要知道从前网络输出张量的shape是看都看不到的，int_shape可以在debug时起到很大作用。&lt;/li&gt;&lt;li&gt;gradients： 求损失函数关于变量的导数，也就是网络的反向计算过程。这个函数在不训练网络而只想用梯度做一点奇怪的事情的时候会很有用，如图像风格转移。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;backend的其他大部分函数的函数名是望而知义的，什么max，min，equal，eval，zeros，ones，conv2d等等。函数的命名方式跟numpy差不多，下次想用时不妨先‘.’一下，说不定就有。 &lt;/p&gt;&lt;h2&gt;models/layers：Keras的核心主题&lt;/h2&gt;&lt;p&gt;使用Keras最常见的目的，当然还是训练一个网络。之前说了网络就是张量到张量的映射，所以Keras的网络，其实是一个由多个子计算图构成的大计算图。当这些子计算图是顺序连接时，称为Sequential，否则就是一般的model，我们称为泛型模型。&lt;/p&gt;&lt;p&gt;模型不但是张量的计算方式，还是层对象的容器，模型用来将它所含有的层整合起来，大家手拉手一起走&lt;/p&gt;&lt;p&gt;模型有两套训练和测试的函数，一套是fit，evaluate等，另一套是fit_generator，evaluate_generator，前者适用于普通情况，后者适用于数据是以迭代器动态生成的情况。迭代器可以在内存/显存不足，实时动态数据提升进行网络训练，所以使用Keras的话，Python的迭代器这一部分是一定要掌握的内容。对模型而言，最核心的函数有两个：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;compile()：编译，模型在训练前必须编译，这个函数用于完成添加正则项啊，确定目标函数啊，确定优化器啊等等一系列模型配置功能。这个函数必须指定的参数是优化器和目标函数，经常还需要指定一个metrics来评价模型。&lt;/li&gt;&lt;li&gt;fit()/fit_generator()：用来训练模型，参数较多，是需要重点掌握的函数，对于keras使用者而言，这个函数的每一个参数都需要掌握。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;其他的函数请自己学习。 &lt;/p&gt;&lt;p&gt;另外，模型还有几个常用的属性和函数：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;layers：该属性是模型全部层对象的列表，是的就是一个普通的python list&lt;/li&gt;&lt;li&gt;get_layer()：这个函数通过名字来返回模型中某个层对象&lt;/li&gt;&lt;li&gt;pop()：这个函数文档里没有，但是可以用。作用是弹出模型的最后一层，从前进行finetune时没有pop，大家一般用model.layers.pop()来完成同样的功能。 &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;当然，因为Model是Layer的子类，Layer的所有属性和方法也自动被Model所有，这些有用的属性稍后介绍。&lt;/p&gt;&lt;p&gt;Keras的层对象是构筑模型的基石，除了卷积层，递归神经网络层，全连接层，激活层这种烂大街的Layer对象外，Keras还有一些不是那么烂大街的东西：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Advanced Activation：高级激活层，主要收录了包括leakyReLU，pReLU，ELU，SReLU等一系列高级激活函数，这些激活函数不是简单的element-wise计算，所以单独拿出来实现一下&lt;/li&gt;&lt;li&gt;Merge层：这个层用于将多个层对象的输出组合起来，支持级联、乘法、余弦等多种计算方式，它还有个小兄弟叫merge，这个函数完成与Merge相同的作用，但输入的对象是张量而不是层对象。&lt;/li&gt;&lt;li&gt;Lambda层：这是一个神奇的层，看名字就知道它用来把一个函数作用在输入张量上。这个层可以大大减少你的工作量，当你需要定义的新层的计算不是那么复杂的时候，可以通过lambda层来实现，而不用自己完全重写。&lt;/li&gt;&lt;li&gt;Highway/Maxout/AtrousConvolution2D层：这个就不多说了，懂的人自然懂，keras还是在一直跟着潮流走的&lt;/li&gt;&lt;li&gt;Wrapper层：Wrapper层用于将一个普通的层对象进行包装升级，赋予其更多功能。目前，Wrapper层里有一个TimeDistributed层，用于将普通的层包装为对时间序列输入处理的层，而Bidirectional可以将输入的递归神经网络层包装为双向的（如把LSTM做成BLSTM） &lt;/li&gt;&lt;li&gt;Input：补一个特殊的层，Input，这个东西实际上是一个Keras tensor的占位符，主要用于在搭建Model模型时作为输入tensor使用，这个Input可以通过keras.layers来import。&lt;/li&gt;&lt;li&gt;stateful与unroll：Keras的递归神经网络层，如SimpleRNN，LSTM等，支持两种特殊的操作。一种是stateful，设置stateful为True意味着训练时每个batch的状态都会被重用于初始化下一个batch的初始状态。另一种是unroll，unroll可以将递归神经网络展开，以空间换取运行时间。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Keras的层对象还有一些有用的属性和方法，比较有用的是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;name：别小看这个，从茫茫层海中搜索一个特定的层，如果你对数数没什么信心，最好是name配合get_layer()来用。&lt;/li&gt;&lt;li&gt;trainable：这个参数确定了层是可训练的还是不可训练的，在迁移学习中我们经常需要把某些层冻结起来而finetune别的层，冻结这个动作就是通过设置trainable来实现的。&lt;/li&gt;&lt;li&gt;input/output：这两个属性是层的输入和输出张量，是Keras tensor的对象，这两个属性在你需要获取中间层输入输出时非常有用&lt;/li&gt;&lt;li&gt;get_weights/set_weights：这是两个方法用于手动取出和载入层的参数，set_weights传入的权重必须与get_weights返回的权重具有同样的shape，一般可以用get_weights来看权重shape，用set_weights来载入权重&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;既然是核心主题，我们就多唠两句，在Keras中经常有的一个需求是需要自己编写一个新的层，如果你的计算比较简单，那可以尝试通过Lambda层来解决，如果你不得不编写一个自己的层，那也不是什么大不了的事儿。前两天群里有朋友想编写一个卷积核大小不一样的卷积层（虽然不知道为啥他这么想不开……活着不好吗？），这个显然就要自己编写层了。 &lt;/p&gt;&lt;p&gt;要在Keras中编写一个自己的层，需要开一个从Layer（或其他层）继承的类，除了__init__以为你需要覆盖三个函数：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;build，这个函数用来确立这个层都有哪些参数，哪些参数是可训练的哪些参数是不可训练的。&lt;/li&gt;&lt;li&gt;call，这个函数在调用层对象时自动使用，里面就是该层的计算逻辑，或计算图了。显然，这个层的核心应该是一段符号式的输入张量到输出张量的计算过程。&lt;/li&gt;&lt;li&gt;get_output_shape_for：如果你的层计算后，输入张量和输出张量的shape不一致，那么你需要把这个函数也重新写一下，返回输出张量的shape，以保证Keras可以进行shape的自动推断&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;其实也不难~是吧，不要忘记Keras是基于Python的框架，你可以随时随地查看那些已经写好的层的代码，模仿着看看你自己的层要怎么写~&lt;/p&gt;&lt;h2&gt;优化器，目标函数，初始化策略，等等... &lt;/h2&gt;&lt;p&gt;和model，layers这种核心功能相比，这些模块的重要性就没有那么大，我们简单介绍一下，里面的具体技术，（下）篇可能会说，也可能不会……我还没想好，但是基本上不说也没什么影响&lt;/p&gt;&lt;p&gt;objectives是优化目标， 它本质上是一个从张量到数值的函数，当然，是用符号式编程表达的。具体的优化目标有mse，mae，交叉熵等等等等，根据具体任务取用即可，当然，也支持自己编写。需要特别说明的一点是，如果选用categorical_crossentropy作为目标函数，需要将标签转换为one-hot编码的形式，这个动作通过utils.np_utils.to_categorical来完成（记得上篇我就提过了）&lt;/p&gt;&lt;p&gt;optimizers是优化器，没什么可说了，如何选用合适的优化器不在本文讨论范畴。注意模型是可以传入优化器对象的，你可以自己配置一个SGD，然后将它传入模型中。 另外，最新版本的Keras为所有优化器额外设置了两个参数clipnorm和clipvalue，用来对梯度进行裁剪。&lt;/p&gt;&lt;p&gt;activation是激活函数，这部分的内容一般不直接使用，而是通过激活层Activation来调用，此处的激活函数是普通的element-wise激活函数，如果想使用高级激活函数，请翻到高级激活函数层。&lt;/p&gt;&lt;p&gt;callback是回调函数，这其实是一个比较重要的模块，回调函数不是一个函数而是一个类，用于在训练过程中收集信息或进行某种动作。比如我们经常想画一下每个epoch的训练误差和测试误差，那这些信息就需要在回调函数中收集。预定义的回调函数中CheckModelpoint，History和EarlyStopping都是比较重要和常用的。其中CheckPoint用于保存模型，History记录了训练和测试的信息，EarlyStopping用于在已经收敛时提前结束训练。回调函数LearningRateScheduler支持按照用户的策略调整学习率，做模型精调或研究优化器的同学可能对这个感兴趣。&lt;/p&gt;&lt;p&gt;值得注意的是，&lt;b&gt;History是模型训练函数fit的返回值&lt;/b&gt;，也就是说即使你没有使用任何回调函数，找一个变量接住model.fit()，还是能得到不少训练过程中的有用信息。&lt;/p&gt;&lt;p&gt;另外，回调函数还支持将信息发送到远程服务器，以及与Tensorflow的tensorboard联动，在网页上动态展示出训练和测试的情况（需要使用tensorflow为后端）&lt;/p&gt;&lt;p&gt;回调函数支持用户自定义，定义方法也非常简单，请参考文档说明编写即可&lt;/p&gt;&lt;p&gt;初始化方法，正则项，约束项，可视化没有什么特别值得注意的，就略过了。Keras中所有的模块都是可以用户自己定义的，这就是开源和Python的魅力，讲真你让我拿C++写这么个东西……我光把结构摸清楚就要吐血了！&lt;/p&gt;&lt;p&gt;另一个文档中没有但实际上有的东西是metrices，这里面定义了一系列用于评价模型的指标，例如“accuracy”。在训练模型时，可以选择一个或多个指标来衡量模型性能。&lt;/p&gt;&lt;h2&gt;数据预处理和utils&lt;/h2&gt;&lt;p&gt;数据预处理是Keras提供的用于预处理图像、文本和序列数据的一套工具，这个地方就属于各回各家各找各妈了，你处理什么问题就从这里面找什么工具。&lt;/p&gt;&lt;p&gt;特别指出的是，数据预处理的图像预处理部分，提供了一套用于实时图像数据提升的工具，这个东西支持用各种各样的方法对输入图像进行数据提升，然后以生成器的形式返回。另外，该工具还支持从文件夹中自动生成数据和标签，简直方便的不要不要的。&lt;/p&gt;&lt;p&gt;utils文档中没有，里面包含的函数也不必怎么了解，除了两个。一个是说了很多遍的to_catgoraical，另一个是convert_kernel。后者的主要作用是把卷积滤波器的卷积核在th和tf之间互相转换。theano和tensorflow相爱想杀，到处搞对抗。其中之一就是卷积核，卷积这个东西，按照信号与系统（哼，才不会告诉你们我是信号系统助教咧）的定义，是翻转-&amp;gt;平移-&amp;gt;相乘-&amp;gt;相加。但反正卷积网络的卷积核都是训练出来的，翻转不翻转有什么关系？&lt;/p&gt;&lt;p&gt;所以有些人没翻转，有些人翻转了。是的，说的就是你俩，theano和tensorflow。于是如果一个网络预训练权重是由其中一种后端训练出来的，又要在另一种后端上跑，那么你就需要用kernel_convert这个函数搞一搞了。&lt;/p&gt;&lt;p&gt;估计这事儿太不地道，作者也看不下去了。现在utils出了一个新的layer_utils，里面有一个convert_all_kernels_in_model函数，用来把一个模型的所有卷积核全部进行转换，以后就用这个吧~&lt;/p&gt;&lt;h2&gt;与scikit-learn联动 &lt;/h2&gt;&lt;p&gt;上一篇有人留言说希望多讲点这块的内容，很抱歉……我……我也不怎么会，原谅我毕竟是一只菜鸡&lt;/p&gt;&lt;p&gt;虽然不怎么会，但是不妨碍我知道这应该是一个非常重要和有潜力的功能。Keras与scikit-learn的协作通过keras.wrapper实现，在这个脚本里定义了两个类，分别是KerasClassifier和KerasRegressor，搭建好Sequential模型（只能是Sequential）将被它们包装为sklearn的分类器和迭代器加入的sklearn的工作流中。&lt;/p&gt;&lt;p&gt;这里有一个使用sklearn对Keras进行超参数调整的例子，大家可以参考这篇文章学习Keras和sklearn的联动：&lt;a class="" data-title="Keras/Python深度学习中的网格搜索超参数调优（附源码）" data-editable="true" href="http://geek.csdn.net/news/detail/95494"&gt;Keras/Python深度学习中的网格搜索超参数调优（附源码）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;中篇就到这里，下篇来介绍Keras的实现原理/原则，常见问题与解答，以及Keras中比较隐蔽和诡异的坑。可能会过一段时间才发哟~最近还是略忙&lt;/p&gt;&lt;p&gt;另外你们觉得我这种菜鸡能找到啥工作啊有没有内推什么的求往脸上砸！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22129301&amp;pixel&amp;useReferer"/&gt;</description><author>BigMoyan</author><pubDate>Tue, 23 Aug 2016 14:07:32 GMT</pubDate></item><item><title>【啄米日常】5：一个不负责任的Keras介绍（上）</title><link>https://zhuanlan.zhihu.com/p/22129946</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/b9e25c524119f16f2376979d3cbf5483_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;如果很长时间不更专栏的话，会不会给大家一种本菜鸡平常不怎么学习的错(zhen)觉(xiang)？&lt;/p&gt;&lt;p&gt;还是稍微更一更吧，听说知乎最近引进了打赏功能……是吧&lt;/p&gt;&lt;p&gt;准备分上下两篇，也可能是上中下三篇，来做一个（不）负责任的Keras介绍~上篇主要从宏观的角度来讲，属于面向新手的。中/下篇细节一点，偏重原理/特色。&lt;/p&gt;&lt;p&gt;另外，最近准备做一个Keras使用过程中常见坑的踩法，如果大家在Keras使用过程中遇到过什么坑，不妨私信告我哦。好的，开始了&lt;/p&gt;&lt;h2&gt;Keras：宏观特性 &lt;/h2&gt;&lt;p&gt;Keras是最近蒸蒸日上的深度学习框架， 非常的蒸蒸日上，5月的时候有这么个图：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" data-rawheight="462" data-rawwidth="690" src="c15027968033d49109d03f2bd82c3069.jpg"&gt;Caffe是老牌选手，Tensorflow有个神爹，不跟这俩比Keras的表现还是十分亮眼的，我想现在如果有排名也会一样出色（注意mxnet是万年老5虽然我一直觉得mxnet其实非常出色……）&lt;/p&gt;&lt;p&gt;那么，Keras有啥特点呢，我想下面这些可能是属于Keras的关键词：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;符号主义&lt;/li&gt;&lt;li&gt;Python &lt;/li&gt;&lt;li&gt;快速原型&lt;/li&gt;&lt;li&gt;轻量级，高度模块化 &lt;/li&gt;&lt;li&gt;易扩展&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Keras事实上是一个基于Theano和Tensorflow上的一个包装，所谓站在巨人的肩膀上也不外如是了。 因为Theano和Tensorflow都是符号主义的东西（下面讲再说这是啥），因此Keras自然也是符号主义的。&lt;/p&gt;&lt;p&gt;Keras由纯Python编写，这意味着它的源代码简单易懂，你可以随时进去看看它都做了什么，怎么做的。并且，当你需要修改源代码的时候，大胆修改就可以了，它会立刻生效。尽管Python的运行效率要低于C++，但Keras只是Tensorflow和Theano的包装而已，这层包装的运行代价是很小的。&lt;/p&gt;&lt;p&gt;Keras是一款高度模块化的框架，使用它搭建网络和训练网络将会非常容易，如果你需要深入模型中控制细节，通常使用Keras提供的一些函数就可以了，很少需要深入到Tensorflow或Theano那一层。因此，Keras适合于快速原型生成，如果你经常需要很快的实现一下自己的idea，Keras会是一个不错的选择。&lt;/p&gt;&lt;p&gt;另外，Keras的预训练模型库也在逐步建设，目前有VGG-16，VGG-19，resnet50，Inceptionv3四种预训练好的模型供大家使用。 &lt;/p&gt;&lt;h2&gt;计算图，符号主义和张量&lt;/h2&gt;&lt;p&gt;符号主义，Google一下会发现是一个机器学习的名词，但我们这说的符号主义不是那个东西，这里说的符号主义，指的是使用&lt;b&gt;符号式编程&lt;/b&gt;的一种方法。 另一种相对的方法是&lt;b&gt;命令式编程&lt;/b&gt;。或者是&lt;/p&gt;&lt;p&gt;要说Theano/Tensorflow/Keras，就不能不提它的符号主义特性&lt;/p&gt;&lt;p&gt;事实上，Theano也好，Tensorflow也好，其实是一款符号主义的计算框架，未必是专为深度学习设计的。假如你有一个与深度学习完全无关的计算任务想运行在GPU上，你完全可以通过Theano/Tensorflow编写和运行。&lt;/p&gt;&lt;p&gt;假如我们要求两个数a和b的和，通常只要把值赋值给a和b，然后计算a+b就可以了，正常人类都是这么写的：&lt;/p&gt;&lt;p&gt;a=3b=5z = a + b &lt;/p&gt;&lt;p&gt;运行到第一行，a真的是3.运行到第2行，b真的是5，然后运行第三行，电脑真的把a和b的值加起来赋给z了。&lt;/p&gt;&lt;p&gt;一点儿都不神奇。 &lt;/p&gt;&lt;p&gt;但总有不正常的，不正常的会这么想问题：a+b这个计算任务，可以分为三步。（1）声明两个变量a，b。建立输出变量z（2）确立a，b和z的计算关系，z=a+b（3）将两个数值a和b赋值到变量中，计算结果z&lt;/p&gt;&lt;p&gt;后面那种“先确定符号以及符号之间的计算关系，然后才放数据进去计算”的办法，就是符号式编程。当你声明a和b时，它们里面是空的。当你确立z=a+b的计算关系时，a，b和z仍然是空的，只有当你真的把数据放入a和b了，程序才开始做计算。&lt;/p&gt;&lt;p&gt;符号之间的运算关系，就称为运算图。 &lt;/p&gt;&lt;p&gt;这样做当然不是闲的无聊，符号式计算的一大优点是，当确立了输入和输出的计算关系后，在进行运算前我们可以对这种运算关系进行自动化简，从而减少计算量，提高计算速度。另一个优势是，运算图一旦确定，整个计算过程就都清楚了，可以用内存复用的方式减少程序占用的内存。&lt;/p&gt;&lt;p&gt;在Keras，theano和Tensorflow中，参与符号运算的那些变量统一称作张量。张量是矩阵的进一步推广。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;规模最小的张量是0阶张量，即标量，也就是一个数。&lt;/p&gt;&lt;p&gt;当我们把一些数有序的排列起来，就形成了1阶张量，也就是一个向量&lt;/p&gt;&lt;p&gt;如果我们继续把一组向量有序的排列起来，就形成了2阶张量，也就是一个矩阵&lt;/p&gt;&lt;p&gt;把矩阵摞起来，就是3阶张量，我们可以称为一个立方体，具有3个颜色通道的彩色图片就是一个这样的立方体&lt;/p&gt;&lt;p&gt;把矩阵摞起来，好吧这次我们真的没有给它起别名了，就叫4阶张量了，不要去试图想像4阶张量是什么样子，它就是个数学上的概念。 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;一言以蔽之，Keras的计算过程，就是建立一个从&lt;b&gt;张量到张量的映射函数&lt;/b&gt;，然后再放入真实数据进行计算。对深度学习而言，这个“映射函数”就是一个神经网络，而神经网络中的每个层自然也都是从张量到张量的映射。&lt;/p&gt;&lt;h2&gt;Keras框架结构&lt;/h2&gt;&lt;p&gt;我想画一个图，可是想了半天画不明白……我就罗列就好了，Keras的结构大致是这样的：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;backend：后端，对Tensorflow和Theano进行封装，完成低层的张量运算、计算图编译等&lt;/li&gt;&lt;li&gt;models：模型，模型是层的有序组合，也是层的“容器”，是“神经网络”的整体表示&lt;/li&gt;&lt;li&gt;layers：层，神经网络的层本质上规定了一种从输入张量到输出张量的计算规则，显然，整个神经网络的模型也是这样一种张量到张量的计算规则，因此keras的model是layer的子类&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;上面的三个模块是Keras最为要紧和核心的三块内容，搭建一个神经网络，就只用上面的内容即可。注意的是，backend虽然很重要，但其内容多而杂，大部分内容都是被其他keras模块调用，而不是被用户直接使用。所以它不是新手马上就应该学的，初学Keras不妨先将backend放一旁，从model和layers学起。&lt;/p&gt;&lt;p&gt;为了训练神经网络，必须定义一个神经网络优化的目标和一套参数更新的方式，这部分就是目标函数和优化器：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;objectives：目标函数，规定了神经网络的优化方向&lt;/li&gt;&lt;li&gt;optimizers：优化器，规定了神经网络的参数如何更新&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;上面的两个模块的内容，是在训练一个网络时必须提供的。此外，Keras提供了一组模块用来对神经网络进行配置：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;initialization：初始化策略，规定了网络参数的初始化方法&lt;/li&gt;&lt;li&gt;regularizers：正则项，提供了一些用于参数正则的方法，以对抗过拟合&lt;/li&gt;&lt;li&gt;constraints：约束项，提供了对网络参数进行约束的方法&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;为了方便调试、分析和使用网络，处理数据，Keras提供了下面的模块：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;callbacks：回调函数，在网络训练的过程中返回一些预定义/自定义的信息&lt;/li&gt;&lt;li&gt;visualization：可视化，用于将网络结构绘制出来，以直观观察&lt;/li&gt;&lt;li&gt;preprocessing：提供了一组用于对文本、图像、序列信号进行预处理的函数 &lt;/li&gt;&lt;li&gt;utils：常用函数库，比较重要的是utils.np_utils中的to_categorical，用于将1D标签转为one-hot的2D标签和convert_kernel函数，用于将卷积核在theano模式和Tensorflow模式之间转换。最新的代码显示utils的utils.layer_utils里提供了将模型中全部卷积核进行模式转换的函数。大部分其他utils的函数你或许很难用到，但有空不妨一读，或有进益。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;最后，为了能让用户一上手就能跑一些模型，Keras提供了一个常用数据库的模块，用来载入常用的数据库：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;datasets：提供了一些常用数据库的接口，用户将通过这些接口下载和载入数据集&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;额外的一点是，如果用户希望将Keras与scikit-learn联动，Keras也提供了这种联动机制，这个模块是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; wrappers.scikit-learn &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;嗯，这个篇幅可以了，这篇先这样。中篇我们把各个模块的东西详细点儿聊一聊，下篇……写着再看，但肯定有下篇的&lt;/p&gt;&lt;p&gt;谢谢大家~&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22129946&amp;pixel&amp;useReferer"/&gt;</description><author>BigMoyan</author><pubDate>Mon, 22 Aug 2016 22:49:57 GMT</pubDate></item><item><title>【啄米日常】4：快来围观Keras的“keras zoo”</title><link>https://zhuanlan.zhihu.com/p/21868244</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/5b3ccc91dbf5fa01097674283bdd805e_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;Caffe 之所以这么流行，我看一大半的功劳要归功于“caffe zoo”。自己编写或训练一个网络总是困难的，而在别人网络的基础上finetune就爽太多了。&lt;/p&gt;&lt;p&gt;Caffe最先出来-&amp;gt;很多研究成果用Caffe实现-&amp;gt;壮大了Caffe Zoo-&amp;gt;follower从caffe zoo下载模型做新的研究-&amp;gt;进一步壮大caffe zoo&lt;/p&gt;&lt;p&gt;良性循环~&lt;/p&gt;&lt;p&gt;Keras终于觉醒啦！&lt;/p&gt;&lt;p&gt;就在&lt;b&gt;十几个小时&lt;/b&gt;之前，fchollet新开了一个github仓库，“deep learning models”，据介绍，这个仓库将收录很多流行网络的&lt;b&gt;Keras实现&lt;/b&gt;和&lt;b&gt;预训练权重&lt;/b&gt;，说白了就是个Keras Zoo了&lt;/p&gt;&lt;p&gt;目前收录的网络有：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;VGG-16&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;VGG-19&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Resnet50 &lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;咦，等等……好像混进去什么了不得的东西！&lt;/p&gt;&lt;p&gt;没错！本菜鸡贡献的50层残差网络在十几个小时之前被merge到Keras中了，而且自恋的想一想，可能正是这个50层残差网络使fchollet神起了建立一个“keras zoo”的念头，然后创建了这个仓库。&lt;/p&gt;&lt;p&gt;啊……好激动，比PR被merge还激动，菜鸡原来也有能改写历史（并没有）的一天吗？&lt;/p&gt;&lt;p&gt;今晚吃顿好的奖励一下自己~&lt;/p&gt;&lt;p&gt;对了，fchollet神的这个版本基于我的代码进行了补充，添加了各种说明，运用了各种花式代码技巧——这些都不是最重要的，最重要的是fchollet神解决了困扰我好久的问题，就是对theano后端的支持。&lt;/p&gt;&lt;p&gt;&lt;b&gt;现在，resnet支持tensorflow和theano两种后端&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Keras Zoo的下一步工作是添加Inception V3网络，欢迎大家持续围观，下载使用！&lt;/p&gt;&lt;p&gt;对了，github地址在这里：&lt;a class="" href="https://github.com/fchollet/deep-learning-models"&gt;https://github.com/fchollet/deep-learning-models&lt;/a&gt;&lt;/p&gt;&lt;p&gt;鞠躬！&lt;/p&gt;&lt;p&gt;（你们说要不要把那个152层的大家伙也弄进去呢……还有一个1000层的咧~）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21868244&amp;pixel&amp;useReferer"/&gt;</description><author>BigMoyan</author><pubDate>Thu, 04 Aug 2016 16:21:58 GMT</pubDate></item></channel></rss>