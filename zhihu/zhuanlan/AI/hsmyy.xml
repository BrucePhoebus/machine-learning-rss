<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>无痛的机器学习 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/hsmyy</link><description>专栏主营业务：让更多人能看的懂的机器学习科普+进阶文章。欢迎各位大神投稿或协助审阅。</description><lastBuildDate>Sun, 05 Mar 2017 16:15:59 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>CTC实现——compute ctc loss（1）</title><link>https://zhuanlan.zhihu.com/p/23293860</link><description>&lt;p&gt;上一次我们介绍了关于CTC的一些基本问题，下面我们还是落到实处，来介绍一个经典的CTC实现代码——来自百度的warp-ctc。这次百度没有作恶……&lt;/p&gt;&lt;h2&gt;CTC：前向计算例子&lt;/h2&gt;&lt;p&gt;这里我们直接使用warp-ctc中的变量进行分析。我们定义T为RNN输出的结果的维数，这个问题的最终输出维度为alphabet_size。而ground_truth的维数为L。也就是说，RNN输出的结果为alphabet_size*T的结果，我们要将这个结果和1*L这个向量进行对比，求出最终的Loss。&lt;/p&gt;&lt;p&gt;我们要一步一步地揭开这个算法的细节……当然这个算法的实现代码有点晦涩……&lt;/p&gt;&lt;p&gt;我们的第一步要顺着test_cpu.cpp的路线来分析代码。第一步我们就是要解析small_test()中的内容。也就是做前向计算，计算对于RNN结果来说，对应最终的ground_truth——t的label的概率。&lt;/p&gt;&lt;p&gt;这个计算过程可以用动态规划的算法求解。我们可以用一个变量来表示动态规划的中间过程，它就是：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\alpha^T_i&lt;/equation&gt;：表示在RNN计算的时间T时刻，这一时刻对应的ground_truth的label为第i个下标的值t[i]的概率。&lt;/p&gt;&lt;p&gt;这样的表示有点抽象，我们用一个实际的例子来讲解：&lt;/p&gt;&lt;p&gt;RNN结果：&lt;equation&gt;[R_1,R_2,R_3,R_4]&lt;/equation&gt;，这里的每一个变量都对应一个列向量。&lt;/p&gt;&lt;p&gt;ground_truth：&lt;equation&gt;[g_1,g_2,g_3]&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;那么&lt;equation&gt;\alpha^2_1&lt;/equation&gt;表示&lt;equation&gt;R_2&lt;/equation&gt;的结果对应着&lt;equation&gt;g_1&lt;/equation&gt;的概率，当然与此同时，前面的结果也都合理地对应完成。&lt;/p&gt;&lt;p&gt;从上面的结果我们可以看出，如果&lt;equation&gt;R_2&lt;/equation&gt;的结果对应着&lt;equation&gt;g_1&lt;/equation&gt;，那么&lt;equation&gt;R_1&lt;/equation&gt;的结果也必然对应着&lt;equation&gt;g_1&lt;/equation&gt;。所以前面的结果是确定的。然而对于其他的一些情况来说，我们的转换存在着一定的不确定性。&lt;/p&gt;&lt;h2&gt;CTC：前向计算具体过程&lt;/h2&gt;&lt;p&gt;我们还是按照上面的例子进行计算，我们把刚才的例子搬过来：&lt;/p&gt;&lt;p&gt;RNN结果：&lt;equation&gt;[R_1,R_2,R_3,R_4]&lt;/equation&gt;，这里的每一个变量都对应一个列向量。&lt;/p&gt;&lt;p&gt;ground_truth：&lt;equation&gt;[g_1,g_2,g_3]&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;alphabet：&lt;equation&gt;[g_0(blank),g_1,g_2,g_3]&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;按照上面介绍的计算方法，第一步我们先做ground_truth的状态扩展，于是我们就把长度从3扩展到了7，现在的ground_truth变成了：&lt;/p&gt;&lt;equation&gt;[blank,g_1,blank,g_2,blank,g_3,blank]&lt;/equation&gt;&lt;p&gt;我们的RNN结果长度为4，也就是说我们会从上面的7个ground_truth状态中进行转移，并最终转移到最终状态。理论上利用动态规划的算法，我们需要计算4*7=28个中间结果。好了，下面我们用&lt;equation&gt;P^T_i&lt;/equation&gt;表示RNN的第T时刻状态为ground_truth中是第i个位置的概率。&lt;/p&gt;&lt;p&gt;那么我们就开始计算了：&lt;/p&gt;&lt;p&gt;T=1时，我们只能选择&lt;equation&gt;g_1&lt;/equation&gt;和blank，所以这一轮我们终结状态只可能落在0和1上。所以第一轮变成了：&lt;/p&gt;&lt;equation&gt;[P^1_0,P^1_1,0,0,0,0,0]&lt;/equation&gt;&lt;p&gt;T=2时，我们可以继续选择&lt;equation&gt;g_1&lt;/equation&gt;，我们同时也可以选择&lt;equation&gt;g_2&lt;/equation&gt;，还可以选择&lt;equation&gt;g_1&lt;/equation&gt;和&lt;equation&gt;g_2&lt;/equation&gt;之间的blank，所以我们可以进一步关注这三个位置的概率，于是我们将其他的位置的概率设为0。&lt;equation&gt;[0,(P^1_0 +P^1_1)P^2_1,P^1_1P^2_2,P^1_1P^2_3,0,0,0]&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;T=3时，留给我们的时间已经不多了，我们还剩2步，要走完整个旅程，我们只能选择&lt;equation&gt;g_2&lt;/equation&gt;，&lt;equation&gt;g_3&lt;/equation&gt;以及它们之间的空格。于是乎我们关心的位置又发生了变化：&lt;/p&gt;&lt;equation&gt;[0,0,0,
(P^1_1P^2_2+P^1_1P^2_3)P^3_3,
P^1_1P^2_3P^3_4,
P^1_1P^2_3P^3_5,
0]&lt;/equation&gt;&lt;p&gt;是不是有点看晕了？没关系，因为还剩最后一步了。下面是最后一步，因为最后一步我们必须要到&lt;equation&gt;g_3&lt;/equation&gt;以及它后面的空格了，所以我们的概率最终计算也就变成了：&lt;/p&gt;&lt;equation&gt;[0,0,0,
0,0,
((P^1_1P^2_2+P^1_1P^2_3)P^3_3+P^1_1P^2_2P^3_4+P^1_1P^2_2P^3_3)P^4_5,
P^1_1P^2_3P^3_5P^4_6]&lt;/equation&gt;&lt;p&gt;好吧，最终的结果我们求出来了，实际上这就是通过时间的推移不断迭代求解出来的。关于迭代求解的公式这里就不再赘述了。我们直接来看一张图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-dc44563d358df08920d55336866c1125.jpg" data-rawwidth="960" data-rawheight="1280"&gt;&lt;p&gt;于是乎我们从这个计算过程中发现一些问题：&lt;/p&gt;&lt;p&gt;首先是一个相对简单的问题，我们看到在计算过程中我们发现了大量的连乘。由于每一个数字都是浮点数，那么这样连乘下去，最终数字有可能非常小而导致underflow。所以我们要将这个计算过程转到对数域上。这样我们就将其中的乘法转变成了加法。但是原本就是加法的计算呢？比方说我们现在计算了loga和logb，我们如何计算log(a+b)呢，这里老司机给出了解决方案，我们假设两个数中a&amp;gt;b，那么有&lt;/p&gt;&lt;equation&gt;log(a+b)=log(a(1+\frac{b}{a}))=loga+log(1+\frac{b}{a})&lt;/equation&gt;&lt;equation&gt;=loga+log(1+exp(log(\frac{b}{a})))=loga+log(1+exp(logb - loga))&lt;/equation&gt;&lt;p&gt;这样我们就利用了loga和logb计算出了log(a+b)来。&lt;/p&gt;&lt;p&gt;另外一个问题就是，我们发现在刚才的计算过程当中，对于每一个时间段，我们实际上并不需要计算每一个ground-truth位置的概率信息，实际上只要计算满足某个条件的某一部分就可以了。所以我们有没有希望在计算前就规划好这条路经，以保证我们只计算最相关的那些值呢？&lt;/p&gt;&lt;h2&gt;如何控制计算的数量？&lt;/h2&gt;&lt;p&gt;不得不说，这一部分warp-ctc写得实在有点晦涩，当然也可能是我在这方面的理解比较渣。我们这里主要关注两个部分——一个是数据的准备，一个是最终的数据的使用。&lt;/p&gt;&lt;p&gt;在介绍数据准备之前，我们先简单说一下这部分计算的大概思路。我们用两个变量start和end表示我们需要计算的状态的起止点，在每一个时间点，我们要更新start和end这两个变量。然后我们更新start和end之间的概率信息。这里我们先要考虑一个问题，start和end的更新有什么规律？&lt;/p&gt;&lt;p&gt;为了简化思考，我们先假设ground_truth中没有重复的label，我们的大脑瞬间得到了解放。好了，下面我们就要给出代码中的两个变量——&lt;/p&gt;&lt;p&gt;T：表示RNN结果中的维度&lt;/p&gt;&lt;p&gt;S/2：ground_truth的维度（S表示了扩展blank之后的维度）&lt;/p&gt;&lt;p&gt;基本上具备一点常识，我们就可以知道T&amp;gt;=S/2。什么？你觉得有可能出现T&amp;lt;S/2的情况？兄弟，这种见鬼的事情如果发生，你难道要我们把RNN的结果拆开给你用？臣妾不太能做得到啊……&lt;/p&gt;&lt;p&gt;好了，既然接受了上面的事实，那么我们就来举几个例子看看：&lt;/p&gt;&lt;p&gt;我们假设T=3，S/2=3，那么说白了，它们之间的对应关系是一一对应，说白了这就和blank位置没啥关系了。在T=1时，我们要转移到第一个结果，T=2，我们要转移到第二个结果……&lt;/p&gt;&lt;p&gt;那么我们还有别的情况么？下回更精彩。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23293860&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Thu, 23 Feb 2017 23:36:43 GMT</pubDate></item><item><title>CTC——下雨天和RNN更配哦</title><link>https://zhuanlan.zhihu.com/p/23308976</link><description>前面我们一直在讲CNN的事情，从今天开始，我们要尝试扩展新方向了！这个新方向就是RNN。介绍RNN的时候，我的内心还是充满紧张的。因为相对而言RNN的理解还是会比CNN弱一些。但是万事开头难，然后中间难，最后结尾难。所以我们一起开车了。&lt;p&gt;RNN的结构我们放在后面介绍，我们先来看一下RNN和CNN相比的特点：对输入输出的维度不必限定。这一点和CNN很不相同。虽然不限定维度的输入输出带来了极大的灵活性，但是也带来了一些其他的问题。当然这些问题我们放在后面慢慢说。下面我们来关注一下CTC的背景。&lt;/p&gt;&lt;p&gt;这个问题就是对齐问题。比方说对于OCR，我们利用RNN进行识别，我们会得到一连串的输出结果，但是真实的结果可能比这一连串输出要少，所以我们需要做一个对齐的工作：哪些RNN输出对应着同一个真实结果？&lt;/p&gt;&lt;p&gt;关于这个问题，我们可以使用CTC解决。我们先来约定下各种变量的名字，不然一会儿会要晕掉的：&lt;/p&gt;&lt;p&gt;我们把RNN输出的内容就称为R，RNN会随着时间的推进不断记忆前面的信息从而后面的结果，我们在此定义RNN经过的时间长度为T，那么RNN输出结果中的一个维度就确定了。&lt;/p&gt;&lt;p&gt;RNN的另一个维度和输出的类别数量有关。在CNN的分类问题中，我们经常使用softmax层输出表示输入内容对所有类别的概率，这里我们同样用RNN输出这样的信息，我们定义类别的数量为C，那么现在我们就可以确定主角之一的三围，哦不是二围了：C*T。&lt;/p&gt;&lt;p&gt;这个二维的RNN输出要匹配的是真正的ground_truth，我们直接给出它的维度：1*L。它的长度为L，其中每一个时刻有一个类别。那么我们的目标就是把C*T和1*L这两个东西对上，并求出RNN输出结果的Loss和梯度。&lt;/p&gt;&lt;p&gt;说到这种形式的序列问题，我们很容易想起ML人民的老朋友：马尔科夫链。实际上在CTC横空出世前，很多人喜欢把两者匹配的问题转换成一个HMM（隐马尔科夫模型）的问题。CTC的套路实际上和HMM有点相近。&lt;/p&gt;&lt;p&gt;好了，下面我们首先来介绍CTC的一个核心思想：&lt;/p&gt;&lt;h2&gt;CTC的核心思想&lt;/h2&gt;&lt;p&gt;CTC的核心思想是什么？好吧我们这里很显然地知道T是大于等于L的（这个很显然吧），那么用脚想也会知道，RNN的结果里肯定存在着一些冗余信息，比方说在相邻的2个时刻，我们实际上预测了同一个类别，这是很容易发生的事情。那么我们就需要制定一个套路，来帮助我们去掉那些冗余信息。&lt;/p&gt;&lt;p&gt;套路第一步，就是定义一个额外的类别，这个类别叫做blank。我们可以想象在进行手写识别时，字与字之间会存在着一些空隙，语音识别时的字与字之间也会存在着一些间断。我们可以把其中的这些间断的状态定义为blank。为了让匹配的过程更加全面，我们可以在ground_truth的每一个输出类别的前后加上blank，以表示这些字之间存在着的某种间断。&lt;/p&gt;&lt;p&gt;当然了，你说我们还会遇到某些极端情况，那就是字与字之间没有blank，这时候硬塞一个blank也不是很合适啊。所以在一般情况下，字与字之间的blank是可有可无的。但是有一个情况除外，那就是重复的字。在中文中，这种看似有点卖萌的叠字实际上还是出镜率很高的。&lt;/p&gt;&lt;p&gt;比方说你在街上看到一只小狗，长得还挺萌的（我觉得泰迪还蛮萌），于是你气沉丹田，说了一个字——“狗～～”，这种间完全没有中断，于是大家除了觉得你有点神经病之外，只会觉得你说了一个字。但是你如果面如桃花般地说“狗狗～”，相信这两个字之间总会存在一个小小的空隙，让人们把你和上面那位气功大师分别开来。而拆成两个字的关键就是着中间有一个停顿。所以为了区分一段时间内的输出是一个状态还是两个状态，我们需要一个停顿，一个blank。&lt;/p&gt;&lt;p&gt;当然，关于blank的好处，官方文章写了好多。这里直接照抄实在没什么意思，大家去看文章就好。好了套路一结束了。&lt;/p&gt;&lt;p&gt;下面是套路二：现在我们要把C*T和1*（2L+1）对应起来了。但是这个时候它们的维度还是不一样，怎么办？下面我们就要开始变身了。首先我们确保重叠字中间有一个blank，这时候ground_truth的长度为L'。理论上这时候的L'还是小于等于T的。但是马上我们的ground_truth要变身了，我们要通过变身让ground_truth的长度最终等于T。变身的规则主要是复制。ground_truth下的每一个输出都可以从原本的1个输出变成多个输出，同时每一对输出之间都可以加入blank。&lt;/p&gt;&lt;p&gt;听上去有点复杂，我们来举个例子。假如我们说了一段话，这话有8个时刻，也就是说T=8，我们实际说出来的字是“无痛的机器学习”，有7个字，根据刚才的规则，我们需要扩充一个字让它的长度也等于8。我们用B表示blank。那么首先扩充的方法是加入blank：&lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;&lt;code lang="text"&gt;B无痛的机器学习    无B痛的机器学习    无痛B的机器学习    无痛的B机器学习
无痛的机B器学习    无痛的机器B学习    无痛的机器学B习    无痛的机器学习B&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;其实真要把上面的复制念出来还是笑点满满的，除了加一个B，我们还可以复制本身存在的字：&lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;&lt;code lang="text"&gt;无无痛的机器学习    无痛痛的机器学习    无痛的的机器学习    无痛的机机器学习
无痛的机器器学习    无痛的机器学学习    无痛的机器学习习
&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;念完了这一组话，我TM脑子里就只剩一个人了——东北F4，你值得拥有：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-b9ba6ef1be3e84e1c9581e24f3bcdd30.jpg" data-rawwidth="219" data-rawheight="300"&gt;好了，通过这种方法我们完成了对齐。既然已经对齐了，我们就来计算ground_truth扩充后所有可能结果的概率，并且把这些概率加起来，得到我们预测结果的总概率。对于每一个序列，由于对齐后序列中每一个输出的内容间相互独立，所以我们直接把softmax层的输出连乘起来就好了。最后就是一个乘加的工作，大家用意念去想一想就会明白。&lt;/p&gt;&lt;p&gt;到这里，CTC的核心思想也就差不多了。下面留给了我们一个艰巨的问题，刚才这个case比较简单，万一遇上复杂的情况，难道我们还要一个一个地把ground_truth扩展么？&lt;/p&gt;&lt;p&gt;看来我们要上点高科技了。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23308976&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Tue, 14 Feb 2017 08:46:24 GMT</pubDate></item><item><title>Dynamic Network Surgery实验</title><link>https://zhuanlan.zhihu.com/p/24307349</link><description>&lt;p&gt;感谢&lt;a href="https://www.zhihu.com/people/03675ab7bf1c28d3d71d2154abb3ddd1" data-hash="03675ab7bf1c28d3d71d2154abb3ddd1" class="member_mention" data-editable="true" data-title="@我爱机器学习" data-hovercard="p$b$03675ab7bf1c28d3d71d2154abb3ddd1"&gt;@我爱机器学习&lt;/a&gt;对本文的审阅。&lt;/p&gt;&lt;p&gt;这一次我们来看看网络压缩的一个方案——来自英特尔实验室的一篇做模型压缩的论文以及相关的实验：Dynamic Network Surgery for Efficient DNNs&lt;/p&gt;&lt;h2&gt;算法原理&lt;/h2&gt;&lt;p&gt;论文中对网络压缩算法的介绍还是很高大上的，但是这些高大上的内容和最终的实现相比还是有一些距离。本着减少痛苦的原则，让我们用大白话来介绍这个算法的具体过程。&lt;/p&gt;&lt;p&gt;模型压缩是希望用最少的参数做同样的事，这对于在一些特殊的设备上运行深度学习模型来说是很有帮助的。比方说在手机上跑一个VGG模型，恐怕存放这个模型的所需的空间就能让人吓一跳。从另外一方面讲，深度学习的模型中不是每一个参数都那么有用，我们可以适当地关闭一些影响力不大的参数，同时又能保持精度不会有大的变动，那么这就是一个令人满意的结果。&lt;/p&gt;&lt;p&gt;具体来说怎么做呢？我们给每一个参数添加一个配对参数——Mask。这个Mask可以和参数进行点乘，从而得到被Mask过滤后的参数。如果Mask上的值为1，那么对应位置的参数将被保留，如果Mask上的值为0，那么对应位置的参数将被清除。&lt;/p&gt;&lt;p&gt;有了Mask，我们问题又来了，每一个参数应该设置成什么值呢？一个直观的方案是根据参数数值的大小来判断，一般来说数值越大，它所起到的作用也越大。所以柿子要捡软的捏，我们可以设定一个阈值，把比阈值小的参数关闭掉，只保留大数值的参数。&lt;/p&gt;&lt;p&gt;但是这又带来了一个新的问题。虽然这些被关闭的参数数值都很小，但是累积起来也是一个很可观的数字，我们这样把它们关闭掉，还是有可能造成精度损失的。于是乎，当我们把一些参数关闭后，我们还需要重新训练，让参数在新的环境适应——也许会有新的参数变得很小，从而被关闭，当然也有可能有些被关闭的参数死灰复燃，变得重要起来。所以设置Mask的值和重新训练两个步骤需要交替进行。&lt;/p&gt;&lt;p&gt;说到这里我们就不由地想起Online Learning来。虽然最终的目的不太相同，但是大家都有个共同的心愿，就是给目标函数加一个参数的L0范数。曾经的截断梯度方法也利用了和上面有点类似的思想：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;每隔一段时间（窗口），我们对所有的参数进行一遍考察，如果参数的数值太小，我们就将其设为0，关闭掉这个特征对应的参数。&lt;/p&gt;&lt;/blockquote&gt;所不同的是，由于截断梯度法所面临的是Online Learning，这个参数说截断就截断了；而我们这里的方法只是加上一个Mask，并不真正删除权重值。不过看上去这两个领域的思想也许能擦出更多的火花。&lt;h2&gt;MNIST实验&lt;/h2&gt;&lt;p&gt;我们来看看文章对应的代码效果如何。代码的位置为&lt;a href="https://github.com/yiwenguo/Dynamic-Network-Surgery" data-editable="true" data-title="yiwenguo/Dynamic-Network-Surgery" class=""&gt;yiwenguo/Dynamic-Network-Surgery&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;代码中存在一点小问题，根据编译的提示进行修改就可以了，这里就埋个雷不详细介绍了，供大家随意踩。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-24721f1825a9378ca2f667b0d944e948.png" data-rawwidth="393" data-rawheight="357"&gt;（基于模板原创的哟……）&lt;/p&gt;&lt;p&gt;压缩的Layer有一些参数，其中我们接下来比较关注的是cRate。当我们把一个已经训练过的模型拿过来以后，我们会计算每个Layer参数的均值mean和标准差std。如果一个参数的绝对值小于：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;threshold1=0.9*(mean+cRate*std)&lt;/equation&gt;时，&lt;/p&gt;&lt;p&gt;这个参数将被关闭，当这个参数的绝对值大于：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;threshold2=1.1*(mean+cRate*std)&lt;/equation&gt;时，&lt;/p&gt;&lt;p&gt;这个参数将会被重新打开。如果cRate越大，我们的压缩效果会越好。&lt;/p&gt;&lt;p&gt;其实关于这个问题我很想换一个复杂的数据集做实验的，但是由于给出的代码没有CUDNN版的实现，而Caffe的非CUDNN版实在慢不少，所以我们就用一个简单的数据集——MNIST举例好了。&lt;/p&gt;&lt;p&gt;首先我们用LeNet训练一个处理MNIST的模型，这个用一个普通版本的caffe就可以训练完成，最终在测试集的精确率是99.16%。&lt;/p&gt;&lt;p&gt;接下来我们要做网络压缩和重训练。我们可以列出一些实验的结果：&lt;/p&gt;&lt;p&gt;当cRate＝1时，在测试集的精度几乎不受任何损失，但参数的比例已经降低到：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;conv1：28.8%
conv2：18.08%
ip1：7.83%
ip2：23.26%&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;当cRate＝2时，在测试集的精度几乎不受任何损失，但参数的比例已经降低到：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;conv1：15.8%
conv2：8.46%
ip1：2.97%
ip2：11.96%&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;当cRate＝3时，精度降到98.71%，但参数的比例已经降低到：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;conv1：4.8%
conv2：2.1%
ip1：1.2%
ip2：4.8%&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;当cRate=4时，直接导致训练失败。基本上这就是模型压缩的极限了，由于参数最多的地方在全连接层，所以最终的模型体积和两个ip层相关。&lt;/p&gt;&lt;p&gt;因为没有CUDNN版的实现，所以想把这个压缩功能用在你的模型上，还需要点时间。这个坑留给各位，让我们自己动手实现一个CUDNN版本的Layer吧！&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;欢迎加入我爱机器学习9群：173718917&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24307349&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Sat, 14 Jan 2017 11:48:43 GMT</pubDate></item><item><title>番外篇(6)——共轭梯度的效果</title><link>https://zhuanlan.zhihu.com/p/23811968</link><description>&lt;p&gt;本文收录在&lt;a href="https://zhuanlan.zhihu.com/p/22464594" data-editable="true" data-title="无痛的机器学习第一季" class=""&gt;无痛的机器学习第一季&lt;/a&gt;。&lt;/p&gt;前面我们聊了许多有关共轭梯度的内容，也基本上推导出了我们想要的算法，不过在此之前我们还是要完成上一回留下的一个小尾巴，那就是证明某一轮迭代的梯度和之前所有轮的优化方向正交而不是共轭正交。没有错，正是因为前面一直再利用共轭正交的性质，到了这里我们才能实现正交的效果。&lt;h2&gt;梯度和优化方向的关系&lt;/h2&gt;&lt;p&gt;在介绍共轭梯度的过程中，我们提出了误差这个概念，当然我们还有一个类似的概念叫做残差。这两个概念还是有很大的不同的，希望大家能够分别。我们多次提到共轭梯度法的基本思想：每一轮把某一方向优化彻底，保证后面的优化不再对这个方向做任何操作。那么我们可以想象，假设我们的算法一共进行了T轮迭代，我们就可以用这T轮求出的优化方向组合成最终的误差。&lt;/p&gt;&lt;p&gt;假设我们的初始参数值为&lt;equation&gt;x_1&lt;/equation&gt;，那么它到最优点的距离为&lt;equation&gt;e_1&lt;/equation&gt;，我们每一步求出的优化方向为&lt;equation&gt;d_t&lt;/equation&gt;，于是乎，根据上面的介绍我们可以给出如下的公式：&lt;/p&gt;&lt;equation&gt;e_1=\sum_{i=1}^T\gamma_id_i&lt;/equation&gt;&lt;p&gt;也就是说我们可以用这样的方式表示误差，同样，对于不同迭代轮数的误差，我们也可以用不同的方式进行表示。很显然，我们可以猜到&lt;equation&gt;\gamma&lt;/equation&gt;和前面我们提到的&lt;equation&gt;\alpha&lt;/equation&gt;步长之间的关系。&lt;/p&gt;&lt;p&gt;那么下面我们就来证明一下：&lt;/p&gt;&lt;equation&gt;d_i^Tg_j=0,(i&amp;lt;j)&lt;/equation&gt;&lt;equation&gt;d_i^Tg_j=d_i^T(AX_j)=d_i^T(AX_j-AX^*)=d_i^TAe_j&lt;/equation&gt;&lt;equation&gt;=d_i^TA(\sum_{t=j}^T\gamma_td_t)&lt;/equation&gt;&lt;p&gt;因为我们前面已经说明任意两个优化方向之前是相互正交的（Gram-Schmidt方法），所以这是式子的结果为0。&lt;/p&gt;&lt;p&gt;所以到这里我们可以说：通过利用共轭正交的性质，我们得到了更好的正交性质，这个正交性质要比前面的最速下降法的正交性质好很多。这就是共轭梯度法最精髓的地方。&lt;/p&gt;&lt;p&gt;在这里不免有些感慨，前辈们的脑洞真心不一般，能够利用一些看上去抽象又无关紧要的定理帮助我们解决一些核心的大问题，这里面存在的不仅仅是数学的美丽，有时甚至是哲学的精彩……&lt;/p&gt;&lt;p&gt;毕竟不是心灵鸡汤文，抒情的套路还是要少一些。我们还是继续来关注这个算法，到这里我们的算法全过程就应该一目了然了：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def conjugateGradient(A, b, x):
	grad = np.dot(A, x) - b
	p = -grad
	while True:
		if abs(np.sum(p)) &amp;lt; 1e-6:
			break
		gradSquare = np.dot(grad, grad)
		Ap = np.dot(A, p)
		alpha = gradSquare / np.dot(p, Ap)
		x += alpha * p
		newGrad = grad + alpha * Ap
		beta = np.dot(newGrad, newGrad) / gradSquare
		p = -newGrad + beta * p
		grad = newGrad
	return x
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;好了，代码放到这里，相关的实验就按下不表了。大家感兴趣可以自己试试看。&lt;/p&gt;&lt;p&gt;好了，这一次的番外篇就到此结束了。我们实际上介绍了两个“全自动”的优化算法，所谓的全自动，就是说它不需要调参。当然为此带来的代价也比较大，我们无法把这些算法用在所有的场景上，而且在同样的场景上，也许还有比他们更好的算法。&lt;/p&gt;&lt;p&gt;优化算法是个大坑，我们要慢慢跳，这次就跳到这里。拜拜了～&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;欢迎加入我爱机器学习9群：173718917！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23811968&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Sat, 07 Jan 2017 22:23:42 GMT</pubDate></item><item><title>番外篇(5)——共轭方向的构建</title><link>https://zhuanlan.zhihu.com/p/23810213</link><description>&lt;p&gt;本文收录在&lt;a href="https://zhuanlan.zhihu.com/p/22464594" data-editable="true" data-title="无痛的机器学习第一季" class=""&gt;无痛的机器学习第一季&lt;/a&gt;。&lt;/p&gt;前面我们聊过了共轭梯度的一些性质，同时我们也推导出了求步长的方法，下面就来看看如何求解出优化方向。&lt;p&gt;我们要解决的主要问题是优化方向的正交问题。由于每一次的优化后，剩下的误差和此次的优化正交（共轭正交），所以可以看出每一个优化方向彼此间都是正交的。那么如何构建一系列的正交方向呢？&lt;/p&gt;&lt;h2&gt;Gram-Schmidt方法&lt;/h2&gt;&lt;p&gt;在线性代数中，我们曾经学过一个向量正交化的方法——Gram-Schmidt方法。这个算法的内容在这里还是要说一下的，算法的输入是N维空间中N个线性无关的向量——线性无关还是有必要的，至于原因……&lt;/p&gt;&lt;p&gt;算法的输出是N个相互正交的向量，也就是我们最终想要的效果。&lt;/p&gt;&lt;p&gt;那么具体怎么做呢？&lt;/p&gt;&lt;p&gt;对于第一个向量，我们保持它不变；&lt;/p&gt;&lt;p&gt;对于第二个向量，我们去掉其中和第一个向量共线的部分；&lt;/p&gt;&lt;p&gt;对于第三个向量，我们去掉其中和第一、二个向量共线的部分；&lt;/p&gt;&lt;p&gt;对于第N个向量，我们去掉其中和第一、二、……N-1个向量共线的部分；&lt;/p&gt;&lt;p&gt;听上去思路还是蛮清晰的，那么具体怎么做呢？&lt;/p&gt;&lt;p&gt;如果输入向量是：{&lt;equation&gt;u_1,u_2,...,u_N&lt;/equation&gt;}&lt;/p&gt;&lt;p&gt;输出向量是：{&lt;equation&gt;d_1,d_2,...,d_N&lt;/equation&gt;}&lt;/p&gt;&lt;p&gt;那么对于每一个向量，我们要做下面的转换：&lt;/p&gt;&lt;equation&gt;d_t=u_t+\sum_{i=1}^{t-1}\beta_{t,i}d_i&lt;/equation&gt;&lt;p&gt;其中的&lt;equation&gt;\beta&lt;/equation&gt;表示向量被去掉的分量，这个数字是要求出来的。那么我们怎么求这个数字呢？我们利用前面提到的性质，向量之间正交（我们这里还是共轭正交），于是有&lt;/p&gt;&lt;equation&gt;d_l^TAd_t=0,(l=1,2,...,t-1)&lt;/equation&gt;&lt;p&gt;于是我们有&lt;/p&gt;&lt;equation&gt;d_l^TAd_t=d_l^TA(u_t+\sum_{i=1}^{t-1}\beta_{t,i}d_i)=d_l^TAu_t+d_l^TA\sum_{i=1}^{t-1}\beta_{t,i}d_i&lt;/equation&gt;&lt;p&gt;利用正交的性质，我们可以把公式化简为：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;=d_l^TAu_t+d_l^TA\beta_{t,i}d_i=0&lt;/equation&gt;，所以最终我们得到：&lt;/p&gt;&lt;equation&gt;\beta_{t,i}=-\frac{d_l^TAu_t}{d_l^TAd_i}&lt;/equation&gt;&lt;p&gt;下面我们就得到了完整的算法流程，这个算法的过程比较繁琐，不过是可行的。&lt;/p&gt;&lt;h2&gt;共轭梯度&lt;/h2&gt;&lt;p&gt;这个算法倒是聊完了，那么问题又来了——&lt;/p&gt;&lt;ol&gt;&lt;li&gt;我们要用什么向量去构建这些正交向量&lt;/li&gt;&lt;li&gt;前面我们看到随着向量数量的增加，我们需要计算的参数也越来越多，我们要计算N方程度个&lt;equation&gt;\beta&lt;/equation&gt;，这个计算的数目还是有点多，那么我们能不能再减少一些呢？&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;上面的问题当然能够解决，而采用的方式也比较简单——就用我们最常见的梯度来解决。这就是算法名字的来源——共轭+梯度。&lt;/p&gt;&lt;p&gt;那么第一个问题解决了，下面主攻第2个问题。第2个问题有点绕，所以我们先给出结论，那就是我们在每次求一个新方向时，只要再求解一个新的&lt;equation&gt;\beta&lt;/equation&gt;就可以了。&lt;/p&gt;&lt;p&gt;下面是推导过程。我们要证明当&lt;equation&gt;u_t=g_t&lt;/equation&gt;时，首先不是去开始证明第2个问题，而是要完成一个前置条件的证明——听上去有点晕啊。那就是由梯度组成的这个向量组是不是一个线性无关的向量组呢？&lt;/p&gt;&lt;p&gt;我们来证明一下。&lt;/p&gt;&lt;p&gt;我们前面提过共轭梯度法的特点——上一轮的优化方向和这一轮的误差之间正交，那么能不能推导出某一轮的误差和之前所有的优化方向全部正交呢？当然可以，不过这里面涉及到一些subspace的思想，我们放在后面再说，我们现在要直接给出一个靠谱的定理，那就是当采用了共轭梯度法之后，对于某一轮t求出的梯度&lt;equation&gt;g_t&lt;/equation&gt;，它和前面t-1轮的梯度正交（这里不是共轭正交，word天……），用形式化的方式描述，就是：&lt;/p&gt;&lt;equation&gt;g_i^Tg_j=0,(i&amp;lt;j)&lt;/equation&gt;&lt;p&gt;我们暂时相信这个定理，后面我们会更详细的分析它（因为进度条快撑不住了……），承认了这个定理，我们就回头看看前面求解的那些&lt;equation&gt;\beta&lt;/equation&gt;：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;g_{j+1}^Tg_i=(AX_{j+1})^Tg_i=(A(X_j+\alpha_jd_j))^Tg_i=g_j^Tg_i+\alpha_jd_j^TAg_i&lt;/equation&gt;（A是对称矩阵）&lt;/p&gt;&lt;p&gt;所以有：&lt;/p&gt;&lt;equation&gt;d_j^TAg_i=\frac{1}{\alpha_j}[g_{j+1}^Tg_i-g_j^Tg_i]&lt;/equation&gt;&lt;p&gt;我们发现上面公式的左边就是&lt;equation&gt;\beta&lt;/equation&gt;的分子，如果我们能够证明其中很多的&lt;equation&gt;\beta&lt;/equation&gt;分子为0，那么我们就完成了证明。&lt;/p&gt;&lt;p&gt;而从前面的公式看，我们必须满足j&amp;lt;i的条件，于是只有当j=i-1是，上面的式子不等于0，所以只有&lt;equation&gt;\beta_{t,t-1}&lt;/equation&gt;这一项不为0，换句话说我们只需要求解这一项，问题就解决了。&lt;/p&gt;&lt;p&gt;好了，进度条阵亡了，我们也得到了答案，下一回我们来看一下算法具体的效果。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;欢迎加入我爱机器学习8群：19517895！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23810213&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Wed, 28 Dec 2016 23:14:37 GMT</pubDate></item><item><title>番外篇(4)——共轭梯度法入坑</title><link>https://zhuanlan.zhihu.com/p/23804838</link><description>&lt;p&gt;本文收录在&lt;a href="https://zhuanlan.zhihu.com/p/22464594" data-editable="true" data-title="无痛的机器学习第一季" class=""&gt;无痛的机器学习第一季&lt;/a&gt;。&lt;/p&gt;前面我们已经把最速下降法的内容介绍的差不多了，下面我们要做的就是介绍这个真正的主角了——共轭梯度法。这个优化方法算是活跃在优化世界的一个经典算法了，它的计算速度还算不错，方法也算相对简单——真的是相对，因为比起梯度下降它还是复杂不少，但是和其他的一些方法比较，那就不算难了。&lt;p&gt;好了，我们直奔主题。在番外篇的开篇我们就提到了机器学习的三个部件。优化算法作为一个黑盒，一般来说是不为人所知的。所以我们的重点就是揭开它的面纱，尽可能清楚地阐述它的原理。&lt;/p&gt;&lt;h2&gt;更高级的约束&lt;/h2&gt;&lt;p&gt;前面在最速下降法中，我们提到了最速下降法的一个性质——那就是相邻两次的优化方向是正交的。乍一听上去，总感觉这个性质很酷，但是看过了一些实际案例，又不免让人心灰——走成"zig-zag"的形状，还好意思标榜这个性质？&lt;/p&gt;&lt;p&gt;于是乎，我们开始对优化方向有了更大的野心——能不能让我们的优化方向更加智能，我们每朝一个方向走，就把这个方向走到极致？所谓的极致，可以理解为在优化的过程中我们再也不需要朝这个方向走了。于是乎我们引出了另外一个变量，叫做误差：&lt;/p&gt;&lt;equation&gt;e_t=x^*-x_t&lt;/equation&gt;&lt;p&gt;这个误差表示了参数的最优点和当前点之间的距离。那么我们的目标就可以更在形式化了，我们希望每一步优化后，当前的误差和我们刚才优化的方向正交！注意，这里我们已经不再使用梯度这个词，而是使用优化方向，因为从现在开始，我们的优化方向不一定是梯度了。当然我们也要换一个变量名比较好，不然会引起误会：&lt;/p&gt;&lt;equation&gt;r_t&lt;/equation&gt;&lt;p&gt;还是写个公式出来比较靠谱：&lt;/p&gt;&lt;equation&gt;r_t^Te_{t+1}=0&lt;/equation&gt;&lt;p&gt;这样我们可以想象，如果我们的优化空间有d维，那么我们最多只需要迭代d轮就可以求解出来了。听上去蛮靠谱的。&lt;/p&gt;&lt;p&gt;好了，大饼画完了，下面就是填坑时间了。&lt;/p&gt;&lt;h2&gt;坑&lt;/h2&gt;&lt;p&gt;理想很丰满，但是现实很骨感。如果我能知道那个误差，我还优化干嘛？直接按误差更新不就完事了？但是想知道误差还是需要一步一步地求解啊？感觉我们掉入了"chicken-egg senario"里面。&lt;/p&gt;&lt;p&gt;但是别着急，我们还有数学武器在手，我们可以用线性代数的知识偷天换日，填满这个坑。于是乎，见证奇迹的时刻就要来临了。&lt;/p&gt;&lt;h2&gt;共轭&lt;/h2&gt;&lt;p&gt;偷天换日的关键就在这个共轭上了。其实一开始看到这个词的时候我是拒绝的，这是什么鬼？这词是啥意思？&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-b16e0493701ee49ee297485a538e481c.jpg" data-rawwidth="480" data-rawheight="360"&gt;一个按原意的解释——轭就是绑在两头牛身上的木头，它让两个本来独立的个体变成了一个整体，属于牵线搭桥之类的关键道具。&lt;/p&gt;&lt;p&gt;好了，我们再回到刚才的问题中，前面我们说了我们新的正交特性，那么放在共轭这个环境下是什么效果呢？我们希望现在的关系变为共轭正交，存在一个矩阵A（A就是轭），使得：&lt;/p&gt;&lt;equation&gt;r_t^TAe_{t+1}=0&lt;/equation&gt;&lt;p&gt;其实看到这里我依然是拒绝的……这又是什么鬼，三观都崩塌了好不好，这么乱搞有什么意义？&lt;/p&gt;&lt;p&gt;别着急，其实如果我们把上面的原始公式中间加一个东西，看上去就舒服很多了：&lt;/p&gt;&lt;equation&gt;r_t^TIe_{t+1}=0&lt;/equation&gt;&lt;p&gt;单位阵不改变结果，现在我们要加一个能改变结果的东东，仅此而已。那么这个矩阵是什么作用呢？&lt;/p&gt;&lt;p&gt;我们可以认为这个矩阵要完成线性变换的作用，将一个向量从一个线性空间转换到另一个空间。转换后的向量可以满足正交的性质，如果这个矩阵一直保持不变，听上去这个性质也是合理的啊。&lt;/p&gt;&lt;p&gt;那么有没有更加实际的例子呢？&lt;/p&gt;&lt;p&gt;比方说我们有两个向量：&lt;/p&gt;&lt;equation&gt;a=[1,1] , b=[1,0.5]^T&lt;/equation&gt;&lt;p&gt;很显然它们不是正交的，但是如果我们多了一个矩阵A：&lt;/p&gt;&lt;equation&gt;A=[[1,0][0, -2]]&lt;/equation&gt;&lt;p&gt;那么我们发现，b经过A转换后就与a正交了。&lt;/p&gt;&lt;p&gt;所以这个新的条件实际上只是多绕了一个弯，和前面的条件差距并不大。&lt;/p&gt;&lt;p&gt;一旦接受了这样的设定，下面的内容就好理解多了。&lt;/p&gt;&lt;h2&gt;共轭梯度法的流程&lt;/h2&gt;&lt;p&gt;好了，我们已经明确了共轭梯度法的目标和特点，那么我就要开始推导算法公式了。当然，共轭梯度法也属于line search的一种，我们的总体思路不变：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;确定优化方向&lt;/li&gt;&lt;li&gt;确定优化步长&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;相对而言，确定优化方向比确定优化步长麻烦，我们放在后面说，现在先来捡个软柿子，那就是第2步，我们从上面提过的公式开始：&lt;/p&gt;&lt;equation&gt;r_t^TAe_{t+1}=0&lt;/equation&gt;&lt;p&gt;好了，开始推导：&lt;/p&gt;&lt;equation&gt;r_t^TAe_{t+1}=r_t^TA[e_{t}+X_t-X_{t+1}]=r_t^TA[e_{t}+\alpha_t r_t]&lt;/equation&gt;&lt;p&gt;&lt;equation&gt;=r_t^TAe_{t}+\alpha_t r_t^TAr_t=0&lt;/equation&gt;，于是&lt;/p&gt;&lt;equation&gt;\alpha_t=-\frac{r_t^TAe_{t}}{r_t^TAr_t}&lt;/equation&gt;&lt;equation&gt;\alpha_t=-\frac{r_t^TA(X^*-X_t)}{r_t^TAr_t}&lt;/equation&gt;&lt;p&gt;我们知道&lt;equation&gt;AX^*=0&lt;/equation&gt;，&lt;equation&gt;g_t=AX_t&lt;/equation&gt;,于是公式最终变为：&lt;/p&gt;&lt;equation&gt;\alpha_t=\frac{r_t^Tg_t}{r_t^TAr_t}&lt;/equation&gt;&lt;p&gt;好了，我们发现我们利用多出来的A把前面的误差e成功地消掉了，这下子步长可解了。让我们记住这个公式——不是背下来，而是大概记住这个形式，后面我们还会用到它。&lt;/p&gt;&lt;p&gt;下一回我们继续来看看后面的推导。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;欢迎加入我爱机器学习8群：19517895！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23804838&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Sun, 25 Dec 2016 11:50:15 GMT</pubDate></item><item><title>番外篇(3)——最速下降法的特点</title><link>https://zhuanlan.zhihu.com/p/23804818</link><description>&lt;p&gt;本文收录在&lt;a href="https://zhuanlan.zhihu.com/p/22464594" data-editable="true" data-title="无痛的机器学习第一季" class=""&gt;无痛的机器学习第一季&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;在前面两回中，我们介绍完最速下降法的的算法内容，并且介绍完了它的下降速度。从结果中我们可以看出，如果矩阵A的最大最小特征值差距大，最速下降法就有可能获得较慢的收敛性，而两者越相近，我们的收敛性也会相应增强。所以我们最终得到了一个结论，那就是最速下降法的收敛性与矩阵的特征值有关。&lt;/p&gt;&lt;p&gt;好了，说了这么多，我们还是来看看它的代码是什么样子：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def steepestGD(A, x):
	while True:
		p = -np.dot(A, x)
		if abs(np.sum(np.abs(p))) &amp;lt; 1e-6:
			break
		gradSquare = np.dot(p, p)
		Ap = np.dot(A, p)
		alpha = gradSquare / np.dot(p, Ap)
		x += alpha * p
	return x
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;一直以来我们在展示完代码后都会顺势给出几个例子，但是这一回我们就不做这样的事情了。关于这个代码的细节我们也不做多的case展示了，我们直接来看看最速下降法当中一个有趣的问题——一步到位问题。&lt;/p&gt;&lt;h2&gt;最速下降法之一步到位攻略&lt;/h2&gt;&lt;p&gt;所谓的一步到位，是指我们经过一轮迭代就能找到最优解，这个速度是非常让人羡慕的，而这其中还包含着一些数学知识。&lt;/p&gt;&lt;p&gt;由于最速下降法的步长和计算过程很有关系，所以我们需要对此进行进一步的分析，来加深对这种优化算法的印象。我们设想一种场景，那就是我们的优化方向刚好包含了最优点，那么我们就有可能在一轮迭代的过程中完成优化。&lt;/p&gt;&lt;p&gt;我们来想想一个简单的场景，经过某一步优化，我们找到了最优值，那么下面的公式一定满足：&lt;/p&gt;&lt;equation&gt;x^*=x_t-\alpha_tg_t&lt;/equation&gt;&lt;p&gt;因为最优值点的梯度为0，所以我们将梯度计算的公式带入，就有了：&lt;/p&gt;&lt;equation&gt;A(x_t-\alpha_tg_t)=0&lt;/equation&gt;&lt;p&gt;经过变换，我们可以得到公式的第一形态：&lt;/p&gt;&lt;equation&gt;g_t-\alpha_tAg_t=0&lt;/equation&gt;&lt;equation&gt;(I-\alpha_tA)g_t=0&lt;/equation&gt;&lt;p&gt;从上面的式子中，我们可以找出最简单的两种解：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;g_t=0&lt;/equation&gt;，当然，这时候我们已经找到了最优解，所以这个解并没有什么意义；&lt;/p&gt;&lt;p&gt;&lt;equation&gt;A=\frac{1}{\alpha}I&lt;/equation&gt;，这个解倒是很有意义，不过太简单了。&lt;/p&gt;&lt;p&gt;比方说当&lt;equation&gt;\alpha=1&lt;/equation&gt;时，原式变成了：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;f(X)=\frac{1}{2}X^TX&lt;/equation&gt;，&lt;/p&gt;&lt;p&gt;这个问题看上去也确实好解，对于2维的空间，这个优化函数的等高线图就是由一个个同心圆组成的，这是最容易优化的一种函数，所以一步解出来倒也不算奇怪。&lt;/p&gt;&lt;p&gt;谈完了上面两种容易找到的解，下面我们看看一些隐藏的解。当然我们需要把公式做一定的转换：&lt;/p&gt;&lt;equation&gt;Ag_t=\frac{1}{\alpha_t}g_t&lt;/equation&gt;&lt;p&gt;嗯……这个公式看上去就熟悉多了，如果&lt;equation&gt;g_t&lt;/equation&gt;是A的特征向量，那么&lt;equation&gt;\frac{1}{\alpha}&lt;/equation&gt;就是它优化的特征值了？当然，一般来说特征值是确定的，特征向量是可以变化的。但总之听上去好有道理。关键时刻还得靠万能的线性代数啊。&lt;/p&gt;&lt;p&gt;所以我们如果能保证我们的参数满足上面的条件，优化过程同样可以做到一步完成。当然，求解特征值的复杂度实在有点高，所以一般来说大家也不会采用这种方法加速求解。上面的问题只是帮助我们更好地理解最速下降法而已。&lt;/p&gt;&lt;p&gt;那么最后一个思考题了，上面我们对梯度做了约束，那么对于参数值呢？想要一步优化，它需要满足什么样的特点呢？&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;欢迎加入我爱机器学习8群：19517895！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23804818&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Mon, 19 Dec 2016 23:42:16 GMT</pubDate></item><item><title>番外篇(2)——无聊的最速下降法推导</title><link>https://zhuanlan.zhihu.com/p/23799012</link><description>本文收录在&lt;a href="https://zhuanlan.zhihu.com/p/22464594" data-editable="true" data-title="无痛的机器学习第一季" class=""&gt;无痛的机器学习第一季&lt;/a&gt;。&lt;p&gt;好吧，我想题目已经说明了一切，虽然说这个过程很无聊，但是我们可以收藏下这段推导，因为它的结论还是很有用的……那不如让我们先来看看结论：&lt;/p&gt;&lt;p&gt;对于上一回我们介绍的那个函数：&lt;/p&gt;&lt;equation&gt;f(X)=\frac{1}{2}X^TQX&lt;/equation&gt;&lt;p&gt;其中Q是一个对称正定矩阵，&lt;equation&gt;\lambda_n&lt;/equation&gt;是Q的n个特征值,并且满足0 &amp;gt; &lt;equation&gt;\lambda_1&lt;/equation&gt;&amp;gt;= &lt;equation&gt;\lambda_2&lt;/equation&gt;&amp;gt;=
&lt;equation&gt;\lambda_n&lt;/equation&gt;，那么有: &lt;/p&gt;&lt;equation&gt;f(X_{k+1})\leq(\frac{\lambda_n-\lambda_1}{\lambda_n+\lambda_1})^2f(X_k)&lt;/equation&gt;&lt;p&gt;下面问题来了，我们求出上面这个公式有毛用？我们可以看出随着优化的不断进行，我们的函数是不断变小的，但是我们需要为函数优化定一个期限。我们希望函数能够尽快地优化到位，那么&lt;equation&gt;\frac{\lambda_n-\lambda_1}{\lambda_n+\lambda_1}&lt;/equation&gt;这部分当然是越小越好了。&lt;/p&gt;&lt;p&gt;如果说大家想知道结论，那么这篇文章到此结束，下面是具体的推导了，前方各种高能，非战斗人员点了赞就可以撤了……&lt;/p&gt;&lt;h2&gt;公式推导&lt;/h2&gt;&lt;p&gt;公式推导总体来说分为几个部分：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;求出步长&lt;equation&gt;\alpha_t&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;整理公式&lt;/li&gt;&lt;li&gt;Kantorovich Inequality&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;下面我们一步一步来，其中的第一步可以跳过了，因为我们在上一篇文章已经做过了，那就是求出步长，这里我们就不再推导一遍了，直接给出结果：&lt;/p&gt;&lt;equation&gt;\alpha_t=\frac{g_t^Tg_t}{g_t^TAg_t}&lt;/equation&gt;&lt;p&gt;下面是第2部。&lt;/p&gt;&lt;h2&gt;整理公式&lt;/h2&gt;&lt;p&gt;这一步的主要目标是去凑那个公式，我们从公式的左边出发，首先要利用更新公式了：&lt;/p&gt;&lt;equation&gt;X_{t+1}=X_t-\alpha_tg_t&lt;/equation&gt;&lt;equation&gt;f(X_{t+1})=\frac{1}{2}X_{t+1}^TAX_{t+1}=\frac{1}{2}(X_t-\alpha_tg_t)^T A (X_t-\alpha_tg_t)&lt;/equation&gt;&lt;equation&gt;=\frac{1}{2}[X_t^TAX_t-X_t^TA\alpha_tg_t-\alpha_tg_t^TAX_t-\alpha_t^2g_t^TAg_t]&lt;/equation&gt;&lt;equation&gt;=\frac{1}{2}[2f(X_t)-X_t^TA\alpha_tg_t-\alpha_tg_t^TAX_t-2\alpha_t^2f(g_t)]&lt;/equation&gt;&lt;p&gt;接下来我们需要两个公式：&lt;/p&gt;&lt;p&gt;首先，我们的矩阵A是对称的，所以&lt;equation&gt;A^T=A&lt;/equation&gt;;&lt;/p&gt;&lt;p&gt;其次，我们的导数公式：&lt;equation&gt;g_t=AX&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;所以上面的公式就变成了：&lt;/p&gt;&lt;equation&gt;=\frac{1}{2}[2f(X_t)-(AX_t)^T\alpha_tg_t-\alpha_tg_t^T(AX_t)+2\alpha_t^2f(g_t)]&lt;/equation&gt;&lt;equation&gt;=\frac{1}{2}[2f(X_t)-\alpha_tg_t^Tg_t-\alpha_tg_t^Tg_t+2\alpha_t^2f(g_t)]&lt;/equation&gt;&lt;equation&gt;=f(X_t)-\alpha_tg_t^Tg_t+\alpha_t^2f(g_t)&lt;/equation&gt;&lt;p&gt;好了，公式整理得差不多了，下面我们要把&lt;equation&gt;\alpha_t&lt;/equation&gt;的公式代入了：&lt;/p&gt;&lt;equation&gt;=f(X_t)-\frac{g_t^Tg_t}{g_t^TAg_t}g_t^Tg_t+(\frac{g_t^Tg_t}{g_t^TAg_t})^2f(g_t)&lt;/equation&gt;&lt;equation&gt;=f(X_t)-\frac{(g_t^Tg_t)^2}{g_t^TAg_t}+(\frac{g_t^Tg_t}{g_t^TAg_t})^2(\frac{1}{2}g_t^TAg_t)&lt;/equation&gt;&lt;equation&gt;=\frac{1}{2}[X_t^TAX_t-\frac{(g_t^Tg_t)^2}{g_t^TAg_t}]&lt;/equation&gt;&lt;p&gt;到这里似乎又推导不动了，下面继续放一个杀招：&lt;/p&gt;&lt;equation&gt;X_t^TAX_t=X_t^TAA^{-1}AX_t=X_t^TA^TA^{-1}AX_t&lt;/equation&gt;&lt;equation&gt;=(AX_t)^TA^{-1}(AX_t)=g_t^TA^{-1}g_t&lt;/equation&gt;&lt;p&gt;于是公式可以进一步整理了：&lt;/p&gt;&lt;equation&gt;=\frac{1}{2}X_t^TAX_t[1-\frac{(g_t^Tg_t)^2}{(g_t^TAg_t)(g_t^TA^{-1}g_t)}]&lt;/equation&gt;&lt;equation&gt;=f(X_t)[1-\frac{(g_t^Tg_t)^2}{(g_t^TAg_t)(g_t^TA^{-1}g_t)}]&lt;/equation&gt;&lt;p&gt;好了，整理到这里我们的第一步工作基本完成了，我们来看看我们成果和最终的结果：&lt;/p&gt;&lt;p&gt;最终结果：&lt;equation&gt;f(x_{k+1})\leq(\frac{\lambda_n-\lambda_1}{\lambda_n+\lambda_1})^2f(x_k)&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;目前结果：&lt;equation&gt;f(X_{t+1})=[1-\frac{(g_t^Tg_t)^2}{(g_t^TAg_t)(g_t^TA^{-1}g_t)}]f(X_t)&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;好了，下面的目标很明确了，我们要做的就是里面那一大堆的变换。&lt;/p&gt;&lt;h2&gt;Kantorovich Inequality&lt;/h2&gt;&lt;p&gt;说实话如果是一般的文章到这里就会结束了，可是这篇推导实在比较恶心，于是我决定一口气推完，不给大家恶心第二次的机会。&lt;/p&gt;&lt;p&gt;下面这部分的转换据说是一个叫做Kantorovich Inequality的定理，但是这么不常见的定理对于我们来说实在陌生，如果拿这个名称糊弄大家就有点不太地道了，于是乎我们就假装不知道这个定理，把这个定理再推导一遍。&lt;/p&gt;&lt;p&gt;我们这一步的目标是证明：&lt;/p&gt;&lt;equation&gt;\frac{(g_t^Tg_t)^2}{(g_t^TAg_t)(g_t^TA^{-1}g_t)}\geq \frac{4\lambda_1\lambda_n}{(\lambda_1+\lambda_n)^2}&lt;/equation&gt;&lt;p&gt;也就是&lt;equation&gt;\frac{(g_t^TAg_t)(g_t^TA^{-1}g_t)}{(g_t^Tg_t)^2}\leq \frac{(\lambda_1+\lambda_n)^2}{4\lambda_1\lambda_n}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;好了，现在我们开始继续推导。左边部分的A可以变为特征值的形式：&lt;/p&gt;&lt;equation&gt;=\frac{(g_t^TS^T\Lambda Sg_t)(g_t^TS^T \Lambda^{-1}Sg_t)}{(g_t^TS^TSg_t)^2}&lt;/equation&gt;&lt;p&gt;令&lt;equation&gt;Sg_t=l&lt;/equation&gt;，可以得到：&lt;/p&gt;&lt;equation&gt;=\frac{\sum_{i=1}^n{\lambda_i * l_i^2}\sum_{i=1}^n{\frac{1}{\lambda_i} * l_i^2}}{(l_t^Tl_t)^2}&lt;/equation&gt;&lt;p&gt;此时的分母相当于一个归一化的因子,所以我们可以将式子进一步化解为:&lt;/p&gt;&lt;equation&gt;\sum_{i=1}^n{\lambda_i * z_i^2}\sum_{i=1}^n{\frac{1}{\lambda_i} * z_i^2}&lt;/equation&gt;&lt;p&gt;given &lt;equation&gt;\sum_{i=1}^n{z_i^2}=1&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;我们可以利用拉格朗日乘子法把这两个公式融合起来,得到:&lt;/p&gt;&lt;equation&gt;F=\sum_{i=1}^n{\lambda_i * z_i^2}\sum_{i=1}^n{\frac{1}{\lambda_i} * z_i^2}+\alpha(\sum_{i=1}^n{z_i^2}-1)&lt;/equation&gt;&lt;p&gt;这里令&lt;/p&gt;&lt;equation&gt;\sigma=\sum_{i=1}^n{\lambda_i * z_i^2}&lt;/equation&gt;&lt;equation&gt;\hat{\sigma}=\sum_{i=1}^n{\frac{1}{\lambda_i} * z_i^2}&lt;/equation&gt;&lt;p&gt;我们可以进行日常的求导工作找到这个函数的极值：&lt;/p&gt;&lt;equation&gt;\frac{\partial F}{\partial z_i}=2(\sigma \frac{1}{\lambda_i}z_i+\hat{\sigma}\lambda_iz_i-\alpha z_i)=0,(i=1...n)&lt;/equation&gt;&lt;equation&gt;\frac{\partial F}{\partial z_i}=z_i(\sigma+\hat{\sigma}\lambda_i^2-\alpha \lambda_i)=0,(i=1...n)&lt;/equation&gt;&lt;p&gt;我们可以给出上面求导的一种解，那就是我们令尽可能多的&lt;equation&gt;z_i&lt;/equation&gt;等于0，而保留其中的两个 &lt;equation&gt;z_i&lt;/equation&gt;不等于0，关于这两个的导数，我们让&lt;equation&gt;(\sigma+\hat{\sigma}\lambda_i^2-\alpha \lambda_i)&lt;/equation&gt;这一部分等于0，同时我们保证所有的变量都满足那个约束，于是我们最原始的公式就变成了：&lt;/p&gt;&lt;equation&gt;F=(\lambda_kz_k^2+\lambda_lz_l^2)(\frac{1}{\lambda_k}z_k^2+\frac{1}{\lambda_l}z_l^2)&lt;/equation&gt;&lt;p&gt;given &lt;equation&gt;z_k^2+z_l^2=1&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;下面就是复杂的各种变换：&lt;/p&gt;&lt;equation&gt; = \frac{1}{4} (\sqrt{\frac{\lambda_k}{\lambda_l}} + \sqrt{\frac{\lambda_l}{\lambda_k}})^2 (z_k^2 + z_l^2)^2 - \frac{1}{4} (\sqrt{\frac{\lambda_k}{\lambda_l}} - \sqrt{\frac{\lambda_l}{\lambda_k}})^2 (z_k^2 - z_l^2)^2&lt;/equation&gt;&lt;p&gt;因为这时右边第2项是小于0的，加上前面given的条件，所以可以得到：&lt;/p&gt;&lt;equation&gt;\leq \frac{1}{4} (\sqrt{\frac{\lambda_k}{\lambda_l}} + \sqrt{\frac{\lambda_l}{\lambda_k}})^2&lt;/equation&gt;&lt;p&gt;我们选择差距最大的两个特征值作为上界，于是又有&lt;/p&gt;&lt;equation&gt;\leq \frac{1}{4} (\sqrt{\frac{\lambda_1}{\lambda_n}} + \sqrt{\frac{\lambda_n}{\lambda_1}})^2&lt;/equation&gt;&lt;p&gt;总之一个是最大的特征值，一个是最小的就好。于是乎又可以得到：&lt;/p&gt;&lt;equation&gt;=\frac{(\lambda_1+\lambda_n)^2}{4\lambda_1\lambda_n}&lt;/equation&gt;&lt;p&gt;我们最终想要的结果马上就要出现了！&lt;/p&gt;&lt;p&gt;于是乎我们得到的完整内容是：&lt;/p&gt;&lt;equation&gt;\frac{(g_t^Tg_t)^2}{(g_t^TAg_t)(g_t^TA^{-1}g_t)}\geq \frac{4\lambda_1\lambda_n}{(\lambda_1+\lambda_n)^2}&lt;/equation&gt;&lt;h2&gt;最终章&lt;/h2&gt;&lt;equation&gt;f(X_{k+1}) \leq (1 - \frac{4 \lambda_1 \lambda_n }{(\lambda_1 + \lambda_n)^2}) f(X_k)&lt;/equation&gt;&lt;equation&gt;= (\frac{(\lambda_1 + \lambda_n)^2}{(\lambda_1 + \lambda_n)^2} - \frac{4 \lambda_1 \lambda_n }{(\lambda_1 + \lambda_n)^2}) f(X_k)&lt;/equation&gt;&lt;equation&gt;= (\frac{\lambda_1^2 - 2 \lambda_1 \lambda_n + \lambda_n^2 }{(\lambda_1 + \lambda_n)^2}) f(X_k)&lt;/equation&gt;&lt;equation&gt;=(\frac{\lambda_1 - \lambda_n}{\lambda_1 + \lambda_n})^2 f(X_k)&lt;/equation&gt;&lt;p&gt;好了，到这里我们的推导结束了，该休息了……&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;欢迎加入我爱机器学习8群：19517895！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23799012&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Tue, 13 Dec 2016 23:44:21 GMT</pubDate></item><item><title>番外篇(1)——最速下降法</title><link>https://zhuanlan.zhihu.com/p/23776390</link><description>本文收录在&lt;a href="https://zhuanlan.zhihu.com/p/22464594" data-editable="true" data-title="无痛的机器学习第一季" class=""&gt;无痛的机器学习第一季&lt;/a&gt;。&lt;p&gt;番外篇正式开始，我们主要利用番外篇的时间聊一些机器学习中的黑盒部分——没错，就是优化算法。之前接受过前辈的教诲，一个机器学习的套路可以分解成三个部分——模型，目标和优化方法。模型用来定义待解决的问题，目标（一般也会被称作损失函数）用来明确评价模型质量的方法，而优化算法则是具体解决求解过程的问题。有了这三个部分，我们可以说在学术的角度上我们基本上就搞定了一个机器学习问题。之所以在前面加上了学术这两个字，是因为在工业界一个机器学习的问题就不止这三部了。&lt;/p&gt;&lt;p&gt;好了回到正题，我们回到前面的三个部分，一般来说第一部分是最灵活的，第二部分也算灵活，但还是有一定的约束的，然而第三部分——一般来说都是非常确定的，而且一般也是以一个黑盒的状态出现的。&lt;/p&gt;&lt;p&gt;于是乎大家一般对第一部分和第二部分更为关注，而对第三部分相对忽视一些。于是乎我们这里就反其道而行，作死地选择和大家聊聊第三部分——优化。实际上在前面的正文中我们已经花了很大的篇幅去聊一些优化算法，下面我们继续聊一些经典的算法。&lt;/p&gt;&lt;h2&gt;最速下降法&lt;/h2&gt;&lt;p&gt;友情提示，下面我们要聊的内容主要用于凸函数。关于凸函数的性质这里就不多说了，大家不懂的去查查资料就好。前面我们提到了梯度下降法，也提到了梯度下降法中那个让人头疼的learning rate，那么我们有没有其他的办法不去计算这个learning rate，又能让剃度下降的每一步尽可能地走好呢？&lt;/p&gt;&lt;p&gt;于是有大神们发明了下面这个方法——最速下降法。所谓的最速下降法，就是在确定下降方向后，从下降方向中找到下降程度最大的一点进行下降。我们可以用形式化的方式来明确表述下：&lt;/p&gt;&lt;p&gt;如果说我们前面的梯度下降法是先求出梯度，再根据预先设定的learning rate完成下降，那么就完成了一轮的优化；&lt;/p&gt;&lt;p&gt;而最速下降法在求出梯度之后，要进行另外一个小优化问题的求解过程，那就是选择最合适的learning rate，使得函数值最小，如果待求的函数为f(x)，当前的迭代轮数为t，那么当前函数的优化的参数解是&lt;equation&gt;x_t&lt;/equation&gt;，这一点的梯度为&lt;equation&gt;-g_t&lt;/equation&gt;，于是我们小优化问题就变成了：&lt;/p&gt;&lt;equation&gt;\alpha_t=argmin_{\alpha_t}f(x_t-\alpha_t*g)&lt;/equation&gt;&lt;equation&gt;x_{t+1}=x_t-\alpha_t*g&lt;/equation&gt;&lt;p&gt;好了，下面就该求解这个问题了。实际上我们同样可以采用求梯度并令梯度值为0的方式求出，但是那种方法并不是很容易推导出一个通用的公式，所以我们可以采用另外的方法求出一个公式来。&lt;/p&gt;&lt;h2&gt;梯度正交&lt;/h2&gt;&lt;p&gt;最速下降法的优化方向有一个特点，那就是相邻两轮迭代的梯度相互正交。这个特点对于推导最终的算法十分重要，我们首先来证明一下。&lt;/p&gt;&lt;p&gt;证明这个问题的最好方法是反证法。我们假设迭代轮数分别为t和t+1的梯度分别为&lt;equation&gt;g_t&lt;/equation&gt;和&lt;equation&gt;g_{t+1}&lt;/equation&gt;，如果两个梯度不正交，那么我们就可以把&lt;equation&gt;g_{t+1}&lt;/equation&gt;分解成两个部分——&lt;/p&gt;&lt;ol&gt;&lt;li&gt;与&lt;equation&gt;g_t&lt;/equation&gt;共线的部分&lt;equation&gt;g_{t+1}^t&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;与&lt;equation&gt;g_t&lt;/equation&gt;正交的部分&lt;equation&gt;g_{t+1}'&lt;/equation&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;那么问题来了——对于被第t轮更新后&lt;equation&gt;x_{t+1}&lt;/equation&gt;来说，在&lt;equation&gt;g_t&lt;/equation&gt;这个方向上应该是最小的了，换句话说在这个点上，关于&lt;equation&gt;g_t&lt;/equation&gt;的方向梯度应该为0，那么&lt;equation&gt;g_{t+1}&lt;/equation&gt;就不应该有&lt;equation&gt;g_t&lt;/equation&gt;方向的分量。如果有就说明上一轮迭代并没有做到最优，和我们的假设矛盾，所以假设不成立，我们最终可以认定，&lt;equation&gt;g_t&lt;/equation&gt;和&lt;equation&gt;g_{t+1}&lt;/equation&gt;是正交的。&lt;/p&gt;&lt;p&gt;知道了正交的特点，我们就可以用这个性质来进行计算了。我们的经典问题是一个二阶优化问题：&lt;/p&gt;&lt;equation&gt;f(X)=\frac{1}{2}X^TAX&lt;/equation&gt;&lt;p&gt;首先为了保证这个问题是凸函数，我们需要对上面的一些内容做约束，具体来说就是A是对称正定的，这样，我们就能保证函数的凸性质了。&lt;/p&gt;&lt;p&gt;于是乎，我们可以得到第一个公式：&lt;/p&gt;&lt;p&gt;g=AX&lt;/p&gt;&lt;p&gt;然后是第二个公式，也就是我们刚才推倒了半天得到的结论：&lt;/p&gt;&lt;equation&gt;g_t^Tg_{t+1}=0&lt;/equation&gt;&lt;p&gt;最后是第三个公式，也是大家喜闻乐见的参数更新公式：&lt;/p&gt;&lt;equation&gt;x_{t+1}=x_t-\alpha_tg_t&lt;/equation&gt;&lt;p&gt;好了，有这三个公式就足够了，我们开始推导：&lt;/p&gt;&lt;equation&gt;g_t^Tg_{t+1}=0&lt;/equation&gt;&lt;equation&gt;g_t^T(AX_{t+1})=0&lt;/equation&gt;&lt;equation&gt;g_t^T(A(X_t-\alpha_t g_t))=0&lt;/equation&gt;&lt;equation&gt;g_t^T(AX_t-A\alpha_t g_t)=0&lt;/equation&gt;&lt;equation&gt;g_t^T(g_t-A\alpha_t g_t)=0&lt;/equation&gt;&lt;equation&gt;\alpha_t=\frac{g_t^Tg_t}{g_t^TAg_t}&lt;/equation&gt;&lt;p&gt;好了，到此我们就把步长的公式求解出来了，下回我们来看看一些细节问题。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;欢迎加入我爱机器学习7群：467165306！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23776390&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Thu, 08 Dec 2016 22:34:55 GMT</pubDate></item><item><title>CNN Dropout的极端实验</title><link>https://zhuanlan.zhihu.com/p/22060265</link><description>&lt;p&gt;本文收录在&lt;a href="https://zhuanlan.zhihu.com/p/22464594" data-editable="true" data-title="无痛的机器学习第一季" class=""&gt;无痛的机器学习第一季&lt;/a&gt;。&lt;/p&gt;有关CNN的故事还有很多，前面我们花了一定的篇幅，讲了有关初始化算法的事情，接下来我们将换一个方向，去看看众位大神在网络结构方面做出的杰出贡献。接下来我们就来看看这一路大神们的杰作之一——Dropout Layer。&lt;p&gt;在训练过程中，Dropout Layer会丢弃一定数量的信息，只让部分数据发挥作用。而且，由于采用随机丢弃的方式，每一次进行前向后向计算时，丢弃掉的数据都会有所不同。这样，模型每一次的前向后向计算的表现都会不同。&lt;/p&gt;&lt;p&gt;而在预测过程中，Dropout Layer将打开所有的参数，让所有的参数发挥作用。这样就相当于把所有的参数的作用同时发挥出来，让模型有点ensemble的效果。&lt;/p&gt;&lt;p&gt;关于Dropout能产生的效果，我们这回来做一个比较激进的实验。&lt;/p&gt;&lt;h2&gt;半字识别&lt;/h2&gt;&lt;p&gt;这次做的实验的主角还是我们熟悉的MNIST，当然，为了让这个实验变得足够刺激，我们要给这个实验加点料。那么要加什么料呢？&lt;/p&gt;&lt;p&gt;我们保持60000张训练数据不变，而将10000张测试数据的上半部分重置成0。那么看上去每一个数据都少了一半，就像这样：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/b54062bae454485dedc8d7b3208cba0c.png" data-rawwidth="203" data-rawheight="475"&gt;这样：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/c68b0914c5fefab46ca1ed6f59d0d55f.png" data-rawwidth="203" data-rawheight="478"&gt;和这样：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/31c2864694d5b5df322a41c6f90550d4.png" data-rawwidth="203" data-rawheight="474"&gt;好了，下面以我们人类的眼光，当我们看完了那些正常的数字后，再来看这些数字，是不是有种蛋疼的感觉？神经病啊……&lt;/p&gt;&lt;p&gt;（P.S. 没奖竞猜上面三个数字是啥，快来猜啊～）&lt;/p&gt;&lt;p&gt;（P.P.S 其实我选择得这几个图好算正常了，兄弟～）&lt;/p&gt;&lt;p&gt;好了，这种人类都觉得蛋疼的问题，交给计算机恐怕也是凶多极少了。这个问题实际上也算是分类问题中遇到的一个十分经典的问题——occlusion。如果我们遮挡了一个东西的一部分，你还能认出它来么？对于人来说，只要不是遮挡住最关键的信息，人类是可以通过局部的信息识别出一个整体的物体的。能做到这一点，说明人类具有利用部分信息进行分析推断的能力。如果希望计算机拥有人的智能，那么它最好也可以拥有这样的能力。&lt;/p&gt;&lt;p&gt;我们先来看看我们之前一直使用的以ReLU做非线性函数的模型的表现（这回我们不黑ReLU了）：&lt;/p&gt;&lt;p&gt;acc = 0.4358&lt;/p&gt;&lt;p&gt;识别率不到一半，不过也算它尽力了。实际上在训练的过程中，某些轮次的测试集精度比这个数还要高一些，但是有时候会出现越训练效果越差的情况。这里面的根本原因是训练集和测试集实际上并不是同样的数据分布和信息容量。&lt;/p&gt;&lt;p&gt;因为我们在训练的时候使用了全部的数据信息，那么在识别的时候每个位置都会被当作识别的特征加以训练；而到了测试部分，我们只有一半的数据，也就是说我们曾经发现的很有把握的特征突然消失了，对于模型这样的耿直boy必然是一脸蒙逼。&lt;/p&gt;&lt;p&gt;这就好比我们在做数学题时，一个公式所需要的关键参数丢失了，我们还怎么把公式求出来？巧妇难为无米之炊啊……&lt;/p&gt;&lt;p&gt;这时候聪明的同学一定想到了，有舍才有得！既然你测试数据只有一半，那我把训练数据也变成只有一半，大家的信息一致，模型用起来一定会舒服不少！于是我们得到了下面的结果：&lt;/p&gt;&lt;p&gt;acc = 0.9044&lt;/p&gt;&lt;p&gt;果然比之前的结果高了不少，模型同学你真的是太耿直了，以后都不敢给你出超纲题了……&lt;/p&gt;&lt;p&gt;这里面倒是也可以说明另一个问题，如果训练过程的数据特性和测试过程的数据特性不同，模型的结果可能会有很大的问题。如果能发现问题并想出自断一半数据的方案固然是好，但是如果没有条件发现这样的情况呢？&lt;/p&gt;&lt;p&gt;这时候，我们不妨用dropout的思想来解决，由于每次训练时我只利用一部分信息，那么我天然就具备了只使用部分信息进行推断预测的能力，这样就更容易和测试数据的形式贴近了。&lt;/p&gt;&lt;p&gt;下面我们就在ip1层的后面加上Dropout Layer，并测试dropout_ratio从0到0.9的效果，最终的结果如下图所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/73c8e9473e3c482c5796493bcafc3ace.png" data-rawwidth="712" data-rawheight="315"&gt;图中的l0表示dropout_ratio为0.0的accuracy，l9表示dropout_ratio为0.9的accuracy。从图中的结果来看，守着所有特征不放的模型精度最差，而dropout最厉害的模型表现最好。这么看来，“割一路更好打”这个战术似乎还是有点道理啊！&lt;/p&gt;&lt;p&gt;不过在这个例子中dropout_ratio=0.9的表现最好也是比较特殊的，因为MNIST的输出类别相对较少，即使dropout_ratio达到0.9也依然能够保证剩下的信息是足以识别这十个数字的，对于一些类别较多，问题较复杂的情况，丢掉这么多信息恐怕会因为必要信息不足导致识别精度下降。&lt;/p&gt;&lt;p&gt;介绍Dropout的论文中提到，Dropout有两种好处：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一定程度上减轻过拟合的情况&lt;/li&gt;&lt;li&gt;使得模型具有多模型融合的效果&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;从上面的实验中，相信我们可以体会到其中一二。&lt;/p&gt;&lt;p&gt;但是——&lt;/p&gt;&lt;p&gt;让训练集合和测试集合的数据保持一致性——这件事情比加不加dropout层要重要得多。&lt;/p&gt;&lt;p&gt;所以，dropout到底该怎么加呢？&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;欢迎加入我爱机器学习7群：467165306！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22060265&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Sat, 03 Dec 2016 23:49:38 GMT</pubDate></item></channel></rss>