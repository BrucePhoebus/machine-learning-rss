<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>无痛的机器学习 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/hsmyy</link><description>专栏主营业务：让更多人能看的懂的机器学习科普+进阶文章。欢迎各位大神投稿或协助审阅。</description><lastBuildDate>Thu, 05 Jan 2017 17:15:59 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>番外篇(5)——共轭方向的构建</title><link>https://zhuanlan.zhihu.com/p/23810213</link><description>前面我们聊过了共轭梯度的一些性质，同时我们也推导出了求步长的方法，下面就来看看如何求解出优化方向。&lt;p&gt;我们要解决的主要问题是优化方向的正交问题。由于每一次的优化后，剩下的误差和此次的优化正交（共轭正交），所以可以看出每一个优化方向彼此间都是正交的。那么如何构建一系列的正交方向呢？&lt;/p&gt;&lt;h2&gt;Gram-Schmidt方法&lt;/h2&gt;&lt;p&gt;在线性代数中，我们曾经学过一个向量正交化的方法——Gram-Schmidt方法。这个算法的内容在这里还是要说一下的，算法的输入是N维空间中N个线性无关的向量——线性无关还是有必要的，至于原因……&lt;/p&gt;&lt;p&gt;算法的输出是N个相互正交的向量，也就是我们最终想要的效果。&lt;/p&gt;&lt;p&gt;那么具体怎么做呢？&lt;/p&gt;&lt;p&gt;对于第一个向量，我们保持它不变；&lt;/p&gt;&lt;p&gt;对于第二个向量，我们去掉其中和第一个向量共线的部分；&lt;/p&gt;&lt;p&gt;对于第三个向量，我们去掉其中和第一、二个向量共线的部分；&lt;/p&gt;&lt;p&gt;对于第N个向量，我们去掉其中和第一、二、……N-1个向量共线的部分；&lt;/p&gt;&lt;p&gt;听上去思路还是蛮清晰的，那么具体怎么做呢？&lt;/p&gt;&lt;p&gt;如果输入向量是：{&lt;equation&gt;u_1,u_2,...,u_N&lt;/equation&gt;}&lt;/p&gt;&lt;p&gt;输出向量是：{&lt;equation&gt;d_1,d_2,...,d_N&lt;/equation&gt;}&lt;/p&gt;&lt;p&gt;那么对于每一个向量，我们要做下面的转换：&lt;/p&gt;&lt;equation&gt;d_t=u_t+\sum_{i=1}^{t-1}\beta_{t,i}d_i&lt;/equation&gt;&lt;p&gt;其中的&lt;equation&gt;\beta&lt;/equation&gt;表示向量被去掉的分量，这个数字是要求出来的。那么我们怎么求这个数字呢？我们利用前面提到的性质，向量之间正交（我们这里还是共轭正交），于是有&lt;/p&gt;&lt;equation&gt;d_l^TAd_t=0,(l=1,2,...,t-1)&lt;/equation&gt;&lt;p&gt;于是我们有&lt;/p&gt;&lt;equation&gt;d_l^TAd_t=d_l^TA(u_t+\sum_{i=1}^{t-1}\beta_{t,i}d_i)=d_l^TAu_t+d_l^TA\sum_{i=1}^{t-1}\beta_{t,i}d_i&lt;/equation&gt;&lt;p&gt;利用正交的性质，我们可以把公式化简为：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;=d_l^TAu_t+d_l^TA\beta_{t,i}d_i=0&lt;/equation&gt;，所以最终我们得到：&lt;/p&gt;&lt;equation&gt;\beta_{t,i}=-\frac{d_l^TAu_t}{d_l^TAd_i}&lt;/equation&gt;&lt;p&gt;下面我们就得到了完整的算法流程，这个算法的过程比较繁琐，不过是可行的。&lt;/p&gt;&lt;h2&gt;共轭梯度&lt;/h2&gt;&lt;p&gt;这个算法倒是聊完了，那么问题又来了——&lt;/p&gt;&lt;ol&gt;&lt;li&gt;我们要用什么向量去构建这些正交向量&lt;/li&gt;&lt;li&gt;前面我们看到随着向量数量的增加，我们需要计算的参数也越来越多，我们要计算N方程度个&lt;equation&gt;\beta&lt;/equation&gt;，这个计算的数目还是有点多，那么我们能不能再减少一些呢？&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;上面的问题当然能够解决，而采用的方式也比较简单——就用我们最常见的梯度来解决。这就是算法名字的来源——共轭+梯度。&lt;/p&gt;&lt;p&gt;那么第一个问题解决了，下面主攻第2个问题。第2个问题有点绕，所以我们先给出结论，那就是我们在每次求一个新方向时，只要再求解一个新的&lt;equation&gt;\beta&lt;/equation&gt;就可以了。&lt;/p&gt;&lt;p&gt;下面是推导过程。我们要证明当&lt;equation&gt;u_t=g_t&lt;/equation&gt;时，首先不是去开始证明第2个问题，而是要完成一个前置条件的证明——听上去有点晕啊。那就是由梯度组成的这个向量组是不是一个线性无关的向量组呢？&lt;/p&gt;&lt;p&gt;我们来证明一下。&lt;/p&gt;&lt;p&gt;我们前面提过共轭梯度法的特点——上一轮的优化方向和这一轮的误差之间正交，那么能不能推导出某一轮的误差和之前所有的优化方向全部正交呢？当然可以，不过这里面涉及到一些subspace的思想，我们放在后面再说，我们现在要直接给出一个靠谱的定理，那就是当采用了共轭梯度法之后，对于某一轮t求出的梯度&lt;equation&gt;g_t&lt;/equation&gt;，它和前面t-1轮的梯度正交（这里不是共轭正交，word天……），用形式化的方式描述，就是：&lt;/p&gt;&lt;equation&gt;g_i^Tg_j=0,(i&amp;lt;j)&lt;/equation&gt;&lt;p&gt;我们暂时相信这个定理，后面我们会更详细的分析它（因为进度条快撑不住了……），承认了这个定理，我们就回头看看前面求解的那些&lt;equation&gt;\beta&lt;/equation&gt;：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;g_{j+1}^Tg_i=(AX_{j+1})^Tg_i=(A(X_j+\alpha_jd_j))^Tg_i=g_j^Tg_i+\alpha_jd_j^TAg_i&lt;/equation&gt;（A是对称矩阵）&lt;/p&gt;&lt;p&gt;所以有：&lt;/p&gt;&lt;equation&gt;d_j^TAg_i=\frac{1}{\alpha_j}[g_{j+1}^Tg_i-g_j^Tg_i]&lt;/equation&gt;&lt;p&gt;我们发现上面公式的左边就是&lt;equation&gt;\beta&lt;/equation&gt;的分子，如果我们能够证明其中很多的&lt;equation&gt;\beta&lt;/equation&gt;分子为0，那么我们就完成了证明。&lt;/p&gt;&lt;p&gt;而从前面的公式看，我们必须满足j&amp;lt;i的条件，于是只有当j=i-1是，上面的式子不等于0，所以只有&lt;equation&gt;\beta_{t,t-1}&lt;/equation&gt;这一项不为0，换句话说我们只需要求解这一项，问题就解决了。&lt;/p&gt;&lt;p&gt;好了，进度条阵亡了，我们也得到了答案，下一回我们来看一下算法具体的效果。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;欢迎加入我爱机器学习8群：19517895！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23810213&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Wed, 28 Dec 2016 23:14:37 GMT</pubDate></item><item><title>番外篇(4)——共轭梯度法入坑</title><link>https://zhuanlan.zhihu.com/p/23804838</link><description>前面我们已经把最速下降法的内容介绍的差不多了，下面我们要做的就是介绍这个真正的主角了——共轭梯度法。这个优化方法算是活跃在优化世界的一个经典算法了，它的计算速度还算不错，方法也算相对简单——真的是相对，因为比起梯度下降它还是复杂不少，但是和其他的一些方法比较，那就不算难了。&lt;p&gt;好了，我们直奔主题。在番外篇的开篇我们就提到了机器学习的三个部件。优化算法作为一个黑盒，一般来说是不为人所知的。所以我们的重点就是揭开它的面纱，尽可能清楚地阐述它的原理。&lt;/p&gt;&lt;h2&gt;更高级的约束&lt;/h2&gt;&lt;p&gt;前面在最速下降法中，我们提到了最速下降法的一个性质——那就是相邻两次的优化方向是正交的。乍一听上去，总感觉这个性质很酷，但是看过了一些实际案例，又不免让人心灰——走成"zig-zag"的形状，还好意思标榜这个性质？&lt;/p&gt;&lt;p&gt;于是乎，我们开始对优化方向有了更大的野心——能不能让我们的优化方向更加智能，我们每朝一个方向走，就把这个方向走到极致？所谓的极致，可以理解为在优化的过程中我们再也不需要朝这个方向走了。于是乎我们引出了另外一个变量，叫做误差：&lt;/p&gt;&lt;equation&gt;e_t=x^*-x_t&lt;/equation&gt;&lt;p&gt;这个误差表示了参数的最优点和当前点之间的距离。那么我们的目标就可以更在形式化了，我们希望每一步优化后，当前的误差和我们刚才优化的方向正交！注意，这里我们已经不再使用梯度这个词，而是使用优化方向，因为从现在开始，我们的优化方向不一定是梯度了。当然我们也要换一个变量名比较好，不然会引起误会：&lt;/p&gt;&lt;equation&gt;r_t&lt;/equation&gt;&lt;p&gt;还是写个公式出来比较靠谱：&lt;/p&gt;&lt;equation&gt;r_t^Te_{t+1}=0&lt;/equation&gt;&lt;p&gt;这样我们可以想象，如果我们的优化空间有d维，那么我们最多只需要迭代d轮就可以求解出来了。听上去蛮靠谱的。&lt;/p&gt;&lt;p&gt;好了，大饼画完了，下面就是填坑时间了。&lt;/p&gt;&lt;h2&gt;坑&lt;/h2&gt;&lt;p&gt;理想很丰满，但是现实很骨感。如果我能知道那个误差，我还优化干嘛？直接按误差更新不就完事了？但是想知道误差还是需要一步一步地求解啊？感觉我们掉入了"chicken-egg senario"里面。&lt;/p&gt;&lt;p&gt;但是别着急，我们还有数学武器在手，我们可以用线性代数的知识偷天换日，填满这个坑。于是乎，见证奇迹的时刻就要来临了。&lt;/p&gt;&lt;h2&gt;共轭&lt;/h2&gt;&lt;p&gt;偷天换日的关键就在这个共轭上了。其实一开始看到这个词的时候我是拒绝的，这是什么鬼？这词是啥意思？&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-b16e0493701ee49ee297485a538e481c.jpg" data-rawwidth="480" data-rawheight="360"&gt;一个按原意的解释——轭就是绑在两头牛身上的木头，它让两个本来独立的个体变成了一个整体，属于牵线搭桥之类的关键道具。&lt;/p&gt;&lt;p&gt;好了，我们再回到刚才的问题中，前面我们说了我们新的正交特性，那么放在共轭这个环境下是什么效果呢？我们希望现在的关系变为共轭正交，存在一个矩阵A（A就是轭），使得：&lt;/p&gt;&lt;equation&gt;r_t^TAe_{t+1}=0&lt;/equation&gt;&lt;p&gt;其实看到这里我依然是拒绝的……这又是什么鬼，三观都崩塌了好不好，这么乱搞有什么意义？&lt;/p&gt;&lt;p&gt;别着急，其实如果我们把上面的原始公式中间加一个东西，看上去就舒服很多了：&lt;/p&gt;&lt;equation&gt;r_t^TIe_{t+1}=0&lt;/equation&gt;&lt;p&gt;单位阵不改变结果，现在我们要加一个能改变结果的东东，仅此而已。那么这个矩阵是什么作用呢？&lt;/p&gt;&lt;p&gt;我们可以认为这个矩阵要完成线性变换的作用，将一个向量从一个线性空间转换到另一个空间。转换后的向量可以满足正交的性质，如果这个矩阵一直保持不变，听上去这个性质也是合理的啊。&lt;/p&gt;&lt;p&gt;那么有没有更加实际的例子呢？&lt;/p&gt;&lt;p&gt;比方说我们有两个向量：&lt;/p&gt;&lt;equation&gt;a=[1,1] , b=[1,0.5]^T&lt;/equation&gt;&lt;p&gt;很显然它们不是正交的，但是如果我们多了一个矩阵A：&lt;/p&gt;&lt;equation&gt;A=[[1,0][0, -2]]&lt;/equation&gt;&lt;p&gt;那么我们发现，b经过A转换后就与a正交了。&lt;/p&gt;&lt;p&gt;所以这个新的条件实际上只是多绕了一个弯，和前面的条件差距并不大。&lt;/p&gt;&lt;p&gt;一旦接受了这样的设定，下面的内容就好理解多了。&lt;/p&gt;&lt;h2&gt;共轭梯度法的流程&lt;/h2&gt;&lt;p&gt;好了，我们已经明确了共轭梯度法的目标和特点，那么我就要开始推导算法公式了。当然，共轭梯度法也属于line search的一种，我们的总体思路不变：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;确定优化方向&lt;/li&gt;&lt;li&gt;确定优化步长&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;相对而言，确定优化方向比确定优化步长麻烦，我们放在后面说，现在先来捡个软柿子，那就是第2步，我们从上面提过的公式开始：&lt;/p&gt;&lt;equation&gt;r_t^TAe_{t+1}=0&lt;/equation&gt;&lt;p&gt;好了，开始推导：&lt;/p&gt;&lt;equation&gt;r_t^TAe_{t+1}=r_t^TA[e_{t}+X_t-X_{t+1}]=r_t^TA[e_{t}+\alpha_t r_t]&lt;/equation&gt;&lt;p&gt;&lt;equation&gt;=r_t^TAe_{t}+\alpha_t r_t^TAr_t=0&lt;/equation&gt;，于是&lt;/p&gt;&lt;equation&gt;\alpha_t=-\frac{r_t^TAe_{t}}{r_t^TAr_t}&lt;/equation&gt;&lt;equation&gt;\alpha_t=-\frac{r_t^TA(X^*-X_t)}{r_t^TAr_t}&lt;/equation&gt;&lt;p&gt;我们知道&lt;equation&gt;AX^*=0&lt;/equation&gt;，&lt;equation&gt;g_t=AX_t&lt;/equation&gt;,于是公式最终变为：&lt;/p&gt;&lt;equation&gt;\alpha_t=\frac{r_t^Tg_t}{r_t^TAr_t}&lt;/equation&gt;&lt;p&gt;好了，我们发现我们利用多出来的A把前面的误差e成功地消掉了，这下子步长可解了。让我们记住这个公式——不是背下来，而是大概记住这个形式，后面我们还会用到它。&lt;/p&gt;&lt;p&gt;下一回我们继续来看看后面的推导。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;欢迎加入我爱机器学习8群：19517895！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23804838&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Sun, 25 Dec 2016 11:50:15 GMT</pubDate></item><item><title>番外篇(3)——最速下降法的特点</title><link>https://zhuanlan.zhihu.com/p/23804818</link><description>&lt;p&gt;在前面两回中，我们介绍完最速下降法的的算法内容，并且介绍完了它的下降速度。从结果中我们可以看出，如果矩阵A的最大最小特征值差距大，最速下降法就有可能获得较慢的收敛性，而两者越相近，我们的收敛性也会相应增强。所以我们最终得到了一个结论，那就是最速下降法的收敛性与矩阵的特征值有关。&lt;/p&gt;&lt;p&gt;好了，说了这么多，我们还是来看看它的代码是什么样子：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def steepestGD(A, x):
	while True:
		p = -np.dot(A, x)
		if abs(np.sum(np.abs(p))) &amp;lt; 1e-6:
			break
		gradSquare = np.dot(p, p)
		Ap = np.dot(A, p)
		alpha = gradSquare / np.dot(p, Ap)
		x += alpha * p
	return x
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;一直以来我们在展示完代码后都会顺势给出几个例子，但是这一回我们就不做这样的事情了。关于这个代码的细节我们也不做多的case展示了，我们直接来看看最速下降法当中一个有趣的问题——一步到位问题。&lt;/p&gt;&lt;h2&gt;最速下降法之一步到位攻略&lt;/h2&gt;&lt;p&gt;所谓的一步到位，是指我们经过一轮迭代就能找到最优解，这个速度是非常让人羡慕的，而这其中还包含着一些数学知识。&lt;/p&gt;&lt;p&gt;由于最速下降法的步长和计算过程很有关系，所以我们需要对此进行进一步的分析，来加深对这种优化算法的印象。我们设想一种场景，那就是我们的优化方向刚好包含了最优点，那么我们就有可能在一轮迭代的过程中完成优化。&lt;/p&gt;&lt;p&gt;我们来想想一个简单的场景，经过某一步优化，我们找到了最优值，那么下面的公式一定满足：&lt;/p&gt;&lt;equation&gt;x^*=x_t-\alpha_tg_t&lt;/equation&gt;&lt;p&gt;因为最优值点的梯度为0，所以我们将梯度计算的公式带入，就有了：&lt;/p&gt;&lt;equation&gt;A(x_t-\alpha_tg_t)=0&lt;/equation&gt;&lt;p&gt;经过变换，我们可以得到公式的第一形态：&lt;/p&gt;&lt;equation&gt;g_t-\alpha_tAg_t=0&lt;/equation&gt;&lt;equation&gt;(I-\alpha_tA)g_t=0&lt;/equation&gt;&lt;p&gt;从上面的式子中，我们可以找出最简单的两种解：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;g_t=0&lt;/equation&gt;，当然，这时候我们已经找到了最优解，所以这个解并没有什么意义；&lt;/p&gt;&lt;p&gt;&lt;equation&gt;A=\frac{1}{\alpha}I&lt;/equation&gt;，这个解倒是很有意义，不过太简单了。&lt;/p&gt;&lt;p&gt;比方说当&lt;equation&gt;\alpha=1&lt;/equation&gt;时，原式变成了：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;f(X)=\frac{1}{2}X^TX&lt;/equation&gt;，&lt;/p&gt;&lt;p&gt;这个问题看上去也确实好解，对于2维的空间，这个优化函数的等高线图就是由一个个同心圆组成的，这是最容易优化的一种函数，所以一步解出来倒也不算奇怪。&lt;/p&gt;&lt;p&gt;谈完了上面两种容易找到的解，下面我们看看一些隐藏的解。当然我们需要把公式做一定的转换：&lt;/p&gt;&lt;equation&gt;Ag_t=\frac{1}{\alpha_t}g_t&lt;/equation&gt;&lt;p&gt;嗯……这个公式看上去就熟悉多了，如果&lt;equation&gt;g_t&lt;/equation&gt;是A的特征向量，那么&lt;equation&gt;\frac{1}{\alpha}&lt;/equation&gt;就是它优化的特征值了？当然，一般来说特征值是确定的，特征向量是可以变化的。但总之听上去好有道理。关键时刻还得靠万能的线性代数啊。&lt;/p&gt;&lt;p&gt;所以我们如果能保证我们的参数满足上面的条件，优化过程同样可以做到一步完成。当然，求解特征值的复杂度实在有点高，所以一般来说大家也不会采用这种方法加速求解。上面的问题只是帮助我们更好地理解最速下降法而已。&lt;/p&gt;&lt;p&gt;那么最后一个思考题了，上面我们对梯度做了约束，那么对于参数值呢？想要一步优化，它需要满足什么样的特点呢？&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;欢迎加入我爱机器学习8群：19517895！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23804818&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Mon, 19 Dec 2016 23:42:16 GMT</pubDate></item><item><title>番外篇(2)——无聊的最速下降法推导</title><link>https://zhuanlan.zhihu.com/p/23799012</link><description>好吧，我想题目已经说明了一切，虽然说这个过程很无聊，但是我们可以收藏下这段推导，因为它的结论还是很有用的……那不如让我们先来看看结论：&lt;p&gt;对于上一回我们介绍的那个函数：&lt;/p&gt;&lt;equation&gt;f(X)=\frac{1}{2}X^TQX&lt;/equation&gt;&lt;p&gt;其中Q是一个对称正定矩阵，&lt;equation&gt;\lambda_n&lt;/equation&gt;是Q的n个特征值,并且满足0 &amp;gt; &lt;equation&gt;\lambda_1&lt;/equation&gt;&amp;gt;= &lt;equation&gt;\lambda_2&lt;/equation&gt;&amp;gt;=
&lt;equation&gt;\lambda_n&lt;/equation&gt;，那么有: &lt;/p&gt;&lt;equation&gt;f(X_{k+1})\leq(\frac{\lambda_n-\lambda_1}{\lambda_n+\lambda_1})^2f(X_k)&lt;/equation&gt;&lt;p&gt;下面问题来了，我们求出上面这个公式有毛用？我们可以看出随着优化的不断进行，我们的函数是不断变小的，但是我们需要为函数优化定一个期限。我们希望函数能够尽快地优化到位，那么&lt;equation&gt;\frac{\lambda_n-\lambda_1}{\lambda_n+\lambda_1}&lt;/equation&gt;这部分当然是越小越好了。&lt;/p&gt;&lt;p&gt;如果说大家想知道结论，那么这篇文章到此结束，下面是具体的推导了，前方各种高能，非战斗人员点了赞就可以撤了……&lt;/p&gt;&lt;h2&gt;公式推导&lt;/h2&gt;&lt;p&gt;公式推导总体来说分为几个部分：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;求出步长&lt;equation&gt;\alpha_t&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;整理公式&lt;/li&gt;&lt;li&gt;Kantorovich Inequality&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;下面我们一步一步来，其中的第一步可以跳过了，因为我们在上一篇文章已经做过了，那就是求出步长，这里我们就不再推导一遍了，直接给出结果：&lt;/p&gt;&lt;equation&gt;\alpha_t=\frac{g_t^Tg_t}{g_t^TAg_t}&lt;/equation&gt;&lt;p&gt;下面是第2部。&lt;/p&gt;&lt;h2&gt;整理公式&lt;/h2&gt;&lt;p&gt;这一步的主要目标是去凑那个公式，我们从公式的左边出发，首先要利用更新公式了：&lt;/p&gt;&lt;equation&gt;X_{t+1}=X_t-\alpha_tg_t&lt;/equation&gt;&lt;equation&gt;f(X_{t+1})=\frac{1}{2}X_{t+1}^TAX_{t+1}=\frac{1}{2}(X_t-\alpha_tg_t)^T A (X_t-\alpha_tg_t)&lt;/equation&gt;&lt;equation&gt;=\frac{1}{2}[X_t^TAX_t-X_t^TA\alpha_tg_t-\alpha_tg_t^TAX_t-\alpha_t^2g_t^TAg_t]&lt;/equation&gt;&lt;equation&gt;=\frac{1}{2}[2f(X_t)-X_t^TA\alpha_tg_t-\alpha_tg_t^TAX_t-2\alpha_t^2f(g_t)]&lt;/equation&gt;&lt;p&gt;接下来我们需要两个公式：&lt;/p&gt;&lt;p&gt;首先，我们的矩阵A是对称的，所以&lt;equation&gt;A^T=A&lt;/equation&gt;;&lt;/p&gt;&lt;p&gt;其次，我们的导数公式：&lt;equation&gt;g_t=AX&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;所以上面的公式就变成了：&lt;/p&gt;&lt;equation&gt;=\frac{1}{2}[2f(X_t)-(AX_t)^T\alpha_tg_t-\alpha_tg_t^T(AX_t)+2\alpha_t^2f(g_t)]&lt;/equation&gt;&lt;equation&gt;=\frac{1}{2}[2f(X_t)-\alpha_tg_t^Tg_t-\alpha_tg_t^Tg_t+2\alpha_t^2f(g_t)]&lt;/equation&gt;&lt;equation&gt;=f(X_t)-\alpha_tg_t^Tg_t+\alpha_t^2f(g_t)&lt;/equation&gt;&lt;p&gt;好了，公式整理得差不多了，下面我们要把&lt;equation&gt;\alpha_t&lt;/equation&gt;的公式代入了：&lt;/p&gt;&lt;equation&gt;=f(X_t)-\frac{g_t^Tg_t}{g_t^TAg_t}g_t^Tg_t+(\frac{g_t^Tg_t}{g_t^TAg_t})^2f(g_t)&lt;/equation&gt;&lt;equation&gt;=f(X_t)-\frac{(g_t^Tg_t)^2}{g_t^TAg_t}+(\frac{g_t^Tg_t}{g_t^TAg_t})^2(\frac{1}{2}g_t^TAg_t)&lt;/equation&gt;&lt;equation&gt;=\frac{1}{2}[X_t^TAX_t-\frac{(g_t^Tg_t)^2}{g_t^TAg_t}]&lt;/equation&gt;&lt;p&gt;到这里似乎又推导不动了，下面继续放一个杀招：&lt;/p&gt;&lt;equation&gt;X_t^TAX_t=X_t^TAA^{-1}AX_t=X_t^TA^TA^{-1}AX_t&lt;/equation&gt;&lt;equation&gt;=(AX_t)^TA^{-1}(AX_t)=g_t^TA^{-1}g_t&lt;/equation&gt;&lt;p&gt;于是公式可以进一步整理了：&lt;/p&gt;&lt;equation&gt;=\frac{1}{2}X_t^TAX_t[1-\frac{(g_t^Tg_t)^2}{(g_t^TAg_t)(g_t^TA^{-1}g_t)}]&lt;/equation&gt;&lt;equation&gt;=f(X_t)[1-\frac{(g_t^Tg_t)^2}{(g_t^TAg_t)(g_t^TA^{-1}g_t)}]&lt;/equation&gt;&lt;p&gt;好了，整理到这里我们的第一步工作基本完成了，我们来看看我们成果和最终的结果：&lt;/p&gt;&lt;p&gt;最终结果：&lt;equation&gt;f(x_{k+1})\leq(\frac{\lambda_n-\lambda_1}{\lambda_n+\lambda_1})^2f(x_k)&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;目前结果：&lt;equation&gt;f(X_{t+1})=[1-\frac{(g_t^Tg_t)^2}{(g_t^TAg_t)(g_t^TA^{-1}g_t)}]f(X_t)&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;好了，下面的目标很明确了，我们要做的就是里面那一大堆的变换。&lt;/p&gt;&lt;h2&gt;Kantorovich Inequality&lt;/h2&gt;&lt;p&gt;说实话如果是一般的文章到这里就会结束了，可是这篇推导实在比较恶心，于是我决定一口气推完，不给大家恶心第二次的机会。&lt;/p&gt;&lt;p&gt;下面这部分的转换据说是一个叫做Kantorovich Inequality的定理，但是这么不常见的定理对于我们来说实在陌生，如果拿这个名称糊弄大家就有点不太地道了，于是乎我们就假装不知道这个定理，把这个定理再推导一遍。&lt;/p&gt;&lt;p&gt;我们这一步的目标是证明：&lt;/p&gt;&lt;equation&gt;\frac{(g_t^Tg_t)^2}{(g_t^TAg_t)(g_t^TA^{-1}g_t)}\geq \frac{4\lambda_1\lambda_n}{(\lambda_1+\lambda_n)^2}&lt;/equation&gt;&lt;p&gt;也就是&lt;equation&gt;\frac{(g_t^TAg_t)(g_t^TA^{-1}g_t)}{(g_t^Tg_t)^2}\leq \frac{(\lambda_1+\lambda_n)^2}{4\lambda_1\lambda_n}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;好了，现在我们开始继续推导。左边部分的A可以变为特征值的形式：&lt;/p&gt;&lt;equation&gt;=\frac{(g_t^TS^T\Lambda Sg_t)(g_t^TS^T \Lambda^{-1}Sg_t)}{(g_t^TS^TSg_t)^2}&lt;/equation&gt;&lt;p&gt;令&lt;equation&gt;Sg_t=l&lt;/equation&gt;，可以得到：&lt;/p&gt;&lt;equation&gt;=\frac{\sum_{i=1}^n{\lambda_i * l_i^2}\sum_{i=1}^n{\frac{1}{\lambda_i} * l_i^2}}{(l_t^Tl_t)^2}&lt;/equation&gt;&lt;p&gt;此时的分母相当于一个归一化的因子,所以我们可以将式子进一步化解为:&lt;/p&gt;&lt;equation&gt;\sum_{i=1}^n{\lambda_i * z_i^2}\sum_{i=1}^n{\frac{1}{\lambda_i} * z_i^2}&lt;/equation&gt;&lt;p&gt;given &lt;equation&gt;\sum_{i=1}^n{z_i^2}=1&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;我们可以利用拉格朗日乘子法把这两个公式融合起来,得到:&lt;/p&gt;&lt;equation&gt;F=\sum_{i=1}^n{\lambda_i * z_i^2}\sum_{i=1}^n{\frac{1}{\lambda_i} * z_i^2}+\alpha(\sum_{i=1}^n{z_i^2}-1)&lt;/equation&gt;&lt;p&gt;这里令&lt;/p&gt;&lt;equation&gt;\sigma=\sum_{i=1}^n{\lambda_i * z_i^2}&lt;/equation&gt;&lt;equation&gt;\hat{\sigma}=\sum_{i=1}^n{\frac{1}{\lambda_i} * z_i^2}&lt;/equation&gt;&lt;p&gt;我们可以进行日常的求导工作找到这个函数的极值：&lt;/p&gt;&lt;equation&gt;\frac{\partial F}{\partial z_i}=2(\sigma \frac{1}{\lambda_i}z_i+\hat{\sigma}\lambda_iz_i-\alpha z_i)=0,(i=1...n)&lt;/equation&gt;&lt;equation&gt;\frac{\partial F}{\partial z_i}=z_i(\sigma+\hat{\sigma}\lambda_i^2-\alpha \lambda_i)=0,(i=1...n)&lt;/equation&gt;&lt;p&gt;我们可以给出上面求导的一种解，那就是我们令尽可能多的&lt;equation&gt;z_i&lt;/equation&gt;等于0，而保留其中的两个 &lt;equation&gt;z_i&lt;/equation&gt;不等于0，关于这两个的导数，我们让&lt;equation&gt;(\sigma+\hat{\sigma}\lambda_i^2-\alpha \lambda_i)&lt;/equation&gt;这一部分等于0，同时我们保证所有的变量都满足那个约束，于是我们最原始的公式就变成了：&lt;/p&gt;&lt;equation&gt;F=(\lambda_kz_k^2+\lambda_lz_l^2)(\frac{1}{\lambda_k}z_k^2+\frac{1}{\lambda_l}z_l^2)&lt;/equation&gt;&lt;p&gt;given &lt;equation&gt;z_k^2+z_l^2=1&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;下面就是复杂的各种变换：&lt;/p&gt;&lt;equation&gt; = \frac{1}{4} (\sqrt{\frac{\lambda_k}{\lambda_l}} + \sqrt{\frac{\lambda_l}{\lambda_k}})^2 (z_k^2 + z_l^2)^2 - \frac{1}{4} (\sqrt{\frac{\lambda_k}{\lambda_l}} - \sqrt{\frac{\lambda_l}{\lambda_k}})^2 (z_k^2 - z_l^2)^2&lt;/equation&gt;&lt;p&gt;因为这时右边第2项是小于0的，加上前面given的条件，所以可以得到：&lt;/p&gt;&lt;equation&gt;\leq \frac{1}{4} (\sqrt{\frac{\lambda_k}{\lambda_l}} + \sqrt{\frac{\lambda_l}{\lambda_k}})^2&lt;/equation&gt;&lt;p&gt;我们选择差距最大的两个特征值作为上界，于是又有&lt;/p&gt;&lt;equation&gt;\leq \frac{1}{4} (\sqrt{\frac{\lambda_1}{\lambda_n}} + \sqrt{\frac{\lambda_n}{\lambda_1}})^2&lt;/equation&gt;&lt;p&gt;总之一个是最大的特征值，一个是最小的就好。于是乎又可以得到：&lt;/p&gt;&lt;equation&gt;=\frac{(\lambda_1+\lambda_n)^2}{4\lambda_1\lambda_n}&lt;/equation&gt;&lt;p&gt;我们最终想要的结果马上就要出现了！&lt;/p&gt;&lt;p&gt;于是乎我们得到的完整内容是：&lt;/p&gt;&lt;equation&gt;\frac{(g_t^Tg_t)^2}{(g_t^TAg_t)(g_t^TA^{-1}g_t)}\geq \frac{4\lambda_1\lambda_n}{(\lambda_1+\lambda_n)^2}&lt;/equation&gt;&lt;h2&gt;最终章&lt;/h2&gt;&lt;equation&gt;f(X_{k+1}) \leq (1 - \frac{4 \lambda_1 \lambda_n }{(\lambda_1 + \lambda_n)^2}) f(X_k)&lt;/equation&gt;&lt;equation&gt;= (\frac{(\lambda_1 + \lambda_n)^2}{(\lambda_1 + \lambda_n)^2} - \frac{4 \lambda_1 \lambda_n }{(\lambda_1 + \lambda_n)^2}) f(X_k)&lt;/equation&gt;&lt;equation&gt;= (\frac{\lambda_1^2 - 2 \lambda_1 \lambda_n + \lambda_n^2 }{(\lambda_1 + \lambda_n)^2}) f(X_k)&lt;/equation&gt;&lt;equation&gt;=(\frac{\lambda_1 - \lambda_n}{\lambda_1 + \lambda_n})^2 f(X_k)&lt;/equation&gt;&lt;p&gt;好了，到这里我们的推导结束了，该休息了……&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;欢迎加入我爱机器学习8群：19517895！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23799012&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Tue, 13 Dec 2016 23:44:21 GMT</pubDate></item><item><title>番外篇(1)——最速下降法</title><link>https://zhuanlan.zhihu.com/p/23776390</link><description>番外篇正式开始，我们主要利用番外篇的时间聊一些机器学习中的黑盒部分——没错，就是优化算法。之前接受过前辈的教诲，一个机器学习的套路可以分解成三个部分——模型，目标和优化方法。模型用来定义待解决的问题，目标（一般也会被称作损失函数）用来明确评价模型质量的方法，而优化算法则是具体解决求解过程的问题。有了这三个部分，我们可以说在学术的角度上我们基本上就搞定了一个机器学习问题。之所以在前面加上了学术这两个字，是因为在工业界一个机器学习的问题就不止这三部了。&lt;p&gt;好了回到正题，我们回到前面的三个部分，一般来说第一部分是最灵活的，第二部分也算灵活，但还是有一定的约束的，然而第三部分——一般来说都是非常确定的，而且一般也是以一个黑盒的状态出现的。&lt;/p&gt;&lt;p&gt;于是乎大家一般对第一部分和第二部分更为关注，而对第三部分相对忽视一些。于是乎我们这里就反其道而行，作死地选择和大家聊聊第三部分——优化。实际上在前面的正文中我们已经花了很大的篇幅去聊一些优化算法，下面我们继续聊一些经典的算法。&lt;/p&gt;&lt;h2&gt;最速下降法&lt;/h2&gt;&lt;p&gt;友情提示，下面我们要聊的内容主要用于凸函数。关于凸函数的性质这里就不多说了，大家不懂的去查查资料就好。前面我们提到了梯度下降法，也提到了梯度下降法中那个让人头疼的learning rate，那么我们有没有其他的办法不去计算这个learning rate，又能让剃度下降的每一步尽可能地走好呢？&lt;/p&gt;&lt;p&gt;于是有大神们发明了下面这个方法——最速下降法。所谓的最速下降法，就是在确定下降方向后，从下降方向中找到下降程度最大的一点进行下降。我们可以用形式化的方式来明确表述下：&lt;/p&gt;&lt;p&gt;如果说我们前面的梯度下降法是先求出梯度，再根据预先设定的learning rate完成下降，那么就完成了一轮的优化；&lt;/p&gt;&lt;p&gt;而最速下降法在求出梯度之后，要进行另外一个小优化问题的求解过程，那就是选择最合适的learning rate，使得函数值最小，如果待求的函数为f(x)，当前的迭代轮数为t，那么当前函数的优化的参数解是&lt;equation&gt;x_t&lt;/equation&gt;，这一点的梯度为&lt;equation&gt;-g_t&lt;/equation&gt;，于是我们小优化问题就变成了：&lt;/p&gt;&lt;equation&gt;\alpha_t=argmin_{\alpha_t}f(x_t-\alpha_t*g)&lt;/equation&gt;&lt;equation&gt;x_{t+1}=x_t-\alpha_t*g&lt;/equation&gt;&lt;p&gt;好了，下面就该求解这个问题了。实际上我们同样可以采用求梯度并令梯度值为0的方式求出，但是那种方法并不是很容易推导出一个通用的公式，所以我们可以采用另外的方法求出一个公式来。&lt;/p&gt;&lt;h2&gt;梯度正交&lt;/h2&gt;&lt;p&gt;最速下降法的优化方向有一个特点，那就是相邻两轮迭代的梯度相互正交。这个特点对于推导最终的算法十分重要，我们首先来证明一下。&lt;/p&gt;&lt;p&gt;证明这个问题的最好方法是反证法。我们假设迭代轮数分别为t和t+1的梯度分别为&lt;equation&gt;g_t&lt;/equation&gt;和&lt;equation&gt;g_{t+1}&lt;/equation&gt;，如果两个梯度不正交，那么我们就可以把&lt;equation&gt;g_{t+1}&lt;/equation&gt;分解成两个部分——&lt;/p&gt;&lt;ol&gt;&lt;li&gt;与&lt;equation&gt;g_t&lt;/equation&gt;共线的部分&lt;equation&gt;g_{t+1}^t&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;与&lt;equation&gt;g_t&lt;/equation&gt;正交的部分&lt;equation&gt;g_{t+1}'&lt;/equation&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;那么问题来了——对于被第t轮更新后&lt;equation&gt;x_{t+1}&lt;/equation&gt;来说，在&lt;equation&gt;g_t&lt;/equation&gt;这个方向上应该是最小的了，换句话说在这个点上，关于&lt;equation&gt;g_t&lt;/equation&gt;的方向梯度应该为0，那么&lt;equation&gt;g_{t+1}&lt;/equation&gt;就不应该有&lt;equation&gt;g_t&lt;/equation&gt;方向的分量。如果有就说明上一轮迭代并没有做到最优，和我们的假设矛盾，所以假设不成立，我们最终可以认定，&lt;equation&gt;g_t&lt;/equation&gt;和&lt;equation&gt;g_{t+1}&lt;/equation&gt;是正交的。&lt;/p&gt;&lt;p&gt;知道了正交的特点，我们就可以用这个性质来进行计算了。我们的经典问题是一个二阶优化问题：&lt;/p&gt;&lt;equation&gt;f(X)=\frac{1}{2}X^TAX&lt;/equation&gt;&lt;p&gt;首先为了保证这个问题是凸函数，我们需要对上面的一些内容做约束，具体来说就是A是对称正定的，这样，我们就能保证函数的凸性质了。&lt;/p&gt;&lt;p&gt;于是乎，我们可以得到第一个公式：&lt;/p&gt;&lt;p&gt;g=AX&lt;/p&gt;&lt;p&gt;然后是第二个公式，也就是我们刚才推倒了半天得到的结论：&lt;/p&gt;&lt;equation&gt;g_t^Tg_{t+1}=0&lt;/equation&gt;&lt;p&gt;最后是第三个公式，也是大家喜闻乐见的参数更新公式：&lt;/p&gt;&lt;equation&gt;x_{t+1}=x_t-\alpha_tg_t&lt;/equation&gt;&lt;p&gt;好了，有这三个公式就足够了，我们开始推导：&lt;/p&gt;&lt;equation&gt;g_t^Tg_{t+1}=0&lt;/equation&gt;&lt;equation&gt;g_t^T(AX_{t+1})=0&lt;/equation&gt;&lt;equation&gt;g_t^T(A(X_t-\alpha_t g_t))=0&lt;/equation&gt;&lt;equation&gt;g_t^T(AX_t-A\alpha_t g_t)=0&lt;/equation&gt;&lt;equation&gt;g_t^T(g_t-A\alpha_t g_t)=0&lt;/equation&gt;&lt;equation&gt;\alpha_t=\frac{g_t^Tg_t}{g_t^TAg_t}&lt;/equation&gt;&lt;p&gt;好了，到此我们就把步长的公式求解出来了，下回我们来看看一些细节问题。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;欢迎加入我爱机器学习7群：467165306！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23776390&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Thu, 08 Dec 2016 22:34:55 GMT</pubDate></item><item><title>CNN Dropout的极端实验</title><link>https://zhuanlan.zhihu.com/p/22060265</link><description>&lt;p&gt;本系列更多文章欢迎点击：&lt;a href="https://zhuanlan.zhihu.com/p/22464594" class=""&gt;无痛的机器学习第一季目录&lt;/a&gt;，现已完整收录。&lt;/p&gt;有关CNN的故事还有很多，前面我们花了一定的篇幅，讲了有关初始化算法的事情，接下来我们将换一个方向，去看看众位大神在网络结构方面做出的杰出贡献。接下来我们就来看看这一路大神们的杰作之一——Dropout Layer。&lt;p&gt;在训练过程中，Dropout Layer会丢弃一定数量的信息，只让部分数据发挥作用。而且，由于采用随机丢弃的方式，每一次进行前向后向计算时，丢弃掉的数据都会有所不同。这样，模型每一次的前向后向计算的表现都会不同。&lt;/p&gt;&lt;p&gt;而在预测过程中，Dropout Layer将打开所有的参数，让所有的参数发挥作用。这样就相当于把所有的参数的作用同时发挥出来，让模型有点ensemble的效果。&lt;/p&gt;&lt;p&gt;关于Dropout能产生的效果，我们这回来做一个比较激进的实验。&lt;/p&gt;&lt;h2&gt;半字识别&lt;/h2&gt;&lt;p&gt;这次做的实验的主角还是我们熟悉的MNIST，当然，为了让这个实验变得足够刺激，我们要给这个实验加点料。那么要加什么料呢？&lt;/p&gt;&lt;p&gt;我们保持60000张训练数据不变，而将10000张测试数据的上半部分重置成0。那么看上去每一个数据都少了一半，就像这样：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/b54062bae454485dedc8d7b3208cba0c.png" data-rawwidth="203" data-rawheight="475"&gt;这样：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/c68b0914c5fefab46ca1ed6f59d0d55f.png" data-rawwidth="203" data-rawheight="478"&gt;和这样：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/31c2864694d5b5df322a41c6f90550d4.png" data-rawwidth="203" data-rawheight="474"&gt;好了，下面以我们人类的眼光，当我们看完了那些正常的数字后，再来看这些数字，是不是有种蛋疼的感觉？神经病啊……&lt;/p&gt;&lt;p&gt;（P.S. 没奖竞猜上面三个数字是啥，快来猜啊～）&lt;/p&gt;&lt;p&gt;（P.P.S 其实我选择得这几个图好算正常了，兄弟～）&lt;/p&gt;&lt;p&gt;好了，这种人类都觉得蛋疼的问题，交给计算机恐怕也是凶多极少了。这个问题实际上也算是分类问题中遇到的一个十分经典的问题——occlusion。如果我们遮挡了一个东西的一部分，你还能认出它来么？对于人来说，只要不是遮挡住最关键的信息，人类是可以通过局部的信息识别出一个整体的物体的。能做到这一点，说明人类具有利用部分信息进行分析推断的能力。如果希望计算机拥有人的智能，那么它最好也可以拥有这样的能力。&lt;/p&gt;&lt;p&gt;我们先来看看我们之前一直使用的以ReLU做非线性函数的模型的表现（这回我们不黑ReLU了）：&lt;/p&gt;&lt;p&gt;acc = 0.4358&lt;/p&gt;&lt;p&gt;识别率不到一半，不过也算它尽力了。实际上在训练的过程中，某些轮次的测试集精度比这个数还要高一些，但是有时候会出现越训练效果越差的情况。这里面的根本原因是训练集和测试集实际上并不是同样的数据分布和信息容量。&lt;/p&gt;&lt;p&gt;因为我们在训练的时候使用了全部的数据信息，那么在识别的时候每个位置都会被当作识别的特征加以训练；而到了测试部分，我们只有一半的数据，也就是说我们曾经发现的很有把握的特征突然消失了，对于模型这样的耿直boy必然是一脸蒙逼。&lt;/p&gt;&lt;p&gt;这就好比我们在做数学题时，一个公式所需要的关键参数丢失了，我们还怎么把公式求出来？巧妇难为无米之炊啊……&lt;/p&gt;&lt;p&gt;这时候聪明的同学一定想到了，有舍才有得！既然你测试数据只有一半，那我把训练数据也变成只有一半，大家的信息一致，模型用起来一定会舒服不少！于是我们得到了下面的结果：&lt;/p&gt;&lt;p&gt;acc = 0.9044&lt;/p&gt;&lt;p&gt;果然比之前的结果高了不少，模型同学你真的是太耿直了，以后都不敢给你出超纲题了……&lt;/p&gt;&lt;p&gt;这里面倒是也可以说明另一个问题，如果训练过程的数据特性和测试过程的数据特性不同，模型的结果可能会有很大的问题。如果能发现问题并想出自断一半数据的方案固然是好，但是如果没有条件发现这样的情况呢？&lt;/p&gt;&lt;p&gt;这时候，我们不妨用dropout的思想来解决，由于每次训练时我只利用一部分信息，那么我天然就具备了只使用部分信息进行推断预测的能力，这样就更容易和测试数据的形式贴近了。&lt;/p&gt;&lt;p&gt;下面我们就在ip1层的后面加上Dropout Layer，并测试dropout_ratio从0到0.9的效果，最终的结果如下图所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/73c8e9473e3c482c5796493bcafc3ace.png" data-rawwidth="712" data-rawheight="315"&gt;图中的l0表示dropout_ratio为0.0的accuracy，l9表示dropout_ratio为0.9的accuracy。从图中的结果来看，守着所有特征不放的模型精度最差，而dropout最厉害的模型表现最好。这么看来，“割一路更好打”这个战术似乎还是有点道理啊！&lt;/p&gt;&lt;p&gt;不过在这个例子中dropout_ratio=0.9的表现最好也是比较特殊的，因为MNIST的输出类别相对较少，即使dropout_ratio达到0.9也依然能够保证剩下的信息是足以识别这十个数字的，对于一些类别较多，问题较复杂的情况，丢掉这么多信息恐怕会因为必要信息不足导致识别精度下降。&lt;/p&gt;&lt;p&gt;介绍Dropout的论文中提到，Dropout有两种好处：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一定程度上减轻过拟合的情况&lt;/li&gt;&lt;li&gt;使得模型具有多模型融合的效果&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;从上面的实验中，相信我们可以体会到其中一二。&lt;/p&gt;&lt;p&gt;但是——&lt;/p&gt;&lt;p&gt;让训练集合和测试集合的数据保持一致性——这件事情比加不加dropout层要重要得多。&lt;/p&gt;&lt;p&gt;所以，dropout到底该怎么加呢？&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;欢迎加入我爱机器学习7群：467165306！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22060265&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Sat, 03 Dec 2016 23:49:38 GMT</pubDate></item><item><title>FCN(6)——从CRF到RNN</title><link>https://zhuanlan.zhihu.com/p/22795755</link><description>前面我们花了大量的篇章介绍了CRF和DenseCRF的内容，下面我们把FCN和CRF串起来。&lt;h2&gt;CRFasRNN&lt;/h2&gt;&lt;p&gt;前面我们在denseCRF中留了一个小尾巴，那就是unary function。为了让FCN结合起来，这里我们做两个设定：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;FCN的结合作为unary function的结果&lt;/li&gt;&lt;li&gt;FCN的结果作为pairwise function中的Q函数的初始值。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这样FCN和CRF就连起来了。下面我们还要解决一个问题，就是为什么是CRFasRNN？&lt;/p&gt;&lt;p&gt;在这篇模型结合的论文中，作者将CRF的求解过程转换成了RNN的形式。由于CRF的求解算法是迭代进行的，因此把算法展开，我们可以将其变成RNN的形式，这里的细节在此就不多说了，大家看看论文基本就能看懂。&lt;/p&gt;&lt;h2&gt;实现&lt;/h2&gt;&lt;p&gt;下面就来看看他的具体实现：&lt;a href="https://github.com/torrvision/crfasrnn" data-editable="true" data-title="GitHub - torrvision/crfasrnn: This repository contains the source code for the semantic image segmentation method described in the ICCV 2015 paper: Conditional Random Fields as Recurrent Neural Networks. http://crfasrnn.torr.vision/" class=""&gt;GitHub - torrvision/crfasrnn: This repository contains the source code for the semantic image segmentation method described in the ICCV 2015 paper: Conditional Random Fields as Recurrent Neural Networks. http://crfasrnn.torr.vision/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;我们先来看看它的prototxt，我们可以用下面这张图表示：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-6aca12386083603e198cff5c9e6f3f05.jpg" data-rawwidth="960" data-rawheight="1280"&gt;&lt;p&gt;可以看出网络的主体部分是由之前的FCN组成。在前面的FCN中我们没有介绍其实现，这里我们就详细看下它的实现。我们从图的左上角出发，到图的左下角，然后再返回到右上角。前面全部是卷积、relu和maxpooling这几部分，后面是不同scale的结果融合。这里详细地展示了这一部分的维度内容。&lt;/p&gt;&lt;p&gt;从实现中，我们可以看出一些值得深思的细节。&lt;/p&gt;&lt;p&gt;首先是一开始的padding=100。为了不让最终的feature map太小，添加一个较大的padding是十分必要的。&lt;/p&gt;&lt;p&gt;其次就是最后一层的MULTI_STAGE_MEANFIELD，这一层我们需要特别介绍一下，虽然在文章中作者提到了CRF as RNN的概念，但是实际上它的实现并没有用RNN的框架，当然Caffe里面并没有这里可用的RNN。这里是将所有的CRF的内容集成到了一个层中，所以这一层会比较复杂，计算量也比较大，其中的反向传播也比较复杂。&lt;/p&gt;&lt;p&gt;首先我们将算法的流程图展示出来：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-0891a04ad4133d9440e5bd97ae8d98fe.jpg" data-rawwidth="960" data-rawheight="1280"&gt;&lt;p&gt;可以看出，这其中的计算主要涉及到两个类别—— MultiStageMeanfieldLayer和MeanfieldIteration，其中的MultiStageMeanfieldLayer类主要负责算法内容的组织，而其中的MeanfieldIteration类主要负责迭代计算过程。这些内容我们都可以从上面的图中读出。&lt;/p&gt;&lt;p&gt;可以看出这其中的细节除了计算高斯滤波部分的Permutohedral比较复杂，属于超纲内容，其他的部分相对比较好理解。关于Permutohedral部分的细节，有机会我们可以开一个CV系列的文章另行讲解。&lt;/p&gt;&lt;p&gt;MeanfieldIteration的反向操作实际上并不复杂，只要记得把这些过程拆解成一个个小部分慢慢算梯度就好，而MultiStageMeanfieldLayer的反向操作也只是把MeanfieldIteration的反向结果合并起来，具体细节可以去看看源码，这里我们就不再赘述了。&lt;/p&gt;&lt;p&gt;到这里，我们已经实现了将CNN和CRF无缝连接起来了。比起之前的单独由CNN组成的FCN，我们已经有了很大的进步。&lt;/p&gt;&lt;p&gt;到这里，实际上我们花了很大的力气在介绍CRF和无向概率图模型的内容，但是我们终于把这部分的内容讲完了。让我休息一下……&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;欢迎加入我爱机器学习7群：467165306！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22795755&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Tue, 29 Nov 2016 21:10:02 GMT</pubDate></item><item><title>FCN(5)——DenseCRF推导</title><link>https://zhuanlan.zhihu.com/p/22464569</link><description>&lt;p&gt;经过前两篇文章，我们了解了CRF的基本概念，了解了许许多多的CRF模型，也了解了Mean field variational inference的基本概念，那么这一回我们开始真刀真枪地进行公式推导。其实公式推导的部分在论文的补充材料里有，但是不够详尽，这里我们尽可能地补充一下，让推导过程更加完整。&lt;/p&gt;&lt;h2&gt;DenseCRF&lt;/h2&gt;&lt;p&gt;前面我们已经看过了DenseCRF的能量函数，如下所示&lt;/p&gt;&lt;equation&gt;E(x)=\sum_i \psi _u(x_i)+\sum_{i&amp;lt;j}\psi_p(x_i,x_j)&lt;/equation&gt;&lt;p&gt;其他内容在这就不说了，我们抓紧时间推导。&lt;/p&gt;&lt;h2&gt;Variational Inference推导&lt;/h2&gt;&lt;p&gt;我们首先给出denseCRF的Gibbs分布：&lt;/p&gt;&lt;equation&gt;P(X)=\frac{1}{Z}\tilde{P}(X)=\frac{1}{Z}exp(\sum_i \psi_u(x_i) + \sum_{i &amp;lt; j}\psi_p(x_i,x_j))&lt;/equation&gt;&lt;p&gt;下面给出KL散度部分的推导，其实就是补充材料中的推导，搬运工来了……&lt;equation&gt;D(Q||P)=\sum_x{Q(x)log(\frac{Q(x)}{P(x)})}&lt;/equation&gt;&lt;equation&gt;=-\sum_xQ(x)logP(x)+\sum_xQ(x)logQ(x)
&lt;/equation&gt;&lt;equation&gt;=-E_{X\in Q}[logP(X)]+E_{X\in Q}[logQ(X)]
&lt;/equation&gt;&lt;equation&gt;=-E_{X\in Q}[log\tilde{P}(X) ]+E_{X \in Q}[logZ]+\sum_iE_{X_i \in Q}[logQ_i(X_i)]&lt;/equation&gt;&lt;/p&gt;&lt;equation&gt;=-E_{X \in Q}[log \tilde{P}(X)]+logZ+\sum_iE_{X_i \in Q_i}[logQ_i(X_i)]&lt;/equation&gt;&lt;p&gt;由于我们要求的是Q，而logZ项中没有Q，所以这一项可以省略。&lt;/p&gt;&lt;p&gt;同时Q还需要满足：&lt;/p&gt;&lt;equation&gt;\sum_{x_i}Q_i(x_i)=1&lt;/equation&gt;&lt;p&gt;所以利用拉格朗日乘子法，可以得到&lt;equation&gt;L(Q_i)=-E_{X_i \in Q}[log \tilde{P}(X)]+\sum_iE_{x_i \in Q_i}[logQ_i(x_i)]+\lambda(\sum_{x_i}Q_i(x_i)-1)&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;这个公式的后面两项相对比较简单，但是前面一项比较复杂，我们单独做一下处理：&lt;/p&gt;&lt;equation&gt;-E_{X_i \in Q}[log \tilde{P}(X)]=-\int{\prod_i{Q_i(x_i)}[log \tilde{P}(X)]dX}&lt;/equation&gt;&lt;equation&gt;=-\int{Q_i(x_i) \prod_{i}{Q(\bar{x}_{i})}[log \tilde{P}(X)]dx_id\bar{X}}&lt;/equation&gt;&lt;equation&gt;=-\int{Q_i(x_i)E_{\bar{X} \in Q}[log \tilde{P}(X)]dx_i}&lt;/equation&gt;&lt;p&gt;经过上面的公式整理，我们可以求出偏导，可得&lt;/p&gt;&lt;equation&gt;\frac{\partial L(Q_i)}{\partial Q_i(x_i)}=-E_{\bar{X} \in Q_i}[log \tilde{P}(X | x_i)]-logQ_i(x_i)-1+\lambda&lt;/equation&gt;&lt;p&gt;令偏导为0，就可以求出极值：&lt;/p&gt;&lt;equation&gt;Q_i(x_i)=exp(\lambda-1)exp(-E_{\bar{X} \in Q_i}[log \tilde{P}(X | x_i)])&lt;/equation&gt;&lt;p&gt;由于每一个Q的&lt;equation&gt;exp(\lambda-1)&lt;/equation&gt;都相同，我们将其当作一个常数项，之后在renormalize的时候将其抵消掉，于是Q函数就等于：&lt;/p&gt;&lt;equation&gt;Q(x_i)=\frac{1}{Z_1}exp(-E_{\bar{X} \in Q_i}[log \tilde{P}(X | x_i)])&lt;/equation&gt;&lt;p&gt;我们将文章开头关于&lt;equation&gt;\tilde{P}&lt;/equation&gt;的定义带入，就得到了&lt;/p&gt;&lt;equation&gt;Q(x_i)=\frac{1}{Z_1}exp(-E_{\bar{X} \in Q}[(\sum_i \psi_u(x_i) + \sum_{j \neq i}\psi_p(x_i,x_j)) | x_i])&lt;/equation&gt;&lt;p&gt;这里面xi的由于是已知的，所以我们可以得到补充材料里的结果（但是变量名不太一样）：&lt;/p&gt;&lt;equation&gt;Q_i(x_i=l)=\frac{1}{Z_i}exp[-\psi_u(l) - \sum_{j \neq i}E_{\bar{X} \in Q_j}\psi_p(l,X_j)]&lt;/equation&gt;&lt;p&gt;继续扩展，就可以得到&lt;/p&gt;&lt;equation&gt;=\frac{1}{Z_i}exp[-\psi_u(l) - \sum_{m=1}^Kw^{(m)}\sum_{j \neq i}E_{X \in Q_j}[\mu(l,X_j)k^{(m)}(f_i,f_j)]]&lt;/equation&gt;&lt;equation&gt;=\frac{1}{Z_i}exp[-\psi_u(l) - \sum_{m=1}^Kw^{(m)}\sum_{j \neq i}\sum_{l' \in L}Q_j(l')\mu(l,l')k^{(m)}(f_i,f_j)]&lt;/equation&gt;&lt;equation&gt;=\frac{1}{Z_i}exp[-\psi_u(l) - \sum_{l' \in L}\mu(l,l')\sum_{m=1}^Kw^{(m)}\sum_{j \neq i}Q_j(l')k^{(m)}(f_i,f_j)]&lt;/equation&gt;&lt;p&gt;这样，一个类似message passing的公式推导就完成了。其中最内层的求和可以用截断的高斯滤波完成。搬运最后的一点公式，可以得：&lt;/p&gt;&lt;equation&gt;\tilde{Q_i^{(m)}(l)}=\sum_{j \neq i}Q_j(l')k^{(m)}(f_i,f_j)=\sum_{j}Q_j(l)k^{(m)}(f_i,f_j)-Q_i(l)&lt;/equation&gt;&lt;p&gt;上面公式的第一项可以转化成卷积操作。&lt;/p&gt;&lt;p&gt;完成了这些推导，下面我们暂时不给出denseCRF的单独结果，我们下面看看FCN和DenseCRF结合的效果，FCN已经等的花都谢了……&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;欢迎加入我爱机器学习7群：467165306！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22464569&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Tue, 22 Nov 2016 21:58:11 GMT</pubDate></item><item><title>无痛的机器学习第一季目录</title><link>https://zhuanlan.zhihu.com/p/22464594</link><description>经过5个月的努力，我终于完成了40篇不高不低还算有些干货的机器学习文章。回首看看这5个月的努力，每一次的写作都充满了开心与痛苦。说开心是因为当自己完成每一个章节的写作后，自己感觉对这一部分的知识有了更加深刻地认识，而痛苦则是对写作过程中一系列事情的恐惧——找不到好选题，对论文细节的困惑，跑不出想要的结果，难以用通俗易懂的语言描述自己所知……好在这一切就要告一段落了。&lt;p&gt;以下就做一个集合贴，展示一下这一季的所有文章，对本专栏文章感兴趣的童鞋，收藏这一篇就足够了（后续未发布的几篇和番外篇会更新上来）：&lt;/p&gt;&lt;h2&gt;文章目录&lt;/h2&gt;&lt;h2&gt;CNN网络基础结构&lt;/h2&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/21525237" data-editable="true" data-title="神经网络-全连接层（1） - 无痛的机器学习 - 知乎专栏" class=""&gt;神经网络-全连接层（1）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/21535703" data-editable="true" data-title="神经网络-全连接层（2） - 无痛的机器学习 - 知乎专栏" class=""&gt;神经网络-全连接层（2）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/21572419" data-editable="true" data-title="神经网络-全连接层（3） - 无痛的机器学习 - 知乎专栏" class=""&gt;神经网络-全连接层（3）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/21609512" data-editable="true" data-title="卷积层（1） - 无痛的机器学习 - 知乎专栏" class=""&gt;卷积层（1） &lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/21675422" data-editable="true" data-title="卷积层（2） - 无痛的机器学习 - 知乎专栏" class=""&gt;卷积层（2） &lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/21737674" data-editable="true" data-title="卷积层（3） - 无痛的机器学习 - 知乎专栏" class=""&gt;卷积层（3）&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;CNN网络上层结构&lt;/h2&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/22197188" data-editable="true" data-title="CNN——架构上的一些数字 - 无痛的机器学习 - 知乎专栏" class=""&gt;CNN——架构上的一些数字&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/22214112" data-editable="true" data-title="CNN--结构上的思考 - 无痛的机器学习 - 知乎专栏" class=""&gt;CNN--结构上的思考 &lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22060265" class="" data-editable="true" data-title="CNN Dropout的极端实验"&gt;CNN Dropout的极端实验&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;Caffe源码分析&lt;/h2&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/21796890" data-editable="true" data-title="Caffe代码阅读——层次结构 " class=""&gt;Caffe代码阅读——层次结构 &lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/21875025" data-editable="true" data-title="Caffe源码阅读——Net组装 - 无痛的机器学习 - 知乎专栏" class=""&gt;Caffe源码阅读——Net组装 &lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/21800004" data-editable="true" data-title="Caffe代码阅读——Solver - 无痛的机器学习 - 知乎专栏" class=""&gt;Caffe代码阅读——Solver &lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/22260935" data-editable="true" data-title="CNN--两个Loss层计算的数值问题 " class=""&gt;CNN--两个Loss层计算的数值问题 &lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22404295" data-editable="true" data-title="Caffe源码阅读——DataLayer " class=""&gt;Caffe源码阅读——DataLayer&amp;amp;Data Transformer&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;生成网络&lt;/h2&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/22386494" class="" data-editable="true" data-title="DCGAN的小尝试（1）"&gt;DCGAN的小尝试（1）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/22389906" class="" data-title="DCGAN的小尝试（2）" data-editable="true"&gt;DCGAN的小尝试（2）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/22464760" class="" data-editable="true" data-title="VAE（1）——从KL说起"&gt;VAE（1）——从KL说起&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/22464764" class="" data-editable="true" data-title="VAE(2)——基本思想"&gt;VAE(2)——基本思想&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/22464768" class="" data-editable="true" data-title="VAE(3)——公式与实现"&gt;VAE(3)——公式与实现&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/22684931" class="" data-editable="true" data-title="VAE（4）——实现"&gt;VAE（4）——实现&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;优化算法&lt;/h2&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/21486804" data-editable="true" data-title="梯度下降是门手艺活…… - 无痛的机器学习 - 知乎专栏" class=""&gt;梯度下降是门手艺活…… &lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/21486826" data-editable="true" data-title="路遥知马力——Momentum - 无痛的机器学习 - 知乎专栏" class=""&gt;路遥知马力——Momentum &lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/22099871" data-editable="true" data-title="CNN——L1正则的稀疏性 " class=""&gt;CNN——L1正则的稀疏性 &lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/22464537" class="" data-editable="true" data-title="Caffe中的SGD的变种优化算法(1)"&gt;Caffe中的SGD的变种优化算法(1)&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/22464551" class="" data-editable="true" data-title="Caffe中的SGD的变种优化算法(2)"&gt;Caffe中的SGD的变种优化算法(2)&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;CNN可视化&lt;/h2&gt;&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22245268" data-editable="true" data-title="CNN-反卷积（1） - 无痛的机器学习 - 知乎专栏" class=""&gt;CNN-反卷积（1）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22293817" data-editable="true" data-title="CNN-卷积反卷积（2） - 无痛的机器学习 - 知乎专栏" class=""&gt;CNN-卷积反卷积（2）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/22464575" class="" data-editable="true" data-title="寻找CNN的弱点"&gt;寻找CNN的弱点&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;CNN数值&lt;/h2&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/22027076" data-editable="true" data-title="CNN的数值实验 - 无痛的机器学习 - 知乎专栏" class=""&gt;CNN的数值实验 - 无痛的机器学习 - 知乎专栏&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/22028079" data-editable="true" data-title="CNN数值——xavier（上） - 无痛的机器学习 - 知乎专栏" class=""&gt;CNN数值——xavier（上） - 无痛的机器学习 - 知乎专栏&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/22044472" data-editable="true" data-title="CNN数值——xavier（下） - 无痛的机器学习 - 知乎专栏" class=""&gt;CNN数值——xavier（下） - 无痛的机器学习 - 知乎专栏&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/22148777" data-editable="true" data-title="CNN数值——ZCA - 无痛的机器学习 - 知乎专栏" class=""&gt;CNN数值——ZCA - 无痛的机器学习 - 知乎专栏&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;FCN&lt;/h2&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/22464571" class="" data-editable="true" data-title="FCN(1)——从分类问题出发"&gt;FCN(1)——从分类问题出发&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/22464581" class="" data-title="FCN(2)——CRF通俗非严谨的入门" data-editable="true"&gt;FCN(2)——CRF通俗非严谨的入门&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/22464586" class="" data-editable="true" data-title="FCN(3)——DenseCRF"&gt;FCN(3)——DenseCRF&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/22887466" class="" data-title="FCN(4)——Mean Field Variational Inference" data-editable="true"&gt;FCN(4)——Mean Field Variational Inference&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22464569" class="" data-editable="true" data-title="FCN(5)——DenseCRF推导"&gt;FCN(5)——DenseCRF推导&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22795755" class="" data-editable="true" data-title="FCN(6)——从CRF到RNN"&gt;FCN(6)——从CRF到RNN&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;Representation&lt;/h2&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/23444100" class="" data-editable="true" data-title="CenterLoss——实战&amp;amp;源码"&gt;CenterLoss——实战&amp;amp;源码&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;GPU&lt;/h2&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/21908564" data-editable="true" data-title="[翻译]Exploring the Complexities of PCIe Connectivity and Peer-to-Peer Communication - 无痛的机器学习 - 知乎专栏" class=""&gt;[翻译]Exploring the Complexities of PCIe Connectivity and Peer-to-Peer Communication - 无痛的机器学习 - 知乎专栏&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;“聊点轻松的”系列&lt;/h2&gt;&lt;p&gt;&lt;a href="http://zhuanlan.zhihu.com/p/21788777" class="" data-editable="true" data-title="聊点轻松的——划个水"&gt;聊点轻松的——划个水&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class="" href="http://zhuanlan.zhihu.com/p/22112582" data-editable="true" data-title="聊点轻松的2——斗图篇"&gt;聊点轻松的2——斗图篇&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class="" href="http://zhuanlan.zhihu.com/p/22421787" data-editable="true" data-title="聊点轻松的3——什么是学习"&gt;聊点轻松的3——什么是学习&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class="" href="http://zhuanlan.zhihu.com/p/22842859" data-editable="true" data-title="聊点轻松的4——这回真的很轻松"&gt;聊点轻松的4——这回真的很轻松&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/23750864" class="" data-editable="true" data-title="聊点轻松的5——这篇写得并不轻松"&gt;聊点轻松的5——这篇写得并不轻松&lt;/a&gt;&lt;/p&gt;&lt;p&gt;番外篇（先给出名字，后面贴链接）&lt;/p&gt;&lt;p&gt;番外篇(1)——最速下降法&lt;/p&gt;&lt;p&gt;番外篇(2)——无聊的最速下降法推导&lt;/p&gt;&lt;p&gt;番外篇(3)——最速下降法的特点&lt;/p&gt;&lt;p&gt;番外篇(4)——共轭梯度法入坑&lt;/p&gt;&lt;p&gt;番外篇(5)——共轭方向的构建&lt;/p&gt;&lt;p&gt;番外篇(6)——共轭梯度的效果&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22464594&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Thu, 17 Nov 2016 23:48:39 GMT</pubDate></item><item><title>聊点轻松的5——这篇写得并不轻松</title><link>https://zhuanlan.zhihu.com/p/23750864</link><description>没错，又是熟悉的开头，不知不觉，我的知乎小庙迎来了5000名看官，已经快过去半年了，每招来1000名看官我就写一篇扯蛋的文章，之前我已经尽力把能扯的内容都扯过了，还把自己不擅长的内容也强扯了（最后果然被打脸了）。但是一切还算顺利，总之感谢大家的支持！这一篇我准备硬扯一下了。&lt;p&gt;在我中学的时候，我对武侠小说异常地痴迷，并且自己撸起袖子写过武侠小说。武侠小说之中，还是最喜欢金庸老先生的小说。而且因为看三联版的原因，我对40这个数字十分喜欢，因为金老先生的小说中有不少被归纳成40章节——比方说射雕三部曲，笑傲江湖。于是我也想着自己可以写出几个“40”来。&lt;/p&gt;&lt;p&gt;于是乎，自己的第一个技术类的40篇就要完结了，也可以说《无痛的机器学习》第一季就要告一段落了（实际上已经写完了，只是为了效果按期放出）。这一季结束后，我会按时间放出一些番外篇的文章，第二季的新鲜文章将在2017年正式放出了。&lt;/p&gt;&lt;p&gt;另外我发现了一个现象，不知道这个现象只发生在我这里还是在知乎中比较普遍的现象，那就是大家更喜欢点“收藏”而不是点“赞”。对我来说大家点哪个都一样，但是说实话一篇篇文章的收藏实在有点累，所以我会在这篇文章后面再一篇这一季文章的目录文，喜欢收藏的朋友直接收藏这一篇就好。传送门：&lt;a href="https://zhuanlan.zhihu.com/p/22464594"&gt;https://zhuanlan.zhihu.com/p/22464594&lt;/a&gt;&lt;/p&gt;&lt;p&gt;写完了这一季，我能感受到自己的成长，对于技术细节的理解，对于技术流程的表述，这些能力还是有了很大的提升。作为机器学习界的一名小学生，我正在茁壮成长。在这一路上要感谢的人很多：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;感谢帮我审阅文章的&lt;a href="https://www.zhihu.com/people/03675ab7bf1c28d3d71d2154abb3ddd1" data-hash="03675ab7bf1c28d3d71d2154abb3ddd1" class="member_mention" data-editable="true" data-title="@我爱机器学习" data-hovercard="p$b$03675ab7bf1c28d3d71d2154abb3ddd1"&gt;@我爱机器学习&lt;/a&gt;和&lt;a href="https://www.zhihu.com/people/08cd1d56513335b13d6a93725db969ad" data-hash="08cd1d56513335b13d6a93725db969ad" class="member_mention" data-title="@夏龙" data-editable="true" data-hovercard="p$b$08cd1d56513335b13d6a93725db969ad"&gt;@夏龙&lt;/a&gt;，他们在第一时间帮我指出文章中不足之处；&lt;/li&gt;&lt;li&gt;感谢很多在评论中指出文章问题的朋友，是你们让这些文章变得更加正确；&lt;/li&gt;&lt;li&gt;同时也感谢评论中提问题的朋友，你们中的一些问题我一开始也是并不清楚的，回答这些问题也是一个提高的过程。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;之前我写过一篇讨论学习的文章，聊过对学习的一些看法，在我看来，在学习机器学习的过程中，要我感到成长最快的就是读源码，做实验。很多时候别人的论文，别人写的技术博客很精彩，但是如果没有动手实践，就总感觉理解得不够透彻明白。而真正看到源代码的实现细节后，我才能明白论文博客中没有着重提到的一些奥妙。&lt;/p&gt;&lt;p&gt;最后，我也想向各位看官征集一下，大家接下来对哪些技术内容感兴趣呢？我会挑选一些大家感兴趣且我能写的内容（原因在这里：&lt;a href="https://www.zhihu.com/question/26865557/answer/126145134" class=""&gt;https://www.zhihu.com/question/26865557/answer/126145134&lt;/a&gt;），尽可能地为大家呈现出来。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23750864&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Thu, 17 Nov 2016 23:47:08 GMT</pubDate></item></channel></rss>