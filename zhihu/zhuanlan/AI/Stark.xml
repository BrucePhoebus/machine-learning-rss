<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>机器鼓励师手册 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/Stark</link><description>我们陪伴着机器成长，我们见证着机器成长</description><lastBuildDate>Wed, 01 Mar 2017 09:17:24 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>氰化物！不怕，我教你怎么检测~~</title><link>https://zhuanlan.zhihu.com/p/23676412</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-d9e77fb16ae27912a141ab658aa236d4_r.jpg"&gt;&lt;/p&gt;氰化物对人体是非常有害的。氰化物进入人体后析出氰离子，与细胞线粒体内氧化型细胞色素氧化酶的三价铁结合，阻止氧化酶中的三价铁还原，妨碍细胞正常呼吸，组织细胞不能利用氧，造成组织缺氧，导致机体陷入内窒息状态。&lt;p&gt;工业排放的污水往往会含有氰化物，如果污染了饮水源，那就......&lt;/p&gt;&lt;p&gt;我最近看了一篇检测液体中氰化物的论文，分享一下。&lt;/p&gt;&lt;h2&gt;检测原理&lt;/h2&gt;&lt;p&gt;文章提出可以用一种银芯金壳的纳米粒子去跟氰化物反应，然后通过反应后的颜色来判断氰化物的浓度。原理上是比较简单的，先看下图&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-32003df2fbac2a7cc3a6e2f5b27895f4.png" data-rawwidth="973" data-rawheight="286"&gt;&lt;p&gt;左边的大球就表示一个银芯金壳纳米粒子。氰化物可以与金、银反应，由于金涂在粒子表层，所以金会先反应，然后再和银反应，最后能消耗多少金银，就看氰化物的浓度了。而粒子的厚度被消耗了多少，在颜色上会反映出来，所以我们就可以通过颜色来判断氰化物的浓度啦。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-618436c5519cc312efb65b1d9267626b.png" data-rawwidth="884" data-rawheight="195"&gt;&lt;p&gt;上图就是不同浓度的氰化物液体在充分反应后的颜色，从左到右浓度依次变大，颜色就从紫、到橙、到黄、再逐渐到无色。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-72c9f852e50f70aa01e32d5638afcd88.png" data-rawwidth="545" data-rawheight="42"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-cfeba3791316b98040968c741b55e981.png" data-rawwidth="534" data-rawheight="29"&gt;&lt;p&gt;以上是金和银与氰化物的反应式子。&lt;/p&gt;&lt;h2&gt;合成银芯金壳纳米粒子&lt;/h2&gt;&lt;p&gt;硝酸银和硼氢化钠反应得到银纳米粒子。&lt;/p&gt;&lt;p&gt;四氯金酸和盐酸羟胺反应得到的金涂在银纳米粒子上，合成银芯金壳纳米粒子。&lt;/p&gt;&lt;h2&gt;聚山梨酯40功能化&lt;/h2&gt;&lt;p&gt;为了使检测液可以在污水（高盐环境）中对氰化物进行检测，需要对粒子进行聚山梨酯40的功能化。&lt;/p&gt;&lt;p&gt;聚山梨酯40（100uL）和胶体银芯金壳纳米粒子（10mL）在室温下混合12小时即成。&lt;/p&gt;&lt;p&gt;题外话：&lt;/p&gt;&lt;p&gt;万一哪天我们一帮人掉落到了一个星球，我们要重新打造工业文明，这个时候我们工程师就责无旁贷了。所以还是多学点东西好^_^&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23676412&amp;pixel&amp;useReferer"/&gt;</description><author>Stark Einstein</author><pubDate>Tue, 15 Nov 2016 12:53:16 GMT</pubDate></item><item><title>RNN Encoder–Decoder的attention机制简介</title><link>https://zhuanlan.zhihu.com/p/22081325</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/fe65dc8624df67eadb75e86f0214642b_r.png"&gt;&lt;/p&gt;先贴原paper：&lt;p&gt;&lt;a href="http://arxiv.org/pdf/1409.0473v7.pdf" data-editable="true" data-title="NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE" class=""&gt;NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE&lt;/a&gt;&lt;/p&gt;&lt;p&gt;对读者要求：&lt;/p&gt;&lt;p&gt;对RNN和Encoder–Decoder有基本的了解。&lt;/p&gt;&lt;p&gt;背景介绍：&lt;/p&gt;&lt;p&gt;神经网络在机器翻译里面比较有名的模型之一就是Encoder–Decoder了，然而在attention机制出来之前，神经网络方法还是没有真正达到传统方法的水平的，用神经网络搞翻译这帮人只敢说“我们还在研究之中~~”。有了attention机制之后，这帮人终于可以扬眉吐气了，神经网络方法做翻译总算敢说超越传统方法了。（这些故事是一位NLP界的同学跟我吹牛时说的，我反正是信了）&lt;/p&gt;&lt;h2&gt;一、原来的Encoder–Decoder&lt;/h2&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/22c17d3f5c59d3b28cbf4018be244b6c.png" data-rawwidth="290" data-rawheight="275"&gt;在这个模型中，encoder只将最后一个输出递给了decoder，这样一来，decoder就相当于对输入只知道梗概意思，而无法得到更多输入的细节，比如输入的位置信息。所以想想就知道了，如果输入的句子比较短、意思比较简单，翻译起来还行，长了复杂了就做不好了嘛。&lt;/p&gt;&lt;h2&gt;二、对齐问题&lt;/h2&gt;&lt;p&gt;前面说了，只给我递来最后一个输出，不好；但如果把每个step的输出都传给我，又有一个问题了，&lt;b&gt;怎么对齐&lt;/b&gt;？&lt;/p&gt;&lt;p&gt;什么是对齐？比如说英文翻译成中文，假设英文有10个词，对应的中文翻译只有6个词，那么就有了哪些英文词对哪些中文词的问题了嘛。&lt;/p&gt;&lt;p&gt;传统的翻译专门有一块是搞对齐的，是一个比较独立的task（传统的NLP基本上每一块都是独立的task啦）。&lt;/p&gt;&lt;h2&gt;三、attention机制&lt;/h2&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/83fe9a9132decae0cb49757f267e7836.png" data-rawwidth="216" data-rawheight="302"&gt;我们从输出端，即decoder部分，倒过来一步一步看公式。&lt;/p&gt;&lt;equation&gt;S_{t} =f(S_{t-1} ,y_{t-1} ,c_{t} )&lt;/equation&gt;                      (1)&lt;p&gt;&lt;equation&gt;S_{t}&lt;/equation&gt;是指decoder在t时刻的状态输出，&lt;equation&gt;S_{t-1}&lt;/equation&gt;是指decoder在t-1时刻的状态输出，&lt;equation&gt;y_{t-1} &lt;/equation&gt;是t-1时刻的label（注意是label，不是我们输出的y），&lt;equation&gt;c_{t} &lt;/equation&gt;看下一个公式，&lt;equation&gt;f&lt;/equation&gt;是一个RNN。&lt;/p&gt;&lt;equation&gt;c_{t} =\sum_{j=1}^{T_{x}}{a _{tj} h _{j}} &lt;/equation&gt;                      (2)&lt;p&gt;&lt;equation&gt; h _{j}&lt;/equation&gt;是指第j个输入在encoder里的输出，&lt;equation&gt;a _{tj}&lt;/equation&gt;是一个权重&lt;/p&gt;&lt;equation&gt;a_{tj} =\frac{exp(e_{tj} )}{\sum_{k=1}^{T_{x} }{exp(e_{tk})} } &lt;/equation&gt;                      (3)&lt;p&gt;这个公式跟softmax是何其相似，道理是一样的，是为了得到条件概率&lt;equation&gt;P(a|e)&lt;/equation&gt;，这个&lt;equation&gt;a &lt;/equation&gt;的意义是当前这一步decoder对齐第j个输入的程度。&lt;/p&gt;&lt;p&gt;最后一个公式，&lt;equation&gt;e_{tj} =g(S_{t-1} ,h_{j})&lt;/equation&gt;                      (4)&lt;/p&gt;&lt;p&gt;这个&lt;equation&gt;g&lt;/equation&gt;可以用一个小型的神经网络来逼近。&lt;/p&gt;&lt;p&gt;好了，把四个公式串起来看，这个attention机制可以总结为一句话，“当前一步输出&lt;equation&gt;S_{t} &lt;/equation&gt;应该对齐哪一步输入，主要取决于前一步输出&lt;equation&gt;S_{t-1} &lt;/equation&gt;和这一步输入的encoder结果&lt;equation&gt;h_{j}&lt;/equation&gt;”。&lt;/p&gt;&lt;p&gt;我当时看了这个方法的感受是，计算力发达的这个年代，真是什么复杂的东西都有人敢试了啊。这要是放在以前，得跑多久才能收敛啊......&lt;/p&gt;&lt;p&gt;神经网络搞NLP虽然还有诸多受限的地方，但这种end-to-end 的one task方式，太吸引人，有前途。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22081325&amp;pixel&amp;useReferer"/&gt;</description><author>Stark Einstein</author><pubDate>Fri, 19 Aug 2016 13:11:36 GMT</pubDate></item><item><title>非负矩阵分解(NMF)简介</title><link>https://zhuanlan.zhihu.com/p/22043930</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/35b21515a5af8d9660b7aa662f9ef166_r.jpg"&gt;&lt;/p&gt;&lt;h2&gt;什么是非负矩阵分解？&lt;/h2&gt;&lt;p&gt;非负矩阵分解，顾名思义就是，将非负的大矩阵分解成两个非负的小矩阵。&lt;/p&gt;&lt;p&gt;&lt;equation&gt;V\approx WH&lt;/equation&gt;                              (1)&lt;/p&gt;&lt;p&gt;回顾矩阵分解本身，在&lt;equation&gt;R^{n} &lt;/equation&gt;空间分布的一堆数据有它们分布的某些规律，那么找一组更能直观反映这种规律的基，再把原来的数据投影到这组基上表示，这样就能便于后续的应用，比如分类等。&lt;/p&gt;&lt;p&gt;公式(1)中的V是一个n*m维的矩阵，其中每一列就是&lt;equation&gt;R^{n} &lt;/equation&gt;空间中的一个向量，共m个向量;W是一个n*k维的矩阵，即k个基；H是k*m的矩阵，每一列为V投影到W上得到的向量。&lt;/p&gt;&lt;p&gt;那么非负和其它的有什么不同呢？下面我就盗一张图来直观地展示几种矩阵分解方法的效果差异。（出自&lt;a href="http://www.cs.wustl.edu/~zhang/teaching/cs517/Spring12/CourseProjects/nmf-Lee1999.pdf" data-editable="true" data-title="Lee and Seung (1999)" class=""&gt;Lee and Seung (1999)&lt;/a&gt;）&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/56079895270388df370b8643f3537dee.png" data-rawwidth="579" data-rawheight="867"&gt;其中original原图即为V中的一列；等式左边第一块是W，其中的每一小块是W的一列；等式左边第二块是H的一列；等式右边的图像是左边相乘还原得到的与original相对应的一个向量（只不过显示成二维图像）。&lt;/p&gt;&lt;p&gt;图中就对比了三种方法的区别。&lt;/p&gt;&lt;p&gt;VQ的约束是要求H的每一列只有一个元素为1，其它为0，因此相当于将m个数据归纳成了k个代表，原数据映射过去就是取k个基当中与原向量距离最小的来重新表示。所以VQ的基都是一张张完整正常的脸，它们都是最具代表性的脸。&lt;/p&gt;&lt;p&gt;PCA大家用得比较多，就是求一组标准正交基，第一个基的方向取原数据方差最大的方向，然后第二个基在与第一个基正交的所有方向里再取方差最大的，这样在跟前面的基都正交的方向上一直取到k个基。所以PCA的基没有直观的物理意义，而且W和H里面的元素都是可正可负的，这就意味着还原时是用W的基加加减减得到的。&lt;/p&gt;&lt;p&gt;NMF因为约束了非负，所以只准把基相加，不能相减，这就意味着基与基是通过拼接组合来还原原象的。所以我们可以看到NMF的每个基实际上在表示着脸部的某个部件，这种性质在许多场景上就有了它特有的用处了。&lt;/p&gt;&lt;p&gt;我们换个角度来想像一下这三种情景。&lt;/p&gt;&lt;p&gt;VQ其实就像kmeans，它的基都落在原数据的最具代表性的位置上。&lt;/p&gt;&lt;p&gt;PCA的基则是指向四向八方的，相互正交着。&lt;/p&gt;&lt;p&gt;NMF的原数据首先就是只分布在非负子空间里面的，然后它的基则在这个非负子空间靠近边缘的区域，像一组长短不一、间隔不一的伞骨。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/33a2a5933596e366bf0ef4a4540db498.png" data-rawwidth="502" data-rawheight="491"&gt;&lt;h2&gt;怎么分解？&lt;/h2&gt;&lt;p&gt;这是个有界优化问题&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/7f9cc6c6fd07501f7dd18362b69160ca.png" data-rawwidth="528" data-rawheight="118"&gt;&lt;p&gt;所以最直观的最简单的方法就是&lt;/p&gt;&lt;p&gt;&lt;b&gt;方法一&lt;/b&gt;：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/5052be4c78557e24a93756c0f45d2656.png" data-rawwidth="384" data-rawheight="99"&gt;嗯，真是简单到几行代码就搞定了，而且跟原问题保持一致。&lt;/p&gt;&lt;p&gt;我们看标准的有界优化问题&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/1830b65f687838577657bd5e6c78fdde.png" data-rawwidth="393" data-rawheight="95"&gt;标准的解法&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7e0d920f064aa60fe14fa4e47533745f.png" data-rawwidth="344" data-rawheight="206"&gt;所以我们方法一是按套路出牌的。&lt;/p&gt;&lt;p&gt;但是，作为有追求有理想的同学们，&lt;a href="http://www.cs.wustl.edu/~zhang/teaching/cs517/Spring12/CourseProjects/nmf-Lee1999.pdf" data-editable="true" data-title="Lee and Seung (1999)" class=""&gt;Lee and Seung (1999)&lt;/a&gt;认为方法一不够好，要来点更快的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;方法二&lt;/b&gt;：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/bfdada349afd5e7d2c9b80fca9a2f655.png" data-rawwidth="461" data-rawheight="134"&gt;这个又叫&lt;b&gt;乘法更新法&lt;/b&gt;，因为人家用了乘法......&lt;/p&gt;&lt;p&gt;推导起来有点麻烦，但写起代码来也是相当简单，几行代码足矣，而且收敛比方法一快一些。&lt;/p&gt;&lt;p&gt;但是，作为有追求有理想的同学们，林智仁（没错，就是搞libsvm那个林智仁）&lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=ACAE3131E5FA09D0798DF7D967F42000?doi=10.1.1.538.997&amp;amp;rep=rep1&amp;amp;type=pdf" data-editable="true" data-title="C Lin(2007)" class=""&gt;C Lin(2007)&lt;/a&gt;摇摇头，“看我的”。&lt;/p&gt;&lt;p&gt;&lt;b&gt;方法三&lt;/b&gt;：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/000926c920d1a3522f20504bfdca9d6d.png" data-rawwidth="1020" data-rawheight="313"&gt;传说中这个方法是快了一些，然而我觉得有方法一和二就妥妥了，像现在这么强计算力的时代，计算省下来那点时间，还抵不回写这复杂代码多花的时间。&lt;/p&gt;&lt;p&gt;林智仁这篇文章&lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=ACAE3131E5FA09D0798DF7D967F42000?doi=10.1.1.538.997&amp;amp;rep=rep1&amp;amp;type=pdf" data-editable="true" data-title="C Lin(2007)" class=""&gt;C Lin(2007)&lt;/a&gt;是后期出来的，review了之前几种主流的方法，再提出自己的新方法，所以这篇的内容比较全（懒人只看这一篇就够了）。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22043930&amp;pixel&amp;useReferer"/&gt;</description><author>Stark Einstein</author><pubDate>Wed, 17 Aug 2016 00:43:43 GMT</pubDate></item><item><title>《计算广告》读书笔记</title><link>https://zhuanlan.zhihu.com/p/20733024</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/462b080d7f7b949cd917fe44789b6894_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;在线广告是我们生活常见的商业中极为重要的一部分。对我们互联网行业算法从业人员来说，广告也是最为常见的业务之一。挑了这本国内计算广告领域名气最大的书《计算广告》刘鹏----其实是我实在没搜到其它像样的......&lt;/p&gt;&lt;p&gt;&lt;u&gt;本文非常多私货，完全不能代表原书，有兴趣可以亲阅原书哈。&lt;/u&gt;&lt;/p&gt;这本书的结构是非常好的，分三大部分，第一部分讲行业背景知识，第二部分讲产品逻辑，第三部分讲关键技术。逻辑非常清晰，宏观微观兼顾得到位，干货十足， 不愧是行业老兵写的，而且丝毫没有夸夸其谈的成分。&lt;h2&gt;&lt;b&gt;第一部分          广告市场与背景&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;什么是广告？&lt;/b&gt;&lt;/p&gt;&lt;blockquote&gt;                  广告的根本目的是广告主通过媒体达到低成本的用户接触。----《当代广告学》&lt;/blockquote&gt;&lt;p&gt;什么是广告呢？作者对广告的理解是，一切付费的信息、产品或服务的传播渠道，都是广告。互联网广告有，横幅广告、富媒体广告、文字链广告、视频广告、社交广告、移动广告、邮件定向营销广告，更宽泛地，团购、游戏联运、固定位导航、返利购买等等，都属于这个范畴。这些都是我们生活中天天见的东西，大量互联网业务营利的重要组成部分就是这些了。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/a842b63d7a380a0a4d281aa594ad4198.png" data-rawwidth="360" data-rawheight="360"&gt;我们来对比一下互联网广告和传统广告。传统广告主要是纸媒、电视、街上各种广告牌（和电线杆上的牛皮藓）等等。互联网广告相比传统广告的优势在于，&lt;b&gt;低成本的定向、方便的竞价、方便的投放、可衡量的效果&lt;/b&gt;。尤其是现在的数据越来越丰富，定向的精准度是越来越高的。&lt;p&gt;&lt;b&gt;大数据与在线广告的关系&lt;/b&gt;&lt;/p&gt;&lt;p&gt;作者从数据量与应用效果的关系将数据问题分为三类：&lt;/p&gt;&lt;p&gt;          A类，数据量小的时候（采样率低）效果会很差，比如个性化推荐和计算广告。如果只采样到一小部分人的数据就无法对所有人都做到精准。&lt;/p&gt;&lt;p&gt;          B类，数据量大到一定规模以后，再增加数据的效果提升不明显，比如文本主题模型。&lt;/p&gt;&lt;p&gt;          C类，数据量比较小的情况下也能达到满足应用需求的效果。比如你用excel就能搞定的问题。&lt;/p&gt;&lt;p&gt;计算广告就属于A类问题，在大数据这个词都烂大街了的今天，计算广告的相关技术是相当的成熟了。某程度上来说，计算广告是推动大数据发展的极为重要的力量。大数据的发展也让这类应用的效果越来越有商业价值。&lt;/p&gt;&lt;p&gt;&lt;b&gt;有效性原理&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" data-rawheight="89" data-rawwidth="570" src="2f855cc9209a8c1fa7675d41eefed1b1.png"&gt;有这张图，貌似也不用我说什么了，相信你看这张图的时候也会点点头的，广告对我们产生作用的过程就是这样子了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;结算方式&lt;/b&gt;--CPM、CPC、CPS/CPA/ROI、CPT &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" data-rawheight="245" data-rawwidth="712" src="0f990dfacb79fc333fbc5dff1f37acb0.png"&gt;&lt;b&gt;CPT&lt;/b&gt;是按时间收费，财大气粗的大品牌会选择这种强曝光的方式来强刷存在感，几乎不定向受众，而是定时间。&lt;/p&gt;&lt;p&gt;&lt;b&gt;CPM&lt;/b&gt;按千次展示收费，通常也是品牌广告比较适用这种收费方式，比如有时候打开爱奇艺首页可能会看到奔驰或者CHANAL的广告。这种方式可以做一定程度的受众定向，但是受众划分不会太细，否则就达不到高曝光的目的了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;CPC&lt;/b&gt;按点击次数收费，与前面的方式相比，CPC使广告平台方与需求方分摊了风险。因为要点了才能赚钱，那如果广告平台对广告投放的点击预测不准的话，广告平台自己就会损失；而需求方只会为实实在在的点击付费，不必再独自去承担投放选择的效果差异带来的损失风险。主流的收费方式是这种，因为风险分摊合理、接入方便灵活、投放精准。&lt;/p&gt;&lt;p&gt;&lt;b&gt;CPS/CPA/ROI&lt;/b&gt;，叫法比较多，总的来说就是广告平台向广告商拿分成，比CPC更加绑定广告商的收益。这种方式比较少见，比较多是有战略合作意义的情况，要么多数就是垂直广告。&lt;/p&gt;&lt;p&gt;&lt;b&gt;效果监测&lt;/b&gt;&lt;/p&gt;&lt;p&gt;作为广告商，我怎么知道广告平台有没有坑我呢，展示或者点击八百次收我一千次的钱怎么办。所以第三方监测公司应运而生，来给曝光量、点击量等指标作技术核实。&lt;/p&gt;&lt;p&gt;&lt;b&gt;第二部分          在线广告产品逻辑&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这部分主要讲：合约广告、搜索与竞价广告、程序化交易广告、移动互联与原生广告。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.　合约广告&lt;/b&gt;&lt;/p&gt;&lt;p&gt;首先是CPT广告，这种基本上就是线下广告的业务逻辑照搬到线上，由广告代理公司和媒体签协议，定时间段定费用。对技术依赖小，只需要简单的广告排期系统（比如某度广告管家）。&lt;/p&gt;&lt;p&gt;CPM虽然也是合约广告，但有进步之处，就是售卖对象从“广告位”进化到了“广告位+人群”。技术上增加的主要复杂度在于，多个合约对投放系统提出的量的约束。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" data-rawheight="261" data-rawwidth="767" src="49af625809b54a8971d92dfabe628570.png"&gt;&lt;b&gt;定向投放&lt;/b&gt;是一种里程碑式的进步，因为像乱枪打鸟式的投放只有土豪才玩得起，有了定向之后，小玩家的投入产出比才能达到可进入的程度。下面列举一些典型的定向方式。&lt;/p&gt;&lt;p&gt;　　（１）地域定向。几乎是广告系统必支持的方式。比如我开店只卖江浙沪，那当然只在江浙沪地区投广告啊。&lt;/p&gt;&lt;p&gt;　　（２）人口属性定向。包括年龄、性别、教育程度、收入水平等。这种特征是比较客观的，相对兴趣标签来说，但不得不说这种数据也是挺脏的，错假数据多，而且缺失数据特别多，要花大量工夫在洗数据和补数据上。虽然这种方式的效果不是非常突出，但是传统广告商熟悉啊，还是要侍候的。&lt;/p&gt;&lt;p&gt;　　（３）频道定向。适用于离转化需求比较近的垂直类媒体，比如汽车、母婴、购物导航等。&lt;/p&gt;&lt;p&gt;　　（４）上下文定向。根据网页的具体内容来匹配相关广告。这种方式的覆盖率高，因为相比用户信息之类，网页内容是最容易得到的。&lt;/p&gt;&lt;p&gt;　　（５）行为定向。根据用户的历史访问行为了解用户兴趣然后进行相关广告投放。跟上下文定向不同的是，上下文定向比较能反映用户的实时意图；　而行为定向就是把用户访问过的内容抽象出来作为这个用户的特征，可以用来表达描述用户的长期兴趣。&lt;/p&gt;&lt;p&gt;　　（6）精确位置定向。利用基站、GPS、WIFI等方法，得到用户比较实时的精确的地理位置，使得大量区域性非常强的小广告主（如餐饮、美容等）可以进来玩广告。&lt;/p&gt;&lt;p&gt;　　（7）重定向。就是给老客再投广告。这种方式的转化率往往是很突出的。&lt;/p&gt;&lt;p&gt;　　（８）新客推荐定向。利用算法预测用户是否潜在客户，老客是一种比较有效的信息可用于算法的学习。&lt;/p&gt;&lt;p&gt;　　（9）团购。作者认为这是一种变相的广告形式。一般这种广告主是有区域性的，所以我们看到的团购基本上都是以城市划分甚至以城区划分的。团购主要是用价格诱惑，降低用户在决策阶段的门槛，对价格敏感的用户转化效果相当明显。&lt;/p&gt;&lt;p&gt;&lt;b&gt;受众定向标签体系&lt;/b&gt;&lt;/p&gt;&lt;p&gt;看了作者对标签体系的介绍，说说我的见解。&lt;/p&gt;&lt;p&gt;在大数据玩得飞起的今天，虽然可以用非常稀疏的高维用户特征、用非常酷炫的模型，但是标签体系还是有它一时半会难以取代的优点。首先是解释性强，广告商能看懂，这就比较容易接受。然后，标签其实是一种经过抽象出来的带有某些现实意义的粗粒度特征，拟合度当然会相对低于高维原始特征，但是泛化能力往往是不错的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;流量预测&lt;/b&gt;&lt;/p&gt;&lt;p&gt;对于CPM来说，广告位是确定的，流量是未知的，那就需要去预测流量。预测流量有三个用途：&lt;/p&gt;&lt;p&gt;　　（１）售前指导，供应方需要自己的供应量才能去接合适量的广告回来投放。不然，供应量严重多于接回来的广告就亏了，或者接的广告严重多于供应量就要违约了。&lt;/p&gt;&lt;p&gt;　　（２）在线流量分配。有的流量会同时适合多个合约，就要合理分配这些流量，从而保证合约都能被满足。&lt;/p&gt;&lt;p&gt;　　（３）出价指导。竞价广告没有量的保证，广告主需要预计什么价格能得到多少流量来决策出价。&lt;/p&gt;&lt;p&gt;&lt;b&gt;流量塑形&lt;/b&gt;&lt;/p&gt;&lt;p&gt;有些情况下，我们可以主动影响流量，利于合约的达到。典型的就是门户网站，它们的流量严重依赖于链接的位置。比如在车展期间，汽车广告需求旺盛，把汽车频道放到流量大的位置上，呵呵，一本满足。&lt;/p&gt;&lt;p&gt;&lt;b&gt;在线分配&lt;/b&gt;&lt;/p&gt;&lt;p&gt;前面流量预测就说到了在线分配。当标签越多时，重叠流量就越少，对流量预测的精度要求就越高。所以标签越丰富，合约广告越难玩，竞价广告就更有价值。作者没有详细谈这部分，我也作不下去了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.　搜索与竞价广告&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.1 搜索广告&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在互联网广告的整个产品谱系中，搜索广告有着特殊重要的地位，它的特点在于：一、搜索能够非常直接地反映用户的意图；长相和自然的搜索内容很接近，用户更容易接受；发展起来与之相适应的高效竞价交易模式。这些特点导致其变现能力最强。&lt;/p&gt;&lt;p&gt;按预测点击率和出价的乘积排序，决定广告透出的位置。所以想让自己的广告出现在好的位置，单纯的高出价不是最有效的，还要高质量的广告内容能够吸引到用户点击。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.2 位置拍卖与机制设计&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;定价方法&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;广义第二高价&lt;/i&gt;&lt;/b&gt;。简单理解就是出价最高者只需要支付比第二高价多一点点的价格。比如，Ａ出价5块，Ｂ出价3块，结果就是Ａ以3.1块拿下。这样一来就解决了大家一门心思去压价的问题，反正不管出多高的价，最终只需要给老二那个价格，竞价竞赢最重要。同理，老二按老三的出价给钱，老三按老四的出价给钱。结果广告平台能赚更多。在多位置拍卖时，这种方法不是理论上最优的，但还是比较容易操作，所以采用这种方法还是最常见的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;VCG定价。&lt;/i&gt;&lt;/b&gt;这个定价是Vickrey、Clarke和Groves研究出来的，所以这样取名......对于赢得了某个位置的广告主，其所付出的成本应该等于他占据这个位置给其他市场参与者带来的价值损害。在单广告位拍卖的情况下，VCG和第二高价策略是等价的。这种定价向广告主收取的费用其实是最少的。虽然看起来很好，但它没有成为主流。首先是太复杂，跟广告主扯不清楚；然后是媒体计算的“给其他市场参与者带来的价值伤害”的正确性很难验证。Facebook是用这个方法的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;市场保留价&lt;/b&gt;&lt;/p&gt;&lt;p&gt;为了控制广告的质量和保持一定的出售单价，往往要设一个拍卖最低价，这个价格就叫市场保留价（MRP）。MRP定得过低或者过高都不利于收益最大化。广告主竞争得比较猛的时候，MRP可以定高一点。可以对整个市场采用同样的MRP，也可以对不同的标的物（比如关键词）设不同的价格。&lt;/p&gt;&lt;p&gt;&lt;b&gt;价格挤压&lt;/b&gt;&lt;/p&gt;&lt;p&gt;基本的收益公式：&lt;equation&gt;r=\mu \cdot bid_{CPC} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;加入价格挤压因子的收益公式：&lt;equation&gt;r=\mu ^{\kappa } \cdot bid_{CPC} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\mu &lt;/equation&gt;是点击率，&lt;equation&gt;bid_{CPC} &lt;/equation&gt;是出价，基本的收益就是点击率和出价的乘积。&lt;/p&gt;&lt;p&gt;价格挤压因子&lt;equation&gt;\kappa &lt;/equation&gt;越大，越以点击率为主导去决定广告排序；反过来，因子越小，越以出价为主导去决定广告排序。&lt;/p&gt;&lt;p&gt;当广告主竞争比较猛的时候，我们平台方是卖方市场，就可以调高挤压因子，从而提高用户体验（用户越愿意点击相当于体验质量越好嘛）；当竞争不够的时候，为了赚钱，只好牺牲一下用户体验了，调低挤压因子，鼓励价格竞争。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.3 广告网络&lt;/b&gt;竞价广告在搜索广告上用爽了，展示广告的流量也就开始进入竞价广告平台了，催生了广告网络。流量大的网站会以合约的方式出售展示广告，剩余的流量再流入广告网络；流量小的网站干脆就全部流量流入广告网络，懒得自己去拉广告合约。&lt;/p&gt;&lt;p&gt;广告网络的投放对媒体方而言是一个黑盒，只需要在广告位的剩余流量上调用广告网络的投放代码或SDK，不用关心每次展示的投放结果。&lt;/p&gt;&lt;p&gt;广告网络的标的主要是人群，广告位被淡化了。&lt;/p&gt;&lt;p&gt;广告网络的广告主一般是要先充值的，合约广告则是结束后付费的，所以用广告网络对运营方现金流也比较有保障。&lt;/p&gt;&lt;p&gt;产品案例：Google Display Network、阿里妈妈旗下的淘宝联盟。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.　程序化交易广告&lt;/b&gt;&lt;/p&gt;&lt;p&gt;前面所讲的广告交易平台，叫ADX。其实相应的采买方或者说需求方的平台没有展开来讲，叫DSP。&lt;/p&gt;&lt;p&gt;ADX卖广告资源，DSP买广告资源。如果没有DSP，广告主要自己去面对ADX，去做复杂的购买决策，而且还难以达到像DSP能达到的那样好的效益。所以将DSP理解为一个智能的广告资源购买平台就可以了。&lt;/p&gt;&lt;p&gt;不管是ADX还是DSP，都要精准的定向和点击率预测，而决定性的因素是什么呢？是数据的来源与质量。可以重点关注的是以下几类数据。&lt;/p&gt;&lt;p&gt;（1）用户标识。顾名思义，就是一种用于识别用户的数据。&lt;/p&gt;&lt;p&gt;        对浏览器行为，最常用的就是cookie。对于同时有多个浏览器、cookie过期或用户主动清cookie的情况，一致性就是一个问题。好在如果运营广告业务的域名同时提供诸如电子邮件、SNS之类的永久身份标识，就可以对cookie做一致性的改善（cookie映射）。&lt;/p&gt;&lt;p&gt;        移动端，ios与android的用户ID有所不同。ios有苹果专门设计的用户标识符（Identifier for Advertising，IDFA），性质与cookie类似。android没有专门的广告用户ID，一般采用android ID或IMEI（international mobile equipment identity）。&lt;/p&gt;&lt;p&gt;        高质量的用户标识数据非常有价值，所以在市场上是可以卖的。&lt;/p&gt;&lt;p&gt;（2）用户行为。按照对广告的有效性分类，可以分为决策行为、主动行为、半主动行为和被动行为。&lt;/p&gt;&lt;p&gt;        决策行为主要是转化和预转化。比如电商网站上，下单就是转化，下单前的搜索、浏览、比价、加入购物车等准备工作就是预转化。这类数据虽然量不大，却是价值最高的。&lt;/p&gt;&lt;p&gt;        主动行为主要是广告点击、搜索。&lt;/p&gt;&lt;p&gt;        半主动行为主要是分享和网页浏览。这类数据，量最大，直接效果不明显，但是有很大的挖掘价值。&lt;/p&gt;&lt;p&gt;        被动行为主要是广告浏览。比如说给你看了好多次汽车类广告，你一次都没点进去，这可以说明你对汽车不感兴趣嘛。所以这类数据通常是作为负样本来让模型学习。&lt;/p&gt;&lt;p&gt;（3）人口属性。人口属性是很常用的定向标签，数据来源很重要。一般来说，实名身份绑定的服务可以获得这种信息。而得不到高质量数据的部分属性就要做缺失补全了。比如可以用语音服务记录的声音信号来判别男女。&lt;/p&gt;&lt;p&gt;（4）地理位置。不同精度，不同用法。IP可以精确到城市；GPS或蜂窝可以精确到几百米，餐饮之类的线下业务可以用得上。&lt;/p&gt;&lt;p&gt;（5）社交关系。物以类聚，人以群分。当一个人的兴趣数据不足的时候，可以用他朋友的数据平滑过来，作为长期稳定的基础数据是可以，但用于短时的购买兴趣就不太适用。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.　移动互联与原生广告&lt;/b&gt;&lt;/p&gt;&lt;b&gt;原生广告&lt;/b&gt;并没有非常严格的定义，软文、搜索广告、社交网络信息流等都有一些原生广告的意味。将商业化内容和非商业化内容混在一起，从形式上难以区分的，都可以叫原生广告。所以有句话叫，内容即广告。&lt;p&gt;信息流广告。典型的就是微信朋友圈，朋友发的消息中间夹着几条推广广告，不仔细看你没发现它是广告呢。信息流里面插广告的好处是，信息流的各条内容本身就没有什么相关性，广告插进去不会显得突兀，对用户体验的伤害不大。&lt;/p&gt;&lt;p&gt;搜索广告。某度最爱干的，搜个东西，前面好多条是广告，不仔细看还不容易认出来那是广告还是搜索内容。反正我是经常不小心点进去才发现是广告，然后被恶心一把。&lt;/p&gt;&lt;p&gt;软文广告。经常看*嗅、*氪之类的媒体，你一定很清楚，软文太多了，有的还不容易辨别。&lt;/p&gt;&lt;p&gt;&lt;b&gt;移动广告&lt;/b&gt;随着移动设备的爆发式增长而迅猛成长，虽然流量红利已经够让人狂欢了，但是作为富于求知精神的我们还是要去关注移动广告本身的特点的。&lt;/p&gt;&lt;p&gt;（1）情境广告的可能性。移动设备一天到晚呆在我们身边，对我们的地理位置、生活状态、需求意图等都可以有更加深入的获知。那么就可以不仅仅根据兴趣推送商品了，完全有可能做到从情境和意图出发。比如下午下班了，就应该赶紧推送各种餐馆各种美食嘛；周五到了，就可以推送一泊二日的周末度假游之类的。&lt;/p&gt;&lt;p&gt;（2）大量潜在的本地化广告主。因为精准的地理信息和情境获知的能力，线下附近的商家就有能力通过这种富有针对性的广告推广来导流到线下，广大的本地商家终于可以翻身做广告主了。&lt;/p&gt;&lt;p&gt;下面列举一下移动广告的形式。嵌入到APP的横幅和插屏；开屏、锁屏；推荐墙、积分墙；还有最近某国产手机把广告做到了天气栏上了。嵌入到APP的广告，通常有点击率虚高、转化较差的问题。开屏是打开APP时出现的全屏广告，品牌价值高，通常以合约方式卖。&lt;/p&gt;&lt;p&gt;移动广告也面临着很多问题。&lt;/p&gt;&lt;p&gt;首先就是&lt;b&gt;数据的割裂&lt;/b&gt;。这个问题在车品觉的《决战大数据》里也强调过。比如你上班的时候，用电脑看了几个网页，然后老板叫你去开会，你在玩手机，开完会再回来用电脑看你的淘宝。这样一来，数据就失去了连贯性，对用户行为分析带来了巨大的困难。&lt;/p&gt;&lt;p&gt;第二个问题是PC时代的广告主&lt;b&gt;移动化还不够&lt;/b&gt;，无法充分消化广告带来的流量。比如PC端的落地页如果照搬到移动端的话，用户体验会非常差，这样就会造成较高的跳失率。&lt;/p&gt;&lt;p&gt;第三是移动广告产品形态需要一次革命。移动端的屏幕这么小，广告和内容分开放在不同的单元的话，用户体验太差。要解决这种广告与内容的对立问题，&lt;b&gt;要原生化&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;终于......讲到技术了。感觉好累，就算我有力气继续写，估计你也没力气看了。&lt;/p&gt;&lt;p&gt;计算广告的关键技术留到下一篇吧。&lt;/p&gt;&lt;img rel="noreferrer" data-rawheight="411" data-rawwidth="324" src="5e7074408822929a55a365f31249af0c.jpg"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/20733024&amp;pixel&amp;useReferer"/&gt;</description><author>Stark Einstein</author><pubDate>Tue, 26 Apr 2016 19:25:03 GMT</pubDate></item></channel></rss>