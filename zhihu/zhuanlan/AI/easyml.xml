<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>炼丹实验室 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/easyml</link><description>会定期更新一些深度学习相关的实践心得。</description><lastBuildDate>Wed, 11 Jan 2017 13:15:50 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>深度学习网络调参技巧</title><link>https://zhuanlan.zhihu.com/p/24720954</link><description>&lt;p&gt;转载请注明：&lt;a href="https://zhuanlan.zhihu.com/easyml" data-editable="true" data-title="知乎专栏" class=""&gt;炼丹实验室&lt;/a&gt;&lt;/p&gt;&lt;p&gt;之前曾经写过一篇文章，讲了一些深度学习训练的技巧，其中包含了部分调参心得：&lt;a href="https://zhuanlan.zhihu.com/p/20767428" data-editable="true" data-title="深度学习训练心得"&gt;深度学习训练心得&lt;/a&gt;。不过由于一般深度学习实验，相比普通机器学习任务，时间较长，因此调参技巧就显得尤为重要。同时个人实践中，又有一些新的调参心得，因此这里单独写一篇文章，谈一下自己对深度学习调参的理解，大家如果有其他技巧，也欢迎多多交流。&lt;/p&gt;&lt;h1&gt;好的实验环境是成功的一半&lt;/h1&gt;&lt;p&gt;由于深度学习实验超参众多，代码风格良好的实验环境，可以让你的人工或者自动调参更加省力，有以下几点可能需要注意：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;将各个参数的设置部分集中在一起。如果参数的设置分布在代码的各个地方，那么修改的过程想必会非常痛苦。&lt;/li&gt;&lt;li&gt;可以输出模型的损失函数值以及训练集和验证集上的准确率。&lt;/li&gt;&lt;li&gt;可以考虑设计一个子程序，可以根据给定的参数，启动训练并监控和周期性保存评估结果。再由一个主程序，分配参数以及并行启动一系列子程序。&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;画图&lt;/h1&gt;&lt;p&gt;画图是一个很好的习惯，一般是训练数据遍历一轮以后，就输出一下训练集和验证集准确率。同时画到一张图上。这样训练一段时间以后，如果模型一直没有收敛，那么就可以停止训练，尝试其他参数了，以节省时间。 如果训练到最后，训练集，测试集准确率都很低，那么说明模型有可能欠拟合。那么后续调节参数方向，就是增强模型的拟合能力。例如增加网络层数，增加节点数，减少dropout值，减少L2正则值等等。 如果训练集准确率较高，测试集准确率比较低，那么模型有可能过拟合，这个时候就需要向提高模型泛化能力的方向，调节参数。&lt;/p&gt;&lt;h1&gt;从粗到细分阶段调参&lt;/h1&gt;&lt;p&gt;实践中，一般先进行初步范围搜索，然后根据好结果出现的地方，再缩小范围进行更精细的搜索。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;建议先参考相关论文，以论文中给出的参数作为初始参数。至少论文中的参数，是个不差的结果。&lt;/li&gt;&lt;li&gt;如果找不到参考，那么只能自己尝试了。可以先从比较重要，对实验结果影响比较大的参数开始，同时固定其他参数，得到一个差不多的结果以后，在这个结果的基础上，再调其他参数。例如学习率一般就比正则值，dropout值重要的话，学习率设置的不合适，不仅结果可能变差，模型甚至会无法收敛。&lt;/li&gt;&lt;li&gt;如果实在找不到一组参数，可以让模型收敛。那么就需要检查，是不是其他地方出了问题，例如模型实现，数据等等。可以参考我写的&lt;a href="https://zhuanlan.zhihu.com/p/20792837" data-editable="true" data-title="深度学习网络调试技巧"&gt;深度学习网络调试技巧&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h1&gt;提高速度&lt;/h1&gt;&lt;p&gt;调参只是为了寻找合适的参数，而不是产出最终模型。一般在小数据集上合适的参数，在大数据集上效果也不会太差。因此可以尝试对数据进行精简，以提高速度，在有限的时间内可以尝试更多参数。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;对训练数据进行采样。例如原来100W条数据，先采样成1W，进行实验看看。&lt;/li&gt;&lt;li&gt;减少训练类别。例如手写数字识别任务，原来是10个类别，那么我们可以先在2个类别上训练，看看结果如何。&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;超参数范围&lt;/h1&gt;&lt;p&gt;建议优先在对数尺度上进行超参数搜索。比较典型的是学习率和正则化项，我们可以从诸如0.001 0.01 0.1 1 10，以10为阶数进行尝试。因为他们对训练的影响是相乘的效果。不过有些参数，还是建议在原始尺度上进行搜索，例如dropout值: 0.3 0.5 0.7)。&lt;/p&gt;&lt;h1&gt;经验参数&lt;/h1&gt;&lt;p&gt;这里给出一些参数的经验值，避免大家调参的时候，毫无头绪。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;learning rate: 1 0.1 0.01 0.001, 一般从1开始尝试。很少见learning rate大于10的。学习率一般要随着训练进行衰减。衰减系数一般是0.5。 衰减时机，可以是验证集准确率不再上升时，或固定训练多少个周期以后。 不过更建议使用自适应梯度的办法，例如adam,adadelta,rmsprop等，这些一般使用相关论文提供的默认值即可，可以避免再费劲调节学习率。对RNN来说，有个经验，如果RNN要处理的序列比较长，或者RNN层数比较多，那么learning rate一般小一些比较好，否则有可能出现结果不收敛，甚至Nan等问题。&lt;/li&gt;&lt;li&gt;网络层数： 先从1层开始。&lt;/li&gt;&lt;li&gt;每层结点数： 16 32 128，超过1000的情况比较少见。超过1W的从来没有见过。&lt;/li&gt;&lt;li&gt;batch size: 128上下开始。batch size值增加，的确能提高训练速度。但是有可能收敛结果变差。如果显存大小允许，可以考虑从一个比较大的值开始尝试。因为batch size太大，一般不会对结果有太大的影响，而batch size太小的话，结果有可能很差。&lt;/li&gt;&lt;li&gt;clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值，就算一个衰减系系数,让value的值等于阈值: 5,10,15&lt;/li&gt;&lt;li&gt;dropout： 0.5&lt;/li&gt;&lt;li&gt;L2正则：1.0，超过10的很少见。&lt;/li&gt;&lt;li&gt;词向量embedding大小：128，256&lt;/li&gt;&lt;li&gt;正负样本比例： 这个是非常忽视，但是在很多分类问题上，又非常重要的参数。很多人往往习惯使用训练数据中默认的正负类别比例，当训练数据非常不平衡的时候，模型很有可能会偏向数目较大的类别，从而影响最终训练结果。除了尝试训练数据默认的正负类别比例之外，建议对数目较小的样本做过采样，例如进行复制。提高他们的比例，看看效果如何，这个对多分类问题同样适用。 在使用mini-batch方法进行训练的时候，尽量让一个batch内，各类别的比例平衡，这个在图像识别等多分类任务上非常重要。&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;自动调参&lt;/h1&gt;&lt;p&gt;人工一直盯着实验，毕竟太累。自动调参当前也有不少研究。下面介绍几种比较实用的办法：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Gird Search. 这个是最常见的。具体说，就是每种参数确定好几个要尝试的值，然后像一个网格一样，把所有参数值的组合遍历一下。优点是实现简单暴力，如果能全部遍历的话，结果比较可靠。缺点是太费时间了，特别像神经网络，一般尝试不了太多的参数组合。&lt;/li&gt;&lt;li&gt;Random Search。Bengio在&lt;a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" data-editable="true" data-title="Random Search for Hyper-Parameter Optimization"&gt;Random Search for Hyper-Parameter Optimization&lt;/a&gt;中指出，Random Search比Gird Search更有效。实际操作的时候，一般也是先用Gird Search的方法，得到所有候选参数，然后每次从中随机选择进行训练。&lt;/li&gt;&lt;li&gt;Bayesian Optimization. 贝叶斯优化，考虑到了不同参数对应的实验结果值，因此更节省时间。和网络搜索相比简直就是老牛和跑车的区别。具体原理可以参考这个论文： &lt;a href="http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf" data-editable="true" data-title="Practical Bayesian Optimization of Machine Learning Algorithms"&gt;Practical Bayesian Optimization of Machine Learning Algorithms&lt;/a&gt; ，这里同时推荐两个实现了贝叶斯调参的Python库，可以上手即用：&lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/jaberg/hyperopt" data-editable="true" data-title="jaberg/hyperopt"&gt;jaberg/hyperopt&lt;/a&gt;, 比较简单。&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/fmfn/BayesianOptimization" data-editable="true" data-title="fmfn/BayesianOptimization"&gt;fmfn/BayesianOptimization&lt;/a&gt;， 比较复杂，支持并行调参。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;总结&lt;/h1&gt;&lt;ul&gt;&lt;li&gt;合理性检查，确定模型，数据和其他地方没有问题。&lt;/li&gt;&lt;li&gt;训练时跟踪损失函数值，训练集和验证集准确率。&lt;/li&gt;&lt;li&gt;使用Random Search来搜索最优超参数，分阶段从粗（较大超参数范围训练较少周期）到细（较小超参数范围训练较长周期）进行搜索。&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;参考资料&lt;/h1&gt;&lt;p&gt;这里列了一些参数资料，大家有时间，可以进一步阅读。 &lt;a href="https://arxiv.org/abs/1206.5533" data-editable="true" data-title="Practical recommendations for gradient-based training of deep architectures by Yoshua Bengio (2012)"&gt;Practical recommendations for gradient-based training of deep architectures by Yoshua Bengio (2012)&lt;/a&gt;&lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" data-editable="true" data-title="Efficient BackProp, by Yann LeCun, Léon Bottou, Genevieve Orr and Klaus-Robert Müller"&gt;Efficient BackProp, by Yann LeCun, Léon Bottou, Genevieve Orr and Klaus-Robert Müller&lt;/a&gt;&lt;a href="http://www.springer.com/computer/theoretical+computer+science/book/978-3-642-35288-1" data-editable="true" data-title="Neural Networks: Tricks of the Trade, edited by Grégoire Montavon, Geneviève Orr, and Klaus-Robert Müller."&gt;Neural Networks: Tricks of the Trade, edited by Grégoire Montavon, Geneviève Orr, and Klaus-Robert Müller.&lt;/a&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24720954&amp;pixel&amp;useReferer"/&gt;</description><author>萧瑟</author><pubDate>Thu, 05 Jan 2017 00:56:47 GMT</pubDate></item><item><title>深度学习模型使用word2vec向量的方法总结</title><link>https://zhuanlan.zhihu.com/p/22018256</link><description>&lt;p&gt;转载请注明：&lt;a href="https://zhuanlan.zhihu.com/easyml" data-editable="true" data-title="知乎专栏" class=""&gt;炼丹实验室&lt;/a&gt;&lt;/p&gt;&lt;p&gt;使用word2vec工具在大规模外部文本语料上训练得到的向量，可以比较精确的衡量词之间的相关程度。一个比较简单的应用，就是利用词之间的向量的cos得分，来找相关词。同时word2vec向量，也可以用于深度学习模型的训练，使深度学习模型可以利用这种相关性，从而提高收敛速度和最终结果。但是实际使用的时候，有很多方式可供选择。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;直接用word2vec向量初始化模型embedding,训练的时候允许embedding向量更新。 这个方法最为常用，但是遇到不在训练语料中的词，就不能借助外部word2vec向量了。&lt;/li&gt;&lt;li&gt;word2vec向量，先连接全连接层（可以是多层），转化后的向量再作为模型的embedding,训练的时候，word2vec向量保持不变，允许全连接层的参数更新。 这个方法，哪怕遇到不在训练语料中的词，只要这个词在外部大规模语料中，能得到word2vec向量，那么就没问题。同时因为word2vec向量在训练的时候固定，因此模型训练涉及的参数会大大减少。 因为word2vec向量的分布，和模型实际需要的向量分布，可能存在差异，因此这个全连接层的作用，就是对word2vec向量的分布进行调整，让他尽可能接近模型需要的向量分布。&lt;/li&gt;&lt;li&gt;将word2vec向量拷贝，得到向量A和向量B，训练的时候，向量A保持不变，允许向量B的参数更新，最终embedding向量是A和B的平均。 具体请参考&lt;a href="https://arxiv.org/abs/1408.5882"&gt;https://arxiv.org/abs/1408.5882&lt;/a&gt;这个idea的想法，其实是限制word2vec向量的调整，避免调整的时候，太偏离原始向量。&lt;/li&gt;&lt;li&gt;&lt;ol&gt;&lt;li&gt;第一轮训练，用word2vec向量初始化embedding,对未知词随机初始化embedding,在训练的时候,固定住word2vec初始化的embedding,而允许未知词的embedding进行调整&lt;/li&gt;&lt;li&gt;第二轮训练，允许所有embedding调整,继续训练 这个idea也可以很好的处理未知词，第一轮的时候，因为固定了word2vec向量，因此模型会尽可能基于word2vec向量的分布来调整自己的参数。但是可能分布差异太大，导致模型参数无论怎么调整，都得不到最好结果。因此第二轮的时候，允许word2vec向量进行适当调整。具体请参考&lt;a href="https://arxiv.org/abs/1507.04808"&gt;https://arxiv.org/abs/1507.04808&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;参考论文&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;a href="https://arxiv.org/abs/1507.04808" data-title="2016-AAAI-Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models" class="" data-editable="true"&gt;2016-AAAI-Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://arxiv.org/abs/1408.5882" data-title="2014-EMNLP-Convolutional Neural Networks for Sentence Classification" class="" data-editable="true"&gt;2014-EMNLP-Convolutional Neural Networks for Sentence Classification&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22018256&amp;pixel&amp;useReferer"/&gt;</description><author>萧瑟</author><pubDate>Mon, 15 Aug 2016 12:52:55 GMT</pubDate></item><item><title>深度学习网络调试技巧</title><link>https://zhuanlan.zhihu.com/p/20792837</link><description>&lt;p&gt;转载请注明：&lt;a href="https://zhuanlan.zhihu.com/easyml" data-editable="true" data-title="知乎专栏" class=""&gt;炼丹实验室&lt;/a&gt;&lt;/p&gt;&lt;p&gt;神经网络的代码，比一般的代码要难调试不少，和编译错误以及运行时程序崩溃相比，神经网络比较棘手的地方，往往在于程序运行正常，但是结果无法收敛，这个检查起来可要麻烦多了。下面是根据我平时调试神经网络的经验，总结的一些比较通用的调试技巧，后续会再写一篇文章，专门介绍一下theano如何进行调试，希望能对大家调试神经网络有所帮助。&lt;/p&gt;&lt;h1&gt;遇到Nan怎么办？&lt;/h1&gt;&lt;p&gt;Nan问题，我相信大部分人都遇到过，一般可能是下面几个原因造成的：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;除0问题。这里实际上有两种可能，一种是被除数的值是无穷大，即Nan，另一种就是除数的值是0。之前产生的Nan或者0，有可能会被传递下去，造成后面都是Nan。请先检查一下神经网络中有可能会有除法的地方，例如softmax层，再认真的检查一下数据。我有一次帮别人调试代码，甚至还遇到过，训练数据文件中，有些值就是Nan。。。这样读进来以后，开始训练，只要遇到Nan的数据，后面也就Nan了。可以尝试加一些日志，把神经网络的中间结果输出出来，看看哪一步开始出现Nan。后面会介绍Theano的处理办法。&lt;/li&gt;&lt;li&gt;梯度过大，造成更新后的值为Nan。特别是RNN，在序列比较长的时候，很容易出现梯度爆炸的问题。一般有以下几个解决办法。&lt;ol&gt;&lt;li&gt;对梯度做clip(梯度裁剪），限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15。&lt;/li&gt;&lt;li&gt;减少学习率。初始学习率过大，也有可能造成这个问题。需要注意的是，即使使用adam之类的自适应学习率算法进行训练，也有可能遇到学习率过大问题，而这类算法，一般也有一个学习率的超参，可以把这个参数改的小一些。&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;初始参数值过大，也有可能出现Nan问题。输入和输出的值，最好也做一下归一化。具体方法可以参考我之前的一篇文章：&lt;a href="http://zhuanlan.zhihu.com/p/20767428" class="" data-editable="true" data-title="深度学习个人炼丹心得 - 炼丹实验室 - 知乎专栏"&gt;深度学习个人炼丹心得 - 炼丹实验室 - 知乎专栏&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h1&gt;神经网络学不出东西怎么办？&lt;/h1&gt;&lt;p&gt;可能我们并没有遇到，或者解决了Nan等问题，网络一直在正常的训练，但是cost降不下来，预测的时候，结果不正常。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;请打印出训练集的cost值和测试集上cost值的变化趋势，正常情况应该是训练集的cost值不断下降，最后趋于平缓，或者小范围震荡，测试集的cost值先下降，然后开始震荡或者慢慢上升。如果训练集cost值不下降，有可能是代码有bug，有可能是数据有问题（本身有问题，数据处理有问题等等），有可能是超参（网络大小，层数，学习率等）设置的不合理。 请人工构造10条数据，用神经网络反复训练，看看cost是否下降，如果还不下降，那么可能网络的代码有bug，需要认真检查了。如果cost值下降，在这10条数据上做预测，看看结果是不是符合预期。那么很大可能网络本身是正常的。那么可以试着检查一下超参和数据是不是有问题。&lt;/li&gt;&lt;li&gt;如果神经网络代码，全部是自己实现的，那么强烈建议做梯度检查。确保梯度计算没有错误。&lt;/li&gt;&lt;li&gt;先从最简单的网络开始实验，不要仅仅看cost值，还要看一看神经网络的预测输出是什么样子，确保能跑出预期结果。例如做语言模型实验的时候，先用一层RNN，如果一层RNN正常，再尝试LSTM，再进一步尝试多层LSTM。&lt;/li&gt;&lt;li&gt;如果可能的话，可以输入一条指定数据，然后自己计算出每一步正确的输出结果，再检查一下神经网络每一步的结果，是不是一样的。&lt;/li&gt;&lt;/ol&gt;&lt;h1&gt;参考资料&lt;/h1&gt;&lt;p&gt;&lt;a href="http://russellsstewart.com/notes/0.html" data-editable="true" data-title="russellsstewart.com 的页面"&gt;http://russellsstewart.com/notes/0.html&lt;/a&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/20792837&amp;pixel&amp;useReferer"/&gt;</description><author>萧瑟</author><pubDate>Sat, 23 Apr 2016 13:26:14 GMT</pubDate></item><item><title>深度学习网络训练技巧汇总</title><link>https://zhuanlan.zhihu.com/p/20767428</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/1c714563438bfc3cf3f2358cb58d8967_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;转载请注明：&lt;a href="https://zhuanlan.zhihu.com/easyml" data-editable="true" data-title="知乎专栏" class=""&gt;炼丹实验室&lt;/a&gt;&lt;/p&gt;&lt;p&gt;新开了一个专栏，为什么叫炼丹实验室呢，因为以后会在这个专栏里分享一些关于深度学习相关的实战心得，而深度学习很多人称它为玄学，犹如炼丹一般。不过即使是炼丹也是可以摸索出一些经验规律的，希望和各位炼丹术士一起多多交流。&lt;/p&gt;训练技巧对深度学习来说是非常重要的，作为一门实验性质很强的科学，同样的网络结构使用不同的训练方法训练，结果可能会有很大的差异。这里我总结了近一年来的炼丹心得，分享给大家，也欢迎大家补充指正。&lt;ol&gt;&lt;li&gt;&lt;p&gt;参数初始化,下面几种方式,随便选一个,结果基本都差不多。但是一定要做。否则可能会减慢收敛速度，影响收敛结果，甚至造成Nan等一系列问题。&lt;/p&gt;&lt;/li&gt;&lt;ol&gt;&lt;li&gt;uniform W = np.random.uniform(low=-scale, high=scale, size=shape)&lt;/li&gt;&lt;li&gt;glorot_uniform scale = np.sqrt(6. / (shape[0] + shape[1])) np.random.uniform(low=-scale, high=scale, size=shape)&lt;/li&gt;&lt;li&gt;高斯初始化: w = np.random.randn(n) / sqrt(n),n为参数数目 激活函数为relu的话,推荐 w = np.random.randn(n) * sqrt(2.0/n)&lt;/li&gt;&lt;li&gt;svd ,对RNN效果比较好,可以有效提高收敛速度.&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;&lt;p&gt;数据预处理方式&lt;/p&gt;&lt;ol&gt;&lt;li&gt;zero-center ,这个挺常用的.X -= np.mean(X, axis = 0) # zero-center X /= np.std(X, axis = 0) # normalize&lt;/li&gt;&lt;li&gt;PCA whitening,这个用的比较少.&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;训练技巧&lt;/p&gt;&lt;ol&gt;&lt;li&gt;要做梯度归一化,即算出来的梯度除以minibatch size&lt;/li&gt;&lt;li&gt;clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15&lt;/li&gt;&lt;li&gt;dropout对小数据防止过拟合有很好的效果,值一般设为0.5,小数据上dropout+sgd在我的大部分实验中，效果提升都非常明显.因此可能的话，建议一定要尝试一下。 dropout的位置比较有讲究, 对于RNN,建议放到输入-&amp;gt;RNN与RNN-&amp;gt;输出的位置.关于RNN如何用dropout,可以参考这篇论文:&lt;a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1409.2329" class="" data-editable="true" data-title="http://arxiv.org/abs/1409.2329"&gt;http://arxiv.org/abs/1409.2329&lt;/a&gt;&lt;/li&gt;&lt;li&gt;adam,adadelta等,在小数据上,我这里实验的效果不如sgd, sgd收敛速度会慢一些，但是最终收敛后的结果，一般都比较好。如果使用sgd的话,可以选择从1.0或者0.1的学习率开始,隔一段时间,在验证集上检查一下,如果cost没有下降,就对学习率减半. 我看过很多论文都这么搞,我自己实验的结果也很好. 当然,也可以先用ada系列先跑,最后快收敛的时候,更换成sgd继续训练.同样也会有提升.据说adadelta一般在分类问题上效果比较好，adam在生成问题上效果比较好。&lt;/li&gt;&lt;li&gt;除了gate之类的地方,需要把输出限制成0-1之外,尽量不要用sigmoid,可以用tanh或者relu之类的激活函数.1. sigmoid函数在-4到4的区间里，才有较大的梯度。之外的区间，梯度接近0，很容易造成梯度消失问题。2. 输入0均值，sigmoid函数的输出不是0均值的。&lt;/li&gt;&lt;li&gt;rnn的dim和embdding size,一般从128上下开始调整. batch size,一般从128左右开始调整.batch size合适最重要,并不是越大越好.&lt;/li&gt;&lt;li&gt;word2vec初始化,在小数据上,不仅可以有效提高收敛速度,也可以可以提高结果.&lt;/li&gt;&lt;li&gt;尽量对数据做shuffle&lt;/li&gt;&lt;li&gt;LSTM 的forget gate的bias,用1.0或者更大的值做初始化,可以取得更好的结果,来自这篇论文:&lt;a href="https://link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" class="" data-editable="true" data-title="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf"&gt;http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf&lt;/a&gt;, 我这里实验设成1.0,可以提高收敛速度.实际使用中,不同的任务,可能需要尝试不同的值.&lt;/li&gt;&lt;li&gt;Batch Normalization据说可以提升效果，不过我没有尝试过，建议作为最后提升模型的手段，参考论文：&lt;a href="http://arxiv.org/abs/1502.03167" class="" data-editable="true" data-title="Accelerating Deep Network Training by Reducing Internal Covariate Shift"&gt;Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt;&lt;/li&gt;&lt;li&gt;如果你的模型包含全连接层（MLP），并且输入和输出大小一样，可以考虑将MLP替换成Highway Network,我尝试对结果有一点提升，建议作为最后提升模型的手段，原理很简单，就是给输出加了一个gate来控制信息的流动，详细介绍请参考论文: &lt;a href="http://arxiv.org/abs/1505.00387" data-editable="true" data-title="arxiv.org 的页面"&gt;http://arxiv.org/abs/1505.00387&lt;/a&gt;&lt;/li&gt;&lt;li&gt;来自&lt;a href="https://www.zhihu.com/people/00c38786ac1d4d806996ee10e8b2912a" data-hash="00c38786ac1d4d806996ee10e8b2912a" class="member_mention" data-editable="true" data-title="@张馨宇" data-hovercard="p$b$00c38786ac1d4d806996ee10e8b2912a"&gt;@张馨宇&lt;/a&gt;的技巧：一轮加正则，一轮不加正则，反复进行。&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Ensemble: 论文刷结果的终极核武器,深度学习中一般有以下几种方式&lt;/p&gt;&lt;ol&gt;&lt;li&gt;同样的参数,不同的初始化方式&lt;/li&gt;&lt;li&gt;不同的参数,通过cross-validation,选取最好的几组&lt;/li&gt;&lt;li&gt;同样的参数,模型训练的不同阶段，即不同迭代次数的模型。&lt;/li&gt;&lt;li&gt;不同的模型,进行线性融合. 例如RNN和传统模型.&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/20767428&amp;pixel&amp;useReferer"/&gt;</description><author>萧瑟</author><pubDate>Mon, 18 Apr 2016 15:45:37 GMT</pubDate></item></channel></rss>