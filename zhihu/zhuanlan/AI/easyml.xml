<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>炼丹实验室 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/easyml</link><description>会定期更新一些深度学习相关的实践心得。</description><lastBuildDate>Sun, 08 Jan 2017 05:15:52 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>深度学习调参技巧</title><link>https://zhuanlan.zhihu.com/p/24720954</link><description>&lt;p&gt;之前曾经写过一篇文章，讲了一些深度学习训练的技巧，其中包含了部分调参心得：&lt;a href="https://zhuanlan.zhihu.com/p/20767428" data-editable="true" data-title="深度学习训练心得"&gt;深度学习训练心得&lt;/a&gt;。不过由于一般深度学习实验，相比普通机器学习任务，时间较长，因此调参技巧就显得尤为重要。同时个人实践中，又有一些新的调参心得，因此这里单独写一篇文章，谈一下自己对深度学习调参的理解，大家如果有其他技巧，也欢迎多多交流。&lt;/p&gt;&lt;h1&gt;好的实验环境是成功的一半&lt;/h1&gt;&lt;p&gt;由于深度学习实验超参众多，代码风格良好的实验环境，可以让你的人工或者自动调参更加省力，有以下几点可能需要注意：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;将各个参数的设置部分集中在一起。如果参数的设置分布在代码的各个地方，那么修改的过程想必会非常痛苦。&lt;/li&gt;&lt;li&gt;可以输出模型的误差或者相应评价指标，至少包括训练集，测试集。&lt;/li&gt;&lt;li&gt;实验环境便携化，方便多机并行调参。&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;确定参数范围&lt;/h1&gt;&lt;p&gt;针对每个参数，需要先确定一个大致的参数范围，这样后面精细调参的时候，可以更节省时间。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;一个比较靠谱的方式，是多参考相关论文，往往论文中给出的参数，是经过大量调参，至少是一个不差的参数。&lt;/li&gt;&lt;li&gt;如果找不到参考，那么只能自己尝试了。可以先固定其他参数，单独调节某一个参数。得到一个大致最佳值以后，在这个的基础上，继续调节其他参数。 虽然很多参数之间是有相关性的，但是我们这里只是为了尽可能确定一个大致的参数范围，为后面的精细调参提供参考，因此这个方法也是可以接受的。参数顺序选择上，可以从经验出发，先从比较重要的参数开始。在调节的时候，参数范围可以考虑指数变化。例如0.1 1 10 100。除非无论怎么调整，实验的loss都很差，基本不怎么变化，例如如果学习率参数初始值是1000，那么网络节点数，可能无论怎么调节，结果都比较差。这个时候可以考虑先调节其他参数。&lt;/li&gt;&lt;li&gt;一定要先找到一组参数，使模型的训练误差可以下降，这样才会有继续调参下去的底气。训练误差一直下降的话，如果是数据，模型实现等地方出了问题，那么调参就毫无意义了。&lt;/li&gt;&lt;li&gt;精简数据，在小数据集上先调参，确定大致参数范围。一方面可以是减少训练数据的数目。例如原来100W条数据，先采样成1W，进行实验看看。另外一方面，也可以减少训练的类别。例如手写数字识别，原来是10个类别，那么我们可以先在2个类别上训练，看看结果如何。&lt;/li&gt;&lt;/ol&gt;&lt;h1&gt;画图&lt;/h1&gt;&lt;p&gt;画图是一个很好的习惯，一般是训练数据遍历一轮以后，就输出一下训练集的误差和测试集的误差。同时画到一张图上。这样训练一段时间以后，如果模型一直没有收敛，那么就可以停止训练，尝试其他参数了，以节省时间。 如果训练到最后，训练集，测试集误差都很高，那么说明模型有可能欠拟合。那么后续调节参数方向，就是增强模型的拟合能力。例如增加网络层数，增加节点数，减少dropout值，减少L2正则值等等。 如果训练集误差很低，测试集误差较高，那么模型有可能过拟合，这个时候就需要向提高模型泛化能力的方向，调节参数。&lt;/p&gt;&lt;h1&gt;经验参数&lt;/h1&gt;&lt;p&gt;这里给出一些参数的经验值，避免大家调参的时候，毫无头绪。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;learning rate: 1 0.1 0.01 0.001, 一般从1开始，越来越小的尝试，比较常见。很少见learning rate大于10的。有很多自适应梯度的办法，例如adam,adadelta,rmsprop等，这些一般使用相关论文提供的默认值即可，可以避免再费劲调节学习率。对RNN来说，有个经验，如果RNN要处理的序列比较长，或者RNN层数比较多，那么learning rate一般小一些比较好，否则有可能出现结果不收敛，甚至Nan等问题。&lt;/li&gt;&lt;li&gt;网络层数： 先从1层开始。&lt;/li&gt;&lt;li&gt;每层结点数： 16 32 128，超过1000的情况比较少见。超过1W的从来没有见过。&lt;/li&gt;&lt;li&gt;batch size: 128上下开始。batch size值增加，的确能提高训练速度。但是有可能收敛结果变差。如果显存大小允许，可以考虑从一个比较大的值开始尝试。因为batch size太大，一般不会对结果有太大的影响，而batch size太小的话，结果有可能很差。&lt;/li&gt;&lt;li&gt;clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值，就算一个衰减系系数,让value的值等于阈值: 5,10,15&lt;/li&gt;&lt;li&gt;dropout： 0.5&lt;/li&gt;&lt;li&gt;L2正则：1.0，超过10的很少见。&lt;/li&gt;&lt;li&gt;词向量embedding大小：128，256&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;自动调参&lt;/h1&gt;&lt;p&gt;人工一直盯着实验，毕竟太累。自动调参当前也有不少研究。下面介绍几种比较实用的办法：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Gird Search. 这个是最常见的。具体说，就是每种参数确定好几个要尝试的值，然后像一个网格一样，把所有参数值的组合遍历一下。优点是实现简单暴力，如果能全部遍历的话，结果比较可靠。缺点是太费时间了，特别像神经网络，一般尝试不了太多的参数组合。&lt;/li&gt;&lt;li&gt;Bayesian Optimization. 贝叶斯优化，考虑到了不同参数对应的实验结果值，因此更节省时间。和网络搜索相比简直就是老牛和跑车的区别。具体原理可以参考这个论文： &lt;a href="http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf" data-editable="true" data-title="Practical Bayesian Optimization of Machine Learning Algorithms"&gt;Practical Bayesian Optimization of Machine Learning Algorithms&lt;/a&gt; ，这里同时推荐两个实现了贝叶斯调参的Python库，可以上手即用：&lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/jaberg/hyperopt" data-editable="true" data-title="jaberg/hyperopt"&gt;jaberg/hyperopt&lt;/a&gt;, 比较简单。&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/fmfn/BayesianOptimization" data-editable="true" data-title="fmfn/BayesianOptimization"&gt;fmfn/BayesianOptimization&lt;/a&gt;， 比较复杂，支持并行调参。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h1&gt;参考资料&lt;/h1&gt;&lt;p&gt;这里列了一些参数资料，大家有时间，可以进一步阅读。 &lt;a href="https://arxiv.org/abs/1206.5533" data-editable="true" data-title="Practical recommendations for gradient-based training of deep architectures by Yoshua Bengio (2012)"&gt;Practical recommendations for gradient-based training of deep architectures by Yoshua Bengio (2012)&lt;/a&gt;&lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" data-editable="true" data-title="Efficient BackProp, by Yann LeCun, Léon Bottou, Genevieve Orr and Klaus-Robert Müller"&gt;Efficient BackProp, by Yann LeCun, Léon Bottou, Genevieve Orr and Klaus-Robert Müller&lt;/a&gt;&lt;a href="http://www.springer.com/computer/theoretical+computer+science/book/978-3-642-35288-1" data-editable="true" data-title="Neural Networks: Tricks of the Trade, edited by Grégoire Montavon, Geneviève Orr, and Klaus-Robert Müller."&gt;Neural Networks: Tricks of the Trade, edited by Grégoire Montavon, Geneviève Orr, and Klaus-Robert Müller.&lt;/a&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24720954&amp;pixel&amp;useReferer"/&gt;</description><author>萧瑟</author><pubDate>Thu, 05 Jan 2017 00:56:47 GMT</pubDate></item><item><title>深度学习模型使用word2vec向量的方法总结</title><link>https://zhuanlan.zhihu.com/p/22018256</link><description>&lt;p&gt;使用word2vec工具在大规模外部文本语料上训练得到的向量，可以比较精确的衡量词之间的相关程度。一个比较简单的应用，就是利用词之间的向量的cos得分，来找相关词。同时word2vec向量，也可以用于深度学习模型的训练，使深度学习模型可以利用这种相关性，从而提高收敛速度和最终结果。但是实际使用的时候，有很多方式可供选择。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;直接用word2vec向量初始化模型embedding,训练的时候允许embedding向量更新。 这个方法最为常用，但是遇到不在训练语料中的词，就不能借助外部word2vec向量了。&lt;/li&gt;&lt;li&gt;word2vec向量，先连接全连接层（可以是多层），转化后的向量再作为模型的embedding,训练的时候，word2vec向量保持不变，允许全连接层的参数更新。 这个方法，哪怕遇到不在训练语料中的词，只要这个词在外部大规模语料中，能得到word2vec向量，那么就没问题。同时因为word2vec向量在训练的时候固定，因此模型训练涉及的参数会大大减少。 因为word2vec向量的分布，和模型实际需要的向量分布，可能存在差异，因此这个全连接层的作用，就是对word2vec向量的分布进行调整，让他尽可能接近模型需要的向量分布。&lt;/li&gt;&lt;li&gt;将word2vec向量拷贝，得到向量A和向量B，训练的时候，向量A保持不变，允许向量B的参数更新，最终embedding向量是A和B的平均。 具体请参考&lt;a href="https://arxiv.org/abs/1408.5882"&gt;https://arxiv.org/abs/1408.5882&lt;/a&gt;这个idea的想法，其实是限制word2vec向量的调整，避免调整的时候，太偏离原始向量。&lt;/li&gt;&lt;li&gt;&lt;ol&gt;&lt;li&gt;第一轮训练，用word2vec向量初始化embedding,对未知词随机初始化embedding,在训练的时候,固定住word2vec初始化的embedding,而允许未知词的embedding进行调整&lt;/li&gt;&lt;li&gt;第二轮训练，允许所有embedding调整,继续训练 这个idea也可以很好的处理未知词，第一轮的时候，因为固定了word2vec向量，因此模型会尽可能基于word2vec向量的分布来调整自己的参数。但是可能分布差异太大，导致模型参数无论怎么调整，都得不到最好结果。因此第二轮的时候，允许word2vec向量进行适当调整。具体请参考&lt;a href="https://arxiv.org/abs/1507.04808"&gt;https://arxiv.org/abs/1507.04808&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;参考论文&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;a href="https://arxiv.org/abs/1507.04808" data-title="2016-AAAI-Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models" class="" data-editable="true"&gt;2016-AAAI-Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://arxiv.org/abs/1408.5882" data-title="2014-EMNLP-Convolutional Neural Networks for Sentence Classification" class="" data-editable="true"&gt;2014-EMNLP-Convolutional Neural Networks for Sentence Classification&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22018256&amp;pixel&amp;useReferer"/&gt;</description><author>萧瑟</author><pubDate>Mon, 15 Aug 2016 12:52:55 GMT</pubDate></item><item><title>深度学习网络调试技巧</title><link>https://zhuanlan.zhihu.com/p/20792837</link><description>&lt;p&gt;神经网络的代码，比一般的代码要难调试不少，和编译错误以及运行时程序崩溃相比，神经网络比较棘手的地方，往往在于程序运行正常，但是结果无法收敛，这个检查起来可要麻烦多了。下面是根据我平时调试神经网络的经验，总结的一些比较通用的调试技巧，后续会再写一篇文章，专门介绍一下theano如何进行调试，希望能对大家调试神经网络有所帮助。&lt;/p&gt;&lt;h1&gt;遇到Nan怎么办？&lt;/h1&gt;&lt;p&gt;Nan问题，我相信大部分人都遇到过，一般可能是下面几个原因造成的：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;除0问题。这里实际上有两种可能，一种是被除数的值是无穷大，即Nan，另一种就是除数的值是0。之前产生的Nan或者0，有可能会被传递下去，造成后面都是Nan。请先检查一下神经网络中有可能会有除法的地方，例如softmax层，再认真的检查一下数据。我有一次帮别人调试代码，甚至还遇到过，训练数据文件中，有些值就是Nan。。。这样读进来以后，开始训练，只要遇到Nan的数据，后面也就Nan了。可以尝试加一些日志，把神经网络的中间结果输出出来，看看哪一步开始出现Nan。后面会介绍Theano的处理办法。&lt;/li&gt;&lt;li&gt;梯度过大，造成更新后的值为Nan。特别是RNN，在序列比较长的时候，很容易出现梯度爆炸的问题。一般有以下几个解决办法。&lt;ol&gt;&lt;li&gt;对梯度做clip(梯度裁剪），限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15。&lt;/li&gt;&lt;li&gt;减少学习率。初始学习率过大，也有可能造成这个问题。需要注意的是，即使使用adam之类的自适应学习率算法进行训练，也有可能遇到学习率过大问题，而这类算法，一般也有一个学习率的超参，可以把这个参数改的小一些。&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;初始参数值过大，也有可能出现Nan问题。输入和输出的值，最好也做一下归一化。具体方法可以参考我之前的一篇文章：&lt;a href="http://zhuanlan.zhihu.com/p/20767428" class="" data-editable="true" data-title="深度学习个人炼丹心得 - 炼丹实验室 - 知乎专栏"&gt;深度学习个人炼丹心得 - 炼丹实验室 - 知乎专栏&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h1&gt;神经网络学不出东西怎么办？&lt;/h1&gt;&lt;p&gt;可能我们并没有遇到，或者解决了Nan等问题，网络一直在正常的训练，但是cost降不下来，预测的时候，结果不正常。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;请打印出训练集的cost值和测试集上cost值的变化趋势，正常情况应该是训练集的cost值不断下降，最后趋于平缓，或者小范围震荡，测试集的cost值先下降，然后开始震荡或者慢慢上升。如果训练集cost值不下降，有可能是代码有bug，有可能是数据有问题（本身有问题，数据处理有问题等等），有可能是超参（网络大小，层数，学习率等）设置的不合理。 请人工构造10条数据，用神经网络反复训练，看看cost是否下降，如果还不下降，那么可能网络的代码有bug，需要认真检查了。如果cost值下降，在这10条数据上做预测，看看结果是不是符合预期。那么很大可能网络本身是正常的。那么可以试着检查一下超参和数据是不是有问题。&lt;/li&gt;&lt;li&gt;如果神经网络代码，全部是自己实现的，那么强烈建议做梯度检查。确保梯度计算没有错误。&lt;/li&gt;&lt;li&gt;先从最简单的网络开始实验，不要仅仅看cost值，还要看一看神经网络的预测输出是什么样子，确保能跑出预期结果。例如做语言模型实验的时候，先用一层RNN，如果一层RNN正常，再尝试LSTM，再进一步尝试多层LSTM。&lt;/li&gt;&lt;li&gt;如果可能的话，可以输入一条指定数据，然后自己计算出每一步正确的输出结果，再检查一下神经网络每一步的结果，是不是一样的。&lt;/li&gt;&lt;/ol&gt;&lt;h1&gt;参考资料&lt;/h1&gt;&lt;p&gt;&lt;a href="http://russellsstewart.com/notes/0.html" data-editable="true" data-title="russellsstewart.com 的页面"&gt;http://russellsstewart.com/notes/0.html&lt;/a&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/20792837&amp;pixel&amp;useReferer"/&gt;</description><author>萧瑟</author><pubDate>Sat, 23 Apr 2016 13:26:14 GMT</pubDate></item><item><title>深度学习个人训练心得</title><link>https://zhuanlan.zhihu.com/p/20767428</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/1c714563438bfc3cf3f2358cb58d8967_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;新开了一个专栏，为什么叫炼丹实验室呢，因为以后会在这个专栏里分享一些关于深度学习相关的实战心得，而深度学习很多人称它为玄学，犹如炼丹一般。不过即使是炼丹也是可以摸索出一些经验规律的，希望和各位炼丹术士一起多多交流。&lt;/p&gt;训练技巧对深度学习来说是非常重要的，作为一门实验性质很强的科学，同样的网络结构使用不同的训练方法训练，结果可能会有很大的差异。这里我总结了近一年来的炼丹心得，分享给大家，也欢迎大家补充指正。&lt;ol&gt;&lt;li&gt;&lt;p&gt;参数初始化,下面几种方式,随便选一个,结果基本都差不多。但是一定要做。否则可能会减慢收敛速度，影响收敛结果，甚至造成Nan等一系列问题。&lt;/p&gt;&lt;/li&gt;&lt;ol&gt;&lt;li&gt;uniform W = np.random.uniform(low=-scale, high=scale, size=shape)&lt;/li&gt;&lt;li&gt;glorot_uniform scale = np.sqrt(6. / (shape[0] + shape[1])) np.random.uniform(low=-scale, high=scale, size=shape)&lt;/li&gt;&lt;li&gt;高斯初始化: w = np.random.randn(n) / sqrt(n),n为参数数目 激活函数为relu的话,推荐 w = np.random.randn(n) * sqrt(2.0/n)&lt;/li&gt;&lt;li&gt;svd ,对RNN效果比较好,可以有效提高收敛速度.&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;&lt;p&gt;数据预处理方式&lt;/p&gt;&lt;ol&gt;&lt;li&gt;zero-center ,这个挺常用的.X -= np.mean(X, axis = 0) # zero-center X /= np.std(X, axis = 0) # normalize&lt;/li&gt;&lt;li&gt;PCA whitening,这个用的比较少.&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;训练技巧&lt;/p&gt;&lt;ol&gt;&lt;li&gt;要做梯度归一化,即算出来的梯度除以minibatch size&lt;/li&gt;&lt;li&gt;clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15&lt;/li&gt;&lt;li&gt;dropout对小数据防止过拟合有很好的效果,值一般设为0.5,小数据上dropout+sgd在我的大部分实验中，效果提升都非常明显.因此可能的话，建议一定要尝试一下。 dropout的位置比较有讲究, 对于RNN,建议放到输入-&amp;gt;RNN与RNN-&amp;gt;输出的位置.关于RNN如何用dropout,可以参考这篇论文:&lt;a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1409.2329" class="" data-editable="true" data-title="http://arxiv.org/abs/1409.2329"&gt;http://arxiv.org/abs/1409.2329&lt;/a&gt;&lt;/li&gt;&lt;li&gt;adam,adadelta等,在小数据上,我这里实验的效果不如sgd, sgd收敛速度会慢一些，但是最终收敛后的结果，一般都比较好。如果使用sgd的话,可以选择从1.0或者0.1的学习率开始,隔一段时间,在验证集上检查一下,如果cost没有下降,就对学习率减半. 我看过很多论文都这么搞,我自己实验的结果也很好. 当然,也可以先用ada系列先跑,最后快收敛的时候,更换成sgd继续训练.同样也会有提升.&lt;/li&gt;&lt;li&gt;除了gate之类的地方,需要把输出限制成0-1之外,尽量不要用sigmoid,可以用tanh或者relu之类的激活函数.&lt;/li&gt;&lt;li&gt;rnn的dim和embdding size,一般从128上下开始调整. batch size,一般从128左右开始调整.batch size合适最重要,并不是越大越好.&lt;/li&gt;&lt;li&gt;word2vec初始化,在小数据上,不仅可以有效提高收敛速度,也可以可以提高结果.&lt;/li&gt;&lt;li&gt;尽量对数据做shuffle&lt;/li&gt;&lt;li&gt;LSTM 的forget gate的bias,用1.0或者更大的值做初始化,可以取得更好的结果,来自这篇论文:&lt;a href="https://link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" class="" data-editable="true" data-title="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf"&gt;http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf&lt;/a&gt;, 我这里实验设成1.0,可以提高收敛速度.实际使用中,不同的任务,可能需要尝试不同的值.&lt;/li&gt;&lt;li&gt;Batch Normalization据说可以提升效果，不过我没有尝试过，建议作为最后提升模型的手段，参考论文：&lt;a href="http://arxiv.org/abs/1502.03167" class=""&gt;http://arxiv.org/abs/1502.03167&lt;/a&gt;&lt;/li&gt;&lt;li&gt;如果你的模型包含全连接层（MLP），并且输入和输出大小一样，可以考虑将MLP替换成Highway Network,我尝试对结果有一点提升，建议作为最后提升模型的手段，原理很简单，就是给输出加了一个gate来控制信息的流动，详细介绍请参考论文: &lt;a href="http://arxiv.org/abs/1505.00387"&gt;http://arxiv.org/abs/1505.00387&lt;/a&gt;&lt;/li&gt;&lt;li&gt;来自&lt;a href="https://www.zhihu.com/people/00c38786ac1d4d806996ee10e8b2912a" data-hash="00c38786ac1d4d806996ee10e8b2912a" class="member_mention" data-editable="true" data-title="@张馨宇" data-hovercard="p$b$00c38786ac1d4d806996ee10e8b2912a"&gt;@张馨宇&lt;/a&gt;的技巧：一轮加正则，一轮不加正则，反复进行。&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Ensemble: 论文刷结果的终极核武器,深度学习中一般有以下几种方式&lt;/p&gt;&lt;ol&gt;&lt;li&gt;同样的参数,不同的初始化方式&lt;/li&gt;&lt;li&gt;不同的参数,通过cross-validation,选取最好的几组&lt;/li&gt;&lt;li&gt;同样的参数,模型训练的不同阶段，即不同迭代次数的模型。&lt;/li&gt;&lt;li&gt;不同的模型,进行线性融合. 例如RNN和传统模型.&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/20767428&amp;pixel&amp;useReferer"/&gt;</description><author>萧瑟</author><pubDate>Mon, 18 Apr 2016 15:45:37 GMT</pubDate></item></channel></rss>