<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>远东轶事 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/yuandong</link><description>“前进！前进！不择手段地前进！”——托马斯·维德</description><lastBuildDate>Mon, 30 Jan 2017 14:16:37 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>2016年总结</title><link>https://zhuanlan.zhihu.com/p/24666144</link><description>&lt;p&gt;2016年是波澜壮阔的一年。&lt;/p&gt;&lt;p&gt;这一年人工智能领域最抢眼的莫过于AlphaGo与李世石的五番棋对决。这一战，极大地扭转了大众对人工智能的认知，注定会载入史册。&lt;/p&gt;&lt;p&gt;我有幸参与了历史。先是开了DarkForest这个项目，在计算机围棋上做了一些工作，开源了代码，给社区做了些贡献；同时在DeepMind公布AlphaGo的论文及人机对决前后，给大家做了一些科普，收获了许多关注。&lt;/p&gt;&lt;p&gt;当然，两边资源投入的量级完全不同。在八月份美国围棋大会上，我有幸见到了AlphaGo的主要贡献者黄士杰(AjaHuang)和樊麾。我问他们，我们用了大概80到90块GPU来训练模型，我是否可以在演讲时说我们用了AlphaGo百分之一的GPU？&lt;/p&gt;&lt;p&gt;那时Aja神秘地笑了笑说：具体数字不能讲。不过，也许小于百分之一吧。&lt;/p&gt;&lt;p&gt;我无言以对。或许当初选择做围棋是个鲁莽的决定，不管是经验上还是资源上，差距都很大，但至少眼光是对的。并且实践过了之后，才知道强化学习（Reinforcement Learning，RL）这个方向的潜力。这个方向虽然有DeepMind和OpenAI的牛人们领头狂奔，但从十年的长远尺度上来说，还有大量的工作可以做。与传统的监督学习相比，强化学习不仅建模“得到数据以训练模型”这个问题，还建模了“如何从世界中得到数据”这个过程，这样天地一下子就广阔了很多。另一方面RL与行为决策直接挂钩，这就在一定程度上跳过了“认识世界”这个可能过于复杂的环节，而直指“改变世界”这样一个终极目标。当然，要让AI真能使用，就得要求它在复杂环境，很少的样本及非常稀缺的外界激励下，做出正确的决定。在这点上，大家还完全没有头绪，所以说未来的空间仍然很大。&lt;/p&gt;&lt;p&gt;今年RL的一个突出特点是各种虚拟环境和新训练方案层出不穷，各家都说自己虚拟平台好，算法效果好，但是否能得到相互间可比较的结果，还是要期待明后年的工作。目前看来，Atari、OpenAI Gym或者参加各种AI比赛，都可以用来评测算法的好坏，但究竟什么样的评测是客观公正的，还需要摸索。因为各类虚拟环境实在太多，样本采集还有随机性，因此好的评测可能比设计计算机视觉中imagenet的数据集更加困难——也许最终都只能放到机器人上，在真实世界中做比较了吧。&lt;/p&gt;&lt;p&gt;RL的另一个有趣的地方是，研究者们需要同时具备强的研究能力、工程能力和数学基础；以后要是机器人大行其道，那就连硬件经验都要一并跟上。RL其实是个很老的跨学科领域，各时代的文章里数学符号和概念都不太一样，要读通需要花一番功夫，要发好文章则更要多思考。在工程上，相比日渐成熟的DL框架，RL的框架另有一些精巧的地方，各种小细节很多，往往错一个则全盘皆输。不过正因为如此，与在现有模型上调参数相比，做RL更具有挑战性。今年我们的Doom AI Bot拿了Track1的冠军是一个惊喜，我很幸运招到了&lt;a href="https://www.zhihu.com/people/5c2b06e8ddc61687e42a1a64fb72bbaf" data-hash="5c2b06e8ddc61687e42a1a64fb72bbaf" class="member_mention" data-hovercard="p$b$5c2b06e8ddc61687e42a1a64fb72bbaf"&gt;@吴育昕&lt;/a&gt; 这样优秀的实习生。&lt;/p&gt;&lt;p&gt;如何让计算机自动写代码则是另一个非常有意思的方向。去年可微计算机（Differentiable machine）非常火爆，大家都设计出带记忆带attention的深度网络模型去学习如何给定程序输入得到算法题的输出，但却忽略了让计算机自动写代码这个更直接，更切合人类思维方式的方案。果不其然，今年年底相关文章井喷，我们也投了一篇，不知道明年又会如何发展。&lt;/p&gt;&lt;p&gt;---------------------------------&lt;/p&gt;&lt;p&gt;总的来说，这一年过得不错。和老婆团聚，第一次被那么多人关注，第一次完成半程马拉松，第一次一个会议投三篇，第一次投稿深度学习的理论。做了那么多以前没能做成的事，真有一种人生才刚刚开始的感觉。&lt;/p&gt;&lt;p&gt;希望接下来的2017年，更加精彩。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24666144&amp;pixel&amp;useReferer"/&gt;</description><author>田渊栋</author><pubDate>Sun, 01 Jan 2017 15:01:40 GMT</pubDate></item><item><title>答读者问</title><link>https://zhuanlan.zhihu.com/p/24377467</link><description>&lt;p&gt;最近收到一些信问“自己是不是适合搞AI，门槛是不是太高”等等问题，也看到“IT是不是泡沫”等等讨论，我就写篇博客给大家分享下我的观点，顺便也分析一下做AI，ML及CS的前景。&lt;/p&gt;&lt;p&gt;其实AI现在算是野蛮生长阶段，英雄不问出身不问门派，只要做的东西效果好，大家立马拜倒。AI做了六七十年，出了很多理论，但都是尝试性和局部性的，并没有一个权威理论去解释一切问题。翻翻各大会议的文章，经常有令人惊讶的结果出现，大牛们往往都不能自圆其说。这种情况往往对应着门槛低而机会多，其实是最适合新人进入的。而一个成熟的领域则有意无意地设置很多门槛，要进入先需要学习五年十年基础知识，或者必须科班出身，或者得要熬年头论资历才可以一点点走上去。&lt;/p&gt;&lt;p&gt;当然反过来说，进入成熟领域，可以借鉴大量前人经验，按部就班就行，节奏也相对慢些；而做AI变数大，对自我学习能力有很强的要求，老师教的和教科书上的往往过时，甚至是刚在会议上听到的权威发言，过几天就被打脸。因此需要不停地总结学习，不仅要有不断突破的意愿和勇气，还要知道如何架梯子搭桥，一点一点地做出成果。&lt;/p&gt;&lt;p&gt;AI作为通用工具，和众多传统领域肯定有交融的一天，并且极大地推动科技进步。现在有很多领域已经用上了ML和AI，比如说今年NIPS上高能物理方向就用上了DL寻找新对撞模式从而找到新粒子。以现在的软硬件发展速度，将来更多领域也会加入，在我们这一辈退休之前，可能大部分行业都会和AI有关，并且因为这个而提高前进的速度。而目前CS，ML和AI等快速迭代的领域需要的能力，是主动而准确地获取新信息及归纳总结，快速学习和利用工具，和不断自我进步自我完善，即“知道如何学习”的技能。这些是做我们这行现在需要的，也会是将来其它领域也需要的能力，不管愿不愿意，总会在将来某时刻与大家产生关联。所以我个人建议，与其将它放到有家有口的将来，不如现在就去主动学会这些技能，是更积极的方案。&lt;/p&gt;&lt;p&gt;当然，并不是所有学AI，ML和CS方向的同学，将来都会从事同类型的工作的。但就算这样，在学习CS及AI过程中掌握的大量工具，都是以后提高工作效率的利器。我和其它领域的人对话，经常痛心效率之低，感到CS可以极大地提高效率。比如说本来需要花费大量人力，为时几年的数据采集工作，往往写几个脚本就可以在几天内更快更好地完成。相比传统的手工操作，CS的思考习惯和工作方式，会让人下意识地思考更高层次的问题，从而开拓新的思路。这里举个Excel的例子，很多非CS专业的同学会经常用Excel画表作图，但在我们看来这效率不高，因为Excel把数据和操作，格式和内容结合在一起，所以每次有新的数据，往往就要重新导入修改画图，这其中的键盘鼠标操作花费的时间，操作员花费的注意力及可能的人为错误造成的代价，都是惊人的。相比之下，写脚本是一个更快更好的处理数据的手段，新数据来了只要命令行下重新跑原来的程序，就能做到自动画图排版甚至生成报表，不需人工，省时省力。另外，在实际生活中一些简单的模式识别问题也可以写正则表达式或者训练分类器来做判断，然后人只要负责特殊情况就行了。当然这些都需要好的基础，能灵活操作各种工具完成自己想要的目标。从这点看来，学这些方向的同学，将来会有比较大的优势。&lt;/p&gt;&lt;p&gt;顺便说一句，为什么这个领域的从业人员有高工资呢？一个原因就是IT人员平日大部分工作是在提高系统盈利的速率，而非维持。一个广告系统做好了，用较小的代价就可以让它持续工作，程序员吃饭睡觉看片打游戏，系统还在不停跑不停地盈利。而程序员改进它则可以提高盈利。相比之下，大部分行业则是不开工则不盈利，开了工只能维持同等的盈利水平。这其中的差别是质上的。不仅如此，将来的AI系统，比如说使用强化学习的系统，还能自动提高盈利的效率，而研究员们现在想的就是如何让它提高得更快些。这其中的差别，又是质上的。所以说这个领域的从业人员能拿高工资是名至实归，并没有什么泡沫在，一个人能躺着服务千万人，肯定得要拿高薪。要是将来有人能做一个吸收阳光，水和土壤，自动生产食物的系统，那他的工资会是多高？我想大家知道答案。这和工作时间多长，干活多勤勉多累，一点关系也没有；而能调动自然规律和社会规律为我所用，才是更重要的，这比一个人的力量，要强大太多了。&lt;/p&gt;&lt;p&gt;看最近的趋势，确实人和人之间的差距在拉大，以后一个牛人以一当百当千绝对是有可能的。而成为那个一还是那个千分之一，就要由自己选择了。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24377467&amp;pixel&amp;useReferer"/&gt;</description><author>田渊栋</author><pubDate>Thu, 15 Dec 2016 10:02:18 GMT</pubDate></item><item><title>求道之人，不问寒暑</title><link>https://zhuanlan.zhihu.com/p/23460569</link><description>&lt;p&gt;我还记得那个决定不给NIPS投稿的傍晚。那时在车里，一个人待着，也不哭，也不笑，看着夕阳西下。七点多了，但因为是六月的关系，天还是亮着。周五傍晚，同事们都走完了，谷歌的校园里很安静，树叶在地上铺开，红的，黄的，绿的。耳朵里只有风吹过的声音。&lt;/p&gt;&lt;p&gt;老婆打电话过来，我说了一通为什么不投稿的原因，我说你指出的是对的，你说的有道理。我说话的时候非常冷静，她说你没事吧，我说我没事的。她挂了电话，宾州那里已经是快11点，该睡了，留我一个人在加州，看着这空无一人的校园。&lt;/p&gt;&lt;p&gt;我给我导师发了封信，说不投了，感谢这几天陪我改这篇糟糕的文章，他之前的建议也是不要急着投稿，我说我想了想，你是对的，我太急太赶了，很多事情要慢慢来。然后我放下电脑，点火，开车，启动，开上shoreline，开上101高速，在亮着灯的车流中穿行。忽然间，就没有执著投稿的念头了，什么也不想，什么也不做，任着车带我走街串巷，就好像一个再平凡不过的周末。&lt;/p&gt;&lt;p&gt;今天过去了，明天还有明天的事。能放下，才可重生。&lt;/p&gt;&lt;p&gt;做数学证明实在是一件非常可怕的任务，前一刻你以为自己胜券在握，后一时刻发现了一个错误否定全盘，立即如坠冰窟，像天塌下来似的；刚刚还像是孙猴子腾挪得舒爽了，觉得自己厉害无比，突然间发现一直在如来的五指山里转圈子，从没有踏出一步。如此几次，才觉得自我的渺小，自然的可怖。这种感觉，书上教不来，别人传达不到，唯有自己走到了，方才悟得。而一旦悟得了，就锋芒尽敛，再也没有傲慢的底气。&lt;/p&gt;&lt;p&gt;《棋魂》里面，佐为有一句话让我印象深刻。“你不害怕，是因为看不到我的剑锋，你害怕了，因为你看到了”。做研究也是如此，不仅先要看到剑锋，还要有迎难而上的勇气，无数次被打趴下后，再无数次从绝望中找到一丝希望，然后费尽九牛二虎之力，从密密麻麻的错误里，一点一点地挖出那米粒般的宝石来——对于证明而言，那必须是完美无瑕的，若有一点瑕疵，就与泥沙无异。这样的事情干得太多，也就无所谓希望和失望，坚持和放弃，我能知道的，就只是时刻做好被从头到脚彻底全盘否定的准备，时刻想着把草稿烧掉，推倒重来。时间久了，有一天我突然发现，我完全无法理解别人守成的习惯，为什么大家都宁愿抱残守缺，而不愿从头开始？&lt;/p&gt;&lt;p&gt;原来这么长时间，已经让我成了一个完全不同的人。&lt;/p&gt;&lt;p&gt;有人问我，梦想如何坚持？梦想破灭了怎么办？我想要回答，但是真要提笔的时候，又不知道如何说起。其实，这世界上没有破灭和未破灭这两种状态，没有是或非两种结论，这世界上有的，只是日升日落，人来人往。你说要有光，那就有光，光在你心里；你要追求什么，那东西就不曾离你而去；而你若忘却，它就消亡。所以，若是要坚持所谓的梦想，那么就如同小说中写的那样——&lt;/p&gt;&lt;p&gt;求道之人，不问寒暑。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23460569&amp;pixel&amp;useReferer"/&gt;</description><author>田渊栋</author><pubDate>Sun, 06 Nov 2016 16:00:31 GMT</pubDate></item><item><title>ICLR总结</title><link>https://zhuanlan.zhihu.com/p/23454387</link><description>ICLR2017的投稿时间终于截止。这次投了三篇文章出去，是我个人做研究以来单次会议投稿数最多的一次。&lt;p&gt;1. 如何用增强学习中的Actor-Critic模型再加上课程学习来训练Doom AI拿到AI比赛冠军：&lt;/p&gt;&lt;p&gt;&lt;a href="http://104.155.136.4:3000/forum?id=Hk3mPK5gg" data-editable="true" data-title="Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning"&gt;Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning&lt;/a&gt;&lt;/p&gt;&lt;p&gt;2. 如何用卷积神经网络让计算机读入一个算法的输入输出，自己写代码：&lt;/p&gt;&lt;p&gt;&lt;a href="http://104.155.136.4:3000/forum?id=r1ryLtqgl" data-editable="true" data-title="Unsupervised Program Induction with Hierarchical Generative Convolutional Neural Networks"&gt;Unsupervised Program Induction with Hierarchical Generative Convolutional Neural Networks&lt;/a&gt;&lt;/p&gt;&lt;p&gt;3. 对于用ReLU作为传递函数的二层非线性神经网络的收敛性分析：&lt;/p&gt;&lt;p&gt;&lt;a href="http://104.155.136.4:3000/forum?id=Hk85q85ee" data-editable="true" data-title="Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with ReLU nonlinearity"&gt;Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with ReLU nonlinearity&lt;/a&gt;&lt;/p&gt;&lt;p&gt;--------------------&lt;/p&gt;&lt;p&gt;ICLR这个会议的投稿是即时公开的，上传完文章之后，任何人都可以看到，不存在任何时延。翻阅投稿的论文，你会看到这个领域的迭代之快，世所罕见。我之前在专栏《&lt;a href="https://zhuanlan.zhihu.com/p/21315591" data-editable="true" data-title="快速迭代的人工智能"&gt;快速迭代的人工智能&lt;/a&gt;》里面提到领域投稿的速度， 等到这次再看看投稿列表，发觉自己可能还低估了。&lt;/p&gt;&lt;p&gt;像“让计算机自己写代码”这个想法，在去年的文章里还出现得不多，并且主要是以构建可微分计算机（比如说DeepMind的神经图灵机，可微分神经计算机）的形式，让神经网络通过端对端的梯度下降的优化方法，学会如求和排序等具体任务。但是这个思路有几个比较大的问题，一个是因为神经网络的黑箱性质，对可微分计算机学习后使用的算法没办法解释；第二个是梯度下降法取决于初值的选取，且优化过程较慢；第三个是扩展性不好，对60个数排序可以，100个数就完全不行，有悖人类的直觉。而让计算机生成代码则没有1和3这两个问题，2也可以通过预先训练神经网络生成代码来克服，不通过优化，而用一遍前向传播就可以了。我们的第二篇投稿（链接）就是基于这个想法，将算法的输入输出的结果抽取特征后，送入卷积神经网络文献中层次式生成图像的经典框架，生成一张二维图，每行就是一行代码，或者更确切地说，是代码的概率分布。有了好的分布，就可以帮助启发式搜索找到正确的程序。而神经网络的训练数据，则由大量的随机代码，随机输入及随机代码执行后得到的输出来提供，所以基本不需要人工干预，算是非监督的办法。&lt;/p&gt;&lt;p&gt;等到今年的ICLR的文章一公布，随便翻一翻就找到了七篇计算机自动生成（或者自动优化）代码的文章。打开一看，引论里全在描述同样的动机。看完之后，我心里暗自庆幸我们及时投了出去，不然等到ICML就要被灭得渣都不剩了。当然方法上来说，大部分使用的还是RNN和Autoencoder，没有像我们这样脑洞大开用卷积神经网络的，而且就算是代码里包含循环和分支，卷积网络也可以给出相对较好（比随机和LSTM的baseline要好挺多）的预测。相比之下，大部分的文章还只是局限于直线式程序（straight-line program），即没有分支和循环的程序。&lt;/p&gt;&lt;p&gt;另外增强学习这个领域目前也是大红大紫。游戏的应用当然是重头戏，这次我们(&lt;a href="https://www.zhihu.com/people/5c2b06e8ddc61687e42a1a64fb72bbaf" data-hash="5c2b06e8ddc61687e42a1a64fb72bbaf" class="member_mention" data-hovercard="p$b$5c2b06e8ddc61687e42a1a64fb72bbaf" data-editable="true" data-title="@吴育昕"&gt;@吴育昕&lt;/a&gt;）的Doom这篇文章用了DeepMind那边最新的A3C训练再加上课程学习（Curriculum Training）的方法，训练出一个比较强的AI，这个AI读取最近的4帧游戏图像，及它们的中心图像（为了瞄准方便），和AI自己的游戏数据（血量和子弹数），然后输出AI应当做的六个动作（向前/左/右，左/右转，开火，注意这里没有后退的选项）。我们自己写了小地图，先在小地图上做了训练，训练时按照AI的成绩去逐渐调节敌人的强弱，以加速训练的进度，然后放进大地图里面再行训练。最后我们还使用了一些人工的规则，先领会AI的意图（比如说想右转），然后用VizDoom里面速度更快的命令取而代之。我们的AI参加了比赛，获得了Track1（已知地图）的冠军。我们没有像第二名那样使用游戏内部数据（比如说敌人是否出现在视野中）进行训练，这让框架通用性较高。当然因为时间所限，目前还没有在Track2（未知地图）上做训练和测试。 &lt;/p&gt;&lt;p&gt;而在Track2上拿了冠军的Intel团队的文章Learning to Act by Predicting the Future则是给定当前图像，当前的各游戏数据（血量，子弹数和分数）及提高这些数据的迫切程度的权值（Goal），对每个动作输出一个提高值f（比如说做这个动作之后，血量提高了多少，或者又杀死了几个敌人），然后用最高的提高值来选下一步动作。这个实际上是Q值网络的变种。他们生成了各种类型的地图做了训练，效果比DQN及A3C都要好些，而且因为迫切程度的权值是一个输入，所以这个模型具有在线改变目标的能力，比如说可以先让它去加血，加完了再去杀敌，当然这个高层逻辑目前还是需要人去控制。&lt;/p&gt;&lt;p&gt;另外增强学习这边还有一些文章值得关注的。比如说有关模拟学习（imitation learning）的一些工作（两篇以上，如Unsupervised Perceptual Rewards for Imitation Learning和Third Person Imitation Learning）。增强学习有个问题是奖励函数需要预先设计好，一般通过手工比较麻烦，这两篇模拟学习的核心思想是通过人类的演示来自动习得增强学习中的奖励函数。&lt;/p&gt;&lt;p&gt;理论这边目前还没有看到特别意思的文章。一般来说深度学习的理论相较实践而言，迭代速度要慢很多，主要是门槛比较高，也并非关注的焦点。我这次投了一篇理论，细节请见《&lt;a href="https://zhuanlan.zhihu.com/p/23454281" data-editable="true" data-title="长长的旅程"&gt;长长的旅程&lt;/a&gt;》。虽说理论比较冷门，我仍然相信对理论的深入了解是有长期的功用，会在将来指导实践。&lt;/p&gt;&lt;p&gt;目前先看到这里，下次再讲其它的内容。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23454387&amp;pixel&amp;useReferer"/&gt;</description><author>田渊栋</author><pubDate>Sun, 06 Nov 2016 09:09:18 GMT</pubDate></item><item><title>长长的旅程</title><link>https://zhuanlan.zhihu.com/p/23454281</link><description>&lt;p&gt;虽然深度学习人人都在喊人人都在用，但是它究竟为什么能有效果，到目前为止都没有初步的结论。具体来说，有两个理论问题没有解决，一个是泛化能力，另一个是收敛性。深度神经网络那么多参数，再配以能拟合世界上大部分函数的强大模型容量，按通常机器学习理论的说法，应该是很容易过拟合才对；但是试看目前的卷积神经网络，往往只要是训练集上有了效果，测试集上就不会太差。同样的参数下其它模型，比如说Random Forest和Adaboost，早就过拟合得不能看了。这是为什么呢？目前学术界没有统一的结论。退一步说，假设我们已经承认了深度神经网络对某个问题有效，就是说存在一个权值的解使得期望误差很小，但因为深度学习优化是个非凸问题，这个存在的解，能否通过梯度下降的优化算法得到，还是会陷入局部极小或者在鞍点停很久？这就牵涉到全局收敛性及收敛速度的问题。&lt;/p&gt;&lt;p&gt;这两个问题都很难很难，大家熟悉的工具在非线性传递函数及非凸问题面前似乎毫无用处，更不用说多层非线性网络带来的分析上的困难。为了能对付这个问题，需要去寻找更有效的数学工具，使用更创造性的分析方法。&lt;/p&gt;&lt;p&gt;我这次终于做了第一篇关于深度学习理论的文章，投给了ICLR（链接见&lt;a href="http://104.155.136.4:3000/pdf?id=Hk85q85ee" data-editable="true" data-title="这里"&gt;这里&lt;/a&gt;）。文章中首先对于单层ReLU下的梯度，在输入数据符合高斯分布的条件下给了一个闭式解，然后用它证明了在某些适当的初始条件下，优化含有ReLU隐层的二层网络也可以收敛到指定解，而不会掉进局部极小，用的方法是常微分方程的一些理论。这个结论很有意思，因为这类二层网络通常有指数量级的对称性，天生非凸，而要得到一个想要的解，就需要去研究不同初值下对称性破缺（Symmetry-Breaking）的结果。我在结果刚开始露头的时候和组里人讨论过，各位大佬们都觉得玩对称性破缺这种东西非常非常难，我也没指望自己再往下挖能做出些什么来，不过在努力下，居然就得到了一些意想不到的结果。&lt;/p&gt;&lt;p&gt;我想着做层次式模型（Hierarchical model）及深度学习的理论已经有一些年了，从2010年开始一篇有关图像对齐的非凸问题的全局最优解拿了CVPR的Oral，到PhD的中间几年开始想着怎么针对图像设计特定的层次式模型，之后在2013年，因为提出了CVPR那篇的层次式拓展，拿了ICCV13的Marr Prize Honorable Mention。后来2013年9月进了谷歌还在业余时间继续做（见&lt;a href="https://zhuanlan.zhihu.com/p/20606385" data-editable="true" data-title="文章收集"&gt;文章收集&lt;/a&gt;中的《业余做研究的心得》系列），险些投了一篇文章出去，可是因为无法将提出的理论框架和梯度下降算法结合起来而放弃；最后在2015年时跳到Facebook人工智能研究所，终于有一些可以专心思考的时间，但就算如此仍然到现在才有一些结果，投了第一篇有关深度学习理论的文章。&lt;/p&gt;&lt;p&gt;我有时候觉得，要是不去想这些核心问题，而只集中精力于深度学习的应用的话，大概还可以提高两三倍的效率，文章会变多，人也会轻松得多，何乐而不为？然而或许是想惯了这个问题，很难不去想现象背后的本质，结果背后的原因，就算是要忙其它的工作，自己有空时也在不停地思考。发《业余做研究的心得》系列，一方面是可以给大家分享一些心得体会，另一方面也是不停地拷问自己，心得写了很多，可我做出了成果没有？&lt;/p&gt;&lt;p&gt;大半年前还在做围棋的时候，有位前辈在开会时问我，找个数据集画个网络图训练模型大家都会，作为一个研究员，你的核心技能是什么？我当时无言以对，心里虽早有答案，可无法说出口。因为我知道，梦想在未成时一文不值。而让它变得有价值，是自己的责任。&lt;/p&gt;&lt;p&gt;现在回想起来，“失败是成功之母”并不对，“不历风雨如何见彩虹”也不对，因为喊着这些口号的时候，依然认为失败或者风雨是世上的稀罕事物，而成功则是要追求的目标。殊不知这些观念，正是阻碍前进的最大原因。当失败到习以为常，当每时每刻都在风雨中穿行，当不再存有失败的概念，而只留下不停尝试的好奇心和不停总结的习惯，成功才可能悄然现身。而伴随而来的，也不是那种梦寐以求的”我也终于牛了一次”的狂喜，而只是“啊，原来如此”的平静。&lt;/p&gt;&lt;p&gt;这篇文章，只是一个小小的开始；前面，还有很长很长的路要走。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23454281&amp;pixel&amp;useReferer"/&gt;</description><author>田渊栋</author><pubDate>Sun, 06 Nov 2016 06:53:14 GMT</pubDate></item><item><title>《机器之心》的采访</title><link>https://zhuanlan.zhihu.com/p/23071317</link><description>&lt;p&gt;应《机器之心》邀请，在半年前所作的采访。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;------------------------------------- &lt;/p&gt;&lt;p&gt;卡耐基梅隆大学机器人系博士、前谷歌无人驾驶汽车项目组研究员、现
Facebook 人工智能组研究员，多重身份的加持和前沿、专业的研究为田渊栋吸引了相当多的目光。&lt;/p&gt;&lt;p&gt;「我们要做的，不是成为高谈阔论的事前事后评论人，而是去当那一两个先行者。他们才是明白事实真相，才是真正改变历史轨迹的人。」&lt;/p&gt;&lt;p&gt;除了前沿研究与技术创新，他还保持了长期的写作习惯。除了早期的个人博客和现今的知乎专栏，田渊栋甚至还完成过一部超过
30 万字的小说，这在以理工科为代表的前沿科技领域是极为罕见的。&lt;/p&gt;&lt;p&gt;近期，机器之心对田渊栋进行了一次独家专访。关于人工智能、个人经历以及前沿技术研究的进展，田博士分享了诸多鲜为人知的故事和观点。&lt;/p&gt;&lt;h2&gt;&lt;u&gt;写小说的人工智能科学家&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;
机器之心：我们注意到您非常喜欢写作，以前也写过小说，这是您的业余爱好吗？&lt;/p&gt;&lt;p&gt;田渊栋：对，这个也算是我在硕士和博士期间的一个爱好，主要的成就是写过一部大概
30 万字的长篇小说，还有一些中篇和短篇 。当然，长篇小说毕竟读得人不多，后来就改成写博客，大家还是愿意看的。&lt;/p&gt;&lt;p&gt;机器之心：那您对文字的爱好，是因为受家庭影响吗？还是您从小就有偏向文科？&lt;/p&gt;&lt;p&gt;田渊栋：我以前受到高中班主任的影响，对历史有兴趣，愿意看些东西，当然写作还是高考作文这种水平。大概
06、07 年的时候，网络小说开始流行，和大家一样我也喜欢看。看多了，我这个人就喜欢动手，自己写写试试。一开始写的时候真的不好写，挤不出几个字来，写的是全是大段对话。但是慢慢就知道怎么写了，时间长了，越写越顺。&lt;/p&gt;&lt;p&gt;机器之心：都是科技题材的内容吗？&lt;/p&gt;&lt;p&gt;田渊栋：都有，玄幻加科技，就是大杂烩嘛。我不是商业写作，所以主角不打怪升级。主要是掺杂了一些个人经历，把自己想写的人物和事情写出来，小说嘛，题材其实无所谓，发生在火星上还是地球上都一样，但人物很重要，是灵魂。这部长篇小说写了五年，一开始是零零散散的写，然后串起来，最后集中精力花三个礼拜把它全部写完，现在回想起来，那段时间太有意思了。&lt;/p&gt;&lt;p&gt;机器之心：您这个领域跨度太大了。&lt;/p&gt;&lt;p&gt;田渊栋：没有，这个也就是业余兴趣，现在比较忙，以写博文和杂文为主。写小说这个经历对我的锻炼很大
，一方面在写人物的时候，要站在人物角度见他所见想他所想，要让人物活起来，这个对于习惯从自我出发的人来说是很好的历练；另一方面语感有很大提高，有了之前的积累，现在写杂文和博文，自然而然会觉得这个地方这么写，会让读者看得顺眼。&lt;/p&gt;&lt;p&gt;机器之心：那您现在的状态是以论文为主吗？&lt;/p&gt;&lt;p&gt;田渊栋：是的，学生时代相对来说空闲一点，也是积累和摸索阶段。现在是当打之年，当然是以论文为主，人生的好时光没有多少的。&lt;/p&gt;&lt;h2&gt;&lt;u&gt;从交大人工智能论坛版主到微软研究院&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;
机器之心：您提到过您对数理化全有兴趣，最后转到计算机。那您在本科的时候读什么专业呢？&lt;/p&gt;&lt;p&gt;田渊栋：我是计算机专业的。当时我进了交大的联读班，
一开始不分专业上基础课，比如说数学物理化学课，还有通信的相关课程 ，到两年之后再选专业
。现在我相信很多学校也开始做这方面的尝试了。比如说第一年不选专业，让你自己去选什么科。我觉得这样对于一个人的发展来说，特别是对学术有喜爱的人来说，是比较好的。&lt;/p&gt;&lt;p&gt;机器之心：您当年本科读完了，就到美国去读博士了？&lt;/p&gt;&lt;p&gt;田渊栋：我当时在交大读了研究生，然后再出国。那个时候我基本上花了一半时间在微软亚洲研究院。&lt;/p&gt;&lt;p&gt;机器之心：那个时候已经在做了？&lt;/p&gt;&lt;p&gt;田渊栋：对，在做这方面的。一开始是做人脸嘛，然后做一些比较广泛的图像识别，图像课程的一些问题，然后就申请了美国的博士。&lt;/p&gt;&lt;p&gt;机器之心：这个方向当时是您在交大的导师帮您选的，还是研究院的，还是您自己的兴趣？&lt;/p&gt;&lt;p&gt;田渊栋：我觉得我真的要感谢我在交大的导师张丽清教授，他给了我自由的发展空间。我说我要去微软亚研院实习半年，一般老师不会同意的。他说：“没关系，去吧”，非常支持。我在交大时做计算机视觉，研究院那边也是做图像识别的，具体来说是人脸识别。当时我想着能去研究院很好了，非常向往，做什么方向也无所谓
。&lt;/p&gt;&lt;p&gt;机器之心：那个时候机器学习有重视，但是没有现在这么热，是吧？&lt;/p&gt;&lt;p&gt;田渊栋：对，那个时候是这样的，学术归学术，
系统归系统，两边分开。机器学习的能力已经开始体现出来了，比如说在特定问题如人脸检测上有很好的解决方案；但是更复杂的物体检测则远远不及人的能力，大家都在讨论什么才是好的视觉表示。那一波其实持续了很长时间，从
01 年开始一直持续到大概 07-08 年 。那时我觉得机器学习有用，但没有像现在这样有广泛的应用。那时基本上是人工设计特征，再让计算机跑个线性模型就完事了。特征还是要人自己去找。 现在就完全不一样了，因为数据量大了，又有深度学习的框架，可以让计算机自己去学到好的特征，效果也好。&lt;/p&gt;&lt;p&gt;除了去亚研院之外，硕士阶段我主要在数学上打下了基础。我当上了交大BBS数学版版主，经常去回答板上提出的各种问题，不能回答的话就会去查资料。作为版主，回答不了问题是会有很遗憾的感觉的，这样就产生一种压力，通过这种方式，我强迫自己不断地学习。时间长了之后就慢慢习惯。另一方面我还开讨论班，我说我主动来讲机器学习和模式识别的一些数学模型，这样大家来听，我就得要准备，准备多了，基础就扎实了。研究生阶段还选了一些其它系的课，比如说广义相对论还有随机过程，一般人不会这么做，但我有兴趣。这样基础就打下了，以后看别的文献就会方便一点。&lt;/p&gt;&lt;p&gt;机器之心：听上去，好像您在学生时代的时候，就已经应该是交大学生团体里面的机器学习，人工智能的一个先锋人物了。&lt;/p&gt;&lt;p&gt;田渊栋：可以算吧。我那时还是人工智能版的版主。版上那时有很多非常有趣的讨论。当然那时候的讨论，现在看起来可能比较幼稚。不过既然是出于兴趣，也不怕人笑话。&lt;/p&gt;&lt;p&gt;机器之心：那个时候您比较确定自己会读这个方向，是吧？&lt;/p&gt;&lt;p&gt;受访者：至少确定将来会做人工智能这一块吧。有兴趣的原因是，我觉得很多问题没有解决。当时我在版里说，人工智能感觉上就像化学史上“燃素说”和“氧化说”争鸣时的状态，还没有系统性的理解，还在黎明前夜。大家现在都在那边低头调参数加特征，只知其然却不知所以然。将来肯定有很多理论框架，但是哪个是对的，现在毫无头绪。&lt;/p&gt;&lt;p&gt;这个就是机会。现在回过头来看，我想的是对的。&lt;/p&gt;&lt;h2&gt;&lt;u&gt;人工智能不该被过度炒作&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;
机器之心：那您觉得人工智能现在的状态呢？&lt;/p&gt;&lt;p&gt;田渊栋：还是那样，还是比较浅层的。当然我们现在有机器也有数据，效果肯定比以前好很多。但是理论这一块，现在还没有太大的突破。&lt;/p&gt;&lt;p&gt;机器之心：所以您专门写文章呼吁不要对人工智能过度炒作，目前理论上的挑战还是非常的艰巨。&lt;/p&gt;&lt;p&gt;田渊栋：对，还是有很多问题。当然了，还存在一种可能，现在机器多了数据多了，不用管理论，一路做应用做到底。在理论还没有掌握之前，应用已经超过人的水平，都是有可能的。&lt;/p&gt;&lt;p&gt;机器之心：那对围棋的研究，你还会继续下去吗？&lt;/p&gt;&lt;p&gt;田渊栋：这块我们还会再做一点，但是现在主要是开一些其他的方向。&lt;/p&gt;&lt;p&gt;机器之心：那你现在最主要的兴趣是在视觉和在语言处理这方面的这个方向吗？&lt;/p&gt;&lt;p&gt;田渊栋：这些方向都会有涉及。但现在时代不同了，不应该把自己限制在视觉或者某个特定方向。&lt;/p&gt;&lt;p&gt;机器之心：不是一个专门的应用。&lt;/p&gt;&lt;p&gt;田渊栋：对，因为感觉上自然语言处理、图像、语音，这些基本上都是应用了。所以说如果必要的话，其实可以在这个中间进行切换，或者做一些交叉的方向。以前做这三个方向，可能需要大量的领域知识，特别是做自然语言处理，要学以前语言学的文献。
要做分词，比如说每个词给一些词性。要做一些语法的分析、语素的分析，有很多很多的步骤。但现在的趋势是从头到尾都让机器学
。&lt;/p&gt;&lt;p&gt;机器之心：就是他们说的
end-to-end。&lt;/p&gt;&lt;p&gt;田渊栋：是的，end-to-end
端对端的学习。比如说自然语言这一块，并没有比以前的效果好太多，但整个流程变得很简单方便，将来进步的速度可能就会变快。比如机器翻译里面，你把一个句子，直接通过神经网络翻译成另外一个语言的句子，这样就比以前快。以前可能要分词呀，词性标注呀，对每个词找到另外一个语言对应的词或者词组，找到之后再重新排列一下，最后才产生一个句子。要通过几个步骤，但是现在在概念上，只要一步就算出来。【注：现在基于神经网络的翻译系统确实比以前好很多了 】。&lt;/p&gt;&lt;p&gt;为什么会出现端对端呢？我觉得主要是因为神经网络这个模型的优点。神经网络模型是个非常灵活可扩充的模型，随便连一下，然后做后向传递就可以了。大家一开始没有意识到它的厉害， 觉得做这个系统得要分几步吧，神经网络只是其中一步，前面和后面还是通过传统方式来做比较安全。后来大家就慢慢意识到，为什么不用神经网络把整个系统打通？那样的话，又省时效果也会更好。自然而然，大家都会思考端对端的思路。我觉得现在基本上端对端的效果，主要体现在整个迭代的速度上，从设计模型到训练，到看到结果，到修改模型这样一个循环的速度会很快，
效果也通常会变得更好。人优化参数的时候，可能半小时优化一次，看看结果如何；机器优化参数，可能一秒就优化几百次。所以这个时间的改进是数量级上的改进。&lt;/p&gt;&lt;p&gt;数据集的获得，现在主要是在网上花钱，人工标注。比如说一张图几块钱的，然后让人去做。就发动群众的力量嘛，看大家有没有空。有空闲着无聊了，就标注两张。这样把力量汇集起来。&lt;/p&gt;&lt;p&gt;机器之心：像您在
Facebook 做的，Facebook 有那么多的图片，然后底下还会有人可能对这个 pictures 做一个评论。&lt;/p&gt;&lt;p&gt;田渊栋：对。&lt;/p&gt;&lt;p&gt;机器之心：那这种东西，你们把它拿来用吗？&lt;/p&gt;&lt;p&gt;田渊栋：这个是有用的，但是具体怎么用，我们现在还在商讨中。&lt;/p&gt;&lt;p&gt;机器之心：因为它没有那么准确？&lt;/p&gt;&lt;p&gt;田渊栋：是的。而且大量的图片，下面的评论可能是杂的、乱的。比如说我们所有的话，下面都可以写一个赞呀。这个评论，其实跟这张图没有关系。&lt;/p&gt;&lt;p&gt;机器之心：需要比如说去噪音这种方式去解决。&lt;/p&gt;&lt;p&gt;田渊栋：对，可能有多少话，一开始说得跟图片有关。但是后面说两句，说到某个人身上，扯远了，离题了，这句话就跟这张图没有关系了，所以这个其实都很难，现在还没有办法做，还需要好好研究。&lt;/p&gt;&lt;p&gt;机器之心：我记得前一阵华为他们那边做了一个小对话的系统，然后进行了简单的归纳。它用的数据其实在微博上取下来的，但是它那个数据像您说的也很乱，它会有一些规则。比如说第多少条回复以后，肯定就绕得不知道哪儿去了，肯定不能要了，还有常见的一些感叹的词语。&lt;/p&gt;&lt;p&gt;受访者：对，肯定不一样。相对来说，你可能需要把剩下的句子提关键词。然后把关键词作为这个图的标注，这是一种方法。或者做一些简单的语音分析。&lt;/p&gt;&lt;p&gt;机器之心：它还有一点，田博士您看到一张图，我们人可以标注它。但是实际上这张图有好多种标注方式而且都是准确的，因为看的角度不同。&lt;/p&gt;&lt;p&gt;田渊栋：对。&lt;/p&gt;&lt;p&gt;机器之心：那在这种标注数据拿给你的时候，一个图会给你多少种标注呢？&lt;/p&gt;&lt;p&gt;田渊栋：这个其实不同领域，有不同的方法。比如说问答系统，可能有一个问题有一个回答。问题不同，回答又不同。所以一张图里面有三个问题，那么就有三个回答。或者一张图有三个问题，有三十个回答。每十个回答对应于一个问题，这是可以的。然后你有这些数据之后，你想办法找到一个比较好的模型去归纳这些数据，这是一种。&lt;/p&gt;&lt;p&gt;比如说还有一些，一张图有几千个标注。一张图里面你可以标注很多属性，里面有猫，有人，有天空，有大地，可能是外景，或者可能是晚上。像这种，每张图上有很多属性，这种也可以拿来的。这种不同的标注方法，目标是不一样的。比如说你做问答系统的话，问题和回答必须成对出现的。&lt;/p&gt;&lt;p&gt;因为这个回答非常依赖问题，如果你没看见图，只看到问题。然后回答的话，其实正确率挺高的，因为可以猜出来。所以你就会发现在不同的情况下，需要的标注是不一样的。&lt;/p&gt;&lt;p&gt;机器之心：这种数据上的处理，不仅需要强有力的技术，还需要更多的思考。&lt;/p&gt;&lt;p&gt;田渊栋：对，所以这一块的思考需要很大很大的力气。有可能一个数据做得不好的话，它的标注出了问题，或者它的采集过程出了问题，就不能用了。现在有很多机构都在做数据集，想办法通过数据来取得进步。做完数据处理之后，大家都会有一个客观的标准来评判他的算法怎么样。然后在数据上提高自己的算法性能，从而达成整个领域的提升。
通过衡量数据上的表现， 来衡量整个领域的进展。&lt;/p&gt;&lt;p&gt;机器之心：那这个趋势看来也是一种需要了，在学术圈，包括像公司这样的级别，尽量去制造好的学习数据，可能会在深度学习这一块取得极大的突破。&lt;/p&gt;&lt;p&gt;田渊栋：这是一个方面，另外一个方面在算法这一块，我们希望深度学习用更少的数据达到相同的效果。这两方面都在做的。&lt;/p&gt;&lt;p&gt;机器之心：小数据这件事，大家很关注。你觉得现在有什么突破口，或者什么思考方法？&lt;/p&gt;&lt;p&gt;田渊栋：现在主要做的是：你先在大数据上，训练一个模型。然后在小数据集上做微调。这样的话，你所要学习的权值数目就变少了。如果这两个问题本身也有相关性，这样就比较容易。或者你把少部分具有足够的健壮性的数据，加上大量的弱标注的数据放在一起训练，这样也是可以的。或者把小数据通过增广变成大数据，比如说旋转缩放图像，里面的物体属性标注保持不变。这样的话，数据增加了对模型的训练过程会有好处。&lt;/p&gt;&lt;p&gt;当然，这些都是权宜之计。真正要解决这个问题的话，需要对深度学习的机制要有很明白清晰的了解。这个很难，还没有办法做出来，大家还在做。之前我去清华做演讲的时候，跟姚教授也在聊，他也觉得这是非常难的问题。&lt;/p&gt;&lt;p&gt;机器之心：他们现在也在关注？&lt;/p&gt;&lt;p&gt;田渊栋：对，他们现在也关注这个。神经网络的训练是一个非凸的优化问题，目前传统的方法没有办法解决它。没有对它的本质理解， 可能没有办法真正解决神经网络训练过程中的疑难杂症。&lt;/p&gt;&lt;p&gt;机器之心：说到这个，有一个大家都在讨论的问题，就是神经网络它的高效性，有一点像黑箱子，里面真正的数学原理大家还不是很清楚。&lt;/p&gt;&lt;p&gt;田渊栋：对。&lt;/p&gt;&lt;p&gt;机器之心：那您对这方面的研究感兴趣吗？&lt;/p&gt;&lt;p&gt;田渊栋：这方面跟我的博士论文是很有关系的。虽然说大家可能因为围棋的工作认识了我，但是我在博士阶段是做理论的，研究如何获得非凸问题的最优解。一般情况下这个问题做不了，但在某些特定情况下是有可能的。我当时做的是如何对齐两张扭曲的图像。对齐是非凸的，局部最小值的分布和图像内容有关，图像里有重复结构，比如说一栋建筑物里有很多窗，那么就对应非常多的局部最小值。那么这个怎么办呢？一种方法是说我们干脆不优化了，就直接把图像用各种已知的扭曲参数生成出来，存到数据库里。然后新的扭曲图像拿进来之后，我就查那个数据库，就可以得到我想要知道的参数。但是这个办法的缺点是需要要非常多的数据，才能够保证得到的参数是准确的。另外一个方法就是传统优化算法，不管它是不是非凸的，我们用梯度下降迭代，但这样可能会陷入局部最小值。我发现了一个折中的方案，结合迭代算法和数据，做一个数据驱动的迭代算法，这种情况下，可以证明用更少的样本达到全局最优解。其中原因就是这个特定的非凸问题有一些特殊的群结构。这就是我博士毕业论文的主要工作。&lt;/p&gt;&lt;p&gt;我当然希望在深度学习上也能看到一些特殊结构，从而揭示它的秘密。但这个仍在探索中。&lt;/p&gt;&lt;h2&gt;&lt;u&gt;沟通和交流能力&lt;/u&gt;&lt;u&gt;是研究的&lt;/u&gt;&lt;u&gt;重要组成部分&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;
机器之心：您的导师对您的影响好像很大，特别是在写作方面。&lt;/p&gt;&lt;p&gt;田渊栋：他的写作和演讲的技巧是很好的。我觉得他对我在博士期间的成长是非常有帮助的。&lt;/p&gt;&lt;p&gt;机器之心：就是他的沟通、交流能力好像很强。&lt;/p&gt;&lt;p&gt;田渊栋：对，他是印度人嘛，大家都知道印度人这方面的能力比较强，他就是这样的。其实我之前是比较内向的，可能大家都有这种刻板印象，认为中国好学生比较安静一点，不愿意说话，比较内向。但是你出国了之后，你会觉得这两个属性不是连在一起的。你可以成为一个好学生，然后你也可以愿意和别人聊天。这些都可以做到的，都可以培养的。比如说一开始上台演讲的时候，有一种恶性循环，上台不知道怎么说，不敢上台。你上台又不知道怎么说，又不敢上台，那你到时候就不敢上台了。一开始要打破这个恶性循环，就要准备非常丰富的，非常好的演讲。第一个演讲说好了，觉得自己有信心了之后，再往上走，就一点点变得非常非常自然了。&lt;/p&gt;&lt;p&gt;机器之心：对，对交流还是有很多的思考。那我个人有一个感触，不知道您是不是认同。就是中国的理科方面的学生，如果有一些追求的话，一定要对语言非常深的深钻。尤其是英语，我觉得国内好像对这个重视不太够。大家好像觉得，大家说论文的英语语言本身不是很复杂。但是我觉得真的，也提到您刚才一个话题。可能你整个思维方式的形成，不仅仅是通过读论文，可能是通过读专著呀，通过读科普的著作呀，跟其他英语的
speaker 进行交流，学术沟通呀。这里面实际上要求你非常强的语言能力，我感觉到您好像是咱们国内华裔学生里面，对这个是有足够重视的。好像有一些学者，尤其在国内没有国外留学经验的人，不知道这个东西价值有多大。&lt;/p&gt;&lt;p&gt;田渊栋：英语只是特定语言，我指的是表达和交流能力，这个价值是非常非常大。中国有句老话叫“酒香不怕巷子深”，其实在现代社会不完全是这样。越是好的东西，越是要说出来，一定要广播，要想办法让大家都知道，才能让别人欣赏你。每年投稿在各大杂志和会议上的文章，基本上以千为单位了，加在一起肯定要上万了。你的文章能否脱颖而出，是一个很大的问题。当然了，如果你做了一个世界上没有人做出来的问题，或者你的效果比别人好太多，那不必多言，大家都觉得你非常厉害对吧。但是很多情况下，你的工作并不能达到世界第一，也有很多工作是分析现有问题，或者表达一个新的思路或者观点，不是硬拼性能的。像这种文章就要靠说了，要靠组织和表达清楚的语言，不然的话，别人看了一头雾水不知道你说什么。&lt;/p&gt;&lt;p&gt;国外有好多的教授，其实这方面的功底是非常深的。比如说咱们CMU有个教授，一篇文章开篇引了福尔摩斯的话。福尔摩斯说：「没有数据支持的任何推理，都是不成立的。」然后他就举例说明数据的重要性。这样的文章，不一定有算法上的贡献，但是他们对别人思维的改变，其实起很大的作用，让别人觉得他这样的思路可能是对的，从而改变自己整个的研究路线。我刚去的时候不适应，觉得这种软文有什么好看的，只会用个最近邻方法，一点技术含量也没有；现在发现这不是吹牛，是对大方向的重要把握。现在深度学习来了，数据更多了，他在文章中提倡的，完全是符合潮流的。
&lt;/p&gt;&lt;p&gt;机器之心：您现在做研究的时候思考，用英语在做吗？&lt;/p&gt;&lt;p&gt;田渊栋：思考并不是依赖于某种语言。英语用得多些，因为这个领域中文有很多词可能还是得翻成英语。&lt;/p&gt;&lt;p&gt;机器之心：甚至超越语言的一种。&lt;/p&gt;&lt;p&gt;田渊栋：对，图像呀，或者一种内在的东西。然后你想到了之后，通过你内部的思考表达出来。&lt;/p&gt;&lt;p&gt;机器之心：可能这是一种，有点像神经网络，它是跨越语言的。&lt;/p&gt;&lt;p&gt;田渊栋：对，拿神经网络的术语来说，它们都映射到同样一个内部表示，然后再翻译过去。&lt;/p&gt;&lt;h2&gt;&lt;u&gt;从 Google 到
Facebook 的身份转变&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;
机器之心：当时田博士您在谷歌无人驾驶的项目里面做过一段时间，后来转到 Facebook。实际上在很多人眼里，谷歌已经是天堂般研究这样一个地方。您怎么会转到
Facebook？&lt;/p&gt;&lt;p&gt;田渊栋：我觉得主要还是因为谷歌是一个比较大的公司嘛。并不是说谷歌每个人都可以做你想做的事情。要看你在哪个组，你是什么地位，你做什么样的方向。大公司有一个问题，去得晚的话，你可能只能做螺丝钉。&lt;/p&gt;&lt;p&gt;机器之心：有一点排资论辈的感觉吗？&lt;/p&gt;&lt;p&gt;田渊栋：其实谷歌已经非常不排资论辈了，已经很开放了，但是还是会存在这样的问题。因为没有办法，无人车已经做了很多年了嘛。你进去之后，东西都做好了，只要修补就行了，你想要搞些有趣的，条件不允许。一开始觉得挺有意思的，但是时间长了，你会觉得没意思。而且还有一个问题，无人车比较保密，想要发表自己的工作就很难。&lt;/p&gt;&lt;p&gt;机器之心：有点受限制。&lt;/p&gt;&lt;p&gt;田渊栋：对，我又是一个比较喜欢写博客的人，你让我这个话不能写，那个话不能写，那怎么写呢。我之前写过一篇有关无人车的博文，不过那篇博文没有涉及到任何细节。后来还是觉得Facebook
相对来说更公开一点，所以就跳走了。&lt;/p&gt;&lt;p&gt;机器之心：您觉得
Facebook 的企业文化有哪些非常值得我们国内企业学习的地方？&lt;/p&gt;&lt;p&gt;田渊栋：我觉得它就比较扁平嘛，小扎就坐在我后面6、7
米的地方，6、7 米都没有，基本上我后面是一个很大的区域。就是 CEO、CTO 还有COO都在后面坐着嘛。去年我的实习生周博磊还被COO雪莉点到了，雪莉带着访问者问他在做什么工作，他回答得非常好。感觉上高层都对人工智能很感兴趣。Facebook总的来说就是比较开放的环境，很多时候比较随意吧，没有那么严格的上下级。在
Facebook 里面，你也看不见别人的级别。相对来说人和人之间平等一点。&lt;/p&gt;&lt;p&gt;机器之心：那你们这个深度学习研究人员和公司的高层坐得这么近，是不是因为高层也是把你们最重视的一个。&lt;/p&gt;&lt;p&gt;田渊栋：有可能是吧，但是这个我也不好说。我觉得这个是公司的安排嘛，所以我不会有什么特别的评论。&lt;/p&gt;&lt;p&gt;机器之心：那他有没有时候会主动地过来，问问你现在在忙什么呀？&lt;/p&gt;&lt;p&gt;田渊栋：他还是比较忙的。每个人都有自己的职责嘛。&lt;/p&gt;&lt;p&gt;机器之心：LeCun
是这个领域的元老，离 Zuckerberg 比较近，就想说有没有从他身上得到一些启发。&lt;/p&gt;&lt;p&gt;田渊栋：LeCun
是一个非常开放的一个人。感觉我们整个组非常民主，你想做什么都可以。如果你愿意做的话，也没有人管你。&lt;/p&gt;&lt;p&gt;机器之心：方向上给你很大的支持。&lt;/p&gt;&lt;p&gt;田渊栋：就像为什么会做围棋嘛，对吧，就是很奇怪的。&lt;/p&gt;&lt;p&gt;机器之心：这个是你自己选的吗？&lt;/p&gt;&lt;p&gt;田渊栋：自己选的，围棋是自己选的，然后一开始数据集呀，整个东西都是我和实习生自己弄的，然后DarkForest的名字也是我自己起的。这个名字比较酷。我们组也是比较开放的，都没有管，说你这个名字一定要跟
Facebook 有关。当时也没有多少人看好这个方向，只是一个试验。&lt;/p&gt;&lt;p&gt;机器之心：所以你们的研究不需要直接跟公司的业务产品相关吗？&lt;/p&gt;&lt;p&gt;田渊栋：能有产品的话，那当然最好，但是还是以研究为主。而且这次我选围棋也证明了眼光是对的。我当时对它有兴趣，是因为看到了两篇文章，当时大家都没有引起重视，就只有圈里人知道。我看了一下觉得这个东西有点意思。&lt;/p&gt;&lt;p&gt;机器之心：就是你的文章可以引用的地方。&lt;/p&gt;&lt;p&gt;田渊栋：对，我觉得这个方向，将来会有一些突破。&lt;/p&gt;&lt;p&gt;机器之心：那当时你意识到他们进度会这么快吗？&lt;/p&gt;&lt;p&gt;田渊栋：当然没有意识到那么快，只是觉得这个方向可能有前景。当时还做了还挺多项目的，没有吊在一棵树上。做研究的风险都很高，所以你必须分几个不同的项目同时做，看哪个项目比较好。这个围棋项目它的效果是不错的，那么就花时间在上面。&lt;/p&gt;&lt;p&gt;机器之心：那除了给你们很多的自由度之外，你觉得他（LeCun）给你最大的帮助是什么呢？或者是收获？&lt;/p&gt;&lt;p&gt;田渊栋：他会有一些比较大的想法和观点分享给大家。比如说他觉得对抗式学习是一个比较重要的方向。他会经常说嘛，让大家觉得这个东西挺重要，这么做可能是有道理的。用这种方式来影响大家。不过他也没有说一定要做这个，一定要做那个，没有。他是个比较宽厚的长者，和大家聊聊。&lt;/p&gt;&lt;p&gt;机器之心：经常会跟你们沟通吗？&lt;/p&gt;&lt;p&gt;田渊栋：还比较多。&lt;/p&gt;&lt;p&gt;机器之心：那你们内部有各种，像研究人员之间的交流，小组这种讨论吗？&lt;/p&gt;&lt;p&gt;田渊栋：你想要研究，想交流很容易嘛，因为大家都坐很近。你可以发个信息过去。或者说直接到他座位上随便聊聊，大家讨论一下。这个还挺重要的，特别是你要做别的方向自己不熟悉的话。你一个做图像的人，突然去做自然语言这一块，那么你对自然语言理解这一块的文献，肯定不那么熟悉。你问别人一个想法，别人会告诉你这个东西做过了。这样的话，你可以慢慢知道这个方向，它的现状怎么样，然后接下来要怎么做，什么地方做过，有哪些地方还没有做过。通过这样的交流方式，你会很快的知道什么东西是应该做的。研究这一块，对方向的确定是很重要的。&lt;/p&gt;&lt;p&gt;机器之心：我看您对研究的方法论，自己非常有成熟的一个看法。&lt;/p&gt;&lt;p&gt;田渊栋：这个也是慢慢总结出来的，碰过钉子嘛，很多时候你都知道了。&lt;/p&gt;&lt;p&gt;机器之心：我们看过一篇文章，就是
LeCun 接受采访的时候，说了一个最不喜欢的对深度学习的描述，就是它像大脑一样的过程。后来记者让他能不能用 8 个单词去描绘一下，然后就想说您能不能用简单的一句话去描述一下深度学习？&lt;/p&gt;&lt;p&gt;受访者：我觉得就是神经网络嘛，现在目前为止还是神经网络为主。就是通过神经网络的多层处理，把数据从一开始的红蓝绿这种非常简单的特征，通过一点点的自组织，变成比较复杂的特征，就是这样一个过程。当然这个想法老早老早就有了，只是最近才在实际数据集上产生了很好的效果，受到了大家的关注。&lt;/p&gt;&lt;p&gt;另外，计算上的神经网络和生物上的神经网络其实没有太大的关系，神经网络里的节点只是对神经元做了最简单的抽象。其实神经元结构太复杂了，一个含各种参数的微分方程，要能快速模拟上亿的神经元，代价很大；另一方面，就算模拟出来效果好，也不知道是哪个原因导致的，反而会拖累对本质的理解。&lt;/p&gt;&lt;p&gt;机器之心：我看您博客里面提到科技树这样一个概念。能不能以科技树的形式给大家梳理一下人工智能，或者图像识别这样一个大体的框架。&lt;/p&gt;&lt;p&gt;田渊栋：我在博客里写的科技树，是一个比方。你看科技树的发展，一开始枝繁叶茂，大家都觉得很有希望，可是发展一会儿就停下来了。等大家没兴趣的时候，过了几年，在某个很不起眼的地方，突然就出现一个突破。所以做一个研究员嘛，最重要的是要&lt;b&gt;于&lt;/b&gt;&lt;b&gt;无声处听惊雷&lt;/b&gt;，就是不能人云亦云，要静下心来找到别人没看见的方向，然后把它挖深，证明这个方向是有效的。一旦大家都觉得这个方向对，大家冲过来接你的棒了，你就是成功的。&lt;/p&gt;&lt;h2&gt;&lt;u&gt;关于未来人工智能行业的一些思考&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;
机器之心：未来深度学习，包括整个人工智能面临的一个挑战，就是非监督系统学习。现在做得最好的监督系统学习，有些人觉得稍微过度，您是认可的吗？&lt;/p&gt;&lt;p&gt;田渊栋：对，这个话我觉得是有道理的。监督学习相对成熟些，但是需要大量的样本，往往是样本翻倍，性能才涨一点点。很多时候，对一个系统而言，光用样本把它的性能提上去就很难。非监督学习要是效果好了，对样本的需求就会少很多。比如说吧，我本来可以对围棋的每一步做一个标签，这步是好棋，这步是臭棋。但是也可以给最后输赢的结果，把这个结果反向传递回去，让算法自己发现哪一步是好棋，哪一步是臭棋。那这样的话，你输入信号变少了，就是一种半监督学习的方法；另一方面，机器也就有了超过人类的潜力。&lt;/p&gt;&lt;p&gt;机器之心：那这种东西也是你们在
Facebook 关注的吗？&lt;/p&gt;&lt;p&gt;田渊栋：我还是比较关注的。&lt;/p&gt;&lt;p&gt;机器之心：那你会花一些时间专门攻这方面的研究吗？&lt;/p&gt;&lt;p&gt;田渊栋：肯定会看一些文章嘛，然后看看有什么东西可以做的。&lt;/p&gt;&lt;p&gt;机器之心：会有一些
paper 出来吗？有一些计划吗？&lt;/p&gt;&lt;p&gt;田渊栋：现在在做，但是能不能出文章，这个不知道，肯定是要边做边看看有什么有趣的。一开始你不熟悉这个领域，你肯定先看文章，然后再选题，然后再看有什么东西可以做。你看多了之后，会慢慢的发现，噢，原来这个有问题。 &lt;/p&gt;&lt;p&gt;机器之心：去年有一篇论文【Human-level concept learning through probabilistic program
induction&lt;/p&gt;&lt;p&gt;】讲到小数据集做得比较好的，甚至它自己在个别的案例上已经超越了（深度学习）。您是怎么评价他这种研究方式和思维方式？&lt;/p&gt;&lt;p&gt;田渊栋：他那篇文章用图模型做One-shot learning，和深度学习作了对比，在生成手写字母这个任务上，在小数据集上比深度学习要好。图模型在推理上比较自然，解释性也比较强，这个是大家公认的。但是相伴地就有另一个问题，就是说设计的模型一定要对，像他写字的模型可以设计得正确，但是对于复杂的真实世界，建一个包罗万象的模型就很困难，未必有深度学习的能力强，计算机视觉这个领域，大家都做了二三十年的模型了，结果还是被卷积神经网络超过了。所以说两者现在各有所长，深度学习长于感知，图模型长于推理，如果我们能把它们连起来会是个很大的突破，是值得我们去发现的。&lt;/p&gt;&lt;p&gt;机器之心：所以您也比较看好这个方向，是吧？&lt;/p&gt;&lt;p&gt;田渊栋：图模型和深度学习如果能够很深地结合起来的话，会是一个很好的方向，现在还是比较浅。&lt;/p&gt;&lt;p&gt;机器之心：所以您自己在这方面愿意做一些探索？&lt;/p&gt;&lt;p&gt;田渊栋：都会看，我肯定不会现在下定论。先了解一下，一点点了解完之后。发现这个有意思，然后再去做。很多时候研究员做的事情，是介于了解、探索、研究之间的。你不知道在看这篇文章的时候，是为了做这个方向呢，还是属于好奇呢，还是审稿呢。所以很多时候你无法界定自己的工作。文章看多了，自然会有一些想法，如果想法有意思，就愿意花时间在上面，然后你就变成从事这个研究方向的人。做研究不像通常的工作，有个老板和你说具体要做什么。可能今天看文章，明天推公式，后天写程序，大后天发现全错了从头再来，自己得分配时间，得要找找准方向。所以啊，这个都不好说。&lt;/p&gt;&lt;p&gt;机器之心：之前有一些深度学习比较小的突破，像注意力模型呀，记忆模型呀，还有深度神经网络简单通俗地解释一下，给一些对这个不太专业的读者，或者做一个形象的比喻。&lt;/p&gt;&lt;p&gt;田渊栋：这个你看一些文章就可以了，很容易懂的。 比如说注意力模型吧， 看一张图，先看左边，再看右边，最后得出图里有什么的结论，和人的行为一样。听起来很有道理吧，但是实际上训练完，往往计算机看一眼就知道图里有什么，猜功太好，让它多看几眼没什么用，可有些情况下又是有用的。所以说实际机制未必和文章描述得一样。
&lt;/p&gt;&lt;p&gt;机器之心：刚才有一点涉及到，正好田博士对物理也非常的有了解。他刚才说很多非常非常多经典的东西，其实当时田博士您记得从经典物理过渡到量子物理，几个地方都在开花。像波粒、活动方程呀，这些东西都在。然后促使了这个量子力学突飞猛进的进展。&lt;/p&gt;&lt;p&gt;田渊栋：对。&lt;/p&gt;&lt;p&gt;机器之心：您觉得现在深度学习的状态和当时从经典力学过度到量子力学那种，比较非常大的状态，能是一种状态吗？还是您个人认为深度学习还是比较平稳，比较缓慢的发展。因为现在媒体对这个的炒作也很热，好像有一点新的科技时代的降临。&lt;/p&gt;&lt;p&gt;田渊栋：不好说吧。量子物理怎么建立的呢？二十世纪初的时候，一个很大的问题是如何建模黑体辐射，一个东西加热到一定温度，会发出什么频率的光。物理学家们提出两种模型，各对了一半，就是拼不起来。然后大家深挖下去，作了夸张的假设，找到量子的方法去解释。相对论也是一样的，一开始大家用以太去解释光速不变，被干涉实验推翻，后来找到狭义相对论，认识到洛伦兹变换是绝对的。这两个都推翻了经典的直觉假设，刷新了大家对世界的认知。对物理来说，从不承认这些假设，到承认这些假设，是一个大突破；认知刷新，是一个大突破。 &lt;/p&gt;&lt;p&gt;我们现在不一样，是工科不是理科，更多是一种经验的东西，也更看重经验的结果。比如说吧，因为数据集不同，模型不同，经验的结果往往是模糊的，渐近的，慢慢地大家意识到这样是对的。这就不像物理学有个明确的分界线，控制完变量后，一个假设一个公式把现象阐述得很清楚，一个实验对不对，改变整个认知，然后宣告胜利。另一方面，你可能对深度学习的认知有突破了，但那时系统性能已经超越人类了，没有人在意。这两点都会让圈外人觉得发展相对平缓，没有像物理学这样的。当然，从人工设计特征到让机器自动发现特征，这是一个比较大的认识上的突破。但是就算如此，大家好像也没有把它当成是革命，而只是默默地记下了继续往前走。也许以后历史学家们会记录成突破吧，就像我们看二十世纪初那样；但是目前看来，身在局中的我们，并不一定会感觉得到，所以大家也不要期望太高嘛。&lt;/p&gt;&lt;p&gt;机器之心：发生得太静悄悄了。&lt;/p&gt;&lt;p&gt;田渊栋：对，有可能某个人某一天宣布，深度学习是这样起作用的，认识上有了突破。然后圈外人觉得，我已经用上了，用上语音识别了，用上图像理解了，用上问答系统了，没有人管了。对他们来说，是一个很平稳的过渡嘛——软件变得越来越牛了。所以这个不像是物理，这个不一样的。物理那边，非常看重对事物的深刻理解。物理是理科，它的目标是发现。为了更新的发现，全世界可以砸钱下去不求回报。而我们这边，总的目标是做一个很好的系统给大家用，AlphaGo战胜了李世石，大家把它当大新闻，就算世界上没人知道 AlphaGo是如何算出好招的，也没有关系，没人管。当然，我个人非常喜欢好的理论，如果对深度学习有一个非常好的突破性理解的话，我会非常非常开心。虽然难，但我相信它迟早会发生的。&lt;/p&gt;&lt;p&gt;机器之心：明白，因为提到一个人工智能进展的问题。您之前写文章，提出大家不要对人工智能有过度的热捧。就是说您觉得现在发展的，它现在最大的瓶颈是什么？&lt;/p&gt;&lt;p&gt;田渊栋：有很多，比如说小数据，非监督学习，比如说对整个深度学习的原理不理解，大家现在就是摸瞎调参数，看怎么样。没有对这个模型有本质的理解，这个其实是一个比较大的问题，这个是需要突破的。我之前说了嘛，这个突破可能对大众来说没有太大的意义，大家都觉得用上了，就用上了。&lt;/p&gt;&lt;p&gt;机器之心：如果我们接下来要在这个无监督学习方面实现一些突破的话，有没有哪些您认为比较好的路径？比如说您刚才说的深度学习和图模型的结合。&lt;/p&gt;&lt;p&gt;田渊栋：对。&lt;/p&gt;&lt;p&gt;机器之心：我们有注意到您之前开发过图像的大系统。&lt;/p&gt;&lt;p&gt;田渊栋：对。&lt;/p&gt;&lt;p&gt;机器之心：我们可能把它看作是图像和自然语言处理的结合，它们这个结合的时候，它的重点在哪个地方？怎么给它结合在一起？&lt;/p&gt;&lt;p&gt;田渊栋：现在还是比较浅的结合，把两边的特征连在一起，或者放进模型里面混合下，就完事了。更深的结合现在还在研究中。&lt;/p&gt;&lt;p&gt;机器之心：那您在这个图像和自然语言处理结合的点，是不是有写论文的计划？&lt;/p&gt;&lt;p&gt;田渊栋：现在在做，但是还早，可以回答一些问题，刚用的人可能会觉得很惊艳，但是用多了就知道它弱在哪里，离真正能用还早。&lt;/p&gt;&lt;p&gt;机器之心：您怎么看待以对话引擎切入的工具，它是不是会取代我们的
App？&lt;/p&gt;&lt;p&gt;田渊栋：这个我也不好评论，我觉得挺好的，可能是一个很好的入口吧，通过更自然的方式来跟别人交流。&lt;/p&gt;&lt;p&gt;机器之心：我们之前看到一篇文章，是科技公司对人才的激烈争夺，您是如何看待这个现象？&lt;/p&gt;&lt;p&gt;田渊栋：我觉得这个对我们来说是好事，对吧，工资肯定会提高。另一方面，这也表明现在人才越来越重要了，以后人工智能能够自动化很多事情，有这方面的人才，能把人工智能运用得好，几个人的小公司能做到跟以前大公司一样，甚至超越，这都是有可能的。技术越发达，可能最后的效果就越好，以一当千当万，都不是天方夜谭。&lt;/p&gt;&lt;p&gt;机器之心：刚才我们聊的有监督学习、强化学习，最后到无监督学习。如果这个过程发展得很顺利的话，我们能够期待这个系统或者机器，能够做一些那些我们现在还不到的事情？&lt;/p&gt;&lt;p&gt;田渊栋：如果这些阶段都能做完的话，那基本上就差不多了。因为人也从无监督中学习，一个婴儿通过有限的监督学习慢慢学到很多技能，对吧。这几块如果能做出来的话，确实会有很大的突破。人脑的核心技术肯定是大大领先现在人类掌握的核心技术，但是工程上仍然有很多可以改进的地方，你要相信进化出来的东西，它是会有很多缺陷的。我们现在就像是原始人去研究一辆二战坦克，怎么看都觉得科技逆天；但等到了我们会造坦克了，改进的路子马上就会想到的。&lt;/p&gt;&lt;h2&gt;&lt;u&gt;关于国内人工智能的发展&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;
机器之心：国内的研究水准，还有国内整个产业环境都不如美国，那您觉得中国有很大机会可以是人工智能存在的地方吗？还是我们只能做一个舶来品拿来应用，您是怎么看待的？&lt;/p&gt;&lt;p&gt;田渊栋：这个问题太大了，说实在的，我也不是太了解国内的很多情况。所以我也不好说，首先第一肯定咱们中国人是非常聪明的。我觉得大家如果有信心，有恒心的话，确实能够做到很好的水平。&lt;/p&gt;&lt;p&gt;机器之心：那像您在清华，还有在交大，您的同行在沟通的时候。您觉得他跟在美国这个领域同行沟通的时候，还是有明显的差距吗？&lt;/p&gt;&lt;p&gt;田渊栋：可能还有相当的差距。这次围棋大热，国内有一些像刘知青教授他们在做。但是除此之外，也没有太多的人在做这个东西。另外围棋本身有深厚的人文背景，两个因素综合起来，问一些比较宽泛的问题，也很正常。所以我想这次回来一方面是探亲，另一方面做一些报告给大家科普一下。我觉得我有资格去说这个东西，因为我正在做。当然很多不一定说得对，只是和大家探讨下。总得来说我觉得国内做得还挺好的。&lt;/p&gt;&lt;p&gt;机器之心：您在深度学习这个领域里面，跟国内的学者沟通的时候，会觉得有什么差异吗？&lt;/p&gt;&lt;p&gt;田渊栋：国外可能更细一点。国外交流的时候，大家都对问题有了解，会谈到很细的内容，会说“这个东西我没有理解，我不知道，我回去查一下资料。我得做了实验才告诉你答案”。但是国内问的问题就比较大一点。&lt;/p&gt;&lt;p&gt;机器之心：因为可能还没有那么深入地了解这个领域，是吗？&lt;/p&gt;&lt;p&gt;田渊栋：可能是吧，特别是围棋这一块 。当然也有可能国内大家都愿意问比较大而宽泛的问题。
&lt;/p&gt;&lt;p&gt;机器之心：那从论文的发表，现在的数目和质量来说，您觉得美国有多大的差距呢？&lt;/p&gt;&lt;p&gt;田渊栋：这个还没有仔细的研究。原创性的，有大跳跃的文章相比还是国外多一点，但是国内跟进很快。大概是这样。&lt;/p&gt;&lt;p&gt;机器之心：你有时候会有国内的某个研究机构出来的文章，让你觉得写得非常漂亮这种感觉吗？&lt;/p&gt;&lt;p&gt;田渊栋：不错的很多啊，何恺明的ResNet大家都在用，做得好管你国内国外，大家都会用的。如果钻研某个方向， 国内做到和国外差不多水平甚至更高，非常正常。国内这种工作的强度，国外是不可想像的。&lt;/p&gt;&lt;p&gt;机器之心：那国内如果有深度学习方面，有浓厚这个热情和兴趣的学生，他想读一个本科。你推荐他哪些院校呢？首先您的母校交大，对吧？&lt;/p&gt;&lt;p&gt;田渊栋：对，是。&lt;/p&gt;&lt;p&gt;机器之心：如果出国留学，你推荐哪几个学校？&lt;/p&gt;&lt;p&gt;田渊栋：我觉得
CMU 其实是很好的一个学校，我觉得卡耐基梅隆大学的一贯风格是做事做得很细，然后大家都很认真，愿意把一些事情做好。&lt;/p&gt;&lt;h2&gt;&lt;u&gt;田渊栋的学习方法论&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;
机器之心：之前您写过一些科研的总结，还有博士的过程。我们发现那些文章的归纳能力特别强。有主线，有要点，非常注重系统性和方法论，这个东西是怎么养成的，或者对于其他的研究人员或者技术人员怎么帮他们更好地做到这一点。&lt;/p&gt;&lt;p&gt;田渊栋：这个其实我自己的经历比较特殊嘛，我之前说过，我自己写过小说的。&lt;/p&gt;&lt;p&gt;机器之心：和这个有关系？&lt;/p&gt;&lt;p&gt;田渊栋：有关系的。
我写过长篇小说，写长了之后，会有一些问题，比如说角色的把握和剧情的走向。你写下来发现这个角色和之前相比， 性格走样了，说的话做的事不像他/她应该做的了。这时候再写下去就越来越糟糕，这时候就要多想想，有些段落虽然写得精彩，但于全局无益的话就得要忍痛割爱。然后反复读，再找到正确的路子写下去。像这样写多了的话会有感觉，会避坑，然后会有一些自己在方法上的总结。写博文也是一样的，一开始一泻千里，东一点西一点，然后收束了，归类了，有些大段大段的直接删掉，迭代几次之后，发表出来的才让人读着舒服。所以这个对我来说是比较特殊的经历。总的来说， 我走过很多弯路，走弯路走多了，你才知道什么地方是对的。&lt;/p&gt;&lt;p&gt;机器之心：靠经验积累。&lt;/p&gt;&lt;p&gt;田渊栋：靠经验积累，如果大家想读博士的话，那还是要通过自己的经验积累，别人说的话再多，都没有自己的教训深刻。不要怕犯错。几个比较简单的经验，动作要快，不要怕犯错，多试几个方向。然后从错误中慢慢总结，知道更多的东西。我觉得现在最重要的是一个人要很聪明，要很会学习，然后愿意去尝试，不要怕犯错，就是这样子。从统计学的角度来说，经验越多，你获得的数据就越多，那你的模型的迭代速度就越快，效果就越好。所以其实就是这样一回事情。&lt;/p&gt;&lt;p&gt;机器之心：我看您对写作的理解就别具一格，好像写作对你来说不是简单的对学习过程的记录，甚至是您的一种思考方式了，对吧？&lt;/p&gt;&lt;p&gt;田渊栋：对，思考方式。&lt;/p&gt;&lt;p&gt;机器之心：您的文章里提到过，有时候可能看起来很平庸的东西，通过写作，可以产生非常好的效果。&lt;/p&gt;&lt;p&gt;田渊栋：写下来之后呢，你会有不一样的感觉。你脑子里面的东西，可能没有那么系统，甚至有自相矛盾的地方。当时没觉得什么，你写下来之后，才发现这个写下来不对，是不是要推倒重来呀，这个地方有问题呀？这就是迭代的过程。人的成长有时候得要抛弃成见，抛弃自己曾经认为十分正确的东西，再作总结，要有这个包容的意识，要知道自己可能全错。写作呢，就是提供了这样一种渠道。写作扩大了记忆力，你可以拿来思考的记忆就那么一点
。你觉得你想到了所有的地方，思路很完美；但事实上是你拿了这个，把那个丢了，拿了那个，把这个丢了。只有全部写下来之后，才会发现有问题。才会去思考。我写博文的时候，第一遍不会直接发到网上的，会反复读几遍，看一看有什么问题。我自己觉得满意了，才会发。很多时候，我会觉得这个地方不通。这个是这个意思，下一段是别的意思，这两段没有连起来。你就会觉得语句有问题，语句有问题，你会自己去调整。在写作上会有这样一个洁癖嘛，你觉得这个文章写得不好，你不愿意发出来。然后这样的话，你可能对你的研究过程有思考，你把这个写下来，会发现这里做得不好，会有这个感觉，会反省，下次会想着要改进。那时间长了以后，自然会有一个比较系统性的方法。
&lt;/p&gt;&lt;p&gt;机器之心：那您发的那么多的论文，背后是不是有特别大量的学习笔记？&lt;/p&gt;&lt;p&gt;田渊栋：有很多，其实我之前有写日记。反正不时就会写一点东西嘛。但说实在的，大部分论文都没有学习笔记，那样太花时间了，很多文章看两眼就过去了。毕竟文章太多，把时间花在刀刃上才是最重要的。&lt;/p&gt;&lt;p&gt;机器之心：这些东西虽然不是特别的系统，或者有一些东西可能还有一些缺陷，您会跟人分享吗？&lt;/p&gt;&lt;p&gt;田渊栋：这个还是不会分享，所以你看到的是冰山一角。能给大家分享的，都是写得比较好的，我比较满意的。你看到我写得特别系统，可能是个幻觉，因为还有大量不系统的堆着，要整理出来太费力。&lt;/p&gt;&lt;p&gt;机器之心：对一些想进入学习机器学领域的年轻人，有没有什么建议？&lt;/p&gt;&lt;p&gt;田渊栋：动作快，然后多学习，多交流，多尝试。不要怕犯错，计算机这一块犯错没有什么问题嘛，犯错就出
bug 嘛，计算机也不会爆炸。出了 bug 也没有关系，就反复调试，对吧。我觉得我们 CS（编者注：计算机科学） 这个领域其实非常好，实验重复性很高。犯错了，也没有任何问题，整个周期非常短。所以我觉得特别适合年轻人学习，我觉得只要你有能动性，你只要抓住机会，多跟别人交流的话，我相信大家都能做得挺好的。&lt;/p&gt;&lt;p&gt;机器之心：最后一个问题，推荐几本您觉得特别好的，技术性强的，或者是科普性强的书给我们的读者。&lt;/p&gt;&lt;p&gt;田渊栋：说实在的，现在看书没有什么大用了。很多时候就是看论文，多了解一下，多跟别人交流，因为现在变化非常大。很多东西都不一样了，所以你看这些书能够知道以前的一些知识。其实你看论文也有同样的目标，比如说看论文第一段，这段里面其实就概括了以前的一些工作。然后你看多了，你自然而然就会对这个领域会有了解。看书当然也会看，比如说你特别想提高一下自己的数学能力的话，就要看一些经典的教材。最近我无聊去看群论，在看为什么一元五次方程得不到根式解。无聊嘛，你可以看看一些有趣的东西，并且深入思考。通过看和思考，你相当于磨炼自己的分析能力，长期不看的话感觉会变钝的，就可能人云亦云了，别人说好，你也觉得好，你作为研究员的价值就没有了。数学这些东西，经典的方法都是十年、百年的积累，不会过时的。所以好多都可以看。但是你要去追人工智能比较好的一些发展的话，其实看论文比较快。多看几遍论文的话，也基本上能够掌握这些方向的一些进展。然后多跟别人交流，我个人建议就是多交朋友。交流是很重要的，别人一句话就顶你看很多书了。你现在不可能看完所有的文章的。&lt;/p&gt;&lt;p&gt;机器之心：那导论性的教材需要看吗？&lt;/p&gt;&lt;p&gt;田渊栋：我觉得像算法这些的，还是可以看一些。&lt;/p&gt;&lt;p&gt;机器之心：就是看一些比较经典的教材，像贝尔萨写的书也是要看的，是吧？&lt;/p&gt;&lt;p&gt;田渊栋：要看，但也取决于你有没有兴趣。你可能没有时间把所有推导都推一遍，这不可能的。但你你可以把整个方法和想法看一遍，把逻辑梳理出来
。看论文的时候，往往跳跃和选择性的看。因为每篇文章的目标是把这个文章卖出去，他会说自己的方法特别好，别人的方法特别差。但是其实不是这样的，对吧。文章往往是有偏向性的，所以要选择性的看。&lt;/p&gt;&lt;p&gt;机器之心：有没有哪些书是你觉得比较值得看的？&lt;/p&gt;&lt;p&gt;田渊栋：这个没有定规啊，每个人的需求都不一样。另外，你可能当时看一下，到用的时候你再去翻，这样可能会好一点，看一本书会花很多时间。没有一个准则，到最后可能就是东看一点，西看一点，关键是把你的知识体系建立起来。比如说这块你觉得不懂，你就看这块不懂的文章。&lt;/p&gt;&lt;p&gt;机器之心：就是有针对性的。&lt;/p&gt;&lt;p&gt;田渊栋：有针对性的去学，可能会比较好。比如说这次做围棋，我之前也没有做过游戏。那怎么办呢？你就看，看David Silver的博士论文，看以前增强学习的文献。你如果要做游戏的话，你看他的博士论文就得要看得比较仔细了。有一些关键的点，一定要搞清楚。相当于你要有选择性的看某些章节，某些文字，某些公式。你如果觉得这个重要的话，你就花时间搞定。如果不重要的话，你可以略过地看。这个就看你的积累了，你的积累肯定会告诉你什么重要，什么不重要。&lt;/p&gt;&lt;p&gt;以最少代价去获得到你的知识体系，没有一定的准则。&lt;/p&gt;&lt;p&gt;搜索能力是很重要的，想看什么就去找。反正现在网上有的是资源，现在已经不是图书馆的时代了，对吧。基本上一搜都搜到，关键怎么样搜。然后你愿意去搜什么样的东西。我觉得搜索是现代人的一个必备技能，不是说去图书馆一本一本啃下来就可以成为专家了，不是这样子的。&lt;/p&gt;&lt;p&gt;另外，科普只是领进门的。我以前是比较喜欢化学，后来喜欢物理。再后面慢慢到数学去，再到做计算机去。所以说这样一条轨迹，基本上化学的专业文献，只要是浅显的我都能看懂。你有基础之后，你再去看科普文，你可以猜出来科普文和专业文献之间，是怎么样的对应关系，为了让外行人看懂，作出了什么样的省略。但是如果一个没有经验的人，只看科普的话是入不了门的，容易被各种名词误导。你需要花时间在专业文献上，让知识构成体系。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23071317&amp;pixel&amp;useReferer"/&gt;</description><author>田渊栋</author><pubDate>Thu, 20 Oct 2016 00:01:40 GMT</pubDate></item><item><title>一些问题的回答</title><link>https://zhuanlan.zhihu.com/p/21620869</link><description>&lt;p&gt;本次问答应“将门创业”之邀所写，专栏版本内容有扩增。&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;strong&gt;问一：能否和大家简要介绍一下你的背景，以及你现在在Facebook的工作内容和强度？&lt;/strong&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;我之前在谷歌无人车组工作了一年三个月，自从2015年1月加入Facebook任人工智能研究所研究员，之前在做围棋，现在在做深度学习和增强学习(reinforcement learning)方面的工作。强度上来说基本上一天十一二个小时吧，一周七天，具体时间上比较自由，可能早上先干一会儿再去上班，或者晚上吃个饭然后继续干到晚上十一二点，或者早晨去健身，周末出去打个球都有可能。睡眠时间一般在七小时左右。&lt;/p&gt;&lt;p&gt;这样长的工作时间当然不是强制的，一方面因为我自己比较喜欢做研究，所以自然而然会比较长；另一方面毕竟年轻无孩，多干点是应该的。工作的内容基本上是看文章想办法写代码，另外还有开会聊天。后者听起来不是正事，但却是与别人沟通的重要一环。&lt;/p&gt;&lt;p&gt;我很少熬夜。熬夜效率非常低，我试过几次，眼睁睁地看着时钟从凌晨一点走到两点，再从两点走到三点，但手上工作还没做多少，伴随着一种非常强烈的挫败感。一般来说，这只适合最后期限（deadline）在第二天早晨的情况，但是如果真是这样，那说明投稿的时间部署还有优化的空间。&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;strong&gt;问二：如果用3个词来形容你自己的性格，你觉得是什么？&lt;/strong&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;三个词概括不了一个人的性格的，写下来反而会给大家刻板印象。如果硬要说的话，我是个不会闲下来的人吧。&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;strong&gt;问三：你曾在一篇文章中提到过，希望奔跑中的自己，不要忘了“梦想”这个词的含义。你的梦想是什么？&lt;/strong&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;我的梦想是在人工智能领域做出影响世界的工作。这其中包括两方面的工作。在理论上，理解复杂人工智能系统，比方说深度学习的工作原理；在应用上，做出效果更好并且切实可用的人工智能系统。目前在这两方面都有尝试并且也有一些成果，比如说在博士阶段做的对特定条件下非凸问题全局最优解的理论分析，还有最近做的DarkForest围棋系统。前者拿了ICCV2013的马尔奖提名，后者获得了很多国内外的媒体报道，拿了一些比赛名次，并且在开源之后惠及他人。&lt;/p&gt;&lt;p&gt;但说实话，目前的这些工作离实现梦想还有很大差距，需要一点一滴的不懈努力。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;u&gt;问四：目前你在知乎的粉丝也有3万多了，在圈内也算小有名气，作为一名科研人员，你觉得出名对你产生了哪些正面和负面的影响？&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;作为一个科研人员，正面的影响是说话有人愿意听，写的文章也有人欣赏。我本质上比较内向，写文章是自己的事情，目的是整理总结过去的经验，以为将来开辟道路，当然我非常高兴有很多人喜欢我的文章，这也算是影响世界的一种办法了吧（笑），从这点来说，出名是很有意义的。希望我以后能给大家带来更多更高质量的文章。负面的影响就是会有很多人来找，会变得忙一些。但另一方面，一个内向的人多和别人聊聊是非常有好处的。所以总的来说没什么负面影响吧。&lt;/p&gt;&lt;p&gt;一句话，做好自己的事，不要被名声牵着走就好了，管自己是涨粉还是掉粉，第二天太阳还是照常升起来的。&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;strong&gt;问五：在AlphaGo战胜李世石之后，你觉得除了Google在此获得了具大的曝光外，对整个产业的发展起到了怎样的作用？&lt;/strong&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;主要还是给大众一个冲击。围棋在国内一直被认为是人类智慧的最后堡垒，是人的智能比机器智能厉害的标杆，现在这个标杆突然间被征服了，多年以来咱们的教科书上写的不对，这个震撼力是不言而喻的。这样就会让普罗大众都来关注人工智能，能起到加速产业发展的作用。&lt;/p&gt;&lt;p&gt;这有好有坏，好的是流进这个领域的钱会变多，需求会增长，工资会变高；但是坏的是大家对我们的期望也变高了。其实如同我之前写的那样，进步没有想像得那么大，很多时候大众看到的只是连成系统的最后一击，这一击非常震撼，但是背后几年甚至十几年的积累大众没有看到，以为会继续连击，结果发现老本都吃完了。&lt;/p&gt;&lt;p&gt;现在就要看我们是不是工作够努力以达成这个期望了，以目前的迭代速度和丰富的资源和工具来看，确实有可能加速发展；但也有可能在基本理论没有突破的情况下试不出什么新货色来，于是发展停滞。在这两种可能中，我个人持乐观主义态度，倾向于前者，当然这个观点是带有偏向性的，因为要是我持悲观态度，那为什么还要在这个领域继续做下去呢？&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;u&gt;问六：现在深度学习在工业界越来越蔓延，其实现在有名的几家DL的公司，都在探索DL可以为社会创造什么样的价值，多大的价值。你认为DL将对社会产生什么样的价值？&lt;/u&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;深度学习（DL）的价值在于它能够免除很多人工设计的麻烦，极大地提高效率。以前花十年手工打造一个为特定场景设计的系统，现在可能花一年就做成通用系统，并且不管是性能还是可维护性都胜过以前手工打造的系统，这个可以说是革命性的。当然目前的问题是DL是黑箱不能解释，在定制和调试方面不如手工打造灵活。不过我相信随着大家的使用，理解会越来越深，更好的理论迟早也会出现，所以从长远上来说不是问题，我相信DL未来的影响会越来越深远，现在仅仅只是个如黎明曙光一般的开始，以后的路还很长。&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;strong&gt;问七：技术的竞争很激烈，快速迭代，而且算法门槛越来越低，大家都在刷数据，简单粗暴。你认为作为技术人员，竞争优势在哪里？&lt;/strong&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;因为DL的便利性，现在是存在这个问题。大家估计都看过这个笑话，Caffe10块钱安装一次，CNN5块钱一行，RNN8块钱一行，好像深度学习门槛很低很低，高中生都可以试试。其实门槛并没有那么低，DL解决了以前的很多问题（比如说设计特征），但是带来了更多的问题（比如设计网络结构，从训练的结果里看出下一步要怎么调整）。&lt;/p&gt;&lt;p&gt;对于工程人员来说，如何以最快速度学习现有工具，掌握它们的脾气，利用它们解决新出现的问题；再上一步，如何设计灵活便利，效率更高的工具，这些都是要思考的。工具变强之后，人就会自然地思考更难的问题，是一直以来的趋势。而计算机这个行业的好处，就是不管工具内部有多复杂的逻辑，接口做好了用起来都一样方便，这是它之前一直在火，以后也会一直火下去的原因。&lt;/p&gt;&lt;p&gt;对于研究人员来说，虽然是绕着DL做文章，但功夫却在DL之外。遍历各大会议的文章，DL虽是很大的主题，但是在DL上翻的花样都和以前的各领域有关，比如说图模型，增强学习，等等，至于理论分析，则更离不开基本功，矩阵论，微分方程，动力系统，随机矩阵谱估计，张量分解，凸优化等等，现在既然不知道它为什么效果好，那么任何领域都得试一下。在这种情况下，能在各个分支上来回切换，并且迅速找到问题的难点，就是研究员的核心能力了。这种能力往往在求学阶段时，静下心来花很多年的时间积累得到，目前还很难被机器被取代。如果翻我的履历，就会发现我以前不是做深度学习的，后来在谷歌无人车组时自己在业余时间做了下，最后拿到了我现在组研究员的Offer。&lt;/p&gt;&lt;p&gt;另一种重要的，目前难以替代的能力是交流能力。其一是业务交流能力及管理能力，单独自己做是永远赶不上一个训练有素的团队的速度的（AlphaGo就是一例）。为此需要广泛地与同行交流与下属交流，明白什么事是自己需要干的，什么事可以交给别人解决，知道什么时候可以妥协需要妥协，什么时候要坚持原则，以最快最有效率的办法解决问题。&lt;/p&gt;&lt;p&gt;其二是跨领域交流能力。如之前我写的《快速迭代的人工智能》所说，相比其它领域以年为单位来衡量的迭代速度，AI这里完全可以用技术爆炸来形容：每天都会出现值得一读的文章，所有教科书都相对过时，各种结论随时可能被推翻，准确率的冠军往往只能保持几月甚至几天。我相信，以后作为高效率工具，机器学习特别是深度学习肯定要进入其它领域的，而如何通过不同领域间的交流，让它充分发挥作用，这是个难点。这就要求博士能真正做到学识渊博，在自己的领域有深入了解的同时，还要对其它领域有所了解，特别对学科框架有所了解，遇到问题能纲举目张，分析到点子上。随便找一个学电子，金融，机械，化工，材料，土木，生物的同学，能不能和他们聊起来？他们现在可能和我们做的一点关系也没有，但还是需要处理数据，需要数学建模，需要模式识别，需要最优化，需要高效地分配和完成任务。这些都是以后的增长点。&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;u&gt;问八：你是如何看待现在大量学术界的人才流向工业界这个现象的？&lt;/u&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;学术界相对来说比较自由，迭代没有那么快，可以做相对长期的课题（比如说一年两年）和相对抽象的课题（比如说建框架，做理论分析），当然资源没有工业界丰富。这里的资源包括计算资源，大量的数据还有有经验的人。工业界这边给钱会比较多，但是要求出活快有实际效果，最好能赚钱。&lt;/p&gt;&lt;p&gt;我们组（Facebook人工智能研究所）目前看起来兼顾学术界和工业界的优点，既有学术界的自由度，又有工业界的资源，是相当不错的，欢迎大家申请。我们这里有以下几类职位。&lt;/p&gt;&lt;p&gt;(1) 研究员（Research Scientist），要求有比较长的相关研究经验（计算机视觉，机器学习，增强学习），并且在自己的研究领域做出有影响力的好成果。主要做基础研究工作。&lt;/p&gt;&lt;p&gt;(2) 研究工程师（Research Engineer），要求代码能力强，对相关方向有一定经验。有相关开源项目更好。&lt;/p&gt;&lt;p&gt;(3) 博士后（Postdoc），与(1)的要求相近但是低些。一般博士毕业应届生只能申请这个职位。&lt;/p&gt;&lt;p&gt;(4) 实习生 (Intern)，为期三个月，一般要求博士在读，已有好工作发表在顶会上，及具备一定的代码能力。&lt;/p&gt;&lt;p&gt;注意(1)(3)(4)都没有文章数量的要求，只看质量。若是看到简历里列出一堆烂会或者低质量的工作，只会是扣分项，有一两篇烂会大家就要考虑一下是不是招，有很多篇烂会那肯定无脑拒了，而反过来，只要有一篇文章在领域里获得了巨大的影响力，那不管是不是中稿都会有对作者有极大的兴趣。总的来说，看一个人的影响力，往往看最好的两三篇文章的质量，看他的招牌成名作，这是以前我导师教我的，也是在美国学术圈里一直看到的评价体系。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;u&gt;问九：目前机器学习方法的成功运用需要依赖于利用大量数据进行算法训练，然而对于不拥有海量数据资源的企业，尤其是初创企业而言门槛很高，这在一定程度上限制了创新的机会。机器学习是否存在减少对数据的依赖的路径？在学术界和工业界最新的实践进展如何？ &lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;减少对数据依赖的办法一定是有的，人类智能就是个现成的例子：一个训练好的模型，能通过几十个或者几百个样本迅速学会新任务。如果以后我们能搞出一个这样的模型来，放在Github上给大家下载，那对初创企业是非常好的帮助。当然，近期内还办不到。&lt;/p&gt;&lt;p&gt;目前减少数据依赖有各种办法，比如说人工设计特征和提炼规则再接以简单的模型训练，各种正则化方法，对模型顶层权值进行fine-tune，绑定权值shared weight, 各种形式的transfer learning，及最近比较流行的建立虚拟世界然后从里面进行数据采样的办法，这些办法各有各的优缺点，不存在万灵药。本质上来说，这是因为我们对DL的机理不清楚，只有模糊的直觉理解，而没有定量理解，所以只好用大量数据把模型硬生生学出来。若是对DL有更深的掌握，那原则上来说能用很少的数据去随意微调已有模型得到新的，将又会是一个突破。对初创公司来说，大数据的限制确实会让人难受，但技术上的问题通过仔细分析，应该是有各种折中办法的，这就要靠各家人才的聪明才智了，基本上是具体问题具体分析，没有什么统一的办法。这是DL研究人员能拿高工资的原因之一。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;u&gt;问十：现在越来越多的企业开始进行人工智能专用处理器的研发。例如IBM公司的TrueNorth，高通公司的Zeroth，Google公司的TPU，KnuEdge公司的KnuPath和中国科学院的寒武纪，中星微的NPU等。这些专用处理器在实际应用中效果如何？人工智能专用处理器的细分市场发展上会有什么样的趋势？&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我不是做硬件的所以无法详细回答这个问题。目前通用的GPU已经让训练速度快10多倍了（这数字可能过时），我相信再往下走可能会出现更快的通用DL芯片，和针对特定应用背景的专用DL芯片吧。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21620869&amp;pixel&amp;useReferer"/&gt;</description><author>田渊栋</author><pubDate>Tue, 02 Aug 2016 02:01:54 GMT</pubDate></item><item><title>第一次半马感想</title><link>https://zhuanlan.zhihu.com/p/21809552</link><description>&lt;p&gt;原来主办方通过Tracker还是能知道冲线的时间。万幸没有白跑这一次半程马拉松，最后成绩是2小时2分47秒。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/b49035403ec3068d05ed396a5ca6c029.png" data-rawwidth="598" data-rawheight="373"&gt;&lt;p&gt;=========================&lt;/p&gt;&lt;p&gt;早上刚跑完旧金山的半程马拉松（21.1公里或13.1迈），用时约2小时1分钟，乘热乎写篇博客（以下一迈等于1.6公里）。&lt;/p&gt;&lt;p&gt;从家里出发去跑步的出发点，事先预约下附近的停车场，然后早上4点35分吃了个面包出门，约5点10分到达，一路走过去，人越来越多，最后到跑点简直是人山人海。整个三番马拉松赛事是早晨5点半开始，运动员们先跑，每隔十分钟再放一波一波的参赛者。周围全都是短打装束，各种装备一应俱全，手机固定在手臂上，腰包上固定各种证件和饮料，还有饮料吸管可用，而相比之下我只穿了条平时锻炼用的短裤，钱包手机和车钥匙放在松松垮垮的裤子口袋里，一晃一晃的，也没有带运动饮料，相比之下业余很多。还好主办方在路上都设有补给点。我在第五波，约6点5分起跑。主席宣布起跑后，整个方队是不动的，要等到前面的人走了，后面的才能慢慢加速。&lt;/p&gt;&lt;p&gt;这次半程马拉松的风景相当不错，先沿着旧金山的海港从Pier 1到Pier 59，然后上金门大桥跑个来回，再一路向南穿过旧金山多山的地形，到达金州公园（Golden State Park）。在金门大桥附近看，细雨霏霏，清晨雾重，将大桥遮去了一半，从高处望下去，长跑的队伍浩浩荡荡，已经有先前跑得快的人从那里回来了。&lt;/p&gt;&lt;p&gt;前面11迈总的来说比较轻松（跑到11迈约7点44分，只用了1小时40分钟左右）。一开始不停地超人，跑得略快。大约在5到6迈时出现了第一个极限，又正是上坡路，腿比较酸，速度慢了下来，但并不难克服。约6迈后上了金门大桥，第一个极限之后跑起来很惬意，金门大桥眨眼就过，当我快下桥看到9迈的标志的时候，有一种“半马也不过如此”的感觉。但后来发现高兴得太早，一过11迈这个坎，第二个极限就来了，虽然心脏还在说“继续跑，没问题”，但肌肉受不了了，迈步和铅一样重，尤其是遇到旧金山市区的上坡路，有一种马上就要大腿抽筋的感觉，我甚至觉得自己坚持不下来。之前一直在超人，但到这个位置，看到别人一个一个地超过自己，什么也干不了。这时候只能对自己说坚持住不能停下来，可以跑得很慢，但是一定要在跑。从11迈到12迈花了12分钟左右，几乎就是快步走的速度。之前训练过两次一小时零几分钟跑7迈（11.2公里），但并没有进行过长达两小时的训练，所以第一次跑起半马就见真章了。&lt;/p&gt;&lt;p&gt;12迈之后稍好一些，但速度仍然上不去，这时候已经进了补给点，喝了口水看到半马/全马的分界线，预示着终点的到来。希望来了，动作快了点。周围的观众越来越多，气氛越来越热烈，我抬着快要断的腿，终于看到了13迈的标志。这对于快跑不动的人来说简直是救命稻草，我憋一口气开始冲刺，超过了之前的几个人，听着旁边的解说员说道“This guy picks a bit speed..."冲过了最后的0.1迈（160米）到达终点，正是8点06分。整个半程马拉松总花时2小时1分钟，对第一次参加的我而言，是非常令人满意的成绩了。&lt;/p&gt;&lt;p&gt;对于长跑的人来说，跑完是很幸福的事情。站着喝水，什么也不用想，什么也不用做，完成了一件大事，真是美好。大家坐在草地上发呆，或者兴奋地交流。回到起始点查成绩，发现随身带的Tracker坏了，只记录了前2.3迈的速度和位置，后面的啥也没有。问了工作人员，他们说Tracker得系在鞋上，而我放在胸前，跑了一阵后被汗水打湿就不起作用了。这是这次美中不足的地方。希望主办方有照片或者视频证明冲线的时间。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/341f964914c49c92e0ffe55dcb383fa0.png" data-rawwidth="851" data-rawheight="1134"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21809552&amp;pixel&amp;useReferer"/&gt;</description><author>田渊栋</author><pubDate>Mon, 01 Aug 2016 03:41:01 GMT</pubDate></item><item><title>两篇DeepMind ICML的点评</title><link>https://zhuanlan.zhihu.com/p/21432542</link><description>&lt;p&gt;【原文应新智元之邀所写】&lt;/p&gt;&lt;p&gt;&lt;b&gt;点评 Dueling Network Architecture for Deep Reinforcement Learning (ICML Best paper)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;DQN系列的方法用的是Reinforcement Learning中的Q-learning, 即给定状态s，给下一步的行动a打分，分数记为Q(s, a)，然后选取分高者作为这一步的最优策略。Q这个函数可以很复杂，特别是当状态由当前图像的像素值直接表示的时候，所以现在流行的方法是用卷积神经网络读取图像s，得到中间特征，然后再加上若干层全相连层去预测Q(s, a)在每个a上的值。而这篇文章的主要贡献很简单，在这个神经网络上稍微改进了一下，在得到中间特征后兵分两路，一路预测估值函数V(s)，另一路预测相对优势函数Advantage function A(s, a)，两个相加才是最终的Q(s, a)。这样做的好处是V(s)和A(s, a)有各自的意义，V(s)是对当前状态的长远判断（Expected Return），而A(s, a)则衡量在当前状态s下，不同行为的相对好坏，一个是远期目标，另一个是近期目标，这就是所谓的Dueling Network Architecture。图2很清楚的显示了这一点，也是本文最有意思的地方。如果状态s1比状态s2总体要好，那么每个Q(s1, a)相对每个Q(s2, a)要高，而需要Q(s, a)的每项都去拟合这种“低频分量”，会在某种程度上费去神经网络的“容量”，不是最优的办法；而将Q(s, a)分解为V(s)及A(s, a)的和就没有这个问题。当然这个只是直觉印象，不一定是真实情况。实验上用了57个Atari Games，算是比较多，也是比较靠谱的，从表1看起来，新方法和老方法相比略好些，相反是各种其它的技巧，如Gradient Clip，和Prioritized Experience Replay，对结果的影响似乎更大。&lt;/p&gt;&lt;p&gt;&lt;b&gt;点评 Continuous Deep Q-Learning with Model-based Acceleration&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这篇也是用Advantage Function去做增强学习的工作，但这次对付的是变量连续的行动空间，这样就直接和自动控制及机器人相关。连续空间上的一个大问题是，即使通过训练得到了用深度网络表达的Q(s, a)，但因为a可以取无穷多个值，在通常情况下无法通过穷举a得到最优的行为。对此该文将Q(s, a)先分解成V(s)和A(s, a)的和，然后将A(s, a)建模成一个关于a的二次函数，而建模这个二次函数的方法是通过建模条件均值mu(a|s)和方差P(a|s)进行的，这样可以用解析方法直接得到给定s后a的最优解。注意该文中用x代表状态，u代表行动，而非s和a，这个是控制论中的通用做法。如果大家想一想可能马上发现mu(a|s)其实就是策略函数pi(a|s)（对计算机围棋而言就是走子网络），那为啥他要这么绕？因为这样的话可以用一个模型同时建模pi(a|s), V(s)和Q(s, a)，这个是比较有意思的地方。&lt;/p&gt;&lt;p&gt;有了这个模型之后，该文另一个大的贡献在于用卡尔曼滤波器配以局部线性模型，来加快经验的获取。在增强学习中训练深度网络是比较慢的，因为一开始深度网络的策略很糟糕，在自我模拟中得到的经验完全没有意义，用这些经验训练得到的网络也就不会太好，如此循环往复，需要很久才能走出这个圈。一个办法是用好的经验去训练策略和估值函数，这可以通过专家已有的经验来获取（如围棋），也可以像该文那样，通过简单模型来获取。局部线性模型是用来预测行动后的下一个状态的，而在有了局部线性模型之后，卡尔曼滤波则告诉你为了达成某个状态（如让猎豹达到下一个跑步的姿势），目前需要采取什么样的控制策略，这两者都是经典控制论中常用的方法。最后的测试是在开源的模拟环境中进行的。效果要比以前他们自己的方法，即使用actor-critic模型的Deep Deterministic Policy Gradient (DDPG)，要好些，我很期望看到只用局部线性加卡尔曼滤波后的结果（即经典控制论的baseline），可惜似乎文章中并未给出。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21432542&amp;pixel&amp;useReferer"/&gt;</description><author>田渊栋</author><pubDate>Mon, 27 Jun 2016 15:16:16 GMT</pubDate></item><item><title>围棋引擎DarkForest开源了</title><link>https://zhuanlan.zhihu.com/p/21381527</link><description>我们的围棋引擎DarkForest开源了。见以下Github链接：&lt;a href="https://github.com/facebookresearch/darkforestGo" class=""&gt;https://github.com/facebookresearch/darkforestGo&lt;/a&gt;，目前主要是围棋的MCTS引擎和训练好的DCNN模型（不加搜索可以直上KGS 3D）及playout模型。另外训练代码会在稍后发布。希望这份代码能给有志于计算机围棋研究和深度学习研究的同仁们一些小小帮助。&lt;p&gt;这个项目从去年五月开始零零碎碎做起，到现在终于公布了内部细节，算来是整整一年了。全心投入的时间大约有四到五个月，其间一波三折，有初始成功的喜悦，拼命工作的漫长，被人甩开的哀伤，对世纪大战的复杂心情和期许，及最后尘埃落地的平静。不管怎么说，我们是中途从视觉研究方向转行，用了别人百分之一甚至千分之一的资源来做这样一项工作，并且能够获得大家的喜欢，认识了很多朋友，已经十分满足了。这次开源后能在短短的一周内收获一千多颗星，一百多次Fork和watch，实在超乎想像。&lt;/p&gt;&lt;p&gt;感谢大家的支持和鼓励！&lt;/p&gt;&lt;p&gt;当然了，第一才有最多的鲜花和掌声，第一之后的人和事，则免不了被批评指摘以及被遗忘的命运。然而即便如此，努力向前总是必要的，因为我们永远也不知道，下一次面对的，是无法战胜的大军，还是无人涉足的宝藏。&lt;/p&gt;&lt;p&gt;能做的，就是再来一次。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/105a99601bf119a6d722847ce1562604.png" data-rawwidth="796" data-rawheight="597"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/212cb8750d87c013f6780b757e1a2904.png" data-rawwidth="790" data-rawheight="578"&gt;&lt;p&gt;---------------------------&lt;/p&gt;&lt;p&gt;PS..用纯C写的原因纯粹偶然：在项目的一开始，我并不知道怎么在Lua里面调用C++的函数，图方便就直接用了C，于是大家看到了现在这个样子。如果我有时间的话，会尽量改成C++11的风格。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21381527&amp;pixel&amp;useReferer"/&gt;</description><author>田渊栋</author><pubDate>Mon, 20 Jun 2016 08:15:47 GMT</pubDate></item></channel></rss>