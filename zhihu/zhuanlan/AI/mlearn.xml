<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>机器学习笔记 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/mlearn</link><description>机器学习的笔记。非常浅显。不得不说ng的课很适合入门，几乎不需要太多的数学基础。</description><lastBuildDate>Fri, 16 Dec 2016 05:16:21 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>别写for循环調参……</title><link>https://zhuanlan.zhihu.com/p/23498425</link><description>刚开始学机器学习的时候，特别是学一些模型的时候，课程要求自己写一些算法（而不是去用别人写好的），有时自己写的渣算法调参有时没法直接从sklearn里引用GridSearchCV 调参，图方便，咱这些菜虫就直接写个for循环……&lt;p&gt;然后机器就跑几天都跑不完了（比如模拟无人车这种，渣电脑模拟一次路况就十来秒。）&lt;/p&gt;&lt;p&gt;那么，在写着很快的情况下，避免用for循环来调参，有哪些写起轻松又好用的方法呢？&lt;/p&gt;&lt;p&gt;首先看参数，一般在调参的时候，影响参数数量的数字有两个，一个是参数的种类有多少，一个是每个参数的范围内有多少。&lt;/p&gt;&lt;p&gt;举个例子，Qlearn这玩意，通常調3个参数，学习率（alpha），折扣因子（gamma）以及探索率（epsilon）。这就是说参数的种类有3种，然后是每个参数范围内有多少，比如探索率这个参数，通常是.0~1.0之间，切为10份来看，那就是10个，如果是切为100粉，那就是该参数范围内有100个。&lt;/p&gt;&lt;p&gt;那么，就这个3类参数，每类100份的情况下，如果写for循环的话，就是100**3=1e6 了。假设渣电脑跑一次1秒钟，得11天半才能跑完……&lt;/p&gt;&lt;p&gt;那么如何改进呢？&lt;/p&gt;&lt;p&gt;如果每个参数中的改变是平滑且只有一个极值的，那么可以考虑用二分法来对每个参数进行缩减（猜数字简单的“大了”，“小了”。），100个数据，可以逼近为2^7==128个步骤内选取，也就是把100个的复杂度缩减到7个了。&lt;/p&gt;&lt;p&gt;然后是3类，这时候如果不进一步缩减，是7^3，如何进一步缩减呢？因为我们假设每数参数中的改变是平滑且只有一个极值的，那么所有参数合起来的图像，也应该是平滑且只有一个极值的。&lt;/p&gt;&lt;p&gt;那么，就可以对每个参数，假设了其他参数的最优值的情况下，单独去选取，比方说，假设3个因子我们给的代数是a,b,c，范围均在0~1之间，步长还是.01，那么，假设3个初始值，比如.5、.5、.5，然后在b=.5，c=.5的情况下，去寻找a的最优值，然后用a的最优值替代初始值。接下来在a和c固定的情况下，去找b的最优值……以此类推。&lt;/p&gt;&lt;p&gt;这样，迭代一次，复杂度就从7^3变成了7*3，如果参数的变化比较简单通常迭代3次以内就能找到最优值，这种情况下，就从7^3=343的复杂度，缩减到了7*3*3=63的复杂度。&lt;/p&gt;&lt;p&gt;两招一起用，原本1e6,一秒算一次得11天半跑完的调参，就变成了63秒能跑出结果的一个简单的调参了。&lt;/p&gt;&lt;p&gt;不过问题又来了，眼睛尖的朋友们想必已经发现了，这招类似于贪婪算法，所以如果函数有多个局部最优解的话，很可能调参调不到全局最优解。怎么办呢？&lt;/p&gt;&lt;p&gt;如果你可以确认哪些参数有几个极值，那么这些参数找到的极值就是最优解。如果不能确认我们参数造成函数有几个极值，但是我们知道在每个局部范围内，参数对函数的影响都是简单，平滑的话，可以随机选取每个参数的初始值，多次尝试，最终选择使整个算法表现最好，且重复出现的那个参数组。&lt;/p&gt;&lt;p&gt;不过这招很多时候也不适用……比如你的参数和结果的关系属于魏尔斯特拉斯函数这类的关系的话……&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23498425&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Tue, 08 Nov 2016 04:21:16 GMT</pubDate></item><item><title>今天我们来给一个剃刀打个广告</title><link>https://zhuanlan.zhihu.com/p/22804193</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-4403c2da8633b002eebbff62a77b4777_r.gif"&gt;&lt;/p&gt;&lt;p&gt;有一把剃刀可厉害了，阿基米德在卫生间里用它剃出了简洁的浮力公式，牛顿从繁琐的稿子里用它剃出出了优美动能公式，爱因斯坦从庞杂的证明中用它剃出了曼妙的质能公式，至于高斯……高斯不长胡子，高斯不需要剃刀……&lt;/p&gt;&lt;p&gt;嘛，大家都猜到了，这叫奥卡姆剃刀。&lt;/p&gt;&lt;p&gt;嘛，这把剃刀肯定不是奥卡姆发明的，一般而言认为是上帝跟高斯商量了以后，回到创世初期，做出了这把剃刀。&lt;/p&gt;&lt;p&gt;那奥卡姆卖剃刀的时候是怎么说的呢？&lt;/p&gt;&lt;p&gt;“如无需要，勿增实体”&lt;/p&gt;&lt;p&gt;这听起来像玄学啊……嘛，剃刀嘛，天气热了，知识青年上山下乡接受劳动人民的再教育，于是咱村头的剃头匠老张头决定开个会解释一下，老张头这么说的：“天气恁个热，要那么多头发爪子嘛，来我给你都剃了，放心剃不到头皮。”&lt;/p&gt;&lt;p&gt;人民群众的智慧是伟大的，老张头解释的简单易懂（大雾），这里的头皮，就是指的有效信息，是真理，头发指的就是蒙蔽住真理的玩意。头发越多，自然风也就更难吹到头皮，熵就越多（热），然而我们只知道真理是埋在头发里面的，具体埋在那里不知道，辣么，剃掉的头发越多，真理也不就越明了？&lt;/p&gt;&lt;p&gt;嘛，想到这里，住在牛棚里的知识青年们折服在了劳动人民的智慧底下，高兴地拍起了肚皮……&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-b17f7bf19ff0a2eb52bea0eb1ef40221.jpg" data-rawwidth="225" data-rawheight="208"&gt;&lt;p&gt;剃头匠老张头微微一笑，捋了捋胡须，来来来，剃刀好处都有啥，谁说对了豆给他。小李被分到园林部门当临时工，负责给决策树裁剪枝条，先开始说了。&lt;/p&gt;&lt;p&gt;小李说，领到为啥安排我给决策树修剪枝条呢？因为啊，领导喜欢到树上摘果子吃。但是呢，咱村这树有点……畸形，虽然每棵树长得差不多，但是一般有上万根枝条，却不是每一根枝条上都有果子，所以我们得沿着树干去找果子，一般长了果子的树枝会有一些特征，我们就能沿着这些特征找到果子，找到了好多果子，领导就不会把我扔到夹边沟去了。&lt;/p&gt;&lt;p&gt;然后我发现了一个问题，如果我每次判断的时候，进果园里，看到一棵树都沿着枝条走到头，然后告诉采果子的二麻子哪样的枝条有果子，二麻子按照我说的去找，因为两棵树相差可能很大，这颗枝叶上有果子，下一棵树并不一定就有。那二麻子爬到决策树上，拿回一堆没果子的枝叶，那就不合适了。&lt;/p&gt;&lt;p&gt;我想了想，嘿，那我就“裁剪”一下决策树，让二狗子每次不需要爬到枝端去拿枝叶，直接把看起来有果子的树枝全给我抱回来，不就行了嘛。果然，这下采到的果子大幅提升，领导也开心了！&lt;/p&gt;&lt;p&gt;小李继续说，其实啊，我描述3根枝条的长度，就能大概说清楚这棵树咋样的枝条结果子了。但是我描述了5跟枝条的长度，事实上说的还是那3根的特点，那我何必说5根呢？既然最小描述长度是说3根，那我就只说3根的就是了嘛。&lt;/p&gt;&lt;p&gt;说到这里，老张头满意地拍起了肚皮，说，好，咱劳动人民就是有智慧。来来来，那这个最小描述长度，二麻子，你体会到了，说一下呗？&lt;/p&gt;&lt;p&gt;二麻子说，好，辣你要我解释最小描述长度，我就先解释一下“贝叶斯定理”吧。&lt;/p&gt;&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22805488" class="" data-editable="true" data-title="小学生都能理解的贝叶斯公式。 - 机器学习笔记 - 知乎专栏"&gt;小学生都能理解的贝叶斯公式。 - 机器学习笔记 - 知乎专栏&lt;/a&gt;（作者懒得再写一遍了自己去看，正好写道了最小描述长度）&lt;/p&gt;&lt;p&gt;老张头不乐意了。嘿，我咋看不懂，你是看不起我小学没毕业是卟？&lt;/p&gt;&lt;p&gt;照你故事里的公式这么说，岂不是任何一个描述语句都应该满足奥卡姆剃刀原则了是卟？你咋能这么绝对捏？你这是严重的左倾主义，信不信老子把你工分扣完？&lt;/p&gt;&lt;p&gt;冤枉啊大爷。所以我说奥卡姆剃刀是玄学啊……&lt;/p&gt;&lt;p&gt;那为毛是玄学呢？&lt;/p&gt;&lt;p&gt;事实上来说，对于奥卡姆剃刀原则，每个人的理解是不同的（但是大多数人很难意识到）对于（像我这种）一般群众来说，一般人对奥卡姆剃刀的理解有3种方式：&lt;/p&gt;&lt;p&gt;1、如果在某个可定义范围内若找出了最优解（最优描述），那么不应当在此范围的周围再去添加任何描述（就算这个描述是对的）。&lt;/p&gt;&lt;p&gt;2、如果我无法分辨出最优描述，那么，在备选的描述中（可容忍描述误差范围内），优先选择更简洁（信息熵最小）的描述。&lt;/p&gt;&lt;p&gt;3、如果我无法分辨出最优描述，那么优先选择更符合直觉经验的描述，而不选择人脑思考起来更累的抽象逻辑描述。&lt;/p&gt;&lt;p&gt;对于这3点的理解不同，造成了很多人在辩论奥卡姆剃刀这个问题上的区别，有的人认为是觉得正确的，有的人认为是模棱两可的，比如这个纸糊问题&lt;a href="https://www.zhihu.com/question/20159241" data-editable="true" data-title="「奥卡姆剃刀原则」是正确的吗？ - 哲学" class=""&gt;「奥卡姆剃刀原则」是正确的吗？ - 哲学&lt;/a&gt;就是，每个答主对奥卡姆剃刀的理解都不一样，看这些人的评论区的辩论真是好玩……有的人认为是玄学。由于我只是一个纸糊小透明菜鸟，并不敢和基督徒或绿绿们讨论他们的神符不符合奥卡姆剃刀原则，所以我只说这3点（删除线）。&lt;/p&gt;&lt;p&gt;现在先从第一点来说，老规矩，咱要说得小学生都能看懂：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1、&lt;/b&gt;&lt;/p&gt;&lt;p&gt;现在有一个描述，你已经得知是最优解了。例如，对于若干个数字1和数字2，组成一个只允许使用加法运算的简单等式，让你描述：&lt;/p&gt;&lt;p&gt;那么，由于只允许使用加法，最简单的，当然是只用一个加法运算符的：&lt;/p&gt;&lt;p&gt;1+1 == 2&lt;/p&gt;&lt;p&gt;然而我们知道，这个也是正确的：&lt;/p&gt;&lt;p&gt;1+1 + 1 == 2 + 1&lt;/p&gt;&lt;p&gt;但是这种情况多用了两个加法运算符，就算这个描述是对的，由于我们只需要组成一个运算符，所以按照剃刀原则，这个应该抹去，而选择第一个描述，也就是对于这个命题下，我们选择的描述为：&lt;/p&gt;&lt;p&gt;1+1 == 2&lt;/p&gt;&lt;p&gt;很容易理解吧？所以，如果已知最优解了，当然应该选择使得信息量最少的最优解，而不是去添加一堆东西。&lt;/p&gt;&lt;p&gt;从第一点来说，因为每一个信息都有一个大于等于0的概率产生杂音，产生杂音就会降低准确率（&lt;a href="https://zhuanlan.zhihu.com/p/22805488" class="" data-editable="true" data-title="小学生都能理解的贝叶斯公式。 - 机器学习笔记 - 知乎专栏"&gt;小学生都能理解的贝叶斯公式&lt;/a&gt;里证明过了），所以在第一种理解前提下，奥卡姆剃刀当然是对的……&lt;/p&gt;&lt;p&gt;然后看第2种理解方式：&lt;/p&gt;&lt;p&gt;&lt;b&gt;2、&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在&lt;a href="https://www.zhihu.com/question/20159241" data-editable="true" data-title="「奥卡姆剃刀原则」是正确的吗？ - 哲学" class=""&gt;「奥卡姆剃刀原则」是正确的吗？ - 哲学&lt;/a&gt;问题中的&lt;a href="https://www.zhihu.com/people/20e911524247b63b55decfbe6080aceb" data-hash="20e911524247b63b55decfbe6080aceb" class="member_mention" data-editable="true" data-title="采铜" data-hovercard="p$b$20e911524247b63b55decfbe6080aceb"&gt;采铜&lt;/a&gt;先生（哎，得一年看不到这哥们的更新也是有点伤感。好怀念哪个剃刚毛的答案……），他对于奥卡姆剃刀的理解，个人感觉就是第二种方式。&lt;/p&gt;&lt;p&gt;那么为什么第二种方式的情况下，奥卡姆剃刀原则就不一定正确了呢？这里我举个例子。&lt;/p&gt;&lt;p&gt;假设，现在小明要向小白证明“我是你爸爸”（咦……我咋又玩起这个梗了……）。小明可以选择两个不同的描述方法集合，第一个描述方法领包含的信息熵为50KB，准确率为50%，第二个描述方法集合描述方法包含的信息熵为50GB，准确率为99.9%&lt;/p&gt;&lt;p&gt;那么，在这种情况下，如果单纯按照奥卡姆剃刀原则，选择了描述方法1，可能小明最后的证明就会以失败告终。而如果选择描述方法2，也许小明向小白传递信息的能力有限，50GB信息传送过去损失了一大半，结果最后准确率还不如99.9%。&lt;/p&gt;&lt;p&gt;那么这个问题怎么解决呢？&lt;/p&gt;&lt;p&gt;所以要考虑小明向小白证明我是你爸爸，需要达到多少的准确率？允许传递的最大信息量是多少？有多少前提条件需要考虑？大家的知识背景是什么……&lt;/p&gt;&lt;p&gt;所以各位看官明白了吧？这个问题，实际上就是因为对于问题的描述简化了，导致下一个问题变得无法解了。想来估计出题的脑残作者也对这个问题的描述使用了并不该使用的奥卡姆剃刀吧……（哎哟，别打脸……）&lt;/p&gt;&lt;p&gt;所以关于理解2，就出现了一个问题，如果并没有办法debug 出最优解，那么，就有可能发生剃胡子刮到肉的情况，这就是现实生活中为什么奥卡姆剃刀原则不是完全适用的。&lt;/p&gt;&lt;p&gt;但是从另一方面考虑的话（&lt;b&gt;接下来才是重点上面大部分信息是我在逗逼&lt;/b&gt;），可以这样理解，对问题的信息熵为I（X），对答案的描述的信息熵为I（Y,X）。&lt;/p&gt;&lt;p&gt;刚才这个解答过程犯得一个明显的错误是，分别单独考虑I（X）和I（Y,X），分别使用奥卡姆剃刀，而不是对 I（X） + I（Y,X）来用剃刀，所以事实上并没有满足所谓的最短信息描述，讲道理不仅没满足最短信息，这一拆开，连贝叶斯公式都没满足了。&lt;/p&gt;&lt;p&gt;也就是说，在理解2中，所谓的   奥卡姆剃刀，并不是最短信息描述。&lt;/p&gt;&lt;p&gt;嘛……但是很多人对于奥卡姆剃刀的理解的确确实就不是最短信息描述啊~~~~~~~~~~~&lt;/p&gt;&lt;p&gt;所以就出现了第三种理解方式：&lt;/p&gt;&lt;p&gt;&lt;b&gt;3、玄学の剃刀&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这就是大多数反剃刀的人理解了。怎么说呢？对于很多人来说，所谓的简单和复杂，并不是基于这个描述的信息熵的，而是基于这个描述我是否能直观看得懂。&lt;/p&gt;&lt;p&gt;举个例子，正太啊不对正态分布，这两种描述方式（都是图）&lt;/p&gt;&lt;p&gt;描述1：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-c7b035a4f2524387bcdce9bd30d580ad.png" data-rawwidth="273" data-rawheight="185"&gt;&lt;/p&gt;&lt;p&gt;描述2：&lt;/p&gt;&lt;equation&gt;\frac{1}{  \sigma\sqrt{2\pi }  } e^{-\frac{(x-\mu )^2}{2\sigma ^2} }&lt;/equation&gt;&lt;p&gt;对于大多数人来说，会直观觉得，嘛，描述1很符合直觉，一下就看得懂。描述2……撒撒撒，这都是些潵。&lt;/p&gt;&lt;p&gt;然而我问了问我家电脑，以他的理解，描述1，电脑认为它的信息量是3.14kb。描述2，电脑认为它的信息量是1.30KB。显然描述2对于电脑来说，是信息熵更低，也就是更简洁的。&lt;/p&gt;&lt;p&gt;当然，这里我不是说描述1和描述2谁更正确。我要说明的是两张表示内容一样（正太分布），表达载体一样（都是图），表达方式不相同，传递的信息量在不同信息接受体（比如人）中直观感受到的，也许并不一样。&lt;/p&gt;&lt;p&gt;而既然接受体都不一样，那自然无法得出一个普世的结论，得到的结论具有主观差异（加上理解2里说了，这玩意这样思考已经不一定满足贝叶斯公式了。），玄而又玄，那到底剃刀原理有没有效呢？这特么就成玄学了。&lt;/p&gt;&lt;p&gt;剃头匠老张头高兴了。嘿，二麻子你小子可以啊，把咱的剃刀说得有板有眼的，咱人民公社要发展轻工业，行嘞，就由二麻子你，负责生产奥卡姆剃刀吧！让全国的剃头匠，都用上咱的剃刀！把全国的男女老少，头都剃的像那红太阳般锃亮锃亮的！&lt;/p&gt;&lt;p&gt;啥……张大爷，这剃刀没法生产啊……&lt;/p&gt;&lt;p&gt;有什么没法的，人有多大胆，剃刀有多大产，有困难，自己克服！&lt;/p&gt;&lt;p&gt;散会&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22804193&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Sun, 09 Oct 2016 08:13:46 GMT</pubDate></item><item><title>小学生都能理解的贝叶斯公式。</title><link>https://zhuanlan.zhihu.com/p/22805488</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-045ce061c63f900e35ab1f3c0796fb16_r.png"&gt;&lt;/p&gt;班主任：你们两个在干什么？班长小红，给我过来，叙述一下事情经过！&lt;p&gt;小红，现在我是小白，你是小明，说一下你们为什么吵架！&lt;/p&gt;&lt;p&gt;好的老师，小明好坏好坏的，他莫名其妙过来，什么前提条件都不给，上来就是一句“我是你爸爸”&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-31ff9648ccaac01b9727458563237c91.jpg" data-rawwidth="136" data-rawheight="128"&gt;你接着就是一巴掌，然后说“你麻痹不给定前提条件，给我的就是个无信息先验分布，等同于前提条件等于正无穷，所以你说我是你爸爸这个结果的符合概率为1/∞≈0”所以说你的命题“我是你爸爸”的概率为0。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-fc1c3a17f58dafd3de901adc45664feb.jpg" data-rawwidth="176" data-rawheight="176"&gt;&lt;p&gt;然而经过我的验证，目前全世界有70亿+1人，而其中一定有一人是你爸爸，我是一个人的概率为1，所以在这个假定条件下，我有理由认为，P（我是你爸爸） = P（这个世界上有一个人是你爸爸）*P（我是一个人）/P（全世界人有70亿）=1*1/70亿的概率，我是你爸爸。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-9268138fac1663f6bb24363fbd476274.jpg" data-rawwidth="136" data-rawheight="128"&gt;&lt;p&gt;然后他摸了摸被打残的脸，微微一笑说，你忽略了一件事，我也是一个人，所以在你的假设条件下，我也有理由认为P（我是你爸爸） = P（这个世界上有一个人是你爸爸）*P（我是一个人）/P（全世界人有70亿）=1*1/70亿的概率，我是你爸爸。所以我是你爸爸的概率等同于你是我爸爸。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-fdab69335781852b2b82ed2079ee671c.jpg" data-rawwidth="176" data-rawheight="176"&gt;&lt;/p&gt;&lt;p&gt;那么，假设我们俩其中有一个人是对方爸爸，现在在这个样本下，我们俩互相是对方爸爸的概率为：&lt;/p&gt;&lt;p&gt;P（我是你爸爸/基于我们俩其中有一个人是对方爸爸） = P（全世界有一个人是你爸爸，这个人是我）/（P（全世界有一个人是你爸爸，这个人是我）+P（全世界有一个人是我爸爸，这个人是你））等于1/2，所以我有50%的概率是你爸爸而你只有1/70亿的概率是我爸爸！所以我是你爸爸。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-bc2e7266313a7a03f8c3b9c50e5996f3.jpg" data-rawwidth="178" data-rawheight="192"&gt;&lt;p&gt;然后你飞起就是一巴掌：你个SB，你的50%的概率建立在已经验证了“基于我们俩其中有一个人是对方爸爸”这个假定条件下，是个后验概率，我的1/70亿的概率基于还没有验证上面哪个假定条件的前提下，属于先验概率，拿后验概率和先验概率样本都不一样来比，你说你四不四潵？？？？？&lt;/p&gt;&lt;p&gt;说到这里，小红说。这时候我实在看不下去了，一会儿我是你爸爸，一会儿全世界有一个人是你爸爸的，这么长，还让不让人吵架了。于是我就上去劝说了一下：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-576b5f0d4694b804b6cfa8441e1f381c.jpg" data-rawwidth="180" data-rawheight="144"&gt;要不这样，我们把你们想要证明的“我是你爸爸”作为结论Y。你们的目的是证明结论Y的合理性，也就是概率，那么，你们要提出一些假设X，我们才能知道你们在假设空间X以下的概率instead of 而不是1除以无穷等于0。&lt;/p&gt;&lt;p&gt;然后呢，你们俩逗逼都是在从人的范畴里找符合定义，所以我们简单认为你们是基于个体为人这个单位个体的均匀先验分布假设这个分布为C，为某一个常数，（就打算是为1吧，反正待会儿要约掉）。&lt;/p&gt;&lt;p&gt;辣么在我们不知道具体数字的时候，我们给这个概率一个标志，既然是在假设空间X中Y的概率，辣么就称之为P（Y，x）。设若你们的所有假设在同一个假设空间C中，那么C就可以约掉，现在我们就考虑X单独发生的概率为P（x），Y单独发生的概率为P（Y），辣么x和Y同时发生的概率，就等于Y和x同时在一个共同的假设空间C发生的概率。也就是说，在假设空间x中，Y发生的概率，乘以假设空间x发生的概率，就等于反过来，在假设空间Y中，X发生的概率，乘以假设空间Y。&lt;/p&gt;&lt;p&gt;即：&lt;equation&gt;P(Y,x)*{P(x)={P(x,Y)*P(Y)
}} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;这样考虑我们要得到的目标P（Y，x），就可以放到等式左边，写为：&lt;equation&gt;P(Y,x)=\frac{P(x,Y)*P(Y)
}{P(x)} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;这就是你们的最佳假设。&lt;/p&gt;&lt;p&gt;然后你们的最佳假设，我们来算算P（Y，x），由于你们认定的全空间为C，那么：&lt;/p&gt;&lt;p&gt;P（Y）=1/C&lt;/p&gt;&lt;p&gt;P（x）=1/C&lt;/p&gt;&lt;p&gt;P（x，Y）=1/X（X为所有x的数量，也就是x所在的假设空间的容量大小）&lt;/p&gt;&lt;p&gt;辣么，就可以算出，P（Y，x）=1/X&lt;/p&gt;&lt;p&gt;小明很生气，辣么，如果我们的假设条件建立在相同的假设空间下，岂不是又是概率一样咯。那我如何向小白证明我是你爸爸呢？难道我们的友好讨论，就变成了提出更多的假设吗？这岂不是和小孩子吵架一样了么？&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-b41ac046ca9c773a2e343c4003a2bd8c.jpg" data-rawwidth="259" data-rawheight="194"&gt;&lt;/p&gt;&lt;p&gt;小红瞪了小明一眼：你们说的话是100%可信的么？不是100%可信不就有噪音么？所以，你们的假设x的概率不应该是你们的假设f（x），而应该&lt;equation&gt;f(x)+\varsigma &lt;/equation&gt;,这个&lt;equation&gt;\varsigma &lt;/equation&gt;表示的就是你们假设的杂音量。一般而言，你们这些正太瞎扯淡的噪音满足正态分布。&lt;/p&gt;&lt;p&gt;所以现在我们就要讨论下一个问题了，在描述了足以确认我是你爸爸的条件下，才能最大化证明假设我是你爸爸的正确性，那么，如何找到这个最大可能性呢？&lt;/p&gt;&lt;p&gt;所以我们做个最大似然假设，hmax，假设满足hmax要提出i个在区间I里的使用x符合要求的基本假设h，那么，这个hmax的概率就可以简单地假设为：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;h_{max} = max(\prod_{i\in I}P(Y,h) )&lt;/equation&gt;也就是说使表达式最大时的&lt;/p&gt;&lt;p&gt;由于我们这里的正确假设为h，那么就可以认为大Y是有一堆小y组成的，其中y=h(x)+&lt;equation&gt;\varsigma &lt;/equation&gt;，我们可以吧&lt;equation&gt;\varsigma &lt;/equation&gt;提到一边去，得到&lt;equation&gt;\varsigma &lt;/equation&gt;=y-h(x)。那么，由于&lt;equation&gt;\varsigma &lt;/equation&gt;满足高斯分布，所以得到&lt;/p&gt;&lt;p&gt;max：&lt;equation&gt;\prod_{i\in I}P(Y,h) = \prod_{i\in I}\frac{1}{\sqrt{2\pi \sigma ^2} } e^{-1/2(y-h(x_i))^2/\sigma ^2}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;嘛，看不懂无所谓。反正要约掉的，由于我们求的是最大值而不是具体值，求得是使该公式最大的时候的参数，所以就可以把杂七杂八的都约了，得到：&lt;/p&gt;&lt;p&gt;max：&lt;equation&gt;\sum_{i\in I}^{}-({y-h(x_i)})^2 &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;换言之，也就是找到&lt;/p&gt;&lt;p&gt;min：&lt;equation&gt;\sum_{i\in I}^{}({y-h(x_i)})^2 &lt;/equation&gt;（找到这个令这个公式最小时的参数）&lt;/p&gt;&lt;p&gt;所以说，你们要证明自己是对方爸爸，就要找到令你的论据，应对与你的假设空间，得到的差值的最小时的论据，这样才能最有可能证明自己是对方爸爸！！！！&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-029b194eb3d97ac7cf33f2d4b8a80fc3.jpg" data-rawwidth="176" data-rawheight="176"&gt;&lt;p&gt;接下来小红又继续说了。然而，你们对于我是你爸爸这个结果的描述（x）越多，得到的杂音根据公式也会越大，也就是说，对于我是你爸爸这条信息的置信度也就越低。相对应的，之前你们也说了，自己提出的假定条件越少，得到我是你爸爸这个结论的概率也会越低。所以，我们要找到一个在证明“我是你爸爸”时，最优的描述。&lt;/p&gt;&lt;p&gt;既然要得到的是，最优的描述，那么我们可以理解为，已经验证的条件下（验证以后发现）这个描述是最优描述，就是验证后概率最大的描述。假设条件为x，则描述得到的概率（最大后验公式），简单写为：P（Y，x）P（x）。&lt;/p&gt;&lt;p&gt;我们的目标就是令这个概率最大对吧？MAX：P（Y，x）P（x）&lt;/p&gt;&lt;p&gt;嘛……既然我们的目标是“描述”，那么也就是关于“信息”的处理，那么就参考一下香农的信息论：（&lt;a href="https://zh.wikipedia.org/wiki/%E4%BF%A1%E6%81%AF%E8%AE%BA" class="" data-editable="true" data-title="wikipedia.org 的页面"&gt;信息论：维基百科&lt;/a&gt;）&lt;/p&gt;&lt;p&gt;嘛……我也懒得看，所以我就随便抓了一个叫做熵的东西过来，熵嘛，这样定义的，&lt;equation&gt;I=-log(p)&lt;/equation&gt;，意思就是概率为p的事情包含的信息量，log的底数取决于信息量的单位，比如比特什么的……嘛。这里管不到。&lt;/p&gt;&lt;p&gt;然后我就就看我们要max的公式嘛，P（Y，x）P（x），取个对数（底是什么随便你）比如我们这里用log，就变成了使&lt;equation&gt;log(P(Y,x))+log(P(x))&lt;/equation&gt;最大,按照哪个熵里面log有个负号，就变成了：&lt;/p&gt;&lt;p&gt;使&lt;equation&gt;-log(P(Y,x))-log(P(x))&lt;/equation&gt;最小。&lt;/p&gt;&lt;p&gt;也就是min:   I(P(Y,x))+I(P(x))&lt;/p&gt;&lt;p&gt;翻译成人话就是，使描述的信息熵，对于描述：结论Y由x的假定条件，以及x的假定条件，总信息量最短的描述，就是最优描述，简称最短信息描述。&lt;/p&gt;&lt;p&gt;以小明和小白的观点就是：要达到证明我是你爸爸最准确，就得让“描述在某条件下，我是你爸爸”的信息，加上“描述某条件的信息”，总体来说最小。&lt;/p&gt;&lt;p&gt;嗯，这个最短信息描述在玄学界还有个别名，叫做奥卡姆剃刀……&lt;/p&gt;&lt;p&gt;老师，我说完了。&lt;/p&gt;&lt;p&gt;班主任：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-50840371889c51043f00f653f76cc473.jpg" data-rawwidth="158" data-rawheight="134"&gt;&lt;p&gt;小明和小白：所以我俩就合伙揍了她一顿。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-edfb8e29414728ce21a3c561c0d8753a.jpg" data-rawwidth="225" data-rawheight="225"&gt;&lt;/p&gt;&lt;p&gt;下课。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22805488&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Sat, 08 Oct 2016 01:31:39 GMT</pubDate></item><item><title>杂谈：（菜鸟如我）用哪些轮子？</title><link>https://zhuanlan.zhihu.com/p/22697926</link><description>其实对于普通人来说，不用分析那么多模型，用各种机器学习库就可以了。再者，对于我们这些小白来说，经常写算法写的不6，一个模型写下来循环多得不得了，测一下时间和复杂度，用人家的库，10秒解决的问题，自己编写的模型，有可能俩小时还没跑出来（别问我怎么知道的……）。&lt;p&gt;那么问题来了，用什么库？&lt;/p&gt;&lt;p&gt;个人是用python，这里推荐几个常用的……估计大家都知道，不过不知道的可以试着去用一下。&lt;/p&gt;&lt;p&gt;首推numpy。原因除了点乘之类的很多东西用这玩意很方便以外，最主要的一个原因……是大量的机器学习库，你如果没安numpy……会没办法运行……（别问我怎么知道的……）&lt;/p&gt;&lt;p&gt;然后是pandas（熊猫），其实可以不用熊猫，比如我就经常用graphlab而不用熊猫。不过如果不是因为替代品的原因的话，类似的结构化数据的库你总得用一个。原因？&lt;/p&gt;&lt;p&gt;我给你举个例子……大量机器学习比赛，会给你一堆生数据，然后你得清洗数据对吧？比如三张表合为一张表，三张表长度不同，顺序是乱的，你要根据用户名来组合。现在你不用结构化数据的库，自己编循环来for和if组合。三张表，至少得三张表的长度相乘的复杂度。假设平均每张表100000行数据（长度10000），你3个for循环if一下，按照一个if语句算一个复杂度，就是10^15的复杂度，假设每个复杂度你要跑1纳秒……嗯……大概12天能跑完。我试了一下，10万行的3张表合到一起，我这渣电脑用熊猫的join函数大概2秒钟吧……当然你用别的什么结构化的库都行，你用SQL也行。&lt;/p&gt;&lt;p&gt;然后随便用个结构化的库处理完了以后，就到机器学习库了。主推sklearn。为啥？便宜啊……便宜到了免费的程度。像我们这些一张高级显卡都要纠结买不买的屌丝，用免费的库再正常不过啦……&lt;/p&gt;&lt;p&gt;不不不，这不是主要原因。其实最主要的原因是因为Sklearn好上手。sklearn的文档非常详细，而且几乎所有的方法都给了范例，稍微跑跑很容易理解，（这点熊猫就做得很不好……在熊猫里大量的方法没给范例……对新人很不友好。）而且因为sklearn是免费库，所以大量的数据分析比赛都允许使用……嗯……说不准哪个四线小城市的比赛人家都在用excel分析数据你就用sklearn去拿了个两百块的奖金呢。&lt;/p&gt;&lt;p&gt;graphlab。&amp;lt;delete&amp;gt;推荐graphlab主要是因为我在coursera上学的另一门课是用graphlab教的所以我这种菜鸡就习惯了用这个&amp;lt;/delete&amp;gt;……其实主要是因为两个原因。1、这玩意的结构化数据库有范例，查起来方便（虽然要翻墙）。2是因为……不知道为什么，这玩意很多时候跑得比sklearn快。不过这玩意你得用要申请（或者买）用户ID，所以简单来说，你如果指着参加点数据分析的比赛拿奖金糊口，那这个只能用来验证模型。具体还得自己编（不过一般验证了模型以后编起来难度已经小多了……）&lt;/p&gt;&lt;p&gt;不过！这玩意有个非常好的好处，倒不是这个库怎么有好处，而是这个GraphLab Create的安装包……直接把常用库一并打包了。对于如我这种电脑三天两头崩溃的来说，重装库简直是灾难。直接安一个GraphLab Create，连着anaconda和jupyter都给安好了，多方便……&lt;/p&gt;&lt;p&gt;最后就是推荐jupyter notebook了。你习惯用其他IDE可以不用这个。推荐这玩意主要是因为……非常方便……谁用谁知道。举个例子，我现在几乎不开计算器和atom算东西了，无论是简单的还是复杂的问题，都用jupyter notebook写。这玩意比计算器用着方便，比文本编辑器用着顺手，比ide加载得快……好处多得我跟你说，给我一瓶啤酒我能吹一晚上……&lt;/p&gt;&lt;p&gt;就这些吧。顺便福利一下GraphLab Create我留在网盘里的备份，懒得一个一个安装库的可以下这个：链接：&lt;a href="http://pan.baidu.com/s/1pLhHWkb" class="" data-editable="true" data-title="baidu.com 的页面"&gt;http://pan.baidu.com/s/1pLhHWkb&lt;/a&gt; 密码：fp0l （啥？你问为啥不发github？当然是因为很多人没法翻墙啦！才不是因为我太菜了git用得不熟练呢。）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22697926&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Thu, 29 Sep 2016 17:07:13 GMT</pubDate></item><item><title>L0，L1，L2正则化</title><link>https://zhuanlan.zhihu.com/p/22505062</link><description>之前说起正则化，我们光说了加一个惩罚项&lt;equation&gt;\sum_{j=1}^{m}{\theta _j^2} &lt;/equation&gt;，用步长&lt;equation&gt;\lambda &lt;/equation&gt;来调节,但是为什么是这样，却没说。&lt;p&gt;那这篇文章就讨论一下为毛是这样，以及常用的别的惩罚项长啥样。&lt;/p&gt;&lt;p&gt;先说&lt;/p&gt;&lt;p&gt;L0正则化：&lt;/p&gt;&lt;p&gt;这玩意长这样：&lt;equation&gt;\sum_{j=1,\theta _j\ne 0}^{m}{\theta _j^0} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;0的0次方没有意义，在这里如果按照L1和L2看显然该定位0，这里讨论就是不参与惩罚项，不参与加权。即所有非零项算作1加起来，然后用步长&lt;equation&gt;\lambda &lt;/equation&gt;调节。意思很明显，每一个对预测产生了贡献的参数，我都惩罚一次，不多不少，大家都一样。就像一个法官判决，你偷了一毛钱，他杀了一个人，法官均以“价值观不正确”为由，把你们判一样的罪……只有一点都没参与的人，才不会被判刑。&lt;/p&gt;&lt;p&gt;很明显有问题，这玩意正则化后，岂不是无论什么样的函数，无论多么复杂，都往一根横着的直线上调节么？&lt;/p&gt;&lt;p&gt;还有一个问题，干嘛叫L0呢？直接调步数不就可以了么……&lt;/p&gt;&lt;p&gt;这个问题后面会说。&lt;/p&gt;&lt;p&gt;L1正则化：&lt;/p&gt;&lt;p&gt;这玩意长这样：&lt;equation&gt;\sum_{j=1}^{m}{|\theta _j|} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;拿法官举例子，就是，法官要按照你们的罪行量刑判罪，但是都得判，无论你影响最终是好是坏（比如你杀了个人，这个人也是个坏人，但是你还是犯了杀人罪得判刑）都按照罪行判罪。于是就都取个绝对值，表示都判，然后按照罪行大小判罪了……&lt;/p&gt;&lt;p&gt;这个地方估计大家可以理解了，惩罚项嘛，按照一个参数的影响来判才对嘛……&lt;/p&gt;&lt;p&gt;不过这还有个问题，x的绝对值，你给我求导一下看看，求出来就是 土1,大于0的时候是1，小于0的时候是-1，一个还好，一个向量X里的话，有神特么多个x，求个导出来能把人累死。&lt;/p&gt;&lt;p&gt;于是就引出L2了：&lt;/p&gt;&lt;equation&gt;\sum_{j=1}^{m}{\theta _j^2} &lt;/equation&gt;&lt;p&gt;就是我们看到的，笔记里记的了。平方了以后嘛，做包括求导在内的各种计算就方便了，啥，你说大了，无所谓啊，反正有个lamda来调节步长，谁在乎呢？&lt;/p&gt;&lt;p&gt;好吧……到这里我就在不用一堆专业术语的情况下，用这种糊弄的方式，勉强把L0，L1，L2解释出来了，啥欧氏距离马氏距离剃刀原则都糊弄过去没说，反正你编程也用不到这些玩意。不过还有一个问题，作为一个喜欢开脑洞的宅，万一某一天你好奇，为毛这要叫L0，L1，L2,为毛不叫L1，L2，L3，不叫个数，拉索和雷吉呢？我还想叫他山岭回归呢……&lt;/p&gt;&lt;p&gt;这个……你可以这样理解：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;L(\theta) = \sum_{j=1}^{m}{(\sqrt{\theta _j^2} )^p} &lt;/equation&gt;当p等于0的时候，就是L0，当P等于1的时候，就是L1，当p等于2的时候，就是L2.嗯……这下就师出有名了啊哈哈哈……&lt;/p&gt;&lt;p&gt;好了不扯淡了。实际上这玩意表示的是距离，比如x点和y点之间的距离：&lt;/p&gt;&lt;equation&gt;L(x,y) = \sum_{j=1}^{m}{(\sqrt{(x_j-y_j)^2} )^p} &lt;/equation&gt;&lt;p&gt;而正则化是表示到哪儿的距离呢？到0点，也就是和完全不改变原函数的区别（图我懒得找了，随便搜图搜个L2norm都能搜出来）。而由于是到0点，所以可以只保留一个参数，那么上面哪个&lt;equation&gt;L(x,y) = \sum_{j=1}^{m}{(\sqrt{(x_j-y_j)^2} )^p} &lt;/equation&gt;就变成了&lt;equation&gt;L(\theta,0) = \sum_{j=1}^{m}{(\sqrt{(\theta _j-0)^2} )^p} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;啊，差不多就糊弄完了……&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22505062&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Mon, 19 Sep 2016 12:24:44 GMT</pubDate></item><item><title>正规方程的推导过程</title><link>https://zhuanlan.zhihu.com/p/22474562</link><description>那啥，之前笔记里这部分是略过的。这里整理一下吧。有兴趣的可以对照看看和你推倒的过程一样不。&lt;p&gt;我们先回顾一下，我们定义观测结果y和预测结果y'之间的差别为Rss:&lt;/p&gt;&lt;equation&gt;Rss = \sum_{i=1}^{n}({y_i-y_i'} )^2= \sum_{i=1}^{n}({y_i-h(x_i)} )^2 = (y-h(X))^T*(y-h(X))&lt;/equation&gt;&lt;p&gt;设若参数的矩阵为&lt;equation&gt;\theta&lt;/equation&gt;,则&lt;equation&gt;h(X)=\theta*X&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;那么&lt;equation&gt;Rss  = (y-h(X))^T*(y-h(X)) =  (y-X*\theta)^T*(y-X*\theta) &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;按照我们的定义，这个Rss的意思是y和y'之间的差，那么当Rss无限趋近于0的时候，则y≈y'，即我们求得的预测结果就等于实际结果。&lt;/p&gt;&lt;p&gt;于是，令Rss等于某一极小值&lt;equation&gt;\delta &lt;/equation&gt;，则&lt;equation&gt;(y-X*\theta)^T*(y-X*\theta) ==\delta &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;对参数&lt;equation&gt;\theta&lt;/equation&gt;求导，得：&lt;/p&gt;&lt;equation&gt;\frac{d}{d(\theta)}(y-X*\theta)^T*(y-X*\theta)== 2X^T*(y-X*\theta)==0&lt;/equation&gt;&lt;p&gt;展开，得&lt;equation&gt; 2X^T*y==2*X^T*X*\theta&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;进而就可以得到&lt;equation&gt;\theta ==(X^T*X)^{-1}*X^T*y&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;于是我们就得到正规方程了。&lt;/p&gt;&lt;p&gt;再讲一个推导方式：&lt;/p&gt;&lt;p&gt;我们可以用矩阵乘法：&lt;/p&gt;&lt;equation&gt;Y=X\theta &lt;/equation&gt;&lt;p&gt;两边同时乘以&lt;equation&gt;X^T&lt;/equation&gt;&lt;/p&gt;&lt;equation&gt;X^TY=X^TX\theta &lt;/equation&gt;&lt;p&gt;然后再乘以&lt;equation&gt;(X^TX)^{-1}&lt;/equation&gt;&lt;/p&gt;&lt;equation&gt;(X^TX)^{-1}X^TY=(X^TX)^{-1}X^TX\theta &lt;/equation&gt;&lt;p&gt;就得到&lt;equation&gt;\theta = (X^TX)^{-1}X^TY&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;……不过这第二种方法是在知道了正规方程是什么以后再推导的。虽然看起来很快，然而并没有告诉你为什么。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22474562&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Sat, 17 Sep 2016 08:44:47 GMT</pubDate></item><item><title>小结及接下来的打算：</title><link>https://zhuanlan.zhihu.com/p/22440031</link><description>目前主干内容已经补充完了。还有大量内容没补充完，今后会继续补充。（比如正规方程怎么推倒啊不对推导的）&lt;p&gt;接下来我有个想法，个人学了几套不同的机器学习课程，感觉光是课堂布置的练习还是不够的，打算自己给自己布置个简单的结业项目。目前的想法是做这两个：&lt;/p&gt;&lt;p&gt;1、做一个简单的房价预测网站。&lt;/p&gt;&lt;p&gt;2、做一个知乎的答案推荐系统。&lt;/p&gt;&lt;p&gt;先做第一个，第一个的计划是这样的，分为3部分：&lt;/p&gt;&lt;p&gt;A、房地产数据获取及数据清洗：嘛，有现成的api最好，找不到的话，就自己爬虫爬，爬来了以后做一个协同过滤系统补完缺失数据。&lt;/p&gt;&lt;p&gt;B、通过数据预测房价。第一个项目就不用神经网络了，用线性回归就行了……做完再慢慢完善嘛。而且就算是个线性回归，不用别人的库，从头开始撸代码要写好了也不是一件容易的事……&lt;/p&gt;&lt;p&gt;C、做个简单的网站通过输入数据可以输出房价。就是做个网站而已。因为光是做了学习系统，以后万一跟别人吹牛逼也没好吹的，放到网站里，以后不管是找工作还是吹牛逼都可以拿出来说了……&lt;/p&gt;&lt;p&gt;目前的想法是，根据这3部分，建立一个学习小组，如果你有兴趣加入，至少得对其中一个部分能有帮助，能编写你的代码并教会小组内其他小伙伴。不接受其他方式加入，做完以后会征求组内意见把学习过程及内容放出来大家同意放出来的部分。&lt;/p&gt;&lt;p&gt;嘛，有人来一起做边做边学最好。没人来的话，反正我这水货的编程能力，用python爬虫也能编，很丑的学习系统也能编，用flask做个小网站也不是不会。只是我自己一个人的话边做边学就会很久咯。&lt;/p&gt;&lt;p&gt;有兴趣的可以联系我。两周内开始。&lt;/p&gt;&lt;p&gt;想要加入请私信我告诉我你能在3部分中，在&lt;u&gt;哪一部分帮助到其他同学&lt;/u&gt;（例：你可以发私信的时候告诉我说你爬虫爬的很不错，会爬数据并整理到sql里云云的都可以。总之门槛很低，但是你必须得能分享知识给别人。除此之外不接受任何加入方式。）。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22440031&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Wed, 14 Sep 2016 08:03:57 GMT</pubDate></item><item><title>第十周笔记：大量数据算法</title><link>https://zhuanlan.zhihu.com/p/22168288</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/ad95d41a014fb95c79712a446925d4d0_r.jpg"&gt;&lt;/p&gt;（第九周笔记还有推荐系统没补充完，不过要补充好推荐系统还得查很多资料，所以暂时没有补充的欲望。）&lt;p&gt;首先一个问题，是否需要用大数据？&lt;/p&gt;&lt;p&gt;记得之前笔记（第六周笔记）中，我们分析过一种情况，就是当样本数量m达到一定的程度时，增加样本数量，对于交叉验证集和训练集之间得出的误差的差距，并没有太多减少的空间时，就不再需要增加样本量。&lt;/p&gt;&lt;p&gt;一句话，当分别计算J_cv和J_train在当前样本数量下的误差时，差距极大时，应当使用更多数据，差距极小时，就不应当使用大量数据了。&lt;/p&gt;&lt;p&gt;那么，大量数据时，用什么算法呢？&lt;/p&gt;&lt;p&gt;数据太多，如果还按照批量梯度下降来玩，估计一个因子都得算个三五天，整个公式出出来，改一改，一个月就过去了……&lt;/p&gt;&lt;p&gt;那就不用所有因子来玩嘛。一步一步局部下降（贪婪算法）也不错嘛！但是贪婪算法也有问题，那就是，如果你的数据凹凸不平的，有很多个局部最优，那就会发生如下情况：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/d0b5aa7bdd8bcab2ff1bcaa1c31e5d7e.png" data-rawwidth="471" data-rawheight="324"&gt;&lt;p&gt;那咋办呢？是的，你可以多重复随机选初始点迭代几次，然后在交叉集上比较，但是万一这种坑坑洼洼的地方很多，那你得重复很多次，运气不好兴许一辈子就过去了~&lt;/p&gt;&lt;p&gt;那我们就不用局部最优，还是得用全部数据，那不就慢了么，怎么办呢？那就每次就一个数据就拿去改变参数，凑合凑合得了。由于很随便，所以就叫它随便，啊不对随机梯度下降……&lt;/p&gt;&lt;p&gt;&lt;b&gt;随机梯度下降：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们先看看以前的批量梯度下降的公式：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\theta_{j}=\theta_{j}-\alpha *\frac{d}{d\theta_{j} } J_{\theta} &lt;/equation&gt;=&lt;equation&gt;\theta_{j}=\theta_{j}-\alpha /m*\sum_{i}^{m}{} (h(x_i) -y_{i}  )*x_{i,j}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;那么，现在我们改成了每次随便挑一个数据，于是公式就成了：&lt;/p&gt;&lt;equation&gt;\theta_{j}=\theta_{j}-\alpha /m*{} (h(x_i) -y_{i}  )*x_{j}&lt;/equation&gt;&lt;p&gt;也就是说，other than 把所有的数据拿来加一遍再改变参数值，我们现在变成了急不可耐地看到一个数据就改变一次参数值……&lt;/p&gt;&lt;p&gt;啥，你说这不就是贪婪算法，并不随机么……？&lt;/p&gt;&lt;p&gt;那好，那就……执行之前，把数据打乱，python的话就randshaffle一下，于是就随机了呗~~~&lt;/p&gt;&lt;p&gt;那么最后还有一个问题，我们以前的批量梯度下降，是一遍又一遍地迭代，直到收敛到某一个范围内。现在，你每个数据就下降一次，需要重复多少次呢？&lt;/p&gt;&lt;p&gt;不重复啊。说了这是给大量数据准备的，我们就假吧意思大量数据来说下降一次就够了。于是我们就总共只下降一次……只是对于其中，每个数据迭代的时候，就直接改变参数值&lt;equation&gt;\theta&lt;/equation&gt;了。所以你也看到了……由于只下降一次，数据不够多时就别用了。误差会很大的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;迷你批量梯度下降（略拗口）：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们也可以换个思路，还是用批量梯度下降，只是这时候不用所有数据，而是和刚才哪个随机梯度下降的思路结合一下。批量一度下降是所有数据一起算一次，改变一次参数值对吧？随机梯度下降时每个观测数据都拿来改变一次参数值对吧？咱中国人讲究中庸嘛。就既不拿所有的，也不光拿一个，而是一小批一小批地拿来下降，比如每次10个数据下降一次啦，每次20个数据下降一次啦。之类的。&lt;/p&gt;&lt;p&gt;那么我们怎么选呢？&lt;/p&gt;&lt;p&gt;把数据随机分成x份，每份里面有10~100个数据。以每份b个数据举例，我们要迭代的参数公式就成了这样：&lt;/p&gt;&lt;p&gt;i=1,b=10(假设b为10)&lt;/p&gt;&lt;p&gt;while i &amp;lt; m&lt;/p&gt;&lt;equation&gt;\theta_{j}=\theta_{j}-\alpha /m*\sum_{i}^{i+b-1}{} (h(x_i) -y_{i}  )*x_{i,j}&lt;/equation&gt;&lt;p&gt;i += b&lt;/p&gt;&lt;p&gt;差不多就是这样……意思就是从每个拿来改变一次参数&lt;equation&gt;\theta&lt;/equation&gt;，变成了每b个拿来改一次参数&lt;equation&gt;\theta&lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;检查收敛：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;那么现在又有一个问题来了。我们数据多了就变懒了，算一次凑合凑合过了，万一不收敛怎么办？所以还得检查收敛……&lt;/p&gt;&lt;p&gt;怎么检查收敛呢？&lt;/p&gt;&lt;p&gt;拿出我们的代价函数：&lt;equation&gt;cost=1/2*(h(x_{i} )-y_{i}  )^2&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;每迭代k次，就计算一下。最后把图画出来，看看收敛不收敛。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/e04f380c74c1942eaeca162e3d2009ff.png" data-rawwidth="720" data-rawheight="419"&gt;蓝色是1000次看一下，红色是5000次看一下。横坐标是看的次数，纵坐标是当前看的时候cost的值。&lt;/p&gt;&lt;p&gt;可以看到，第一幅图是在收敛的表现，到达最低点后开始震荡。第二幅图意思是你看的范围越大，看到的振幅越小（批量梯度下降看起来就很光滑了。相反，k=1的时候，看起来就心电图了……）第三附图表示的意思是，如果你看着震荡不确定到最小没有，增加k，就可以知道到底是应该优化算法还是只是杂音造成的震荡。第四附图当然就是发散了，这时候可以试试小一点的&lt;equation&gt;\alpha &lt;/equation&gt;值。&lt;/p&gt;&lt;p&gt;接下来又有一个问题来了。大量数据一般都不是一次就给完的，很多时候，是连续给你赛数据，算法也要与时俱进嘛。当然，这时候你就可以直接用上面说的两种算法，新数据直接分成几份，算了就马上拿去改变参数值。这里可以让客户端完成一部分简单的数据处理再交给服务器，比如服务器每天计算一次参数，客户端通过这些参数，把缺失数据补充完整，再返回给客户端。又例如如果网络恨不好，而数据量足够客户端依据数据计算出一个代价函数，返回这个代价函数而不是数据，也许能提高一定的效率。嘛……再多的就要学分布式处理了。这里就不多扯淡了、&lt;/p&gt;&lt;p&gt;第11周的笔记内容很少，大致就是讲一下如何涉及一个机器学习的解决计划，以及感谢收看云云。在这里一并写了吧。基本思路就是，把一个复杂的机器学习问题分解成n个小问题（当然你也可以让神经网络自己分解去……）例如你要让机器自己看书，你就可以这样分解问题，让机器识别出书，让机器通过书识别出段落，语句，让机器通过段落语句识别其中的文字关联，进而识别出其抽象化的意思。让机器通过抽象化的意思重新组合合成你需要的信息。这种，把一个复杂的问题拆分成n个细小的问题一个一个解决。&lt;/p&gt;&lt;p&gt;常见的，例如我们要自动分析一个地区菜价，拿上来的数据不可能100%完整，那么就可以分为两套来做，第一个系统用来补充不完整信息（天气，道路交通，政府政策等），第二个系统用第一个系统已经加工好的信息来分析出想要的数据，第三个系统根据这些数据来预测未来的菜价。&lt;/p&gt;&lt;p&gt;实际过程中，由于每次系统都有偏差，最后总准确率会不断下降。例如上面说的哪个分析菜价的，也许总准确率为80%，然后如果是给完善的信息，不考虑第1个系统的误差，准确率是90%，如果给完善的数据，直接用来预测，准确率是98%。那么就可以简单的理解为，系统1可改善上限为10%，系统2可改善上限为8%，系统3可改善上限为2%。假如时间有限，也许就应该把宝贵的时间主要用以改善系统1和系统2。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22168288&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Tue, 13 Sep 2016 08:46:36 GMT</pubDate></item><item><title>第九周笔记：密度估计</title><link>https://zhuanlan.zhihu.com/p/21898453</link><description>考虑一个产品，每个工厂生产线都有一定的概率产生次品。假设在用户退货之前，我们没法知道一样东西是否是次品。那么我们只能通过产品的各项指标估计（重量，硬度，曲率，发热量等，不同产品指标不一样）将合格的某项指标画在图上，也许会得到这样一个图（左）：&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/606054110fbd2fbdcc26ea5e5aaf8356.png" data-rawwidth="500" data-rawheight="250"&gt;把图按照浓度梯度描红，得到右边的这个区域。按照聚类的思考方式，大概就可以理解为，越接近高浓度区域，其正品概率越高。而越不太可能产生落点的区域，其次品的概率越高。&lt;/p&gt;&lt;p&gt;用数学语言描述就是：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;p(x)&amp;lt;\xi &lt;/equation&gt;时，产品判断为次品。&lt;equation&gt;\xi &lt;/equation&gt;为异常判断参数，&lt;equation&gt;p(x)&lt;/equation&gt;为当前特征的产品产生概率。&lt;/p&gt;&lt;p&gt;&lt;equation&gt;p(x)=\prod_{j=1}^{n} p(x_j,\mu _j,\sigma _j)&lt;/equation&gt;，其中&lt;equation&gt;\mu _i = \frac{1}{m} \sum_{i=1}^{m}{x_j^{(i)}} &lt;/equation&gt;，&lt;equation&gt;\sigma _j^2=\frac{1}{m} \sum_{1}^{m}{(x_j^{(i)}-\mu _j)^2}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;当然，你可以简单地把正常和异常记作y=1和y=0.&lt;/p&gt;&lt;p&gt;选&lt;equation&gt;\xi &lt;/equation&gt;的方法，就是之前（第六周笔记）里计算F值，取F值最大时的&lt;equation&gt;\xi &lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;那么问题来了，既然可以用y=1和y=0来标记出正常和异常，为什么不用监督学习，而要用密度估计呢？&lt;/p&gt;&lt;p&gt;因为大多数情况下，生产线上的异常值相对正常值来说，是极其少量的。在这种情况下，如果用监督学习，那么就没有足够的负样本用于训练集。此时，为了准确，我们就可以只把负样本用于交叉验证集与测验集，而训练集不用负样本。既然不用负样本，那么训练的时候也只要用非监督学习了……&lt;/p&gt;&lt;p&gt;还有一种可能，负样本相对与数据来说，非常奇怪，每一个负样本都不一样，或者未来可能产生的负样本具有不确定性，那么这种情况下，负样本无法建模，没法判断，只能通过和正样本的差异来判断是否为负样本。这种情况下，显然训练时，只用正样本的非监督学习是优于监督学习的……&lt;/p&gt;&lt;p&gt;反之，如果样本特征明显，可预测，未来样本可确定，且正负样本均足够多的情况下，用监督学习更优。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21898453&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Wed, 10 Aug 2016 18:46:02 GMT</pubDate></item><item><title>第八周笔记：聚类（clustering）</title><link>https://zhuanlan.zhihu.com/p/21798972</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/2e901f902f1351d6824c7d5e27b9d8aa_r.jpg"&gt;&lt;/p&gt;聚类很好理解。你左手在地上撒一把盐，右手在地上撒一把糖。假设你分不清盐和糖，但是你分别是用左右手撒的，所以两个东西位置不同，你就可以通过俩玩意的位置，判断出两个东西是两类（左手撒的，右手撒的）。然而能不能区别出是糖还是盐？不行。你只能分出这是两类而已。但是分成两类以后再去分析，就比撒地上一堆分析容易多了。&lt;p&gt;聚类是典型的非监督学习。上面例子中，如果把盐和糖改成白色盐和染成黄色的糖，你可以通过颜色分析，颜色就是标签，有标签就是监督学习。没标签就是非监督学习。&lt;/p&gt;&lt;p&gt;聚类算法常用K均值算法：&lt;/p&gt;&lt;p&gt;随机选择包含K个中心的cluster(&lt;equation&gt;\mu_1,\mu_2,\mu_3,\mu_4,...,\mu_k&lt;/equation&gt; ,&lt;equation&gt;K\in R&lt;/equation&gt;)&lt;/p&gt;&lt;p&gt;以K=3为例，假设特征量是啥我也不知道……反正有特征量，现在画3个圈圈：&lt;/p&gt;&lt;p&gt;随机找3个点（图中①，②，③的蓝色点）当然，你找的点不一定就马上在中心了。有可能3个点都跑一个cluster里面去，无所谓……&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/7f7dbc4a792c474dc10b8b5159b05770.png" data-rawwidth="359" data-rawheight="372"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/c98d20495d645ca6b8c27c8c5d33e0be.png" data-rawwidth="385" data-rawheight="398"&gt;&lt;p&gt;然后开始迭代，每次迭代方法如下：&lt;/p&gt;&lt;p&gt;对于每个特征点&lt;equation&gt;x_i&lt;/equation&gt;，找到和特征点&lt;equation&gt;x_i&lt;/equation&gt;最近的那个&lt;equation&gt;\mu _i&lt;/equation&gt;（例如&lt;equation&gt;\mu _1&lt;/equation&gt;），把每个和&lt;equation&gt;\mu _1&lt;/equation&gt;最近的点放到一起。对于每个&lt;equation&gt;\mu _i&lt;/equation&gt;都这个操作，例如你选了3个点，就是3个&lt;equation&gt;\mu _i&lt;/equation&gt;（&lt;equation&gt;\mu _1&lt;/equation&gt;，&lt;equation&gt;\mu _2&lt;/equation&gt;，&lt;equation&gt;\mu _3&lt;/equation&gt;）&lt;/p&gt;&lt;p&gt;然后对于每个&lt;equation&gt;\mu _i&lt;/equation&gt;的最近点&lt;equation&gt;x_i&lt;/equation&gt;，取个平均值&lt;equation&gt;\bar{x_i} &lt;/equation&gt;，当作该&lt;equation&gt;\mu _i&lt;/equation&gt;的新值。然后你会神奇的发现，每个&lt;equation&gt;\mu _i&lt;/equation&gt;都朝着一个cluster的中心近了一步！（一点也不神奇好吗……）&lt;/p&gt;&lt;p&gt;重复n多次，直到收敛为止……&lt;/p&gt;&lt;p&gt;这是别人网站上可视化的具体收敛过程，做得很好这里推荐一下：&lt;a href="https://www.naftaliharris.com/blog/visualizing-k-means-clustering/" data-editable="true" data-title="Visualizing K-Means Clustering"&gt;Visualizing K-Means Clustering&lt;/a&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/d22d2960559a5a5ac8503b94c8f1fb01.png" data-rawwidth="354" data-rawheight="382"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/788ce87a02bf4de1903c6e4470d52ce4.png" data-rawwidth="377" data-rawheight="387"&gt;&lt;p&gt;你就会神奇地发现这3个点把区域按照某种神奇的规则分出来了（这个规则就是K均值最小……）这3个不同的区域就是3个cluster。&lt;/p&gt;&lt;p&gt;具体的算式如下：&lt;/p&gt;&lt;equation&gt;J=\frac{1}{m} \sum_{m}^{i=1}{(x_i-\mu _c^{(i)})} &lt;/equation&gt;&lt;p&gt;由于这次咱不是监督算法了，所以没法去压导数去迭代。上文也说了怎么迭代，所以这次迭代方法是：&lt;equation&gt;minJ(c^{(1)}......c^{(m)};\mu ^{(1)}......\mu ^{(k)})&lt;/equation&gt;不和导数扯上关系了，自己根据每个点算最小值不断迭代到收敛就是了……&lt;/p&gt;&lt;p&gt;然后就是选每个初始K的点也有技巧，一旦选不好，有可能3个点选到一个簇，本来应该是分成3个圆形片区，结果把整张图片分成3个长方块的也不是不可能……&lt;/p&gt;所以要随机选，为了保险，可以多随机几次。&lt;p&gt;然后就是另一个问题了：&lt;/p&gt;&lt;p&gt;&lt;b&gt;K元素选几个？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;选少了，偏差很大。选多了，过拟合了。如果愿意画图，有个方法叫肘子方法（我饿了……）&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/4695f9282ddbcac507e6697fda29ac78.png" data-rawwidth="395" data-rawheight="287"&gt;诺，就是画这么一个图，那个大概是肘子的地方就是我们要选的K的个数……不过肘子方法缺点很明显，老画图人工去看，一点也不机器。&lt;/p&gt;&lt;p&gt;&lt;b&gt;维数缩减（Rimensionality Reduction ）：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;举个例子。你做房子的数据的爬虫。假设全是正方形房子，爬虫中有这3个特征量：面积，长度，宽度。那么因为面积和长度宽度相乘相等，就可以缩减了。再例如，你爬到了一个是摄氏度，一个是华氏度，由于数据本身是一样的，也可以缩减了。把实际内核为同一个特征的的缩减到同一个，就是维数缩减。估计你也看出来了。K的个数就是维度嘛……所以维数缩减一定程度上可以看作选择k的个数。&lt;/p&gt;&lt;p&gt;当然……既然数据缩减了，储存需要的空间也就小了。（对于我们这种屌丝来说，省一张硬盘就是几百块钱，四舍五入就是一个亿啊……多缩减几次数据就省出一个王思聪了。）&lt;/p&gt;&lt;p&gt;实际做法，将J维的数据投影到一个I维空间上。&lt;/p&gt;&lt;p&gt;比如这样：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/61b75d03e002a7a17b8b875ae1d39ea8.jpg" data-rawwidth="224" data-rawheight="225"&gt;二向箔攻击！（容我中二一下……）三维空间里的数据就被压缩成了二维的了……~&lt;p&gt;通常要可视化数据的话，由于我们人类很傻，只能看到3维，所以一般要把数据降到3维或者二维……举个例子……把一个国家杂七杂八的经济发展指标压缩成一个GDP，然后分别看人口和GDP关系，工人比率和GDP关系……这种。&lt;/p&gt;&lt;p&gt;&lt;b&gt;主成分分析算法（PCA）：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;上文说了一个可视化数据要压缩，那怎么知道压缩了以后不会产生极大偏差呢？要投影数据，那么投影到哪个屏幕啊不对平面上呢？&lt;/p&gt;&lt;p&gt;实际做法：找能使投影误差最小的那个维度（平面）&lt;/p&gt;&lt;p&gt;定义i个向量，将J个特征量投影到这i个向量组成的子空间上。&lt;/p&gt;&lt;p&gt;如下：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\mu _i = \frac{1}{m} \sum_{i=1}^{m}{x_j^{(i)}} &lt;/equation&gt;，&lt;equation&gt;\mu _i &lt;/equation&gt;这玩意就是我们说的向量。&lt;/p&gt;&lt;p&gt;然后把原来的&lt;equation&gt;x_j^{(i)}&lt;/equation&gt;变成&lt;equation&gt;\frac{x_j{(i)}-\mu _i}{s_j} &lt;/equation&gt;。&lt;equation&gt;s_j&lt;/equation&gt;是数据归一化用的玩意（例如方差，最大值什么的。随便你选个你喜欢的）。&lt;/p&gt;&lt;p&gt;然后问题来了，咱算数据肯定是一列一列算，谁闲着没事一个一个算。那么如何一列一列算呢？&lt;/p&gt;&lt;p&gt;定义sigma，&lt;equation&gt;Sigma=\frac{1}{m}\sum_{a}^{b}{(x^{(i)}*x^{(i)T})}  &lt;/equation&gt;拿矩阵说的话就是&lt;equation&gt;Sigma=\frac{1}{m}*X'*X &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;然后用一个Octave里面的算法svd，神奇的事情就发生了……&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;[U,S,V]=svd(Sigma)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后我们就得到了包含n个（特征量个数）&lt;equation&gt;\mu _i &lt;/equation&gt;的一个矩阵U了。然后我们要缩减到k个纬度，就从这个矩阵U里面提取前k个就行了（误差好大的感觉……不过算了。压缩数据本来误差就大）。&lt;/p&gt;&lt;p&gt;用Octave的话就是这么个玩意,假设我们压缩到k个纬度。：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;U_reduce=U(:,1:k);
Z=U_reduce'*X
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后就可以用这个U_reduce开开心心地投影出了我们要的z(压缩过的特征量)了。&lt;/p&gt;&lt;p&gt;那假如我后悔了。我不该压缩我亲爱的数据的，我一个臭屌丝又没钱买硬盘所以并没有数据的备份，那我想要回最初的数据咋办呢？&lt;/p&gt;&lt;p&gt;那就……那就算一算原数据吧……&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;由于：Z=U_reduce'*X&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;1所以我们可以反推X：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;X=U_reduce*Z&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这个X当然是有误差的。所以就别写作X了，写作X_approx好了……&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;X_approx=U_reduce*Z&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后就是自动选K的问题。自动选K这事很有趣，如果X被压缩以后，解压缩回来误差很小，那大概可以认为此时选取的k是比较合适的。方法如下：&lt;/p&gt;&lt;p&gt;计算误差&lt;equation&gt;error=\frac{\sum_{i=1}^{m}{(x_i-x_{i,approx})}^2 }{\sum_{i=1}^{m}{x_i} ^2} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;如果error&amp;lt;0.01的话，那这时候的k大概就在肘子那里了……当然，你也可以写个循环，找出error最小时的k。&lt;/p&gt;&lt;p&gt;顺便说一句，PCA也可以用于监督学习中，以便我们这些穷B减少自己心爱的计算机的负荷（好心酸。我要买个显卡坞来增加运算能力，谁也别拦着我！）具体的做法就是把&lt;equation&gt;(x_i,y_i)&lt;/equation&gt;PCA成&lt;equation&gt;(z_i,y_i)&lt;/equation&gt;，然后代入假定函数中。当然，既然特征量已经从x变成z了。记得在用交叉函数J_cv和测试函数J_test的时候，也要把x变成z。因为特征量的数据并不同所以需要再变一次，这地方容易出错……&lt;/p&gt;&lt;p&gt;由于PCA整个方案都没用到y，所以过拟合问题并不能用PCA来降维攻击，还是老老实实的用正则化吧……正则化简单粗暴还不&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21798972&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Sat, 30 Jul 2016 23:03:40 GMT</pubDate></item></channel></rss>