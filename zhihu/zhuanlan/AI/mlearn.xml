<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>机器学习笔记 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/mlearn</link><description>机器学习的笔记。非常浅显。不得不说ng的课很适合入门，几乎不需要太多的数学基础。</description><lastBuildDate>Fri, 10 Mar 2017 20:16:15 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>小数据想用机器学习咋整？</title><link>https://zhuanlan.zhihu.com/p/25566686</link><description>&lt;p&gt;“子楠，我今天谈这个公司打算试着做一下土壤调节人工智能，包括室内动态室内水培这类的东西。”&lt;/p&gt;&lt;p&gt;“好，数据拿来在咱的快速验证系统上跑一下，看看可行性。”&lt;/p&gt;&lt;p&gt;“呃……那么问题来了。只有20组数据……”&lt;/p&gt;&lt;p&gt;……&lt;/p&gt;这半个月来北京和&lt;a href="http://www.zhihu.com/people/901ad779582a43a79f86a55004f0410f" data-hash="901ad779582a43a79f86a55004f0410f" class="member_mention" data-title="@许铁-巡洋舰科技" data-editable="true" data-hovercard="p$b$901ad779582a43a79f86a55004f0410f"&gt;@许铁-巡洋舰科技&lt;/a&gt;一起做项目（整天敲代码所以没啥时间刷知乎了），较为深刻的一点感受就是，很多公司想抓住机器学习人工智能的风口走一个，然而这些公司里，有很多人对数据的存留方式几乎可以用“落后”来形容……从几十万张纸片上的信息，到各路联络员脑子里的记忆，讲道理，客户说他有十万组数据，清洗完能有一千行可用的都谢天谢地了。&lt;p&gt;就比如说之前接的甲方预测土地报价的项目，甲方给我们做模型用的数据，大大小小清洗完，也就剩下了六百来个可用的……这可用的里面还有某些莫名其妙的原因导致应该肉眼去除的数据，比如北京某块十几亩的地一万块钱卖了这种，就算数据真实……对于我们想要构建的模型来说他也是杂音。&lt;/p&gt;&lt;p&gt;再处理一下，不到600个能用的数据……特征dummy化以后光省市区都300来个……这是要搞事情么……&lt;/p&gt;&lt;p&gt;那么第一步……就应该找同类网站的数据，利用别人的数据，来训练模型，用这600做交叉验证，测试集什么的……要不就迁移学习一下……这里为了方便看，就把甲方的数据叫做A，其他地方的BCDE往后跑吧……&lt;/p&gt;&lt;p&gt;于是我来之前，就一直在写各种模型，以MAPE做参数调整，从之前在0.6左右，后来调到了0.55。&lt;/p&gt;&lt;p&gt;但是MAPE这个判断参数有一个很严重的问题。在数据集不一样的情况下，同一个模型，MAPE会完全不同。受到杂音影响非常大。果不其然，A数据集上0.55的MAPE，到B数据集上就1300多，C数据集上甚至有3000多，这特么的……没法判断模型有效性了啊。&lt;/p&gt;&lt;p&gt;所以得改一个判断有效性的方法。接下来我们就选择了非常简单有效的“范围内准确率”。比如这块地叫价50万，人一般会网高了叫10万，那么，预测40~60万都认为是准确的。&lt;/p&gt;&lt;p&gt;在范围内准确率这个概念上，对模型验证以后，发现一件很恶心的事情，在之前MAPE表现最优的模型和数据集上，准确率在我们设定的标准范围内，在本数据集的测试集上只有30%……&lt;/p&gt;&lt;p&gt;对报价的预测只有30%的准确率，甲方显然很难满意。不过此时这个准确率受到杂音的影响变小了，我们可以开开心心地在其他数据集上试试了……&lt;/p&gt;&lt;p&gt;然而并不开心……&lt;/p&gt;&lt;p&gt;B数据集训练的模型，在C上只有5%的准确率。&lt;/p&gt;&lt;p&gt;C数据集上的模型，在B上也只有7%左右的准确率……&lt;/p&gt;&lt;p&gt;总而言之就是，无论是B,C,D,E上面的数据集，对于其他的数据集，预测准确率都不到10%&lt;/p&gt;&lt;p&gt;……&lt;/p&gt;&lt;p&gt;那么，问题来了。&lt;/p&gt;&lt;p&gt;到底是过拟合了，还是数据的原因呢？&lt;/p&gt;&lt;p&gt;如果是过拟合，那么为何在本数据集上的测试分数也那么低？难道不同源的数据，对同一个问题的解释不一样么？&lt;/p&gt;&lt;p&gt;如果是数据的原因，那么具体是什么原因呢？是人们乱报价，还是网站出于某些原因（比如一开始客户太少，罗玉龙，马甲甲什么的人，就伪造销售量，伪造评论来装作有客户）生成的虚假数据呢？&lt;/p&gt;&lt;p&gt;过拟合这个问题基本上不用考虑了……本数据集内测试集上准确率都低于30%（几十个模型全试过了）……我个人认为应该是杂音问题。然而方差，聚类，肉眼看到北京10万元一公顷的地这类方法除杂音都试过了，再除杂音，除非认为数据基本上大部分都是杂音，这咋整呢？&lt;/p&gt;&lt;p&gt;这就是小数据常见的问题了。数据少，质量差，机器很难学习。这种时候，就需要去了解甲方的信息来源，了解数据从“质”上面的问题，以找到改进方法。&lt;/p&gt;&lt;p&gt;于是&lt;a href="http://www.zhihu.com/people/901ad779582a43a79f86a55004f0410f" data-hash="901ad779582a43a79f86a55004f0410f" class="member_mention" data-hovercard="p$b$901ad779582a43a79f86a55004f0410f"&gt;@许铁-巡洋舰科技&lt;/a&gt;出差了一趟和联络员面基（大误（了解信息），我留在办公室继续敲代码……&lt;/p&gt;&lt;p&gt;回来以后告诉我一个很有趣的事情，农村土地价格预测，往往会往高了叫2~5倍的价格，有些地方叫到十几倍都有可能……纯粹是看心情。&lt;/p&gt;&lt;p&gt;这……甲方要我们预测报价……&lt;/p&gt;&lt;p&gt;于是我们把准确率判定范围改为了“2~5”倍，忽然准确率就到90%了。&lt;/p&gt;&lt;p&gt;当然，你说我们预测50万块钱的地范围在10~250万内，甲方肯定要冲上来揍你。他才不管实际人家报价就是会高叫2~5倍呢……&lt;/p&gt;&lt;p&gt;那么我们又考虑了相对不会乱报价的租地预测……很容易就达到了60%的准确率。稍微一调参，70%的准确率就有了。&lt;/p&gt;&lt;p&gt;……&lt;/p&gt;&lt;p&gt;为毛甲方要我们预测报价呢……为什么不给我们成交数据预测成交价或者让我们预测土地出租价格呢……我很迷茫啊。&lt;/p&gt;&lt;p&gt;算了，想不通就不想了。甲方永远是对的。&lt;/p&gt;&lt;p&gt;我们要做的就是，如何把这30%的准确率提高上去那么一丢丢……&lt;/p&gt;&lt;p&gt;杂音大，数据少，神经网络是指望不上了。那么，我们能否把一些客观经验加进去，以此来增加模型中的有效信息，进而提高模型效果呢？&lt;/p&gt;&lt;p&gt;比如，在其他条件不变的情况下，面积加大一倍，售价提高一倍这种客观经验，在模型中是否有效呢？&lt;/p&gt;&lt;p&gt;嘛……试试就知道了嘛。反正提前拿出一部分数据藏起来作test集，然后再去对train集随便折腾。比如我的方法就是，copy一份train的数据出来，其他特征不变，面积和价格全部增加1.1倍。然后看看准确率是否得到了提高……&lt;/p&gt;&lt;p&gt;结果，果不其然（令人惊讶）的是，在A,B,C,D的每个数据集中，准确率都有提高，多的10%，低的2%，平均提高了5%左右（虽说也就是从30%提高到了35%，甲方肯定还是不买账）&lt;/p&gt;&lt;p&gt;然而既然发现了利用客观经验先验地强化数据的方法是有效的，那么，我们就可以把所有连续的数据全部找到其客观规律来玩一道。比如年限和土地价格的关系，存在一个贴现率，为5%。于是我们把这个5%加到模型里，把所有土地的年份提高1年，价格按照贴现率的一个函数计算出来……准确率果然也提高了。&lt;/p&gt;&lt;p&gt;到目前为止，在甲方的A数据集上，利用对训练集加两倍数据的辅助方式进行学习，每次重新随机测试集藏起来，稳定能够达到50%的准确率了（土地报价乱叫价的情况下感觉撑死也就这么高了）。然而这种生成数据的思想，却是可以推衍的。&lt;/p&gt;&lt;p&gt;大量的中小型企业都存在这种数据少，质量差的情况，然而各个中小型企业能维生，主要是因为大家脑海中有个“经验公式”。那么，把这个经验公式取出来，用某种方法来强化数据，这也许也是一条独特的，中国企业利用人工智能辅助自己业务的一条不错的手段呢？&lt;/p&gt;&lt;p&gt;毕竟大公司都在搞大数据，但是全国大多数还是只有小数据的小公司，也许这部分小公司利用好自己的经验，强化数据，运用机器学习，也是一种人工智能走向千家万户的一种方式呢。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/25566686&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Sat, 04 Mar 2017 23:16:23 GMT</pubDate></item><item><title>不要太纠结小样本中的正确率</title><link>https://zhuanlan.zhihu.com/p/25109768</link><description>这两天做深度学习文本情绪分析（经典的深度学习入门项目），发现这么一个问题。&lt;p&gt;比如常见词汇的杂音处理，作为一个菜B，我马上想到了以下两种方法：&lt;/p&gt;&lt;p&gt;1、超高频词语削掉（比如空格这种高频词语几乎就是杂音……直接砍掉，不加到训练用的网络里）&lt;/p&gt;&lt;p&gt;2、倒数出现次数的词袋（在训练时，对当前train中某词语的个数乘以1/这个词语的总出现次数，比如空格虽然出现了30几次，但是在全样本中出现了30万次，这个时候它在对这次train的贡献只有万分之一。而“二狗子”这个词语虽然在这里只出现了一次，但是在全样本中总共出现也就两次，对这次train的贡献就是.5，那么就相当于变相增加了“二狗子”的重要程度，削减了空格的杂音）&lt;/p&gt;&lt;p&gt;那么，问题就来了。&lt;/p&gt;&lt;p&gt;先讲第一个。&lt;/p&gt;&lt;p&gt;1.如果把超高频词语削掉，准确率下降了怎么办？举个例子，像“的”，“and”，“so”，“你”，“这”，之类的这种高频词语，不一定是杂音，虽说是语气词，在特定的环境下可能表达出了有效的语意思比如“你这孙子”显然不应该把“你这”俩高频词语去掉，只留“孙子”。有可能“你这孙子”明显是情绪为负的，但是“孙子”就会识别成情绪为正。&lt;/p&gt;&lt;p&gt;所以，多高的频率以上去掉，就是一个问题了。&lt;/p&gt;&lt;p&gt;然后，如果我们要提高准确率，通常会做这样一件事，把每个字，和之前的字，之后的字，保留顺序组成词语，比如“今天天气真好”作为单字，处理出来是6个特征，但是把前后字的2字组合加上，就是6+5个特征，再加上3字特征的话，就是6+5+4个特征。&lt;/p&gt;&lt;p&gt;很明显，这个中间的“今天”是有意义的，但是也很明显“气真”这个词就是无意义的。&lt;/p&gt;&lt;p&gt;如果用了这种组词的方法，因为“今天”出现的频率更高，很有可能把无意义的“气真”留下，把有意义的“今天”去掉。&lt;/p&gt;&lt;p&gt;这种情况怎么办呢？&lt;/p&gt;&lt;p&gt;当然，我们可以加深层，识别更多特征（简单理解，比如识别出形容词，动词这类特征，就可以避免留下“气真”，去掉“今天”的错误），但是这样耗时耗力，很可能机器识别的特征和我们理解的有偏差，无法迁移。&lt;/p&gt;&lt;p&gt;这种时候，转个角度思考，我们一开始为什么纠结呢？因为高频词语削弱了，准确率下降了（比如95%的准确率下降到了92%），但是换个角度考虑，把高频词语砍了，神经网络的负荷自然也小了，运转速度更快了（有可能提升数十倍的速度）。&lt;/p&gt;&lt;p&gt;那么，如果不纠结于小样本中的准确率，把这提升的数十倍的速度，换个角度考虑，增加数十倍的样本训练呢？准确率比之前更低还是更高呢？&lt;/p&gt;&lt;p&gt;如果这个速度，用在迭代更多次数，更多样本上，准确率更高，那么，削弱了高频词语导致概率空间的缩小，也许并不是很重要的事。&lt;/p&gt;&lt;p&gt;某种程度上来说，机器学习就像是有限制时间的盲人摸象，大概摸摸，把全身摸了，能八九不离十地知道这是个动物。然而仔仔细细，一点一点的摸，也许仔细了半天，只摸了一条腿，反而把象识别成了柱子。&lt;/p&gt;&lt;p&gt;所以说，不要太纠结于小样本的准确率，也许你也就拿了个小腿在那里摸呢？&lt;/p&gt;&lt;p&gt;用比较玄的词来描述的话，不纠结于小范围的对错，跳出这个小样本，寻找到更大的空间里，广阔天地大有作为，大概就是格局吧。&lt;/p&gt;&lt;p&gt;exciteing！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/25109768&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Mon, 06 Feb 2017 18:02:07 GMT</pubDate></item><item><title>【增强学习】简单的Qlearn入门</title><link>https://zhuanlan.zhihu.com/p/24995280</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-f4d98a207e8af6b70b108909fd780ad1_r.gif"&gt;&lt;/p&gt;很多时候，第一时间无法获取足够的信息（比如观察一支新发行股什么状态下值得买），我们需要建立一个模型，随着时间的改变，强化其对信息的处理和描述能力。&lt;p&gt;比如，小时候步步高上的围棋越下越厉害，其实现在回头去看，那个围棋游戏用的蒙特卡洛算法，就是一种增强学习，嘛……不谈公式，如何简单理解增强学习呢？&lt;/p&gt;&lt;p&gt;&lt;b&gt;公众号链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA5NTc4NDk5OQ==&amp;amp;mid=2651877235&amp;amp;idx=1&amp;amp;sn=90e81c3ef8152352193fa75efc10e053&amp;amp;chksm=8b5ed63bbc295f2dd001571c340c4c1228bd5707953d1e59ececa5dae22ca66b769a15c0c49d#rd" data-editable="true" data-title="【增强学习】最简单的Qlearn解释" class=""&gt;【增强学习】最简单的Qlearn解释&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;1、先举个简单的例子，玩猜拳。&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;假设你和电脑玩猜拳，已经玩了10次了，接着玩。电脑记录了你这10次的动作，但是这电脑忒笨，它记不住那么多东西，假设除了已经预处理好的学习系统以外，吝啬的程序员只给你留了足够储存27个状态的空间（计数系统预设好了）。&lt;/p&gt;&lt;p&gt;27个状态……正好石头剪子布各3个。&lt;/p&gt;&lt;p&gt;于是你决定，把当前出的手势，和前两个手势，共3个状态，存在一起，记录一次。因为每个手势有3种可能，3*3*3，正好27。&lt;/p&gt;&lt;p&gt;于是电脑这样记录，每进行一次猜拳，把当前的手势，和前两次的手势，这3个手势产生在一起，按照时间顺序，找到所在的状态，加一。&lt;/p&gt;&lt;p&gt;比如，你之前出了石头，石头，接下来出了剪刀，记作：石石—剪：1&lt;/p&gt;&lt;p&gt;当你再进行这个行动的时候，把后面那个数字加一，比如石石—剪：2&lt;/p&gt;&lt;p&gt;……&lt;/p&gt;&lt;p&gt;每进行一次就加一。&lt;/p&gt;&lt;p&gt;这样，渐渐的，由于有了前两次你的猜拳记录，你第三次的3个状态，已经发生的次数就出来了：&lt;/p&gt;&lt;p&gt;比如这次你又出了石头，石头：&lt;/p&gt;&lt;p&gt;那么，也许，记录了前两个是石头的3个状态，如下：&lt;/p&gt;&lt;p&gt;石石—石：25&lt;/p&gt;&lt;p&gt;石石—剪：55&lt;/p&gt;&lt;p&gt;石石—布：37&lt;/p&gt;&lt;p&gt;这样，由于你出的剪刀的次数最多，那么计算机就认为，你出了两次石头，第三次你出剪刀的可能性更大，它就会选择出石头，来赢你。&lt;/p&gt;&lt;p&gt;这就是一种简单的增强学习方法，描述你的状态（出了2次石头），记录你这个状态下的信息（第三次出的手势）和这个信息出现的次数（石头25次，剪刀55次，布37次），找到出现次数最多的信息，认为这个信息会复现。&lt;/p&gt;&lt;p&gt;好，是不是感觉很容易懂呢？&lt;/p&gt;&lt;p&gt;你想要知道的信息（下一个手势出啥），你预测这个信息依托的状态（前两个手势），你预测这个信息依托的数据（前两个手势下第三个手势出现的次数）。&lt;/p&gt;&lt;p&gt;不过猜拳这个例子理解有限。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2、再举个例子，小红想知道男朋友爱不爱她：&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;接下来我们再举个例子，假设你是小红，你男朋友叫小白，你想知道你男朋友明天爱不爱你。怎么办？&lt;/p&gt;&lt;p&gt;假设，小红认为以下这个行为可以证明他爱你：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1、给你买礼物回家。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;以下这个行为证明他不爱你：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1、和别的女人啪啪啪&lt;/b&gt;&lt;/p&gt;&lt;p&gt;然后现在小红突然作死病犯了，想要预测他明天爱不爱你，怎么办？&lt;/p&gt;&lt;p&gt;把以前他做的事记录下来，看看他第二天，会表现“爱你”的举动，还是“不爱你”的举动，和猜拳一样嘛！&lt;/p&gt;&lt;p&gt;这例子有啥好举的呢？&lt;/p&gt;&lt;p&gt;和猜拳不同，人的感情是会随着时间淡化的，所以以前的经验不应该按照权重值为1，与今天的经验等价处理，应该做个衰减。&lt;/p&gt;&lt;p&gt;举个例子，小白在认识小红之前有个女友，那时他和前女友啪啪啪之前的一天，经常会加班加点把工作提前完成。&lt;/p&gt;&lt;p&gt;那么，小红发现小白加班加点把工作提前完成，就认为小白要和“前女友”啪啪啪，是不是很荒谬？（当然……这里可以用迁移学习把“前女友”上的状态信息，迁移到小红上，判断小红和“前女友”的重要信息相似度就好了，比如都是女人，这样就可以理解为小白把工作提前完成，就是要和小红啪啪啪，概率等于重要指标相似性乘以原模型预测概率……这篇文章不讲这个。）&lt;/p&gt;&lt;p&gt;就算硬要说这里要用到迁移学习，小白把工作提前做完，就是为了今天要和小红啪啪啪，也很荒谬对吧？&lt;/p&gt;&lt;p&gt;再举个例子，小白以前是个富二代，去酒吧回来都给小红带（酒吧送的）礼物，去茶馆回来很大可能不给小红买礼物，小红用“买礼物”来预测，就会认为小白去了酒吧，相比于去茶馆，明天可能更爱他。后来娶了小红以后没那么多前造了，于是不再去酒吧，天天去茶馆，小红再用以前的模型做预测，觉得“不去酒吧”意味着回来没礼物，意味着不爱她了，就很不合理。&lt;/p&gt;&lt;p&gt;所以要做一个衰减，以前的经验，渐渐衰减，以后的经验才是应该重点学习一个的。&lt;/p&gt;&lt;p&gt;怎么衰减呢？&lt;/p&gt;&lt;p&gt;&lt;b&gt;两种方法：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1、&lt;/b&gt;&lt;/p&gt;&lt;p&gt;比如，在猜拳里，我们给每次的状态都是加1：&lt;/p&gt;&lt;p&gt;石石—剪：1&lt;/p&gt;&lt;p&gt;再出现一次&lt;/p&gt;&lt;p&gt;石石—剪：2&lt;/p&gt;&lt;p&gt;现在呢，就可以这样，把时间函数算上，每次猜拳，加上现在是第几次。&lt;/p&gt;&lt;p&gt;由于第一次和第二次没有前俩数据，暂不加。&lt;/p&gt;&lt;p&gt;那么第3次开始，两个石后除了剪刀：&lt;/p&gt;&lt;p&gt;石石—剪：0+3&lt;/p&gt;&lt;p&gt;接下来，出了石，由于这是第4次猜拳就是&lt;/p&gt;&lt;p&gt;石石—剪：3&lt;/p&gt;&lt;p&gt;石剪—石：0+4&lt;/p&gt;&lt;p&gt;接下来又除了石，这是第5次，那么：&lt;/p&gt;&lt;p&gt;石石—剪：3&lt;/p&gt;&lt;p&gt;石剪—石：4&lt;/p&gt;&lt;p&gt;剪石—石：0+5&lt;/p&gt;&lt;p&gt;接下来出了剪刀，由于这是第6次，那么：&lt;/p&gt;&lt;p&gt;石石—剪：3 + 6 = 9&lt;/p&gt;&lt;p&gt;石剪—石：4&lt;/p&gt;&lt;p&gt;剪石—石：5&lt;/p&gt;&lt;p&gt;这样，我们就随着时间，增加了权重。&lt;/p&gt;&lt;p&gt;当然……这种加权的方法类似的有很多，比如把加改成乘什么的……&lt;/p&gt;&lt;p&gt;&lt;b&gt;2、&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;第二种方法呢？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;假设小红很健忘，记不住那么多数字（第300天以后岂不是要加300？）&lt;/p&gt;&lt;p&gt;小红认为，过去的事就过去了，未来的才重要，未来的每件事，我都要按照50%的重要性算，以前的事加起来记住（1-50%）就好，那么，对于小白去酒吧和去茶馆这件事：&lt;/p&gt;&lt;p&gt;一开始，去了酒吧，有礼物：&lt;/p&gt;&lt;p&gt;去酒吧——有礼物： +1&lt;/p&gt;&lt;p&gt;第2天，去了茶馆，没礼物：&lt;/p&gt;&lt;p&gt;去酒吧——有礼物： 1&lt;/p&gt;&lt;p&gt;去茶馆——有礼物：+0&lt;/p&gt;&lt;p&gt;第3天，去了酒吧，有礼物：&lt;/p&gt;&lt;p&gt;去酒吧——有礼物： 1*0.5+1*0.5 = 1&lt;/p&gt;&lt;p&gt;去茶馆——有礼物：0&lt;/p&gt;&lt;p&gt;第4天，去了茶馆，有礼物：&lt;/p&gt;&lt;p&gt;去酒吧——有礼物：1&lt;/p&gt;&lt;p&gt;去茶馆——有礼物：0*0.5+1*0.5 = 0.5&lt;/p&gt;&lt;p&gt;第5天，去了酒吧，没礼物：&lt;/p&gt;&lt;p&gt;去酒吧——有礼物： 1*0.5 + 0*0.5 = 0.5&lt;/p&gt;&lt;p&gt;去茶馆——有礼物：0.5&lt;/p&gt;&lt;p&gt;发现没有？虽然去酒吧了3次，带了2次礼物，去茶馆了2次，带了1次礼物，从数学期望上来看，去酒吧带回礼物的概率应该是0.67，去茶馆是0.5，但由于“以前”去酒吧带来礼物，以前没有现在重要，所以把以前的权重衰减了，把现在的权重提高了。&lt;/p&gt;&lt;p&gt;于是去酒吧和去茶馆带礼物回来一样了，那么，虽然不知道现在发生了啥事，起码她知道，去酒吧和去茶馆之间，对于现在来说，带礼物回来的可能性差不多。&lt;/p&gt;&lt;p&gt;到这里你看到了第一个参数：50%，这个在Qlearn里叫做学习率（α），表示你每次从新知识里吸收的信息，占权重的多少，这个参数越高，对环境的变化越敏感（小红马上知道现在去酒吧并不一定会带礼物回来），越低越不敏感（小红觉得得观望一段时间，万一只是这次忘了呢？）。&lt;/p&gt;&lt;p&gt;那么，现在你知道了由于世界在变化，环境在变化，以前的经验会不如现在的经验，问题又来了。&lt;/p&gt;&lt;p&gt;小红需要刷新现在的经验。但是假设小红没法知道小白是否和别人啪啪啪了，但是又想知道某个状态下是否能判断小白不爱她，如果距离小白上一次和其他女孩啪啪啪，已经是3年前还不认识小红的时候了，那小红就需要“创造条件”判断某个状态。&lt;/p&gt;&lt;p&gt;举个例子，小红希望知道，小白很疲惫了，做完老板的项目回来，根本不理自己，那么第二天还爱不爱她。&lt;/p&gt;&lt;p&gt;于是小红就开始探索了。她花钱请了个朋友勾引小白去宾馆，假设勾引成功了，就说明小白不爱她了。假设没勾引成功，就说明小白依然爱她。&lt;/p&gt;&lt;p&gt;这个就叫“探索”，探索当前状态下，期望事件按照预料发生的概率。呐，假设学习率为50%，那么，假设接下来4次，小白很疲惫的第二天，让朋友勾引小白：&lt;/p&gt;&lt;p&gt;第1次，很疲惫——勾引失败：&lt;/p&gt;&lt;p&gt;不爱：+0&lt;/p&gt;&lt;p&gt;第2次，很疲惫——勾引成功：&lt;/p&gt;&lt;p&gt;不爱：0*0.5+1*0.5 = 0.5&lt;/p&gt;&lt;p&gt;第3次，很疲惫——勾引失败：&lt;/p&gt;&lt;p&gt;不爱：0.5*0.5 + 0*0.5 = 0.25&lt;/p&gt;&lt;p&gt;第4次，很疲惫——勾引失败：&lt;/p&gt;&lt;p&gt;不爱：0.25*0.5 + 0*0.5 = 0.125&lt;/p&gt;&lt;p&gt;但是小白火大了，你特么的还让不让人上班了，这个项目这周截至必须赶出来，5个工作日4天你一天到晚找不认识的人来烦我，我不搭理也不行搭理也不行。&lt;/p&gt;&lt;p&gt;小红一想，也是。老试探也不好，我的观测行为本身就会影响系统，而且这周都4天没和闺蜜去逛商场了。&lt;/p&gt;&lt;p&gt;但是老不试探呢？也不行。小红会纠结小白到底爱不爱她。&lt;/p&gt;&lt;p&gt;于是小红决定，每次遇到某个状态想试探了，掷骰子，假设掷骰到1，就试探，否则不试探。于是小红试探小白的概率为1/6，16.6667%…&lt;/p&gt;&lt;p&gt;这个概率就叫探索率，假设我们行为的结果明天才能看到，我们今天只能且必须选择一个行为，就必须要选择探索或者不探索，完全不探索无法更新信息，模型会失效，一天到晚探索会失去。&lt;/p&gt;&lt;p&gt;探索率在Qlearn里面一般用epsilon，ε来表示&lt;/p&gt;&lt;p&gt;嘛……到这里Qlearn的概念已经足够写个增强学习选股agent了，不过本着话得说完的原则，还有一个参数得说完。&lt;/p&gt;&lt;p&gt;再举个例子……&lt;/p&gt;&lt;p&gt;假设小红不是作，而是在犹豫一件事。&lt;/p&gt;&lt;p&gt;小红最近趁小白上班的时候，会和小明约会。但是小红心里会感到愧疚，但是小孩又很享受在小白不知道的情况下，和小明约会。&lt;/p&gt;&lt;p&gt;如果知道小白还爱自己，那么和小明约会，心里就会愧疚，收益就较低。&lt;/p&gt;&lt;p&gt;如果知道小白不爱自己了，那么和小明约会，心里就不会那么愧疚，收益就较高。&lt;/p&gt;&lt;p&gt;所以，小红之所以想知道小白爱不爱自己，其实是想知道自己和小明约会的收益如何。&lt;/p&gt;&lt;p&gt;假设知道小白还爱自己，那么，和小明约会，由于愧疚，收益为0。&lt;/p&gt;&lt;p&gt;而知道小白不爱自己了，那么，和小明约会收益为1。&lt;/p&gt;&lt;p&gt;由于我们一开始知道了“学习率”，那么，在预测的时候，小红认为自己在预测明天的收益时，也应该“类比学习率”考虑以往的收益比率。&lt;/p&gt;&lt;p&gt;假设小红认为这次约会的收益很重要，但是以前和小明一起的快乐经历也很重要啊~那么就各按50%重要性考虑吧~&lt;/p&gt;&lt;p&gt;小明也不是每天都有空和小红约会，那么，回到上一个例子中，小白很疲惫，小红找人去勾引小白的例子：&lt;/p&gt;&lt;p&gt;第1次，很疲惫——找人勾引小白失败，和小明约会：&lt;/p&gt;&lt;p&gt;不爱：+0&lt;/p&gt;&lt;p&gt;约会收益：+0&lt;/p&gt;&lt;p&gt;和小明约会，收益为&lt;/p&gt;&lt;p&gt;第2次，很疲惫——找人勾引小白成功，和小明约会：&lt;/p&gt;&lt;p&gt;不爱：0*0.5+1*0.5 = 0.5&lt;/p&gt;&lt;p&gt;约会收益：0*0.5 + 1*0.5 = 0.5&lt;/p&gt;&lt;p&gt;第3次，很疲惫——找人勾引小白失败，没和小明约会：&lt;/p&gt;&lt;p&gt;不爱：0.5*0.5 + 0*0.5 = 0.25&lt;/p&gt;&lt;p&gt;约会收益：0.5&lt;/p&gt;&lt;p&gt;第4次，很疲惫——找人勾引小白失败，和小明约会：&lt;/p&gt;&lt;p&gt;不爱：0.25*0.5 + 0*0.5 = 0.125&lt;/p&gt;&lt;p&gt;约会收益：0.5*0.5 + 0.125*1 = 0.375&lt;/p&gt;&lt;p&gt;这样，聪明的小红就可以在小白工作一天很疲惫以后，预测第二天自己和小明约会的收益，是否值得约了。&lt;/p&gt;&lt;p&gt;这个考虑过去收益的参数，一般作为gamma，γ虽然现在这样理解了……不过大部分人不会在设置gamma的时候，对于当前的和以往的作一个比，而是，把当前的直接折算，今天的收益完全考虑，以前的收益按按照γ折减一下。这样对于某状态下的收益，就是呈递增趋势，在当前收益权重最高的情况下，不断强化了对收益的感受……（而不是本例子中50%+50%为1，收益的感受从统计的角度上并没得到强化）&lt;/p&gt;&lt;p&gt;然而实际操作的时候如果环境变化比较快，γ可以不要的（状态不恒定，不应当过度强化，快速调整反而更好，比如某些地方的股市，说不准来个什么政策就应当重新训练模型了）……&lt;/p&gt;&lt;p&gt;好，概念上你就大概理解Qlearn是什么了……&lt;/p&gt;&lt;p&gt;很多人觉得机器学习就是“对了给颗糖，错了打一鞭子”，但是实际上，什么如何给糖？如何抽鞭子？以前对的万一现在错了怎么办？鞭子抽多了打傻了怎么办？&lt;/p&gt;&lt;p&gt;一定程度上来说，Qlearn回答这个问题……&lt;/p&gt;&lt;p&gt;……呼~好不容易不用公式把Qlearn讲完了。接下来具体写一个class，用Qlearn来实现观察以及判断是否购买股票的agent（草版的代码是已经写好的了，回测收益为正，跑赢了大盘，但是初期随机性很大，有兴趣的可以在我写完之前先找来看看。）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24995280&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Mon, 23 Jan 2017 08:15:04 GMT</pubDate></item><item><title>使用机器学习模型对大盘指数进行预测</title><link>https://zhuanlan.zhihu.com/p/24417597</link><description>&lt;p&gt;用数学模型分析策略，可以避免由于情绪波动的造成的影响，避免一些因此产生的非理性策略。从这点上来说，机器学习可以很好地避免这种主观情绪造成的非理性决策。&lt;/p&gt;&lt;p&gt;大多数人在炒股的时候会觉得，如果我能判断大盘涨跌，大盘涨的时候，我就买，下跌的时候，我就全卖了，等下次涨，那肯定能赚钱。那么，本篇文章就用3个模型（SVM，决策树，adaboost）来对HS300指数进行预测，记录数据的获取，清洗，模型选用，以及如何调参。&lt;/p&gt;&lt;h2&gt;1、获取数据：&lt;/h2&gt;&lt;p&gt;首先获取数据，这里我使用优矿的api：DataAPI.MktIdxdGet来获取历史数据。获取在fields中已定义的数据，本项目，我选择的数据有：tradeDate，交易日、closeIndex，收盘指数、highestIndex，当日最大指数，lowestIndex，当日最小指数，CHG，当日最大涨跌幅。&lt;/p&gt;&lt;p&gt;我们获取数据选用的是从2006年3月1日到2015年3月1日的所有交易日，一共有2127行的HS300指数数据。&lt;/p&gt;&lt;h2&gt;2、 处理数据：&lt;/h2&gt;&lt;p&gt;首先我把交易日设定为index，然后将预测用的交易日的前30日数据提取出来，找出前30日的最大指数，找出前30天最小指数，定义当日指数差为当日最大指数减去当日最小指数，找出前30天最大日指数差，加上之前通过api选择到的数据，作为特征值。&lt;/p&gt;&lt;p&gt;除去我们用作索引的交易日期，经过处理后，一共有11列数据，其中10个为特征值，一个是待处理的标签。我认为目前市场并不是完全的有效市场，少数异常值可以对预测的时候提供市场信息，所以我认为，不应当把任何值当作异常值去除。&lt;/p&gt;&lt;p&gt;处理完数据后，我们查看一下数据的统计描述：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-facb2b63733b72169636461949430e37.png" data-rawwidth="971" data-rawheight="515"&gt;&lt;p&gt;然后，我们找到预测用的30个交易日后的收盘价，用这个数字当作当前交易日需要预测的目标。设定lables表，为布尔值，如果这个预测目标大于当前交易日的收盘价，则设定为true，否则设定为False。&lt;/p&gt;&lt;p&gt;到这里数据集合就处理完了，待会儿可以直接切割这个集合用来做预测。&lt;/p&gt;&lt;h2&gt;3、有效性验证：&lt;/h2&gt;&lt;p&gt;为了验证模型的有效性，本项目采取了两个指标：&lt;/p&gt;&lt;p&gt;1、 模型交叉验证的score值。（这里的score值简单地计算测试机中准确预测的比率有多少。score值的范围为0~1，由于是二元分类问题，所以socre越接近1，模型表现越优秀。由于是二元分类问题，所以如果score小于等于0.5，那么可以认为模型失效。）&lt;/p&gt;&lt;p&gt;2、与纯随机策略对比的夏普比率（夏普比率是一个综合考虑风险和收益的计算数据，可以简单的理解为收益/风险，本模型中夏普比率采取的计算方式是（收益-基准收益）/标准差）。&lt;/p&gt;&lt;p&gt;首先，在训练模型的时候，对数据集进行交叉验证，获取score。以此来验证模型对于训练用数据集的准确率，如果小于等于0.5，则认为该模型对于该问题无效。如果score大于0.5，使用数据集时间点之后的数据进行回测，比对用了模型验证后，预测涨则随机选股，否则不选。对比没用模型时，纯粹随机选股的夏普比率，如果明显大于纯随即选股的夏普比率，则认为“如果大盘涨我就买，大盘跌就不买”这种思路是有效的。&lt;/p&gt;&lt;h2&gt;4、阈值设定：&lt;/h2&gt;&lt;p&gt;模型有效性经过验证的情况下，也就是说，“如果大盘涨我就买，大盘跌就不买”这种思路是有效的，且未经调参的模型score大于0.5的情况下。我认为经过调参后，模型的score要稳定大于0.8，才能证明该模型能够明显有效。&lt;/p&gt;&lt;h2&gt;5、纯随机策略回测图：&lt;/h2&gt;&lt;p&gt;用2016年1月到12月来回测，得到下图：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-14c0f6fc27cfc166c68f815523198b6e.png" data-rawwidth="910" data-rawheight="333"&gt;&lt;/p&gt;&lt;p&gt;夏普指数为-0.72，我们在验证有效性时，使用该纯随机策略，只添加“如果预测1个月后上涨，则进行交易”这一个条件，其他不变，依然是随机购买。&lt;/p&gt;&lt;h2&gt;6、SVM预测：&lt;/h2&gt;&lt;p&gt;&lt;b&gt;简要介绍：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;SVM又名支持向量机，对二元分类问题表现尤其良好。某种程度上来说，SVM是把数据映射到高维空间，然后对空间进行切割，所以训练点之间间隔越大，SVM效果越好。作为一个大间隔分类器，SVM可以最小化经验误差，降低结构化风险。SVM的计算复杂度取决于其映射产生的支持向量，故不易发生维数灾难这种问题（这很重要，因为本题定义了很多特征量）。而最终结果取决于少数重要向量，所以一定程度上增减向量，不会对模型造成太大损害。&lt;/p&gt;&lt;p&gt;当样本数量过多时，SVM的训练时间会大幅增加，同时，SVM对多元分类问题处理存在困难。&lt;/p&gt;&lt;p&gt;单看指数的话，股票交易10年也不过2500个左右的交易日，所以数据量并不大。而当前研究的问题也确实是二元分类问题，所以这里首先选用SVM模型进行测试。&lt;/p&gt;&lt;p&gt;&lt;b&gt;模型测试：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这里我们直接使用sklearn的SVM包中的分类模型SVC。在训练模型后，使用score函数，获得的预测准确率为：0.68。超过了0.5，所以可以认为该模型是初试有效的，值得进一步测试。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-7c278a0dbe380388f55ba1cce98ca0d9.png" data-rawwidth="973" data-rawheight="396"&gt;&lt;p&gt;我们用2016年1月到12月来回测，得到上图，发现该模型是有效的，此时的夏普比率为0.99.，大于纯随机策略的-0.72，同时，我们的策略线（蓝线）确实明显优于HS300大盘指数（黑线）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;调整参数：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;接下来，我将调整SVM的参数C，以便进一步提高SVM对于该模型的表现，观察随着C的变化，Score值的变化趋势：&lt;/p&gt;&lt;p&gt;我首先做了一个图，观察在C值在1~1000范围内，得到的Score值的变化：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-78476e65e0393939f5740c5792795fd0.png" data-rawwidth="296" data-rawheight="197"&gt;&lt;p&gt;我们发现，模型表现随着C值的提高而提高，在C值为360左右的时候，表现达到了最高。&lt;/p&gt;&lt;p&gt;接下来调整gamma，也是相似的方法，我们观察gamma在0~10之间的变化，得到下图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-4e00552913729cafcd06750c7c2e3e49.png" data-rawwidth="271" data-rawheight="184"&gt;&lt;p&gt;可以看出，gamma在1.8左右的时候，score表现得最好。&lt;/p&gt;&lt;p&gt;至于核函数，通常都是默认核函数最佳，这里由于是第一个模型，还是检验一下吧：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-c55ee8d81af067841f41927c52b64202.png" data-rawwidth="259" data-rawheight="197"&gt;&lt;p&gt;果不其然是rbf（默认的高斯核函数）是最佳的。&lt;/p&gt;&lt;p&gt;接下来我们用GridSearchCV来确切获得最佳的C值和gamma值：运行函数后，我们得到最佳的C值为300，最佳的gamma值为1.03。根据这两个参数，此时我们模型test后获得的score为0.76，明显高于之前的0.68。&lt;/p&gt;&lt;p&gt;接下来通过不同的数据集（改变数据集中的数据数量）的方法测试score的方法，判断模型是否稳健，得到下图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-355b8a9f2fcb685d07d864dc209b359a.png" data-rawwidth="303" data-rawheight="220"&gt;&lt;p&gt;发现根据数据集的不同，准确率上下摆动，摆动幅度在0.1左右。但是始终没有低于过0.72，0.72大于0.5，所以可以认为模型一定程度上是稳健的。&lt;/p&gt;&lt;p&gt;

但是由于0.72小于0.8，低于了我们设定的阈值，所以认为SVM模型对于该问题的解决表现不够良好。&lt;/p&gt;&lt;h2&gt;7、决策树预测：&lt;/h2&gt;&lt;p&gt;&lt;b&gt;简要介绍：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;由于决策树是归纳型算法，所以当其预测的数据集如果是人类很容易理解的信息，那么决策树可以表现良好。决策树可以清晰地处理大量数据，了解不同特征的影响重要性。这种算法在特征明确，杂音小，特别熟数据量较大时，效果较好。&lt;/p&gt;&lt;p&gt;决策树属于局部贪婪的算法，容易过拟合，有时无法保持全局最优，所以泛化能力较差。在股票交易中使用时，应当随时更新数据，否则有可能过拟合过去的经验，对未来的预测能力下降。&lt;/p&gt;&lt;p&gt;&lt;b&gt;模型测试：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这里我们直接使用sklearn的tree包中的分类模型DecisionTreeClassifier。在训练模型后，使用score函数，获得的预测准确率为：0.88。超过了0.5，所以可以认为该模型是初步有效的。&lt;/p&gt;&lt;p&gt;我们用2016年1月到12月来回测，得到下图&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-61b9d567ef6557b3c1a3c371d09e68df.png" data-rawwidth="554" data-rawheight="212"&gt;&lt;p&gt;发现该模型是有效的，此时的夏普比率为-0.09.大于纯随机策略的-0.72，我们的策略线（蓝线）也在HS300大盘指数（黑线）上面，所以可以认为相比于纯随机策略，模型是有效的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;调整参数：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;接下来我们调整决策树的参数。首先我们调整最大深度，在0~100之间，获得下图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-a2791c682dc01a1273c46114006c817d.png" data-rawwidth="240" data-rawheight="172"&gt;&lt;p&gt;发现最大深度在15左右的时候，开始趋于稳定，在18左右的时候，模型表现最好。&lt;/p&gt;&lt;p&gt;然后我们调节min_samples_leaf参数，从0到10之间，获得下图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-86931e17ab1d26a461d6486658aeb35f.png" data-rawwidth="243" data-rawheight="175"&gt;&lt;p&gt;发现min_samples_leaf震荡幅度很大，总体来说，随着min_samples_leaf增加，模型score降低。所以这里我们就选用默认值1。&lt;/p&gt;&lt;p&gt;接下来调整参数min_samples_split，范围在0~50之间，获得下图：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-f1693c3e83cb7c52ff373893e01a111a.png" data-rawwidth="230" data-rawheight="168"&gt;&lt;/p&gt;&lt;p&gt;发现随着min_samples_split的增加，模型在测试集上获得的score降低。&lt;/p&gt;&lt;p&gt;然后调整参数min_weight_fraction_leaf，范围从0~0.5，获得下图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-b28a8a221be4507d6c41faa1f72d0351.png" data-rawwidth="237" data-rawheight="176"&gt;&lt;p&gt;在可选范围内震荡过大，不具有明显规律，所以这时我们就选用默认值0.0。&lt;/p&gt;&lt;p&gt;接下来我们依靠上面选用的参数范围，使用GridSearchCV函数，选取表现出了趋势的参数max_depth和min_samples_split的最优值，我们得到的反馈为max_depth为32，min_samples_split为3.&lt;/p&gt;&lt;p&gt;依靠这两个参数重新进行一次在test集上的测试，这时我们得到的预测准确率为0.9，略高于之前的0.88&lt;/p&gt;&lt;p&gt;接下来通过不同的数据集（改变数据集中的数据数量）的方法测试score的方法，判断模型是否稳健，通过改变测试集，得到下图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-3050cdd68eff9a67f03e11bd73fed70c.png" data-rawwidth="253" data-rawheight="185"&gt;&lt;p&gt;发现总体来说，模型表现在0.82~0.94之间，1000次的测试稳定高于0.8，0.8是大于0.5的，所以可以认为我们的预测模型是稳健有效的。&lt;/p&gt;&lt;p&gt;同时因为我们1000次的测试中，模型准确率从未低于我们设定的阈值，0.8。所以认为我们的模型对于问题的解决表现良好。&lt;/p&gt;&lt;p&gt;既然表现良好，那么我们就来看看哪些特征影响最大，我生成了一下特征在决策树中的重要性，得到下面得到得到下面表格与图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-6411e83b20443534d34a39d5245894e2.png" data-rawwidth="219" data-rawheight="256"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-c635897e6999cd0fc7ffd44a56ff1f08.png" data-rawwidth="256" data-rawheight="261"&gt;&lt;p&gt;发现对于未来30天股市涨跌来说，最重要的预测指数是30天内的最高值和30天内的最低值，其次是30天内的最大日波动，这3项的重要性超过了50%。也就是说，对于未来股市涨跌的预测，决策树模型认为，股市的波动是最重要的。&lt;/p&gt;&lt;h2&gt;8、
Adaboost预测：&lt;/h2&gt;&lt;p&gt;&lt;b&gt;简要介绍：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;adaboost是一种通过训练多个不同的弱分类器，装配到一起的办法，形成一个较强的分类器的模型。它根据每个分类器上样本的准确性，来给特征分配权值，然后再把修改过权重值后的特征传入下一个分类器，依次迭代，最终融合成一个决策分类器。可以简单地把adaboost算法理解为“三个臭皮匠赛过诸葛亮”。&lt;/p&gt;&lt;p&gt;adaboost可以很好地对特征权重值进行筛选，一定程度排除无效的训练数据特征造成的干扰，增加关键数据的权重，可以较好的避免过拟合。&lt;/p&gt;

adaboost的缺点也很明显，弱分类器太少则训练结果不够好，太多则训练时间过长。&lt;p&gt;&lt;b&gt;模型测试：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这里我们直接使用sklearn的ensemble包中的分类模型AdaBoostClassifier。在训练模型后，使用score函数，获得的预测准确率为：0.78。超过了0.5，所以可以认为该模型是有效的，有调参的价值。&lt;/p&gt;&lt;p&gt;我们用2016年1月到12月来回测，得到下图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-f62b0c30c93a0fd086ac6cf2d14592a7.png" data-rawwidth="554" data-rawheight="251"&gt;&lt;p&gt;发现该模型是有效的，我们的策略线（蓝线）确实明显优于HS300大盘指数（黑线），此时的夏普比率为-0.13.大于纯随机策略的-0.72，所以可以认为相比于纯随机策略，模型是有效的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;调整参数：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;接下来我们先调整adaboost的参数，首先调整装配数量n_estimators，我们选择0~200范围，发现得到下图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-6a527fb0427a798390e71b51c1142828.png" data-rawwidth="263" data-rawheight="198"&gt;&lt;p&gt;果不其然是随着装配的弱分类器数量的增加，模型表现效果越好，而且依然在增加，由于依然在增加，我们接下来比对一下200~300范围内，模型的score表&lt;/p&gt;&lt;p&gt;现结果：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-9bc1105b3e64cbb8fd14d390a7204e12.png" data-rawwidth="229" data-rawheight="165"&gt;发现产生了较大的震动，那么可以认为，模型的最优n_estimators大致在这个范围之内。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-2a2018ec773506bc8c77c307b0ad7e66.png" data-rawwidth="223" data-rawheight="167"&gt;&lt;/p&gt;&lt;p&gt;为了保险起见，我又做了一个200~400范围内的图，发现震荡依然很明显，略有提高，并且在350以后近似于稳定，此时计算起来已经相当慢了。以及此时相比于200~300的平均提高，已经没有超过自身的波动范围，可以认为没有进一步尝试更高的参数的意义。&lt;/p&gt;&lt;p&gt;接下来我们调整参learning_rate，在0~10的范围内，得到下图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-169aff24038e2d2a6477700ace96a7a7.png" data-rawwidth="256" data-rawheight="187"&gt;&lt;p&gt;发现在1.7左右，表现最好。&lt;/p&gt;&lt;p&gt;综合以上两个大致范围，接下来我选用GridSearchCV在n_estimators为340时，learning_rate在1.5~2之间，寻找最优值。得到最优结果为：n_estimators=340，learning_rate = 1.53的情况下。此时预测准确率为0.91。&lt;/p&gt;&lt;p&gt;然后通过不同的数据集（改变数据集中的数据数量）的方法测试score的方法，判断模型是否稳健，通过改变测试集，得到下图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-2f90a7c507a21a17e30750d6cd336120.png" data-rawwidth="260" data-rawheight="190"&gt;&lt;p&gt;确率在0.86~0.94之间波动，未低于0.84，平均在0.9左右，由于0.84大于0.5，所以模型是有效的。总体来说，模型的稳健性还不错。&lt;/p&gt;&lt;p&gt;同时，由于准确率的波动从未低于过我们设定的阈值0.8，所以可以认为模型表现良好，对于解决问题有积极作用。&lt;/p&gt;&lt;p&gt;由于我们这次装配准确率很不错，所以我又再次生成了一次特征重要性的表格与图，来看看哪些因子影响更大。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-8b592b4ac3c411a021fa9653825d68a3.png" data-rawwidth="212" data-rawheight="251"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-fb9c3e879ee4dc9b9a30af5c5cb25812.png" data-rawwidth="244" data-rawheight="253"&gt;&lt;p&gt;从表格与图中可以看出，这次影响最大依然是前30天的最小值，不过第二大的是30天内的最大日波动，第三大的因素才是30天内的当日最大指数。与决策树相比，前三位影响最大的因子依然是这3个。（不过与决策树相比，交易量不再是影响第四大的因素，在adaboost中，交易量成为了影响最小的因素了。）&lt;/p&gt;&lt;p&gt;值得注意的是，无论是在决策树模型，还是在adaboost模型中，能体现前30日的特征，在对未来30日后的涨跌预测中，起到了巨大的影响。&lt;/p&gt;&lt;h2&gt;9、总结&lt;/h2&gt;&lt;p&gt;通过添加了3个模型对大盘指数的预测后的随机策略，与纯随机策略的夏普指数相比，可以发现，对大盘预测之后，夏普指数是大于纯随机策略的，也就是说，预测大盘的涨跌，对于股票交易的收益提升以及风险降低，是有积极作用的。用通俗的话就是&lt;/p&gt;&lt;p&gt;&lt;b&gt;“如果你能预测大盘涨跌后再瞎J8买，确实比纯粹的瞎J8买效果更好”&lt;/b&gt;&lt;/p&gt;&lt;p&gt;通过比较3个模型，我们发现，对于大盘的预测，SVM模型表现不够好，决策树和adaboost的表现经过调参之后，稳定大于我们设定的阈值，表现良好。&lt;/p&gt;&lt;p&gt;同时，在对达到了阈值的两个模型的参数比重分析中，我们发现了一个有趣的现象：“前30天的最小指数”，“30天内的最大日波动”与“30天内的最大日波动”这3个指数作为特征，对于模型预测的权重加起来达到了50%，我是这样理解的，前30日的股市波动，对股市未来30天的涨跌有巨大的影响。&lt;/p&gt;&lt;h2&gt;10、增加样本外数据再次验证模型&lt;/h2&gt;&lt;p&gt;不行，我一直在考虑时间序列这个问题，不写代码来验证一下睡不着。 虽说写好代码测试完都2点了，希望大家耐心看完吧。&lt;/p&gt;&lt;p&gt;于是这次我切割了样本外数据，通过预留最后100~228个数据用作样本外数据测试，再次查看模型效果。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a6dfd6ba055521089e717ac33633ed16.png" data-rawwidth="369" data-rawheight="282"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-22dbb545c6c24f0b31ab863474858ef2.png" data-rawwidth="369" data-rawheight="282"&gt;&lt;p&gt;发现经过再次测试以后，adaboost和SVM的模型稳健性很差了，上下摇摆，波动不定，决策树的稳健性依然还在，但是预测样本外数据的准确率（0.8左右），明显低于预测样本内数据的准确率（0.9）&lt;/p&gt;&lt;p&gt;然而又有了新的问题，对于稳健性很差的那两个模型，一般而言，100个数据预测准确率为0.8，再增加一个数据，就算预测错了，预测准确率也应该只是变为0.79啊，为毛从图中看，变动的时候，直接就从0.8的准确率，变到0.2了呢？&lt;/p&gt;&lt;p&gt;这到底是为什么呢？目前我对这个现象的假设是这多的一个数据因为我分割的方法，它到了训练集里面，然后对模型产生了巨大的影响，恰好改变了模型之前对上涨和下跌的预测。产生这个问题的主要原因应该是我很SB的，直接根据大于或者小于算涨跌的定义标签的方式，导致分类器在涨跌那条很细的线上纠结，所以稳健性变差了。&lt;/p&gt;&lt;p&gt;解决方案：预留一个阈值，比方预留为5%，那么30天后的收盘价，大于今天的105%才算涨，低于今天的95%才算跌。否则设定为None。&lt;/p&gt;&lt;p&gt;然后我又想起来了一个问题，这个准确率是一段时间内的统计准确率，如果我减少时间周期，不是集合到一起统计准确率，而是100天，100天这样间隔统计准确率，准确率又如何呢？于是这次我得到了这样一张图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-216c479f2a17a4f252cf58b215eab86e.png" data-rawwidth="386" data-rawheight="266"&gt;&lt;p&gt;说明模型的稳健性并不够高，而且随着时间周期，模型的准确率会突变。有趣的是，不同模型依然保持一个加起来准确率为1的规律。&lt;/p&gt;&lt;p&gt;本文只是抛砖引玉，别妄想看完本文你就能拿去直接预测大盘了。还有一堆模型和一堆数据可以进一步使用的。&lt;/p&gt;&lt;h2&gt;11、代码地址：&lt;/h2&gt;&lt;p&gt;由于数据是在优矿上提取的，纸糊又不允许我这种小透明上传附件，已经保存到桌面上处理好的csv文件没法上传上来，优矿的api只能在优矿里调用，所以我只好在那上面发布instead of上传到github了（我才不会告诉你主要是因为我不会git）。在网页里点克隆自己改着玩吧。&lt;/p&gt;&lt;p&gt;SVM模型的代码：&lt;a href="https://uqer.io/community/share/584f652f6740ec004f2bd542" data-editable="true" data-title="优矿" class=""&gt;https://uqer.io/community/share/584f652f6740ec004f2bd542&lt;/a&gt;&lt;/p&gt;&lt;p&gt;决策树模型的代码：&lt;a href="https://uqer.io/community/share/5853f6bd954fa20047b771e3" data-editable="true" data-title="优矿" class=""&gt;https://uqer.io/community/share/5853f6bd954fa20047b771e3&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Adaboost模型的代码：&lt;a href="https://uqer.io/community/share/58541c566a5e6d0051dc33f5" data-editable="true" data-title="优矿" class=""&gt;https://uqer.io/community/share/58541c566a5e6d0051dc33f5&lt;/a&gt;&lt;/p&gt;&lt;p&gt;重新比较3个模型预测大盘：&lt;a href="https://uqer.io/community/share/58557c8a954fa20050b77496" data-editable="true" data-title="优矿" class=""&gt;https://uqer.io/community/share/58557c8a954fa20050b77496&lt;/a&gt;&lt;/p&gt;&lt;p&gt;所使用的机器学习库均为sklearn的库，可以直接在谷歌上搜。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24417597&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Sat, 17 Dec 2016 15:07:02 GMT</pubDate></item><item><title>别写for循环調参……</title><link>https://zhuanlan.zhihu.com/p/23498425</link><description>刚开始学机器学习的时候，特别是学一些模型的时候，课程要求自己写一些算法（而不是去用别人写好的），有时自己写的渣算法调参有时没法直接从sklearn里引用GridSearchCV 调参，图方便，咱这些菜虫就直接写个for循环……&lt;p&gt;然后机器就跑几天都跑不完了（比如模拟无人车这种，渣电脑模拟一次路况就十来秒。）&lt;/p&gt;&lt;p&gt;那么，在写着很快的情况下，避免用for循环来调参，有哪些写起轻松又好用的方法呢？&lt;/p&gt;&lt;p&gt;首先看参数，一般在调参的时候，影响参数数量的数字有两个，一个是参数的种类有多少，一个是每个参数的范围内有多少。&lt;/p&gt;&lt;p&gt;举个例子，Qlearn这玩意，通常調3个参数，学习率（alpha），折扣因子（gamma）以及探索率（epsilon）。这就是说参数的种类有3种，然后是每个参数范围内有多少，比如探索率这个参数，通常是.0~1.0之间，切为10份来看，那就是10个，如果是切为100粉，那就是该参数范围内有100个。&lt;/p&gt;&lt;p&gt;那么，就这个3类参数，每类100份的情况下，如果写for循环的话，就是100**3=1e6 了。假设渣电脑跑一次1秒钟，得11天半才能跑完……&lt;/p&gt;&lt;p&gt;那么如何改进呢？&lt;/p&gt;&lt;p&gt;如果每个参数中的改变是平滑且只有一个极值的，那么可以考虑用二分法来对每个参数进行缩减（猜数字简单的“大了”，“小了”。），100个数据，可以逼近为2^7==128个步骤内选取，也就是把100个的复杂度缩减到7个了。&lt;/p&gt;&lt;p&gt;然后是3类，这时候如果不进一步缩减，是7^3，如何进一步缩减呢？因为我们假设每数参数中的改变是平滑且只有一个极值的，那么所有参数合起来的图像，也应该是平滑且只有一个极值的。&lt;/p&gt;&lt;p&gt;那么，就可以对每个参数，假设了其他参数的最优值的情况下，单独去选取，比方说，假设3个因子我们给的代数是a,b,c，范围均在0~1之间，步长还是.01，那么，假设3个初始值，比如.5、.5、.5，然后在b=.5，c=.5的情况下，去寻找a的最优值，然后用a的最优值替代初始值。接下来在a和c固定的情况下，去找b的最优值……以此类推。&lt;/p&gt;&lt;p&gt;这样，迭代一次，复杂度就从7^3变成了7*3，如果参数的变化比较简单通常迭代3次以内就能找到最优值，这种情况下，就从7^3=343的复杂度，缩减到了7*3*3=63的复杂度。&lt;/p&gt;&lt;p&gt;两招一起用，原本1e6,一秒算一次得11天半跑完的调参，就变成了63秒能跑出结果的一个简单的调参了。&lt;/p&gt;&lt;p&gt;不过问题又来了，眼睛尖的朋友们想必已经发现了，这招类似于贪婪算法，所以如果函数有多个局部最优解的话，很可能调参调不到全局最优解。怎么办呢？&lt;/p&gt;&lt;p&gt;如果你可以确认哪些参数有几个极值，那么这些参数找到的极值就是最优解。如果不能确认我们参数造成函数有几个极值，但是我们知道在每个局部范围内，参数对函数的影响都是简单，平滑的话，可以随机选取每个参数的初始值，多次尝试，最终选择使整个算法表现最好，且重复出现的那个参数组。&lt;/p&gt;&lt;p&gt;不过这招很多时候也不适用……比如你的参数和结果的关系属于魏尔斯特拉斯函数这类的关系的话……&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23498425&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Tue, 08 Nov 2016 04:21:16 GMT</pubDate></item><item><title>今天我们来给一个剃刀打个广告</title><link>https://zhuanlan.zhihu.com/p/22804193</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-4403c2da8633b002eebbff62a77b4777_r.gif"&gt;&lt;/p&gt;&lt;p&gt;有一把剃刀可厉害了，阿基米德在卫生间里用它剃出了简洁的浮力公式，牛顿从繁琐的稿子里用它剃出出了优美动能公式，爱因斯坦从庞杂的证明中用它剃出了曼妙的质能公式，至于高斯……高斯不长胡子，高斯不需要剃刀……&lt;/p&gt;&lt;p&gt;嘛，大家都猜到了，这叫奥卡姆剃刀。&lt;/p&gt;&lt;p&gt;嘛，这把剃刀肯定不是奥卡姆发明的，一般而言认为是上帝跟高斯商量了以后，回到创世初期，做出了这把剃刀。&lt;/p&gt;&lt;p&gt;那奥卡姆卖剃刀的时候是怎么说的呢？&lt;/p&gt;&lt;p&gt;“如无需要，勿增实体”&lt;/p&gt;&lt;p&gt;这听起来像玄学啊……嘛，剃刀嘛，天气热了，知识青年上山下乡接受劳动人民的再教育，于是咱村头的剃头匠老张头决定开个会解释一下，老张头这么说的：“天气恁个热，要那么多头发爪子嘛，来我给你都剃了，放心剃不到头皮。”&lt;/p&gt;&lt;p&gt;人民群众的智慧是伟大的，老张头解释的简单易懂（大雾），这里的头皮，就是指的有效信息，是真理，头发指的就是蒙蔽住真理的玩意。头发越多，自然风也就更难吹到头皮，熵就越多（热），然而我们只知道真理是埋在头发里面的，具体埋在那里不知道，辣么，剃掉的头发越多，真理也不就越明了？&lt;/p&gt;&lt;p&gt;嘛，想到这里，住在牛棚里的知识青年们折服在了劳动人民的智慧底下，高兴地拍起了肚皮……&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-b17f7bf19ff0a2eb52bea0eb1ef40221.jpg" data-rawwidth="225" data-rawheight="208"&gt;&lt;p&gt;剃头匠老张头微微一笑，捋了捋胡须，来来来，剃刀好处都有啥，谁说对了豆给他。小李被分到园林部门当临时工，负责给决策树裁剪枝条，先开始说了。&lt;/p&gt;&lt;p&gt;小李说，领到为啥安排我给决策树修剪枝条呢？因为啊，领导喜欢到树上摘果子吃。但是呢，咱村这树有点……畸形，虽然每棵树长得差不多，但是一般有上万根枝条，却不是每一根枝条上都有果子，所以我们得沿着树干去找果子，一般长了果子的树枝会有一些特征，我们就能沿着这些特征找到果子，找到了好多果子，领导就不会把我扔到夹边沟去了。&lt;/p&gt;&lt;p&gt;然后我发现了一个问题，如果我每次判断的时候，进果园里，看到一棵树都沿着枝条走到头，然后告诉采果子的二麻子哪样的枝条有果子，二麻子按照我说的去找，因为两棵树相差可能很大，这颗枝叶上有果子，下一棵树并不一定就有。那二麻子爬到决策树上，拿回一堆没果子的枝叶，那就不合适了。&lt;/p&gt;&lt;p&gt;我想了想，嘿，那我就“裁剪”一下决策树，让二狗子每次不需要爬到枝端去拿枝叶，直接把看起来有果子的树枝全给我抱回来，不就行了嘛。果然，这下采到的果子大幅提升，领导也开心了！&lt;/p&gt;&lt;p&gt;小李继续说，其实啊，我描述3根枝条的长度，就能大概说清楚这棵树咋样的枝条结果子了。但是我描述了5跟枝条的长度，事实上说的还是那3根的特点，那我何必说5根呢？既然最小描述长度是说3根，那我就只说3根的就是了嘛。&lt;/p&gt;&lt;p&gt;说到这里，老张头满意地拍起了肚皮，说，好，咱劳动人民就是有智慧。来来来，那这个最小描述长度，二麻子，你体会到了，说一下呗？&lt;/p&gt;&lt;p&gt;二麻子说，好，辣你要我解释最小描述长度，我就先解释一下“贝叶斯定理”吧。&lt;/p&gt;&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22805488" class="" data-editable="true" data-title="小学生都能理解的贝叶斯公式。 - 机器学习笔记 - 知乎专栏"&gt;小学生都能理解的贝叶斯公式。 - 机器学习笔记 - 知乎专栏&lt;/a&gt;（作者懒得再写一遍了自己去看，正好写道了最小描述长度）&lt;/p&gt;&lt;p&gt;老张头不乐意了。嘿，我咋看不懂，你是看不起我小学没毕业是卟？&lt;/p&gt;&lt;p&gt;照你故事里的公式这么说，岂不是任何一个描述语句都应该满足奥卡姆剃刀原则了是卟？你咋能这么绝对捏？你这是严重的左倾主义，信不信老子把你工分扣完？&lt;/p&gt;&lt;p&gt;冤枉啊大爷。所以我说奥卡姆剃刀是玄学啊……&lt;/p&gt;&lt;p&gt;那为毛是玄学呢？&lt;/p&gt;&lt;p&gt;事实上来说，对于奥卡姆剃刀原则，每个人的理解是不同的（但是大多数人很难意识到）对于（像我这种）一般群众来说，一般人对奥卡姆剃刀的理解有3种方式：&lt;/p&gt;&lt;p&gt;1、如果在某个可定义范围内若找出了最优解（最优描述），那么不应当在此范围的周围再去添加任何描述（就算这个描述是对的）。&lt;/p&gt;&lt;p&gt;2、如果我无法分辨出最优描述，那么，在备选的描述中（可容忍描述误差范围内），优先选择更简洁（信息熵最小）的描述。&lt;/p&gt;&lt;p&gt;3、如果我无法分辨出最优描述，那么优先选择更符合直觉经验的描述，而不选择人脑思考起来更累的抽象逻辑描述。&lt;/p&gt;&lt;p&gt;对于这3点的理解不同，造成了很多人在辩论奥卡姆剃刀这个问题上的区别，有的人认为是觉得正确的，有的人认为是模棱两可的，比如这个纸糊问题&lt;a href="https://www.zhihu.com/question/20159241" data-editable="true" data-title="「奥卡姆剃刀原则」是正确的吗？ - 哲学" class=""&gt;「奥卡姆剃刀原则」是正确的吗？ - 哲学&lt;/a&gt;就是，每个答主对奥卡姆剃刀的理解都不一样，看这些人的评论区的辩论真是好玩……有的人认为是玄学。由于我只是一个纸糊小透明菜鸟，并不敢和基督徒或绿绿们讨论他们的神符不符合奥卡姆剃刀原则，所以我只说这3点（删除线）。&lt;/p&gt;&lt;p&gt;现在先从第一点来说，老规矩，咱要说得小学生都能看懂：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1、&lt;/b&gt;&lt;/p&gt;&lt;p&gt;现在有一个描述，你已经得知是最优解了。例如，对于若干个数字1和数字2，组成一个只允许使用加法运算的简单等式，让你描述：&lt;/p&gt;&lt;p&gt;那么，由于只允许使用加法，最简单的，当然是只用一个加法运算符的：&lt;/p&gt;&lt;p&gt;1+1 == 2&lt;/p&gt;&lt;p&gt;然而我们知道，这个也是正确的：&lt;/p&gt;&lt;p&gt;1+1 + 1 == 2 + 1&lt;/p&gt;&lt;p&gt;但是这种情况多用了两个加法运算符，就算这个描述是对的，由于我们只需要组成一个运算符，所以按照剃刀原则，这个应该抹去，而选择第一个描述，也就是对于这个命题下，我们选择的描述为：&lt;/p&gt;&lt;p&gt;1+1 == 2&lt;/p&gt;&lt;p&gt;很容易理解吧？所以，如果已知最优解了，当然应该选择使得信息量最少的最优解，而不是去添加一堆东西。&lt;/p&gt;&lt;p&gt;从第一点来说，因为每一个信息都有一个大于等于0的概率产生杂音，产生杂音就会降低准确率（&lt;a href="https://zhuanlan.zhihu.com/p/22805488" class="" data-editable="true" data-title="小学生都能理解的贝叶斯公式。 - 机器学习笔记 - 知乎专栏"&gt;小学生都能理解的贝叶斯公式&lt;/a&gt;里证明过了），所以在第一种理解前提下，奥卡姆剃刀当然是对的……&lt;/p&gt;&lt;p&gt;然后看第2种理解方式：&lt;/p&gt;&lt;p&gt;&lt;b&gt;2、&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在&lt;a href="https://www.zhihu.com/question/20159241" data-editable="true" data-title="「奥卡姆剃刀原则」是正确的吗？ - 哲学" class=""&gt;「奥卡姆剃刀原则」是正确的吗？ - 哲学&lt;/a&gt;问题中的&lt;a href="https://www.zhihu.com/people/20e911524247b63b55decfbe6080aceb" data-hash="20e911524247b63b55decfbe6080aceb" class="member_mention" data-editable="true" data-title="采铜" data-hovercard="p$b$20e911524247b63b55decfbe6080aceb"&gt;采铜&lt;/a&gt;先生（哎，得一年看不到这哥们的更新也是有点伤感。好怀念哪个剃刚毛的答案……），他对于奥卡姆剃刀的理解，个人感觉就是第二种方式。&lt;/p&gt;&lt;p&gt;那么为什么第二种方式的情况下，奥卡姆剃刀原则就不一定正确了呢？这里我举个例子。&lt;/p&gt;&lt;p&gt;假设，现在小明要向小白证明“我是你爸爸”（咦……我咋又玩起这个梗了……）。小明可以选择两个不同的描述方法集合，第一个描述方法领包含的信息熵为50KB，准确率为50%，第二个描述方法集合描述方法包含的信息熵为50GB，准确率为99.9%&lt;/p&gt;&lt;p&gt;那么，在这种情况下，如果单纯按照奥卡姆剃刀原则，选择了描述方法1，可能小明最后的证明就会以失败告终。而如果选择描述方法2，也许小明向小白传递信息的能力有限，50GB信息传送过去损失了一大半，结果最后准确率还不如99.9%。&lt;/p&gt;&lt;p&gt;那么这个问题怎么解决呢？&lt;/p&gt;&lt;p&gt;所以要考虑小明向小白证明我是你爸爸，需要达到多少的准确率？允许传递的最大信息量是多少？有多少前提条件需要考虑？大家的知识背景是什么……&lt;/p&gt;&lt;p&gt;所以各位看官明白了吧？这个问题，实际上就是因为对于问题的描述简化了，导致下一个问题变得无法解了。想来估计出题的脑残作者也对这个问题的描述使用了并不该使用的奥卡姆剃刀吧……（哎哟，别打脸……）&lt;/p&gt;&lt;p&gt;所以关于理解2，就出现了一个问题，如果并没有办法debug 出最优解，那么，就有可能发生剃胡子刮到肉的情况，这就是现实生活中为什么奥卡姆剃刀原则不是完全适用的。&lt;/p&gt;&lt;p&gt;但是从另一方面考虑的话（&lt;b&gt;接下来才是重点上面大部分信息是我在逗逼&lt;/b&gt;），可以这样理解，对问题的信息熵为I（X），对答案的描述的信息熵为I（Y,X）。&lt;/p&gt;&lt;p&gt;刚才这个解答过程犯得一个明显的错误是，分别单独考虑I（X）和I（Y,X），分别使用奥卡姆剃刀，而不是对 I（X） + I（Y,X）来用剃刀，所以事实上并没有满足所谓的最短信息描述，讲道理不仅没满足最短信息，这一拆开，连贝叶斯公式都没满足了。&lt;/p&gt;&lt;p&gt;也就是说，在理解2中，所谓的   奥卡姆剃刀，并不是最短信息描述。&lt;/p&gt;&lt;p&gt;嘛……但是很多人对于奥卡姆剃刀的理解的确确实就不是最短信息描述啊~~~~~~~~~~~&lt;/p&gt;&lt;p&gt;所以就出现了第三种理解方式：&lt;/p&gt;&lt;p&gt;&lt;b&gt;3、玄学の剃刀&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这就是大多数反剃刀的人理解了。怎么说呢？对于很多人来说，所谓的简单和复杂，并不是基于这个描述的信息熵的，而是基于这个描述我是否能直观看得懂。&lt;/p&gt;&lt;p&gt;举个例子，正太啊不对正态分布，这两种描述方式（都是图）&lt;/p&gt;&lt;p&gt;描述1：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-c7b035a4f2524387bcdce9bd30d580ad.png" data-rawwidth="273" data-rawheight="185"&gt;&lt;/p&gt;&lt;p&gt;描述2：&lt;/p&gt;&lt;equation&gt;\frac{1}{  \sigma\sqrt{2\pi }  } e^{-\frac{(x-\mu )^2}{2\sigma ^2} }&lt;/equation&gt;&lt;p&gt;对于大多数人来说，会直观觉得，嘛，描述1很符合直觉，一下就看得懂。描述2……撒撒撒，这都是些潵。&lt;/p&gt;&lt;p&gt;然而我问了问我家电脑，以他的理解，描述1，电脑认为它的信息量是3.14kb。描述2，电脑认为它的信息量是1.30KB。显然描述2对于电脑来说，是信息熵更低，也就是更简洁的。&lt;/p&gt;&lt;p&gt;当然，这里我不是说描述1和描述2谁更正确。我要说明的是两张表示内容一样（正太分布），表达载体一样（都是图），表达方式不相同，传递的信息量在不同信息接受体（比如人）中直观感受到的，也许并不一样。&lt;/p&gt;&lt;p&gt;而既然接受体都不一样，那自然无法得出一个普世的结论，得到的结论具有主观差异（加上理解2里说了，这玩意这样思考已经不一定满足贝叶斯公式了。），玄而又玄，那到底剃刀原理有没有效呢？这特么就成玄学了。&lt;/p&gt;&lt;p&gt;剃头匠老张头高兴了。嘿，二麻子你小子可以啊，把咱的剃刀说得有板有眼的，咱人民公社要发展轻工业，行嘞，就由二麻子你，负责生产奥卡姆剃刀吧！让全国的剃头匠，都用上咱的剃刀！把全国的男女老少，头都剃的像那红太阳般锃亮锃亮的！&lt;/p&gt;&lt;p&gt;啥……张大爷，这剃刀没法生产啊……&lt;/p&gt;&lt;p&gt;有什么没法的，人有多大胆，剃刀有多大产，有困难，自己克服！&lt;/p&gt;&lt;p&gt;散会&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22804193&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Sun, 09 Oct 2016 08:13:46 GMT</pubDate></item><item><title>小学生都能理解的贝叶斯公式。</title><link>https://zhuanlan.zhihu.com/p/22805488</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-045ce061c63f900e35ab1f3c0796fb16_r.png"&gt;&lt;/p&gt;班主任：你们两个在干什么？班长小红，给我过来，叙述一下事情经过！&lt;p&gt;小红，现在我是小白，你是小明，说一下你们为什么吵架！&lt;/p&gt;&lt;p&gt;好的老师，小明好坏好坏的，他莫名其妙过来，什么前提条件都不给，上来就是一句“我是你爸爸”&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-31ff9648ccaac01b9727458563237c91.jpg" data-rawwidth="136" data-rawheight="128"&gt;你接着就是一巴掌，然后说“你麻痹不给定前提条件，给我的就是个无信息先验分布，等同于前提条件等于正无穷，所以你说我是你爸爸这个结果的符合概率为1/∞≈0”所以说你的命题“我是你爸爸”的概率为0。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-fc1c3a17f58dafd3de901adc45664feb.jpg" data-rawwidth="176" data-rawheight="176"&gt;&lt;p&gt;然而经过我的验证，目前全世界有70亿+1人，而其中一定有一人是你爸爸，我是一个人的概率为1，所以在这个假定条件下，我有理由认为，P（我是你爸爸） = P（这个世界上有一个人是你爸爸）*P（我是一个人）/P（全世界人有70亿）=1*1/70亿的概率，我是你爸爸。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-9268138fac1663f6bb24363fbd476274.jpg" data-rawwidth="136" data-rawheight="128"&gt;&lt;p&gt;然后他摸了摸被打残的脸，微微一笑说，你忽略了一件事，我也是一个人，所以在你的假设条件下，我也有理由认为P（我是你爸爸） = P（这个世界上有一个人是你爸爸）*P（我是一个人）/P（全世界人有70亿）=1*1/70亿的概率，我是你爸爸。所以我是你爸爸的概率等同于你是我爸爸。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-fdab69335781852b2b82ed2079ee671c.jpg" data-rawwidth="176" data-rawheight="176"&gt;&lt;/p&gt;&lt;p&gt;那么，假设我们俩其中有一个人是对方爸爸，现在在这个样本下，我们俩互相是对方爸爸的概率为：&lt;/p&gt;&lt;p&gt;P（我是你爸爸/基于我们俩其中有一个人是对方爸爸） = P（全世界有一个人是你爸爸，这个人是我）/（P（全世界有一个人是你爸爸，这个人是我）+P（全世界有一个人是我爸爸，这个人是你））等于1/2，所以我有50%的概率是你爸爸而你只有1/70亿的概率是我爸爸！所以我是你爸爸。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-bc2e7266313a7a03f8c3b9c50e5996f3.jpg" data-rawwidth="178" data-rawheight="192"&gt;&lt;p&gt;然后你飞起就是一巴掌：你个SB，你的50%的概率建立在已经验证了“基于我们俩其中有一个人是对方爸爸”这个假定条件下，是个后验概率，我的1/70亿的概率基于还没有验证上面哪个假定条件的前提下，属于先验概率，拿后验概率和先验概率样本都不一样来比，你说你四不四潵？？？？？&lt;/p&gt;&lt;p&gt;说到这里，小红说。这时候我实在看不下去了，一会儿我是你爸爸，一会儿全世界有一个人是你爸爸的，这么长，还让不让人吵架了。于是我就上去劝说了一下：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-576b5f0d4694b804b6cfa8441e1f381c.jpg" data-rawwidth="180" data-rawheight="144"&gt;要不这样，我们把你们想要证明的“我是你爸爸”作为结论Y。你们的目的是证明结论Y的合理性，也就是概率，那么，你们要提出一些假设X，我们才能知道你们在假设空间X以下的概率instead of 而不是1除以无穷等于0。&lt;/p&gt;&lt;p&gt;然后呢，你们俩逗逼都是在从人的范畴里找符合定义，所以我们简单认为你们是基于个体为人这个单位个体的均匀先验分布假设这个分布为C，为某一个常数，（就打算是为1吧，反正待会儿要约掉）。&lt;/p&gt;&lt;p&gt;辣么在我们不知道具体数字的时候，我们给这个概率一个标志，既然是在假设空间X中Y的概率，辣么就称之为P（Y，x）。设若你们的所有假设在同一个假设空间C中，那么C就可以约掉，现在我们就考虑X单独发生的概率为P（x），Y单独发生的概率为P（Y），辣么x和Y同时发生的概率，就等于Y和x同时在一个共同的假设空间C发生的概率。也就是说，在假设空间x中，Y发生的概率，乘以假设空间x发生的概率，就等于反过来，在假设空间Y中，X发生的概率，乘以假设空间Y。&lt;/p&gt;&lt;p&gt;即：&lt;equation&gt;P(Y,x)*{P(x)={P(x,Y)*P(Y)
}} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;这样考虑我们要得到的目标P（Y，x），就可以放到等式左边，写为：&lt;equation&gt;P(Y,x)=\frac{P(x,Y)*P(Y)
}{P(x)} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;这就是你们的最佳假设。&lt;/p&gt;&lt;p&gt;然后你们的最佳假设，我们来算算P（Y，x），由于你们认定的全空间为C，那么：&lt;/p&gt;&lt;p&gt;P（Y）=1/C&lt;/p&gt;&lt;p&gt;P（x）=1/C&lt;/p&gt;&lt;p&gt;P（x，Y）=1/X（X为所有x的数量，也就是x所在的假设空间的容量大小）&lt;/p&gt;&lt;p&gt;辣么，就可以算出，P（Y，x）=1/X&lt;/p&gt;&lt;p&gt;小明很生气，辣么，如果我们的假设条件建立在相同的假设空间下，岂不是又是概率一样咯。那我如何向小白证明我是你爸爸呢？难道我们的友好讨论，就变成了提出更多的假设吗？这岂不是和小孩子吵架一样了么？&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-b41ac046ca9c773a2e343c4003a2bd8c.jpg" data-rawwidth="259" data-rawheight="194"&gt;&lt;/p&gt;&lt;p&gt;小红瞪了小明一眼：你们说的话是100%可信的么？不是100%可信不就有噪音么？所以，你们的假设x的概率不应该是你们的假设f（x），而应该&lt;equation&gt;f(x)+\varsigma &lt;/equation&gt;,这个&lt;equation&gt;\varsigma &lt;/equation&gt;表示的就是你们假设的杂音量。一般而言，你们这些正太瞎扯淡的噪音满足正态分布。&lt;/p&gt;&lt;p&gt;所以现在我们就要讨论下一个问题了，在描述了足以确认我是你爸爸的条件下，才能最大化证明假设我是你爸爸的正确性，那么，如何找到这个最大可能性呢？&lt;/p&gt;&lt;p&gt;所以我们做个最大似然假设，hmax，假设满足hmax要提出i个在区间I里的使用x符合要求的基本假设h，那么，这个hmax的概率就可以简单地假设为：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;h_{max} = max(\prod_{i\in I}P(Y,h) )&lt;/equation&gt;也就是说使表达式最大时的&lt;/p&gt;&lt;p&gt;由于我们这里的正确假设为h，那么就可以认为大Y是有一堆小y组成的，其中y=h(x)+&lt;equation&gt;\varsigma &lt;/equation&gt;，我们可以吧&lt;equation&gt;\varsigma &lt;/equation&gt;提到一边去，得到&lt;equation&gt;\varsigma &lt;/equation&gt;=y-h(x)。那么，由于&lt;equation&gt;\varsigma &lt;/equation&gt;满足高斯分布，所以得到&lt;/p&gt;&lt;p&gt;max：&lt;equation&gt;\prod_{i\in I}P(Y,h) = \prod_{i\in I}\frac{1}{\sqrt{2\pi \sigma ^2} } e^{-1/2(y-h(x_i))^2/\sigma ^2}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;嘛，看不懂无所谓。反正要约掉的，由于我们求的是最大值而不是具体值，求得是使该公式最大的时候的参数，所以就可以把杂七杂八的都约了，得到：&lt;/p&gt;&lt;p&gt;max：&lt;equation&gt;\sum_{i\in I}^{}-({y-h(x_i)})^2 &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;换言之，也就是找到&lt;/p&gt;&lt;p&gt;min：&lt;equation&gt;\sum_{i\in I}^{}({y-h(x_i)})^2 &lt;/equation&gt;（找到这个令这个公式最小时的参数）&lt;/p&gt;&lt;p&gt;所以说，你们要证明自己是对方爸爸，就要找到令你的论据，应对与你的假设空间，得到的差值的最小时的论据，这样才能最有可能证明自己是对方爸爸！！！！&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-029b194eb3d97ac7cf33f2d4b8a80fc3.jpg" data-rawwidth="176" data-rawheight="176"&gt;&lt;p&gt;接下来小红又继续说了。然而，你们对于我是你爸爸这个结果的描述（x）越多，得到的杂音根据公式也会越大，也就是说，对于我是你爸爸这条信息的置信度也就越低。相对应的，之前你们也说了，自己提出的假定条件越少，得到我是你爸爸这个结论的概率也会越低。所以，我们要找到一个在证明“我是你爸爸”时，最优的描述。&lt;/p&gt;&lt;p&gt;既然要得到的是，最优的描述，那么我们可以理解为，已经验证的条件下（验证以后发现）这个描述是最优描述，就是验证后概率最大的描述。假设条件为x，则描述得到的概率（最大后验公式），简单写为：P（Y，x）P（x）。&lt;/p&gt;&lt;p&gt;我们的目标就是令这个概率最大对吧？MAX：P（Y，x）P（x）&lt;/p&gt;&lt;p&gt;嘛……既然我们的目标是“描述”，那么也就是关于“信息”的处理，那么就参考一下香农的信息论：（&lt;a href="https://zh.wikipedia.org/wiki/%E4%BF%A1%E6%81%AF%E8%AE%BA" class="" data-editable="true" data-title="wikipedia.org 的页面"&gt;信息论：维基百科&lt;/a&gt;）&lt;/p&gt;&lt;p&gt;嘛……我也懒得看，所以我就随便抓了一个叫做熵的东西过来，熵嘛，这样定义的，&lt;equation&gt;I=-log(p)&lt;/equation&gt;，意思就是概率为p的事情包含的信息量，log的底数取决于信息量的单位，比如比特什么的……嘛。这里管不到。&lt;/p&gt;&lt;p&gt;然后我就就看我们要max的公式嘛，P（Y，x）P（x），取个对数（底是什么随便你）比如我们这里用log，就变成了使&lt;equation&gt;log(P(Y,x))+log(P(x))&lt;/equation&gt;最大,按照哪个熵里面log有个负号，就变成了：&lt;/p&gt;&lt;p&gt;使&lt;equation&gt;-log(P(Y,x))-log(P(x))&lt;/equation&gt;最小。&lt;/p&gt;&lt;p&gt;也就是min:   I(P(Y,x))+I(P(x))&lt;/p&gt;&lt;p&gt;翻译成人话就是，使描述的信息熵，对于描述：结论Y由x的假定条件，以及x的假定条件，总信息量最短的描述，就是最优描述，简称最短信息描述。&lt;/p&gt;&lt;p&gt;以小明和小白的观点就是：要达到证明我是你爸爸最准确，就得让“描述在某条件下，我是你爸爸”的信息，加上“描述某条件的信息”，总体来说最小。&lt;/p&gt;&lt;p&gt;嗯，这个最短信息描述在玄学界还有个别名，叫做奥卡姆剃刀……&lt;/p&gt;&lt;p&gt;老师，我说完了。&lt;/p&gt;&lt;p&gt;班主任：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-50840371889c51043f00f653f76cc473.jpg" data-rawwidth="158" data-rawheight="134"&gt;&lt;p&gt;小明和小白：所以我俩就合伙揍了她一顿。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-edfb8e29414728ce21a3c561c0d8753a.jpg" data-rawwidth="225" data-rawheight="225"&gt;&lt;/p&gt;&lt;p&gt;下课。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22805488&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Sat, 08 Oct 2016 01:31:39 GMT</pubDate></item><item><title>杂谈：（菜鸟如我）用哪些轮子？</title><link>https://zhuanlan.zhihu.com/p/22697926</link><description>其实对于普通人来说，不用分析那么多模型，用各种机器学习库就可以了。再者，对于我们这些小白来说，经常写算法写的不6，一个模型写下来循环多得不得了，测一下时间和复杂度，用人家的库，10秒解决的问题，自己编写的模型，有可能俩小时还没跑出来（别问我怎么知道的……）。&lt;p&gt;那么问题来了，用什么库？&lt;/p&gt;&lt;p&gt;个人是用python，这里推荐几个常用的……估计大家都知道，不过不知道的可以试着去用一下。&lt;/p&gt;&lt;p&gt;首推numpy。原因除了点乘之类的很多东西用这玩意很方便以外，最主要的一个原因……是大量的机器学习库，你如果没安numpy……会没办法运行……（别问我怎么知道的……）&lt;/p&gt;&lt;p&gt;然后是pandas（熊猫），其实可以不用熊猫，比如我就经常用graphlab而不用熊猫。不过如果不是因为替代品的原因的话，类似的结构化数据的库你总得用一个。原因？&lt;/p&gt;&lt;p&gt;我给你举个例子……大量机器学习比赛，会给你一堆生数据，然后你得清洗数据对吧？比如三张表合为一张表，三张表长度不同，顺序是乱的，你要根据用户名来组合。现在你不用结构化数据的库，自己编循环来for和if组合。三张表，至少得三张表的长度相乘的复杂度。假设平均每张表100000行数据（长度10000），你3个for循环if一下，按照一个if语句算一个复杂度，就是10^15的复杂度，假设每个复杂度你要跑1纳秒……嗯……大概12天能跑完。我试了一下，10万行的3张表合到一起，我这渣电脑用熊猫的join函数大概2秒钟吧……当然你用别的什么结构化的库都行，你用SQL也行。&lt;/p&gt;&lt;p&gt;然后随便用个结构化的库处理完了以后，就到机器学习库了。主推sklearn。为啥？便宜啊……便宜到了免费的程度。像我们这些一张高级显卡都要纠结买不买的屌丝，用免费的库再正常不过啦……&lt;/p&gt;&lt;p&gt;不不不，这不是主要原因。其实最主要的原因是因为Sklearn好上手。sklearn的文档非常详细，而且几乎所有的方法都给了范例，稍微跑跑很容易理解，（这点熊猫就做得很不好……在熊猫里大量的方法没给范例……对新人很不友好。）而且因为sklearn是免费库，所以大量的数据分析比赛都允许使用……嗯……说不准哪个四线小城市的比赛人家都在用excel分析数据你就用sklearn去拿了个两百块的奖金呢。&lt;/p&gt;&lt;p&gt;graphlab。&amp;lt;delete&amp;gt;推荐graphlab主要是因为我在coursera上学的另一门课是用graphlab教的所以我这种菜鸡就习惯了用这个&amp;lt;/delete&amp;gt;……其实主要是因为两个原因。1、这玩意的结构化数据库有范例，查起来方便（虽然要翻墙）。2是因为……不知道为什么，这玩意很多时候跑得比sklearn快。不过这玩意你得用要申请（或者买）用户ID，所以简单来说，你如果指着参加点数据分析的比赛拿奖金糊口，那这个只能用来验证模型。具体还得自己编（不过一般验证了模型以后编起来难度已经小多了……）&lt;/p&gt;&lt;p&gt;不过！这玩意有个非常好的好处，倒不是这个库怎么有好处，而是这个GraphLab Create的安装包……直接把常用库一并打包了。对于如我这种电脑三天两头崩溃的来说，重装库简直是灾难。直接安一个GraphLab Create，连着anaconda和jupyter都给安好了，多方便……&lt;/p&gt;&lt;p&gt;最后就是推荐jupyter notebook了。你习惯用其他IDE可以不用这个。推荐这玩意主要是因为……非常方便……谁用谁知道。举个例子，我现在几乎不开计算器和atom算东西了，无论是简单的还是复杂的问题，都用jupyter notebook写。这玩意比计算器用着方便，比文本编辑器用着顺手，比ide加载得快……好处多得我跟你说，给我一瓶啤酒我能吹一晚上……&lt;/p&gt;&lt;p&gt;就这些吧。顺便福利一下GraphLab Create我留在网盘里的备份，懒得一个一个安装库的可以下这个：链接：&lt;a href="http://pan.baidu.com/s/1pLhHWkb" class="" data-editable="true" data-title="baidu.com 的页面"&gt;http://pan.baidu.com/s/1pLhHWkb&lt;/a&gt; 密码：fp0l （啥？你问为啥不发github？当然是因为很多人没法翻墙啦！才不是因为我太菜了git用得不熟练呢。）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22697926&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Thu, 29 Sep 2016 17:07:13 GMT</pubDate></item><item><title>L0，L1，L2正则化</title><link>https://zhuanlan.zhihu.com/p/22505062</link><description>之前说起正则化，我们光说了加一个惩罚项&lt;equation&gt;\sum_{j=1}^{m}{\theta _j^2} &lt;/equation&gt;，用步长&lt;equation&gt;\lambda &lt;/equation&gt;来调节,但是为什么是这样，却没说。&lt;p&gt;那这篇文章就讨论一下为毛是这样，以及常用的别的惩罚项长啥样。&lt;/p&gt;&lt;p&gt;先说&lt;/p&gt;&lt;p&gt;L0正则化：&lt;/p&gt;&lt;p&gt;这玩意长这样：&lt;equation&gt;\sum_{j=1,\theta _j\ne 0}^{m}{\theta _j^0} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;0的0次方没有意义，在这里如果按照L1和L2看显然该定位0，这里讨论就是不参与惩罚项，不参与加权。即所有非零项算作1加起来，然后用步长&lt;equation&gt;\lambda &lt;/equation&gt;调节。意思很明显，每一个对预测产生了贡献的参数，我都惩罚一次，不多不少，大家都一样。就像一个法官判决，你偷了一毛钱，他杀了一个人，法官均以“价值观不正确”为由，把你们判一样的罪……只有一点都没参与的人，才不会被判刑。&lt;/p&gt;&lt;p&gt;很明显有问题，这玩意正则化后，岂不是无论什么样的函数，无论多么复杂，都往一根横着的直线上调节么？&lt;/p&gt;&lt;p&gt;还有一个问题，干嘛叫L0呢？直接调步数不就可以了么……&lt;/p&gt;&lt;p&gt;这个问题后面会说。&lt;/p&gt;&lt;p&gt;L1正则化：&lt;/p&gt;&lt;p&gt;这玩意长这样：&lt;equation&gt;\sum_{j=1}^{m}{|\theta _j|} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;拿法官举例子，就是，法官要按照你们的罪行量刑判罪，但是都得判，无论你影响最终是好是坏（比如你杀了个人，这个人也是个坏人，但是你还是犯了杀人罪得判刑）都按照罪行判罪。于是就都取个绝对值，表示都判，然后按照罪行大小判罪了……&lt;/p&gt;&lt;p&gt;这个地方估计大家可以理解了，惩罚项嘛，按照一个参数的影响来判才对嘛……&lt;/p&gt;&lt;p&gt;不过这还有个问题，x的绝对值，你给我求导一下看看，求出来就是 土1,大于0的时候是1，小于0的时候是-1，一个还好，一个向量X里的话，有神特么多个x，求个导出来能把人累死。&lt;/p&gt;&lt;p&gt;于是就引出L2了：&lt;/p&gt;&lt;equation&gt;\sum_{j=1}^{m}{\theta _j^2} &lt;/equation&gt;&lt;p&gt;就是我们看到的，笔记里记的了。平方了以后嘛，做包括求导在内的各种计算就方便了，啥，你说大了，无所谓啊，反正有个lamda来调节步长，谁在乎呢？&lt;/p&gt;&lt;p&gt;好吧……到这里我就在不用一堆专业术语的情况下，用这种糊弄的方式，勉强把L0，L1，L2解释出来了，啥欧氏距离马氏距离剃刀原则都糊弄过去没说，反正你编程也用不到这些玩意。不过还有一个问题，作为一个喜欢开脑洞的宅，万一某一天你好奇，为毛这要叫L0，L1，L2,为毛不叫L1，L2，L3，不叫个数，拉索和雷吉呢？我还想叫他山岭回归呢……&lt;/p&gt;&lt;p&gt;这个……你可以这样理解：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;L(\theta) = \sum_{j=1}^{m}{(\sqrt{\theta _j^2} )^p} &lt;/equation&gt;当p等于0的时候，就是L0，当P等于1的时候，就是L1，当p等于2的时候，就是L2.嗯……这下就师出有名了啊哈哈哈……&lt;/p&gt;&lt;p&gt;好了不扯淡了。实际上这玩意表示的是距离，比如x点和y点之间的距离：&lt;/p&gt;&lt;equation&gt;L(x,y) = \sum_{j=1}^{m}{(\sqrt{(x_j-y_j)^2} )^p} &lt;/equation&gt;&lt;p&gt;而正则化是表示到哪儿的距离呢？到0点，也就是和完全不改变原函数的区别（图我懒得找了，随便搜图搜个L2norm都能搜出来）。而由于是到0点，所以可以只保留一个参数，那么上面哪个&lt;equation&gt;L(x,y) = \sum_{j=1}^{m}{(\sqrt{(x_j-y_j)^2} )^p} &lt;/equation&gt;就变成了&lt;equation&gt;L(\theta,0) = \sum_{j=1}^{m}{(\sqrt{(\theta _j-0)^2} )^p} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;啊，差不多就糊弄完了……&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22505062&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Mon, 19 Sep 2016 12:24:44 GMT</pubDate></item><item><title>正规方程的推导过程</title><link>https://zhuanlan.zhihu.com/p/22474562</link><description>那啥，之前笔记里这部分是略过的。这里整理一下吧。有兴趣的可以对照看看和你推倒的过程一样不。&lt;p&gt;我们先回顾一下，我们定义观测结果y和预测结果y'之间的差别为Rss:&lt;/p&gt;&lt;equation&gt;Rss = \sum_{i=1}^{n}({y_i-y_i'} )^2= \sum_{i=1}^{n}({y_i-h(x_i)} )^2 = (y-h(X))^T*(y-h(X))&lt;/equation&gt;&lt;p&gt;设若参数的矩阵为&lt;equation&gt;\theta&lt;/equation&gt;,则&lt;equation&gt;h(X)=\theta*X&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;那么&lt;equation&gt;Rss  = (y-h(X))^T*(y-h(X)) =  (y-X*\theta)^T*(y-X*\theta) &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;按照我们的定义，这个Rss的意思是y和y'之间的差，那么当Rss无限趋近于0的时候，则y≈y'，即我们求得的预测结果就等于实际结果。&lt;/p&gt;&lt;p&gt;于是，令Rss等于某一极小值&lt;equation&gt;\delta &lt;/equation&gt;，则&lt;equation&gt;(y-X*\theta)^T*(y-X*\theta) ==\delta &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;对参数&lt;equation&gt;\theta&lt;/equation&gt;求导，得：&lt;/p&gt;&lt;equation&gt;\frac{d}{d(\theta)}(y-X*\theta)^T*(y-X*\theta)== 2X^T*(y-X*\theta)==0&lt;/equation&gt;&lt;p&gt;展开，得&lt;equation&gt; 2X^T*y==2*X^T*X*\theta&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;进而就可以得到&lt;equation&gt;\theta ==(X^T*X)^{-1}*X^T*y&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;于是我们就得到正规方程了。&lt;/p&gt;&lt;p&gt;再讲一个推导方式：&lt;/p&gt;&lt;p&gt;我们可以用矩阵乘法：&lt;/p&gt;&lt;equation&gt;Y=X\theta &lt;/equation&gt;&lt;p&gt;两边同时乘以&lt;equation&gt;X^T&lt;/equation&gt;&lt;/p&gt;&lt;equation&gt;X^TY=X^TX\theta &lt;/equation&gt;&lt;p&gt;然后再乘以&lt;equation&gt;(X^TX)^{-1}&lt;/equation&gt;&lt;/p&gt;&lt;equation&gt;(X^TX)^{-1}X^TY=(X^TX)^{-1}X^TX\theta &lt;/equation&gt;&lt;p&gt;就得到&lt;equation&gt;\theta = (X^TX)^{-1}X^TY&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;……不过这第二种方法是在知道了正规方程是什么以后再推导的。虽然看起来很快，然而并没有告诉你为什么。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22474562&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Sat, 17 Sep 2016 08:44:47 GMT</pubDate></item></channel></rss>