<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>机器学习笔记 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/mlearn</link><description>机器学习的笔记。非常浅显。不得不说ng的课很适合入门，几乎不需要太多的数学基础。</description><lastBuildDate>Tue, 20 Dec 2016 22:16:18 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>使用机器学习模型对大盘指数进行预测</title><link>https://zhuanlan.zhihu.com/p/24417597</link><description>&lt;p&gt;用数学模型分析策略，可以避免由于情绪波动的造成的影响，避免一些因此产生的非理性策略。从这点上来说，机器学习可以很好地避免这种主观情绪造成的非理性决策。&lt;/p&gt;&lt;p&gt;大多数人在炒股的时候会觉得，如果我能判断大盘涨跌，大盘涨的时候，我就买，下跌的时候，我就全卖了，等下次涨，那肯定能赚钱。那么，本篇文章就用3个模型（SVM，决策树，adaboost）来对HS300指数进行预测，记录数据的获取，清洗，模型选用，以及如何调参。&lt;/p&gt;&lt;h2&gt;1、获取数据：&lt;/h2&gt;&lt;p&gt;首先获取数据，这里我使用优矿的api：DataAPI.MktIdxdGet来获取历史数据。获取在fields中已定义的数据，本项目，我选择的数据有：tradeDate，交易日、closeIndex，收盘指数、highestIndex，当日最大指数，lowestIndex，当日最小指数，CHG，当日最大涨跌幅。&lt;/p&gt;&lt;p&gt;我们获取数据选用的是从2006年3月1日到2015年3月1日的所有交易日，一共有2127行的HS300指数数据。&lt;/p&gt;&lt;h2&gt;2、 处理数据：&lt;/h2&gt;&lt;p&gt;首先我把交易日设定为index，然后将预测用的交易日的前30日数据提取出来，找出前30日的最大指数，找出前30天最小指数，定义当日指数差为当日最大指数减去当日最小指数，找出前30天最大日指数差，加上之前通过api选择到的数据，作为特征值。&lt;/p&gt;&lt;p&gt;除去我们用作索引的交易日期，经过处理后，一共有11列数据，其中10个为特征值，一个是待处理的标签。我认为目前市场并不是完全的有效市场，少数异常值可以对预测的时候提供市场信息，所以我认为，不应当把任何值当作异常值去除。&lt;/p&gt;&lt;p&gt;处理完数据后，我们查看一下数据的统计描述：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-facb2b63733b72169636461949430e37.png" data-rawwidth="971" data-rawheight="515"&gt;&lt;p&gt;然后，我们找到预测用的30个交易日后的收盘价，用这个数字当作当前交易日需要预测的目标。设定lables表，为布尔值，如果这个预测目标大于当前交易日的收盘价，则设定为true，否则设定为False。&lt;/p&gt;&lt;p&gt;到这里数据集合就处理完了，待会儿可以直接切割这个集合用来做预测。&lt;/p&gt;&lt;h2&gt;3、有效性验证：&lt;/h2&gt;&lt;p&gt;为了验证模型的有效性，本项目采取了两个指标：&lt;/p&gt;&lt;p&gt;1、 模型交叉验证的score值。（这里的score值简单地计算测试机中准确预测的比率有多少。score值的范围为0~1，由于是二元分类问题，所以socre越接近1，模型表现越优秀。由于是二元分类问题，所以如果score小于等于0.5，那么可以认为模型失效。）&lt;/p&gt;&lt;p&gt;2、与纯随机策略对比的夏普比率（夏普比率是一个综合考虑风险和收益的计算数据，可以简单的理解为收益/风险，本模型中夏普比率采取的计算方式是（收益-基准收益）/标准差）。&lt;/p&gt;&lt;p&gt;首先，在训练模型的时候，对数据集进行交叉验证，获取score。以此来验证模型对于训练用数据集的准确率，如果小于等于0.5，则认为该模型对于该问题无效。如果score大于0.5，使用数据集时间点之后的数据进行回测，比对用了模型验证后，预测涨则随机选股，否则不选。对比没用模型时，纯粹随机选股的夏普比率，如果明显大于纯随即选股的夏普比率，则认为“如果大盘涨我就买，大盘跌就不买”这种思路是有效的。&lt;/p&gt;&lt;h2&gt;4、阈值设定：&lt;/h2&gt;&lt;p&gt;模型有效性经过验证的情况下，也就是说，“如果大盘涨我就买，大盘跌就不买”这种思路是有效的，且未经调参的模型score大于0.5的情况下。我认为经过调参后，模型的score要稳定大于0.8，才能证明该模型能够明显有效。&lt;/p&gt;&lt;h2&gt;5、纯随机策略回测图：&lt;/h2&gt;&lt;p&gt;用2016年1月到12月来回测，得到下图：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-14c0f6fc27cfc166c68f815523198b6e.png" data-rawwidth="910" data-rawheight="333"&gt;&lt;/p&gt;&lt;p&gt;夏普指数为-0.72，我们在验证有效性时，使用该纯随机策略，只添加“如果预测1个月后上涨，则进行交易”这一个条件，其他不变，依然是随机购买。&lt;/p&gt;&lt;h2&gt;6、SVM预测：&lt;/h2&gt;&lt;p&gt;&lt;b&gt;简要介绍：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;SVM又名支持向量机，对二元分类问题表现尤其良好。某种程度上来说，SVM是把数据映射到高维空间，然后对空间进行切割，所以训练点之间间隔越大，SVM效果越好。作为一个大间隔分类器，SVM可以最小化经验误差，降低结构化风险。SVM的计算复杂度取决于其映射产生的支持向量，故不易发生维数灾难这种问题（这很重要，因为本题定义了很多特征量）。而最终结果取决于少数重要向量，所以一定程度上增减向量，不会对模型造成太大损害。&lt;/p&gt;&lt;p&gt;当样本数量过多时，SVM的训练时间会大幅增加，同时，SVM对多元分类问题处理存在困难。&lt;/p&gt;&lt;p&gt;单看指数的话，股票交易10年也不过2500个左右的交易日，所以数据量并不大。而当前研究的问题也确实是二元分类问题，所以这里首先选用SVM模型进行测试。&lt;/p&gt;&lt;p&gt;&lt;b&gt;模型测试：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这里我们直接使用sklearn的SVM包中的分类模型SVC。在训练模型后，使用score函数，获得的预测准确率为：0.68。超过了0.5，所以可以认为该模型是初试有效的，值得进一步测试。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-7c278a0dbe380388f55ba1cce98ca0d9.png" data-rawwidth="973" data-rawheight="396"&gt;&lt;p&gt;我们用2016年1月到12月来回测，得到上图，发现该模型是有效的，此时的夏普比率为0.99.，大于纯随机策略的-0.72，同时，我们的策略线（蓝线）确实明显优于HS300大盘指数（黑线）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;调整参数：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;接下来，我将调整SVM的参数C，以便进一步提高SVM对于该模型的表现，观察随着C的变化，Score值的变化趋势：&lt;/p&gt;&lt;p&gt;我首先做了一个图，观察在C值在1~1000范围内，得到的Score值的变化：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-78476e65e0393939f5740c5792795fd0.png" data-rawwidth="296" data-rawheight="197"&gt;&lt;p&gt;我们发现，模型表现随着C值的提高而提高，在C值为360左右的时候，表现达到了最高。&lt;/p&gt;&lt;p&gt;接下来调整gamma，也是相似的方法，我们观察gamma在0~10之间的变化，得到下图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-4e00552913729cafcd06750c7c2e3e49.png" data-rawwidth="271" data-rawheight="184"&gt;&lt;p&gt;可以看出，gamma在1.8左右的时候，score表现得最好。&lt;/p&gt;&lt;p&gt;至于核函数，通常都是默认核函数最佳，这里由于是第一个模型，还是检验一下吧：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-c55ee8d81af067841f41927c52b64202.png" data-rawwidth="259" data-rawheight="197"&gt;&lt;p&gt;果不其然是rbf（默认的高斯核函数）是最佳的。&lt;/p&gt;&lt;p&gt;接下来我们用GridSearchCV来确切获得最佳的C值和gamma值：运行函数后，我们得到最佳的C值为300，最佳的gamma值为1.03。根据这两个参数，此时我们模型test后获得的score为0.76，明显高于之前的0.68。&lt;/p&gt;&lt;p&gt;接下来通过不同的数据集（改变数据集中的数据数量）的方法测试score的方法，判断模型是否稳健，得到下图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-355b8a9f2fcb685d07d864dc209b359a.png" data-rawwidth="303" data-rawheight="220"&gt;&lt;p&gt;发现根据数据集的不同，准确率上下摆动，摆动幅度在0.1左右。但是始终没有低于过0.72，0.72大于0.5，所以可以认为模型一定程度上是稳健的。&lt;/p&gt;&lt;p&gt;

但是由于0.72小于0.8，低于了我们设定的阈值，所以认为SVM模型对于该问题的解决表现不够良好。&lt;/p&gt;&lt;h2&gt;7、决策树预测：&lt;/h2&gt;&lt;p&gt;&lt;b&gt;简要介绍：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;由于决策树是归纳型算法，所以当其预测的数据集如果是人类很容易理解的信息，那么决策树可以表现良好。决策树可以清晰地处理大量数据，了解不同特征的影响重要性。这种算法在特征明确，杂音小，特别熟数据量较大时，效果较好。&lt;/p&gt;&lt;p&gt;决策树属于局部贪婪的算法，容易过拟合，有时无法保持全局最优，所以泛化能力较差。在股票交易中使用时，应当随时更新数据，否则有可能过拟合过去的经验，对未来的预测能力下降。&lt;/p&gt;&lt;p&gt;&lt;b&gt;模型测试：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这里我们直接使用sklearn的tree包中的分类模型DecisionTreeClassifier。在训练模型后，使用score函数，获得的预测准确率为：0.88。超过了0.5，所以可以认为该模型是初步有效的。&lt;/p&gt;&lt;p&gt;我们用2016年1月到12月来回测，得到下图&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-61b9d567ef6557b3c1a3c371d09e68df.png" data-rawwidth="554" data-rawheight="212"&gt;&lt;p&gt;发现该模型是有效的，此时的夏普比率为-0.09.大于纯随机策略的-0.72，我们的策略线（蓝线）也在HS300大盘指数（黑线）上面，所以可以认为相比于纯随机策略，模型是有效的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;调整参数：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;接下来我们调整决策树的参数。首先我们调整最大深度，在0~100之间，获得下图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-a2791c682dc01a1273c46114006c817d.png" data-rawwidth="240" data-rawheight="172"&gt;&lt;p&gt;发现最大深度在15左右的时候，开始趋于稳定，在18左右的时候，模型表现最好。&lt;/p&gt;&lt;p&gt;然后我们调节min_samples_leaf参数，从0到10之间，获得下图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-86931e17ab1d26a461d6486658aeb35f.png" data-rawwidth="243" data-rawheight="175"&gt;&lt;p&gt;发现min_samples_leaf震荡幅度很大，总体来说，随着min_samples_leaf增加，模型score降低。所以这里我们就选用默认值1。&lt;/p&gt;&lt;p&gt;接下来调整参数min_samples_split，范围在0~50之间，获得下图：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-f1693c3e83cb7c52ff373893e01a111a.png" data-rawwidth="230" data-rawheight="168"&gt;&lt;/p&gt;&lt;p&gt;发现随着min_samples_split的增加，模型在测试集上获得的score降低。&lt;/p&gt;&lt;p&gt;然后调整参数min_weight_fraction_leaf，范围从0~0.5，获得下图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-b28a8a221be4507d6c41faa1f72d0351.png" data-rawwidth="237" data-rawheight="176"&gt;&lt;p&gt;在可选范围内震荡过大，不具有明显规律，所以这时我们就选用默认值0.0。&lt;/p&gt;&lt;p&gt;接下来我们依靠上面选用的参数范围，使用GridSearchCV函数，选取表现出了趋势的参数max_depth和min_samples_split的最优值，我们得到的反馈为max_depth为32，min_samples_split为3.&lt;/p&gt;&lt;p&gt;依靠这两个参数重新进行一次在test集上的测试，这时我们得到的预测准确率为0.9，略高于之前的0.88&lt;/p&gt;&lt;p&gt;接下来通过不同的数据集（改变数据集中的数据数量）的方法测试score的方法，判断模型是否稳健，通过改变测试集，得到下图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-3050cdd68eff9a67f03e11bd73fed70c.png" data-rawwidth="253" data-rawheight="185"&gt;&lt;p&gt;发现总体来说，模型表现在0.82~0.94之间，1000次的测试稳定高于0.8，0.8是大于0.5的，所以可以认为我们的预测模型是稳健有效的。&lt;/p&gt;&lt;p&gt;同时因为我们1000次的测试中，模型准确率从未低于我们设定的阈值，0.8。所以认为我们的模型对于问题的解决表现良好。&lt;/p&gt;&lt;p&gt;既然表现良好，那么我们就来看看哪些特征影响最大，我生成了一下特征在决策树中的重要性，得到下面得到得到下面表格与图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-6411e83b20443534d34a39d5245894e2.png" data-rawwidth="219" data-rawheight="256"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-c635897e6999cd0fc7ffd44a56ff1f08.png" data-rawwidth="256" data-rawheight="261"&gt;&lt;p&gt;发现对于未来30天股市涨跌来说，最重要的预测指数是30天内的最高值和30天内的最低值，其次是30天内的最大日波动，这3项的重要性超过了50%。也就是说，对于未来股市涨跌的预测，决策树模型认为，股市的波动是最重要的。&lt;/p&gt;&lt;h2&gt;8、
Adaboost预测：&lt;/h2&gt;&lt;p&gt;&lt;b&gt;简要介绍：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;adaboost是一种通过训练多个不同的弱分类器，装配到一起的办法，形成一个较强的分类器的模型。它根据每个分类器上样本的准确性，来给特征分配权值，然后再把修改过权重值后的特征传入下一个分类器，依次迭代，最终融合成一个决策分类器。可以简单地把adaboost算法理解为“三个臭皮匠赛过诸葛亮”。&lt;/p&gt;&lt;p&gt;adaboost可以很好地对特征权重值进行筛选，一定程度排除无效的训练数据特征造成的干扰，增加关键数据的权重，可以较好的避免过拟合。&lt;/p&gt;

adaboost的缺点也很明显，弱分类器太少则训练结果不够好，太多则训练时间过长。&lt;p&gt;&lt;b&gt;模型测试：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这里我们直接使用sklearn的ensemble包中的分类模型AdaBoostClassifier。在训练模型后，使用score函数，获得的预测准确率为：0.78。超过了0.5，所以可以认为该模型是有效的，有调参的价值。&lt;/p&gt;&lt;p&gt;我们用2016年1月到12月来回测，得到下图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-f62b0c30c93a0fd086ac6cf2d14592a7.png" data-rawwidth="554" data-rawheight="251"&gt;&lt;p&gt;发现该模型是有效的，我们的策略线（蓝线）确实明显优于HS300大盘指数（黑线），此时的夏普比率为-0.13.大于纯随机策略的-0.72，所以可以认为相比于纯随机策略，模型是有效的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;调整参数：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;接下来我们先调整adaboost的参数，首先调整装配数量n_estimators，我们选择0~200范围，发现得到下图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-6a527fb0427a798390e71b51c1142828.png" data-rawwidth="263" data-rawheight="198"&gt;&lt;p&gt;果不其然是随着装配的弱分类器数量的增加，模型表现效果越好，而且依然在增加，由于依然在增加，我们接下来比对一下200~300范围内，模型的score表&lt;/p&gt;&lt;p&gt;现结果：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-9bc1105b3e64cbb8fd14d390a7204e12.png" data-rawwidth="229" data-rawheight="165"&gt;发现产生了较大的震动，那么可以认为，模型的最优n_estimators大致在这个范围之内。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-2a2018ec773506bc8c77c307b0ad7e66.png" data-rawwidth="223" data-rawheight="167"&gt;&lt;/p&gt;&lt;p&gt;为了保险起见，我又做了一个200~400范围内的图，发现震荡依然很明显，略有提高，并且在350以后近似于稳定，此时计算起来已经相当慢了。以及此时相比于200~300的平均提高，已经没有超过自身的波动范围，可以认为没有进一步尝试更高的参数的意义。&lt;/p&gt;&lt;p&gt;接下来我们调整参learning_rate，在0~10的范围内，得到下图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-169aff24038e2d2a6477700ace96a7a7.png" data-rawwidth="256" data-rawheight="187"&gt;&lt;p&gt;发现在1.7左右，表现最好。&lt;/p&gt;&lt;p&gt;综合以上两个大致范围，接下来我选用GridSearchCV在n_estimators为340时，learning_rate在1.5~2之间，寻找最优值。得到最优结果为：n_estimators=340，learning_rate = 1.53的情况下。此时预测准确率为0.91。&lt;/p&gt;&lt;p&gt;然后通过不同的数据集（改变数据集中的数据数量）的方法测试score的方法，判断模型是否稳健，通过改变测试集，得到下图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-2f90a7c507a21a17e30750d6cd336120.png" data-rawwidth="260" data-rawheight="190"&gt;&lt;p&gt;确率在0.86~0.94之间波动，未低于0.84，平均在0.9左右，由于0.84大于0.5，所以模型是有效的。总体来说，模型的稳健性还不错。&lt;/p&gt;&lt;p&gt;同时，由于准确率的波动从未低于过我们设定的阈值0.8，所以可以认为模型表现良好，对于解决问题有积极作用。&lt;/p&gt;&lt;p&gt;由于我们这次装配准确率很不错，所以我又再次生成了一次特征重要性的表格与图，来看看哪些因子影响更大。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-8b592b4ac3c411a021fa9653825d68a3.png" data-rawwidth="212" data-rawheight="251"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-fb9c3e879ee4dc9b9a30af5c5cb25812.png" data-rawwidth="244" data-rawheight="253"&gt;&lt;p&gt;从表格与图中可以看出，这次影响最大依然是前30天的最小值，不过第二大的是30天内的最大日波动，第三大的因素才是30天内的当日最大指数。与决策树相比，前三位影响最大的因子依然是这3个。（不过与决策树相比，交易量不再是影响第四大的因素，在adaboost中，交易量成为了影响最小的因素了。）&lt;/p&gt;&lt;p&gt;值得注意的是，无论是在决策树模型，还是在adaboost模型中，能体现前30日的特征，在对未来30日后的涨跌预测中，起到了巨大的影响。&lt;/p&gt;&lt;h2&gt;9、总结&lt;/h2&gt;&lt;p&gt;通过添加了3个模型对大盘指数的预测后的随机策略，与纯随机策略的夏普指数相比，可以发现，对大盘预测之后，夏普指数是大于纯随机策略的，也就是说，预测大盘的涨跌，对于股票交易的收益提升以及风险降低，是有积极作用的。用通俗的话就是&lt;/p&gt;&lt;p&gt;&lt;b&gt;“如果你能预测大盘涨跌后再瞎J8买，确实比纯粹的瞎J8买效果更好”&lt;/b&gt;&lt;/p&gt;&lt;p&gt;通过比较3个模型，我们发现，对于大盘的预测，SVM模型表现不够好，决策树和adaboost的表现经过调参之后，稳定大于我们设定的阈值，表现良好。&lt;/p&gt;&lt;p&gt;同时，在对达到了阈值的两个模型的参数比重分析中，我们发现了一个有趣的现象：“前30天的最小指数”，“30天内的最大日波动”与“30天内的最大日波动”这3个指数作为特征，对于模型预测的权重加起来达到了50%，我是这样理解的，前30日的股市波动，对股市未来30天的涨跌有巨大的影响。&lt;/p&gt;&lt;h2&gt;10、增加样本外数据再次验证模型&lt;/h2&gt;&lt;p&gt;不行，我一直在考虑时间序列这个问题，不写代码来验证一下睡不着。 虽说写好代码测试完都2点了，希望大家耐心看完吧。&lt;/p&gt;&lt;p&gt;于是这次我切割了样本外数据，通过预留最后100~228个数据用作样本外数据测试，再次查看模型效果。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a6dfd6ba055521089e717ac33633ed16.png" data-rawwidth="369" data-rawheight="282"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-22dbb545c6c24f0b31ab863474858ef2.png" data-rawwidth="369" data-rawheight="282"&gt;&lt;p&gt;发现经过再次测试以后，adaboost和SVM的模型稳健性很差了，上下摇摆，波动不定，决策树的稳健性依然还在，但是预测样本外数据的准确率（0.8左右），明显低于预测样本内数据的准确率（0.9）&lt;/p&gt;&lt;p&gt;然而又有了新的问题，对于稳健性很差的那两个模型，一般而言，100个数据预测准确率为0.8，再增加一个数据，就算预测错了，预测准确率也应该只是变为0.79啊，为毛从图中看，变动的时候，直接就从0.8的准确率，变到0.2了呢？&lt;/p&gt;&lt;p&gt;这到底是为什么呢？目前我对这个现象的假设是这多的一个数据因为我分割的方法，它到了训练集里面，然后对模型产生了巨大的影响，恰好改变了模型之前对上涨和下跌的预测。产生这个问题的主要原因应该是我很SB的，直接根据大于或者小于算涨跌的定义标签的方式，导致分类器在涨跌那条很细的线上纠结，所以稳健性变差了。&lt;/p&gt;&lt;p&gt;解决方案：预留一个阈值，比方预留为5%，那么30天后的收盘价，大于今天的105%才算涨，低于今天的95%才算跌。否则设定为None。&lt;/p&gt;&lt;p&gt;然后我又想起来了一个问题，这个准确率是一段时间内的统计准确率，如果我减少时间周期，不是集合到一起统计准确率，而是100天，100天这样间隔统计准确率，准确率又如何呢？于是这次我得到了这样一张图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-216c479f2a17a4f252cf58b215eab86e.png" data-rawwidth="386" data-rawheight="266"&gt;&lt;p&gt;说明模型的稳健性并不够高，而且随着时间周期，模型的准确率会突变。有趣的是，不同模型依然保持一个加起来准确率为1的规律。&lt;/p&gt;&lt;p&gt;本文只是抛砖引玉，别妄想看完本文你就能拿去直接预测大盘了。还有一堆模型和一堆数据可以进一步使用的。&lt;/p&gt;&lt;h2&gt;11、代码地址：&lt;/h2&gt;&lt;p&gt;由于数据是在优矿上提取的，纸糊又不允许我这种小透明上传附件，已经保存到桌面上处理好的csv文件没法上传上来，优矿的api只能在优矿里调用，所以我只好在那上面发布instead of上传到github了（我才不会告诉你主要是因为我不会git）。在网页里点克隆自己改着玩吧。&lt;/p&gt;&lt;p&gt;SVM模型的代码：&lt;a href="https://uqer.io/community/share/584f652f6740ec004f2bd542" data-editable="true" data-title="优矿" class=""&gt;https://uqer.io/community/share/584f652f6740ec004f2bd542&lt;/a&gt;&lt;/p&gt;&lt;p&gt;决策树模型的代码：&lt;a href="https://uqer.io/community/share/5853f6bd954fa20047b771e3" data-editable="true" data-title="优矿" class=""&gt;https://uqer.io/community/share/5853f6bd954fa20047b771e3&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Adaboost模型的代码：&lt;a href="https://uqer.io/community/share/58541c566a5e6d0051dc33f5" data-editable="true" data-title="优矿" class=""&gt;https://uqer.io/community/share/58541c566a5e6d0051dc33f5&lt;/a&gt;&lt;/p&gt;&lt;p&gt;重新比较3个模型预测大盘：&lt;a href="https://uqer.io/community/share/58557c8a954fa20050b77496" data-editable="true" data-title="优矿" class=""&gt;https://uqer.io/community/share/58557c8a954fa20050b77496&lt;/a&gt;&lt;/p&gt;&lt;p&gt;所使用的机器学习库均为sklearn的库，可以直接在谷歌上搜。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24417597&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Sat, 17 Dec 2016 15:07:02 GMT</pubDate></item><item><title>别写for循环調参……</title><link>https://zhuanlan.zhihu.com/p/23498425</link><description>刚开始学机器学习的时候，特别是学一些模型的时候，课程要求自己写一些算法（而不是去用别人写好的），有时自己写的渣算法调参有时没法直接从sklearn里引用GridSearchCV 调参，图方便，咱这些菜虫就直接写个for循环……&lt;p&gt;然后机器就跑几天都跑不完了（比如模拟无人车这种，渣电脑模拟一次路况就十来秒。）&lt;/p&gt;&lt;p&gt;那么，在写着很快的情况下，避免用for循环来调参，有哪些写起轻松又好用的方法呢？&lt;/p&gt;&lt;p&gt;首先看参数，一般在调参的时候，影响参数数量的数字有两个，一个是参数的种类有多少，一个是每个参数的范围内有多少。&lt;/p&gt;&lt;p&gt;举个例子，Qlearn这玩意，通常調3个参数，学习率（alpha），折扣因子（gamma）以及探索率（epsilon）。这就是说参数的种类有3种，然后是每个参数范围内有多少，比如探索率这个参数，通常是.0~1.0之间，切为10份来看，那就是10个，如果是切为100粉，那就是该参数范围内有100个。&lt;/p&gt;&lt;p&gt;那么，就这个3类参数，每类100份的情况下，如果写for循环的话，就是100**3=1e6 了。假设渣电脑跑一次1秒钟，得11天半才能跑完……&lt;/p&gt;&lt;p&gt;那么如何改进呢？&lt;/p&gt;&lt;p&gt;如果每个参数中的改变是平滑且只有一个极值的，那么可以考虑用二分法来对每个参数进行缩减（猜数字简单的“大了”，“小了”。），100个数据，可以逼近为2^7==128个步骤内选取，也就是把100个的复杂度缩减到7个了。&lt;/p&gt;&lt;p&gt;然后是3类，这时候如果不进一步缩减，是7^3，如何进一步缩减呢？因为我们假设每数参数中的改变是平滑且只有一个极值的，那么所有参数合起来的图像，也应该是平滑且只有一个极值的。&lt;/p&gt;&lt;p&gt;那么，就可以对每个参数，假设了其他参数的最优值的情况下，单独去选取，比方说，假设3个因子我们给的代数是a,b,c，范围均在0~1之间，步长还是.01，那么，假设3个初始值，比如.5、.5、.5，然后在b=.5，c=.5的情况下，去寻找a的最优值，然后用a的最优值替代初始值。接下来在a和c固定的情况下，去找b的最优值……以此类推。&lt;/p&gt;&lt;p&gt;这样，迭代一次，复杂度就从7^3变成了7*3，如果参数的变化比较简单通常迭代3次以内就能找到最优值，这种情况下，就从7^3=343的复杂度，缩减到了7*3*3=63的复杂度。&lt;/p&gt;&lt;p&gt;两招一起用，原本1e6,一秒算一次得11天半跑完的调参，就变成了63秒能跑出结果的一个简单的调参了。&lt;/p&gt;&lt;p&gt;不过问题又来了，眼睛尖的朋友们想必已经发现了，这招类似于贪婪算法，所以如果函数有多个局部最优解的话，很可能调参调不到全局最优解。怎么办呢？&lt;/p&gt;&lt;p&gt;如果你可以确认哪些参数有几个极值，那么这些参数找到的极值就是最优解。如果不能确认我们参数造成函数有几个极值，但是我们知道在每个局部范围内，参数对函数的影响都是简单，平滑的话，可以随机选取每个参数的初始值，多次尝试，最终选择使整个算法表现最好，且重复出现的那个参数组。&lt;/p&gt;&lt;p&gt;不过这招很多时候也不适用……比如你的参数和结果的关系属于魏尔斯特拉斯函数这类的关系的话……&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23498425&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Tue, 08 Nov 2016 04:21:16 GMT</pubDate></item><item><title>今天我们来给一个剃刀打个广告</title><link>https://zhuanlan.zhihu.com/p/22804193</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-4403c2da8633b002eebbff62a77b4777_r.gif"&gt;&lt;/p&gt;&lt;p&gt;有一把剃刀可厉害了，阿基米德在卫生间里用它剃出了简洁的浮力公式，牛顿从繁琐的稿子里用它剃出出了优美动能公式，爱因斯坦从庞杂的证明中用它剃出了曼妙的质能公式，至于高斯……高斯不长胡子，高斯不需要剃刀……&lt;/p&gt;&lt;p&gt;嘛，大家都猜到了，这叫奥卡姆剃刀。&lt;/p&gt;&lt;p&gt;嘛，这把剃刀肯定不是奥卡姆发明的，一般而言认为是上帝跟高斯商量了以后，回到创世初期，做出了这把剃刀。&lt;/p&gt;&lt;p&gt;那奥卡姆卖剃刀的时候是怎么说的呢？&lt;/p&gt;&lt;p&gt;“如无需要，勿增实体”&lt;/p&gt;&lt;p&gt;这听起来像玄学啊……嘛，剃刀嘛，天气热了，知识青年上山下乡接受劳动人民的再教育，于是咱村头的剃头匠老张头决定开个会解释一下，老张头这么说的：“天气恁个热，要那么多头发爪子嘛，来我给你都剃了，放心剃不到头皮。”&lt;/p&gt;&lt;p&gt;人民群众的智慧是伟大的，老张头解释的简单易懂（大雾），这里的头皮，就是指的有效信息，是真理，头发指的就是蒙蔽住真理的玩意。头发越多，自然风也就更难吹到头皮，熵就越多（热），然而我们只知道真理是埋在头发里面的，具体埋在那里不知道，辣么，剃掉的头发越多，真理也不就越明了？&lt;/p&gt;&lt;p&gt;嘛，想到这里，住在牛棚里的知识青年们折服在了劳动人民的智慧底下，高兴地拍起了肚皮……&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-b17f7bf19ff0a2eb52bea0eb1ef40221.jpg" data-rawwidth="225" data-rawheight="208"&gt;&lt;p&gt;剃头匠老张头微微一笑，捋了捋胡须，来来来，剃刀好处都有啥，谁说对了豆给他。小李被分到园林部门当临时工，负责给决策树裁剪枝条，先开始说了。&lt;/p&gt;&lt;p&gt;小李说，领到为啥安排我给决策树修剪枝条呢？因为啊，领导喜欢到树上摘果子吃。但是呢，咱村这树有点……畸形，虽然每棵树长得差不多，但是一般有上万根枝条，却不是每一根枝条上都有果子，所以我们得沿着树干去找果子，一般长了果子的树枝会有一些特征，我们就能沿着这些特征找到果子，找到了好多果子，领导就不会把我扔到夹边沟去了。&lt;/p&gt;&lt;p&gt;然后我发现了一个问题，如果我每次判断的时候，进果园里，看到一棵树都沿着枝条走到头，然后告诉采果子的二麻子哪样的枝条有果子，二麻子按照我说的去找，因为两棵树相差可能很大，这颗枝叶上有果子，下一棵树并不一定就有。那二麻子爬到决策树上，拿回一堆没果子的枝叶，那就不合适了。&lt;/p&gt;&lt;p&gt;我想了想，嘿，那我就“裁剪”一下决策树，让二狗子每次不需要爬到枝端去拿枝叶，直接把看起来有果子的树枝全给我抱回来，不就行了嘛。果然，这下采到的果子大幅提升，领导也开心了！&lt;/p&gt;&lt;p&gt;小李继续说，其实啊，我描述3根枝条的长度，就能大概说清楚这棵树咋样的枝条结果子了。但是我描述了5跟枝条的长度，事实上说的还是那3根的特点，那我何必说5根呢？既然最小描述长度是说3根，那我就只说3根的就是了嘛。&lt;/p&gt;&lt;p&gt;说到这里，老张头满意地拍起了肚皮，说，好，咱劳动人民就是有智慧。来来来，那这个最小描述长度，二麻子，你体会到了，说一下呗？&lt;/p&gt;&lt;p&gt;二麻子说，好，辣你要我解释最小描述长度，我就先解释一下“贝叶斯定理”吧。&lt;/p&gt;&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22805488" class="" data-editable="true" data-title="小学生都能理解的贝叶斯公式。 - 机器学习笔记 - 知乎专栏"&gt;小学生都能理解的贝叶斯公式。 - 机器学习笔记 - 知乎专栏&lt;/a&gt;（作者懒得再写一遍了自己去看，正好写道了最小描述长度）&lt;/p&gt;&lt;p&gt;老张头不乐意了。嘿，我咋看不懂，你是看不起我小学没毕业是卟？&lt;/p&gt;&lt;p&gt;照你故事里的公式这么说，岂不是任何一个描述语句都应该满足奥卡姆剃刀原则了是卟？你咋能这么绝对捏？你这是严重的左倾主义，信不信老子把你工分扣完？&lt;/p&gt;&lt;p&gt;冤枉啊大爷。所以我说奥卡姆剃刀是玄学啊……&lt;/p&gt;&lt;p&gt;那为毛是玄学呢？&lt;/p&gt;&lt;p&gt;事实上来说，对于奥卡姆剃刀原则，每个人的理解是不同的（但是大多数人很难意识到）对于（像我这种）一般群众来说，一般人对奥卡姆剃刀的理解有3种方式：&lt;/p&gt;&lt;p&gt;1、如果在某个可定义范围内若找出了最优解（最优描述），那么不应当在此范围的周围再去添加任何描述（就算这个描述是对的）。&lt;/p&gt;&lt;p&gt;2、如果我无法分辨出最优描述，那么，在备选的描述中（可容忍描述误差范围内），优先选择更简洁（信息熵最小）的描述。&lt;/p&gt;&lt;p&gt;3、如果我无法分辨出最优描述，那么优先选择更符合直觉经验的描述，而不选择人脑思考起来更累的抽象逻辑描述。&lt;/p&gt;&lt;p&gt;对于这3点的理解不同，造成了很多人在辩论奥卡姆剃刀这个问题上的区别，有的人认为是觉得正确的，有的人认为是模棱两可的，比如这个纸糊问题&lt;a href="https://www.zhihu.com/question/20159241" data-editable="true" data-title="「奥卡姆剃刀原则」是正确的吗？ - 哲学" class=""&gt;「奥卡姆剃刀原则」是正确的吗？ - 哲学&lt;/a&gt;就是，每个答主对奥卡姆剃刀的理解都不一样，看这些人的评论区的辩论真是好玩……有的人认为是玄学。由于我只是一个纸糊小透明菜鸟，并不敢和基督徒或绿绿们讨论他们的神符不符合奥卡姆剃刀原则，所以我只说这3点（删除线）。&lt;/p&gt;&lt;p&gt;现在先从第一点来说，老规矩，咱要说得小学生都能看懂：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1、&lt;/b&gt;&lt;/p&gt;&lt;p&gt;现在有一个描述，你已经得知是最优解了。例如，对于若干个数字1和数字2，组成一个只允许使用加法运算的简单等式，让你描述：&lt;/p&gt;&lt;p&gt;那么，由于只允许使用加法，最简单的，当然是只用一个加法运算符的：&lt;/p&gt;&lt;p&gt;1+1 == 2&lt;/p&gt;&lt;p&gt;然而我们知道，这个也是正确的：&lt;/p&gt;&lt;p&gt;1+1 + 1 == 2 + 1&lt;/p&gt;&lt;p&gt;但是这种情况多用了两个加法运算符，就算这个描述是对的，由于我们只需要组成一个运算符，所以按照剃刀原则，这个应该抹去，而选择第一个描述，也就是对于这个命题下，我们选择的描述为：&lt;/p&gt;&lt;p&gt;1+1 == 2&lt;/p&gt;&lt;p&gt;很容易理解吧？所以，如果已知最优解了，当然应该选择使得信息量最少的最优解，而不是去添加一堆东西。&lt;/p&gt;&lt;p&gt;从第一点来说，因为每一个信息都有一个大于等于0的概率产生杂音，产生杂音就会降低准确率（&lt;a href="https://zhuanlan.zhihu.com/p/22805488" class="" data-editable="true" data-title="小学生都能理解的贝叶斯公式。 - 机器学习笔记 - 知乎专栏"&gt;小学生都能理解的贝叶斯公式&lt;/a&gt;里证明过了），所以在第一种理解前提下，奥卡姆剃刀当然是对的……&lt;/p&gt;&lt;p&gt;然后看第2种理解方式：&lt;/p&gt;&lt;p&gt;&lt;b&gt;2、&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在&lt;a href="https://www.zhihu.com/question/20159241" data-editable="true" data-title="「奥卡姆剃刀原则」是正确的吗？ - 哲学" class=""&gt;「奥卡姆剃刀原则」是正确的吗？ - 哲学&lt;/a&gt;问题中的&lt;a href="https://www.zhihu.com/people/20e911524247b63b55decfbe6080aceb" data-hash="20e911524247b63b55decfbe6080aceb" class="member_mention" data-editable="true" data-title="采铜" data-hovercard="p$b$20e911524247b63b55decfbe6080aceb"&gt;采铜&lt;/a&gt;先生（哎，得一年看不到这哥们的更新也是有点伤感。好怀念哪个剃刚毛的答案……），他对于奥卡姆剃刀的理解，个人感觉就是第二种方式。&lt;/p&gt;&lt;p&gt;那么为什么第二种方式的情况下，奥卡姆剃刀原则就不一定正确了呢？这里我举个例子。&lt;/p&gt;&lt;p&gt;假设，现在小明要向小白证明“我是你爸爸”（咦……我咋又玩起这个梗了……）。小明可以选择两个不同的描述方法集合，第一个描述方法领包含的信息熵为50KB，准确率为50%，第二个描述方法集合描述方法包含的信息熵为50GB，准确率为99.9%&lt;/p&gt;&lt;p&gt;那么，在这种情况下，如果单纯按照奥卡姆剃刀原则，选择了描述方法1，可能小明最后的证明就会以失败告终。而如果选择描述方法2，也许小明向小白传递信息的能力有限，50GB信息传送过去损失了一大半，结果最后准确率还不如99.9%。&lt;/p&gt;&lt;p&gt;那么这个问题怎么解决呢？&lt;/p&gt;&lt;p&gt;所以要考虑小明向小白证明我是你爸爸，需要达到多少的准确率？允许传递的最大信息量是多少？有多少前提条件需要考虑？大家的知识背景是什么……&lt;/p&gt;&lt;p&gt;所以各位看官明白了吧？这个问题，实际上就是因为对于问题的描述简化了，导致下一个问题变得无法解了。想来估计出题的脑残作者也对这个问题的描述使用了并不该使用的奥卡姆剃刀吧……（哎哟，别打脸……）&lt;/p&gt;&lt;p&gt;所以关于理解2，就出现了一个问题，如果并没有办法debug 出最优解，那么，就有可能发生剃胡子刮到肉的情况，这就是现实生活中为什么奥卡姆剃刀原则不是完全适用的。&lt;/p&gt;&lt;p&gt;但是从另一方面考虑的话（&lt;b&gt;接下来才是重点上面大部分信息是我在逗逼&lt;/b&gt;），可以这样理解，对问题的信息熵为I（X），对答案的描述的信息熵为I（Y,X）。&lt;/p&gt;&lt;p&gt;刚才这个解答过程犯得一个明显的错误是，分别单独考虑I（X）和I（Y,X），分别使用奥卡姆剃刀，而不是对 I（X） + I（Y,X）来用剃刀，所以事实上并没有满足所谓的最短信息描述，讲道理不仅没满足最短信息，这一拆开，连贝叶斯公式都没满足了。&lt;/p&gt;&lt;p&gt;也就是说，在理解2中，所谓的   奥卡姆剃刀，并不是最短信息描述。&lt;/p&gt;&lt;p&gt;嘛……但是很多人对于奥卡姆剃刀的理解的确确实就不是最短信息描述啊~~~~~~~~~~~&lt;/p&gt;&lt;p&gt;所以就出现了第三种理解方式：&lt;/p&gt;&lt;p&gt;&lt;b&gt;3、玄学の剃刀&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这就是大多数反剃刀的人理解了。怎么说呢？对于很多人来说，所谓的简单和复杂，并不是基于这个描述的信息熵的，而是基于这个描述我是否能直观看得懂。&lt;/p&gt;&lt;p&gt;举个例子，正太啊不对正态分布，这两种描述方式（都是图）&lt;/p&gt;&lt;p&gt;描述1：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-c7b035a4f2524387bcdce9bd30d580ad.png" data-rawwidth="273" data-rawheight="185"&gt;&lt;/p&gt;&lt;p&gt;描述2：&lt;/p&gt;&lt;equation&gt;\frac{1}{  \sigma\sqrt{2\pi }  } e^{-\frac{(x-\mu )^2}{2\sigma ^2} }&lt;/equation&gt;&lt;p&gt;对于大多数人来说，会直观觉得，嘛，描述1很符合直觉，一下就看得懂。描述2……撒撒撒，这都是些潵。&lt;/p&gt;&lt;p&gt;然而我问了问我家电脑，以他的理解，描述1，电脑认为它的信息量是3.14kb。描述2，电脑认为它的信息量是1.30KB。显然描述2对于电脑来说，是信息熵更低，也就是更简洁的。&lt;/p&gt;&lt;p&gt;当然，这里我不是说描述1和描述2谁更正确。我要说明的是两张表示内容一样（正太分布），表达载体一样（都是图），表达方式不相同，传递的信息量在不同信息接受体（比如人）中直观感受到的，也许并不一样。&lt;/p&gt;&lt;p&gt;而既然接受体都不一样，那自然无法得出一个普世的结论，得到的结论具有主观差异（加上理解2里说了，这玩意这样思考已经不一定满足贝叶斯公式了。），玄而又玄，那到底剃刀原理有没有效呢？这特么就成玄学了。&lt;/p&gt;&lt;p&gt;剃头匠老张头高兴了。嘿，二麻子你小子可以啊，把咱的剃刀说得有板有眼的，咱人民公社要发展轻工业，行嘞，就由二麻子你，负责生产奥卡姆剃刀吧！让全国的剃头匠，都用上咱的剃刀！把全国的男女老少，头都剃的像那红太阳般锃亮锃亮的！&lt;/p&gt;&lt;p&gt;啥……张大爷，这剃刀没法生产啊……&lt;/p&gt;&lt;p&gt;有什么没法的，人有多大胆，剃刀有多大产，有困难，自己克服！&lt;/p&gt;&lt;p&gt;散会&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22804193&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Sun, 09 Oct 2016 08:13:46 GMT</pubDate></item><item><title>小学生都能理解的贝叶斯公式。</title><link>https://zhuanlan.zhihu.com/p/22805488</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-045ce061c63f900e35ab1f3c0796fb16_r.png"&gt;&lt;/p&gt;班主任：你们两个在干什么？班长小红，给我过来，叙述一下事情经过！&lt;p&gt;小红，现在我是小白，你是小明，说一下你们为什么吵架！&lt;/p&gt;&lt;p&gt;好的老师，小明好坏好坏的，他莫名其妙过来，什么前提条件都不给，上来就是一句“我是你爸爸”&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-31ff9648ccaac01b9727458563237c91.jpg" data-rawwidth="136" data-rawheight="128"&gt;你接着就是一巴掌，然后说“你麻痹不给定前提条件，给我的就是个无信息先验分布，等同于前提条件等于正无穷，所以你说我是你爸爸这个结果的符合概率为1/∞≈0”所以说你的命题“我是你爸爸”的概率为0。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-fc1c3a17f58dafd3de901adc45664feb.jpg" data-rawwidth="176" data-rawheight="176"&gt;&lt;p&gt;然而经过我的验证，目前全世界有70亿+1人，而其中一定有一人是你爸爸，我是一个人的概率为1，所以在这个假定条件下，我有理由认为，P（我是你爸爸） = P（这个世界上有一个人是你爸爸）*P（我是一个人）/P（全世界人有70亿）=1*1/70亿的概率，我是你爸爸。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-9268138fac1663f6bb24363fbd476274.jpg" data-rawwidth="136" data-rawheight="128"&gt;&lt;p&gt;然后他摸了摸被打残的脸，微微一笑说，你忽略了一件事，我也是一个人，所以在你的假设条件下，我也有理由认为P（我是你爸爸） = P（这个世界上有一个人是你爸爸）*P（我是一个人）/P（全世界人有70亿）=1*1/70亿的概率，我是你爸爸。所以我是你爸爸的概率等同于你是我爸爸。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-fdab69335781852b2b82ed2079ee671c.jpg" data-rawwidth="176" data-rawheight="176"&gt;&lt;/p&gt;&lt;p&gt;那么，假设我们俩其中有一个人是对方爸爸，现在在这个样本下，我们俩互相是对方爸爸的概率为：&lt;/p&gt;&lt;p&gt;P（我是你爸爸/基于我们俩其中有一个人是对方爸爸） = P（全世界有一个人是你爸爸，这个人是我）/（P（全世界有一个人是你爸爸，这个人是我）+P（全世界有一个人是我爸爸，这个人是你））等于1/2，所以我有50%的概率是你爸爸而你只有1/70亿的概率是我爸爸！所以我是你爸爸。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-bc2e7266313a7a03f8c3b9c50e5996f3.jpg" data-rawwidth="178" data-rawheight="192"&gt;&lt;p&gt;然后你飞起就是一巴掌：你个SB，你的50%的概率建立在已经验证了“基于我们俩其中有一个人是对方爸爸”这个假定条件下，是个后验概率，我的1/70亿的概率基于还没有验证上面哪个假定条件的前提下，属于先验概率，拿后验概率和先验概率样本都不一样来比，你说你四不四潵？？？？？&lt;/p&gt;&lt;p&gt;说到这里，小红说。这时候我实在看不下去了，一会儿我是你爸爸，一会儿全世界有一个人是你爸爸的，这么长，还让不让人吵架了。于是我就上去劝说了一下：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-576b5f0d4694b804b6cfa8441e1f381c.jpg" data-rawwidth="180" data-rawheight="144"&gt;要不这样，我们把你们想要证明的“我是你爸爸”作为结论Y。你们的目的是证明结论Y的合理性，也就是概率，那么，你们要提出一些假设X，我们才能知道你们在假设空间X以下的概率instead of 而不是1除以无穷等于0。&lt;/p&gt;&lt;p&gt;然后呢，你们俩逗逼都是在从人的范畴里找符合定义，所以我们简单认为你们是基于个体为人这个单位个体的均匀先验分布假设这个分布为C，为某一个常数，（就打算是为1吧，反正待会儿要约掉）。&lt;/p&gt;&lt;p&gt;辣么在我们不知道具体数字的时候，我们给这个概率一个标志，既然是在假设空间X中Y的概率，辣么就称之为P（Y，x）。设若你们的所有假设在同一个假设空间C中，那么C就可以约掉，现在我们就考虑X单独发生的概率为P（x），Y单独发生的概率为P（Y），辣么x和Y同时发生的概率，就等于Y和x同时在一个共同的假设空间C发生的概率。也就是说，在假设空间x中，Y发生的概率，乘以假设空间x发生的概率，就等于反过来，在假设空间Y中，X发生的概率，乘以假设空间Y。&lt;/p&gt;&lt;p&gt;即：&lt;equation&gt;P(Y,x)*{P(x)={P(x,Y)*P(Y)
}} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;这样考虑我们要得到的目标P（Y，x），就可以放到等式左边，写为：&lt;equation&gt;P(Y,x)=\frac{P(x,Y)*P(Y)
}{P(x)} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;这就是你们的最佳假设。&lt;/p&gt;&lt;p&gt;然后你们的最佳假设，我们来算算P（Y，x），由于你们认定的全空间为C，那么：&lt;/p&gt;&lt;p&gt;P（Y）=1/C&lt;/p&gt;&lt;p&gt;P（x）=1/C&lt;/p&gt;&lt;p&gt;P（x，Y）=1/X（X为所有x的数量，也就是x所在的假设空间的容量大小）&lt;/p&gt;&lt;p&gt;辣么，就可以算出，P（Y，x）=1/X&lt;/p&gt;&lt;p&gt;小明很生气，辣么，如果我们的假设条件建立在相同的假设空间下，岂不是又是概率一样咯。那我如何向小白证明我是你爸爸呢？难道我们的友好讨论，就变成了提出更多的假设吗？这岂不是和小孩子吵架一样了么？&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-b41ac046ca9c773a2e343c4003a2bd8c.jpg" data-rawwidth="259" data-rawheight="194"&gt;&lt;/p&gt;&lt;p&gt;小红瞪了小明一眼：你们说的话是100%可信的么？不是100%可信不就有噪音么？所以，你们的假设x的概率不应该是你们的假设f（x），而应该&lt;equation&gt;f(x)+\varsigma &lt;/equation&gt;,这个&lt;equation&gt;\varsigma &lt;/equation&gt;表示的就是你们假设的杂音量。一般而言，你们这些正太瞎扯淡的噪音满足正态分布。&lt;/p&gt;&lt;p&gt;所以现在我们就要讨论下一个问题了，在描述了足以确认我是你爸爸的条件下，才能最大化证明假设我是你爸爸的正确性，那么，如何找到这个最大可能性呢？&lt;/p&gt;&lt;p&gt;所以我们做个最大似然假设，hmax，假设满足hmax要提出i个在区间I里的使用x符合要求的基本假设h，那么，这个hmax的概率就可以简单地假设为：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;h_{max} = max(\prod_{i\in I}P(Y,h) )&lt;/equation&gt;也就是说使表达式最大时的&lt;/p&gt;&lt;p&gt;由于我们这里的正确假设为h，那么就可以认为大Y是有一堆小y组成的，其中y=h(x)+&lt;equation&gt;\varsigma &lt;/equation&gt;，我们可以吧&lt;equation&gt;\varsigma &lt;/equation&gt;提到一边去，得到&lt;equation&gt;\varsigma &lt;/equation&gt;=y-h(x)。那么，由于&lt;equation&gt;\varsigma &lt;/equation&gt;满足高斯分布，所以得到&lt;/p&gt;&lt;p&gt;max：&lt;equation&gt;\prod_{i\in I}P(Y,h) = \prod_{i\in I}\frac{1}{\sqrt{2\pi \sigma ^2} } e^{-1/2(y-h(x_i))^2/\sigma ^2}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;嘛，看不懂无所谓。反正要约掉的，由于我们求的是最大值而不是具体值，求得是使该公式最大的时候的参数，所以就可以把杂七杂八的都约了，得到：&lt;/p&gt;&lt;p&gt;max：&lt;equation&gt;\sum_{i\in I}^{}-({y-h(x_i)})^2 &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;换言之，也就是找到&lt;/p&gt;&lt;p&gt;min：&lt;equation&gt;\sum_{i\in I}^{}({y-h(x_i)})^2 &lt;/equation&gt;（找到这个令这个公式最小时的参数）&lt;/p&gt;&lt;p&gt;所以说，你们要证明自己是对方爸爸，就要找到令你的论据，应对与你的假设空间，得到的差值的最小时的论据，这样才能最有可能证明自己是对方爸爸！！！！&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-029b194eb3d97ac7cf33f2d4b8a80fc3.jpg" data-rawwidth="176" data-rawheight="176"&gt;&lt;p&gt;接下来小红又继续说了。然而，你们对于我是你爸爸这个结果的描述（x）越多，得到的杂音根据公式也会越大，也就是说，对于我是你爸爸这条信息的置信度也就越低。相对应的，之前你们也说了，自己提出的假定条件越少，得到我是你爸爸这个结论的概率也会越低。所以，我们要找到一个在证明“我是你爸爸”时，最优的描述。&lt;/p&gt;&lt;p&gt;既然要得到的是，最优的描述，那么我们可以理解为，已经验证的条件下（验证以后发现）这个描述是最优描述，就是验证后概率最大的描述。假设条件为x，则描述得到的概率（最大后验公式），简单写为：P（Y，x）P（x）。&lt;/p&gt;&lt;p&gt;我们的目标就是令这个概率最大对吧？MAX：P（Y，x）P（x）&lt;/p&gt;&lt;p&gt;嘛……既然我们的目标是“描述”，那么也就是关于“信息”的处理，那么就参考一下香农的信息论：（&lt;a href="https://zh.wikipedia.org/wiki/%E4%BF%A1%E6%81%AF%E8%AE%BA" class="" data-editable="true" data-title="wikipedia.org 的页面"&gt;信息论：维基百科&lt;/a&gt;）&lt;/p&gt;&lt;p&gt;嘛……我也懒得看，所以我就随便抓了一个叫做熵的东西过来，熵嘛，这样定义的，&lt;equation&gt;I=-log(p)&lt;/equation&gt;，意思就是概率为p的事情包含的信息量，log的底数取决于信息量的单位，比如比特什么的……嘛。这里管不到。&lt;/p&gt;&lt;p&gt;然后我就就看我们要max的公式嘛，P（Y，x）P（x），取个对数（底是什么随便你）比如我们这里用log，就变成了使&lt;equation&gt;log(P(Y,x))+log(P(x))&lt;/equation&gt;最大,按照哪个熵里面log有个负号，就变成了：&lt;/p&gt;&lt;p&gt;使&lt;equation&gt;-log(P(Y,x))-log(P(x))&lt;/equation&gt;最小。&lt;/p&gt;&lt;p&gt;也就是min:   I(P(Y,x))+I(P(x))&lt;/p&gt;&lt;p&gt;翻译成人话就是，使描述的信息熵，对于描述：结论Y由x的假定条件，以及x的假定条件，总信息量最短的描述，就是最优描述，简称最短信息描述。&lt;/p&gt;&lt;p&gt;以小明和小白的观点就是：要达到证明我是你爸爸最准确，就得让“描述在某条件下，我是你爸爸”的信息，加上“描述某条件的信息”，总体来说最小。&lt;/p&gt;&lt;p&gt;嗯，这个最短信息描述在玄学界还有个别名，叫做奥卡姆剃刀……&lt;/p&gt;&lt;p&gt;老师，我说完了。&lt;/p&gt;&lt;p&gt;班主任：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-50840371889c51043f00f653f76cc473.jpg" data-rawwidth="158" data-rawheight="134"&gt;&lt;p&gt;小明和小白：所以我俩就合伙揍了她一顿。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-edfb8e29414728ce21a3c561c0d8753a.jpg" data-rawwidth="225" data-rawheight="225"&gt;&lt;/p&gt;&lt;p&gt;下课。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22805488&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Sat, 08 Oct 2016 01:31:39 GMT</pubDate></item><item><title>杂谈：（菜鸟如我）用哪些轮子？</title><link>https://zhuanlan.zhihu.com/p/22697926</link><description>其实对于普通人来说，不用分析那么多模型，用各种机器学习库就可以了。再者，对于我们这些小白来说，经常写算法写的不6，一个模型写下来循环多得不得了，测一下时间和复杂度，用人家的库，10秒解决的问题，自己编写的模型，有可能俩小时还没跑出来（别问我怎么知道的……）。&lt;p&gt;那么问题来了，用什么库？&lt;/p&gt;&lt;p&gt;个人是用python，这里推荐几个常用的……估计大家都知道，不过不知道的可以试着去用一下。&lt;/p&gt;&lt;p&gt;首推numpy。原因除了点乘之类的很多东西用这玩意很方便以外，最主要的一个原因……是大量的机器学习库，你如果没安numpy……会没办法运行……（别问我怎么知道的……）&lt;/p&gt;&lt;p&gt;然后是pandas（熊猫），其实可以不用熊猫，比如我就经常用graphlab而不用熊猫。不过如果不是因为替代品的原因的话，类似的结构化数据的库你总得用一个。原因？&lt;/p&gt;&lt;p&gt;我给你举个例子……大量机器学习比赛，会给你一堆生数据，然后你得清洗数据对吧？比如三张表合为一张表，三张表长度不同，顺序是乱的，你要根据用户名来组合。现在你不用结构化数据的库，自己编循环来for和if组合。三张表，至少得三张表的长度相乘的复杂度。假设平均每张表100000行数据（长度10000），你3个for循环if一下，按照一个if语句算一个复杂度，就是10^15的复杂度，假设每个复杂度你要跑1纳秒……嗯……大概12天能跑完。我试了一下，10万行的3张表合到一起，我这渣电脑用熊猫的join函数大概2秒钟吧……当然你用别的什么结构化的库都行，你用SQL也行。&lt;/p&gt;&lt;p&gt;然后随便用个结构化的库处理完了以后，就到机器学习库了。主推sklearn。为啥？便宜啊……便宜到了免费的程度。像我们这些一张高级显卡都要纠结买不买的屌丝，用免费的库再正常不过啦……&lt;/p&gt;&lt;p&gt;不不不，这不是主要原因。其实最主要的原因是因为Sklearn好上手。sklearn的文档非常详细，而且几乎所有的方法都给了范例，稍微跑跑很容易理解，（这点熊猫就做得很不好……在熊猫里大量的方法没给范例……对新人很不友好。）而且因为sklearn是免费库，所以大量的数据分析比赛都允许使用……嗯……说不准哪个四线小城市的比赛人家都在用excel分析数据你就用sklearn去拿了个两百块的奖金呢。&lt;/p&gt;&lt;p&gt;graphlab。&amp;lt;delete&amp;gt;推荐graphlab主要是因为我在coursera上学的另一门课是用graphlab教的所以我这种菜鸡就习惯了用这个&amp;lt;/delete&amp;gt;……其实主要是因为两个原因。1、这玩意的结构化数据库有范例，查起来方便（虽然要翻墙）。2是因为……不知道为什么，这玩意很多时候跑得比sklearn快。不过这玩意你得用要申请（或者买）用户ID，所以简单来说，你如果指着参加点数据分析的比赛拿奖金糊口，那这个只能用来验证模型。具体还得自己编（不过一般验证了模型以后编起来难度已经小多了……）&lt;/p&gt;&lt;p&gt;不过！这玩意有个非常好的好处，倒不是这个库怎么有好处，而是这个GraphLab Create的安装包……直接把常用库一并打包了。对于如我这种电脑三天两头崩溃的来说，重装库简直是灾难。直接安一个GraphLab Create，连着anaconda和jupyter都给安好了，多方便……&lt;/p&gt;&lt;p&gt;最后就是推荐jupyter notebook了。你习惯用其他IDE可以不用这个。推荐这玩意主要是因为……非常方便……谁用谁知道。举个例子，我现在几乎不开计算器和atom算东西了，无论是简单的还是复杂的问题，都用jupyter notebook写。这玩意比计算器用着方便，比文本编辑器用着顺手，比ide加载得快……好处多得我跟你说，给我一瓶啤酒我能吹一晚上……&lt;/p&gt;&lt;p&gt;就这些吧。顺便福利一下GraphLab Create我留在网盘里的备份，懒得一个一个安装库的可以下这个：链接：&lt;a href="http://pan.baidu.com/s/1pLhHWkb" class="" data-editable="true" data-title="baidu.com 的页面"&gt;http://pan.baidu.com/s/1pLhHWkb&lt;/a&gt; 密码：fp0l （啥？你问为啥不发github？当然是因为很多人没法翻墙啦！才不是因为我太菜了git用得不熟练呢。）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22697926&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Thu, 29 Sep 2016 17:07:13 GMT</pubDate></item><item><title>L0，L1，L2正则化</title><link>https://zhuanlan.zhihu.com/p/22505062</link><description>之前说起正则化，我们光说了加一个惩罚项&lt;equation&gt;\sum_{j=1}^{m}{\theta _j^2} &lt;/equation&gt;，用步长&lt;equation&gt;\lambda &lt;/equation&gt;来调节,但是为什么是这样，却没说。&lt;p&gt;那这篇文章就讨论一下为毛是这样，以及常用的别的惩罚项长啥样。&lt;/p&gt;&lt;p&gt;先说&lt;/p&gt;&lt;p&gt;L0正则化：&lt;/p&gt;&lt;p&gt;这玩意长这样：&lt;equation&gt;\sum_{j=1,\theta _j\ne 0}^{m}{\theta _j^0} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;0的0次方没有意义，在这里如果按照L1和L2看显然该定位0，这里讨论就是不参与惩罚项，不参与加权。即所有非零项算作1加起来，然后用步长&lt;equation&gt;\lambda &lt;/equation&gt;调节。意思很明显，每一个对预测产生了贡献的参数，我都惩罚一次，不多不少，大家都一样。就像一个法官判决，你偷了一毛钱，他杀了一个人，法官均以“价值观不正确”为由，把你们判一样的罪……只有一点都没参与的人，才不会被判刑。&lt;/p&gt;&lt;p&gt;很明显有问题，这玩意正则化后，岂不是无论什么样的函数，无论多么复杂，都往一根横着的直线上调节么？&lt;/p&gt;&lt;p&gt;还有一个问题，干嘛叫L0呢？直接调步数不就可以了么……&lt;/p&gt;&lt;p&gt;这个问题后面会说。&lt;/p&gt;&lt;p&gt;L1正则化：&lt;/p&gt;&lt;p&gt;这玩意长这样：&lt;equation&gt;\sum_{j=1}^{m}{|\theta _j|} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;拿法官举例子，就是，法官要按照你们的罪行量刑判罪，但是都得判，无论你影响最终是好是坏（比如你杀了个人，这个人也是个坏人，但是你还是犯了杀人罪得判刑）都按照罪行判罪。于是就都取个绝对值，表示都判，然后按照罪行大小判罪了……&lt;/p&gt;&lt;p&gt;这个地方估计大家可以理解了，惩罚项嘛，按照一个参数的影响来判才对嘛……&lt;/p&gt;&lt;p&gt;不过这还有个问题，x的绝对值，你给我求导一下看看，求出来就是 土1,大于0的时候是1，小于0的时候是-1，一个还好，一个向量X里的话，有神特么多个x，求个导出来能把人累死。&lt;/p&gt;&lt;p&gt;于是就引出L2了：&lt;/p&gt;&lt;equation&gt;\sum_{j=1}^{m}{\theta _j^2} &lt;/equation&gt;&lt;p&gt;就是我们看到的，笔记里记的了。平方了以后嘛，做包括求导在内的各种计算就方便了，啥，你说大了，无所谓啊，反正有个lamda来调节步长，谁在乎呢？&lt;/p&gt;&lt;p&gt;好吧……到这里我就在不用一堆专业术语的情况下，用这种糊弄的方式，勉强把L0，L1，L2解释出来了，啥欧氏距离马氏距离剃刀原则都糊弄过去没说，反正你编程也用不到这些玩意。不过还有一个问题，作为一个喜欢开脑洞的宅，万一某一天你好奇，为毛这要叫L0，L1，L2,为毛不叫L1，L2，L3，不叫个数，拉索和雷吉呢？我还想叫他山岭回归呢……&lt;/p&gt;&lt;p&gt;这个……你可以这样理解：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;L(\theta) = \sum_{j=1}^{m}{(\sqrt{\theta _j^2} )^p} &lt;/equation&gt;当p等于0的时候，就是L0，当P等于1的时候，就是L1，当p等于2的时候，就是L2.嗯……这下就师出有名了啊哈哈哈……&lt;/p&gt;&lt;p&gt;好了不扯淡了。实际上这玩意表示的是距离，比如x点和y点之间的距离：&lt;/p&gt;&lt;equation&gt;L(x,y) = \sum_{j=1}^{m}{(\sqrt{(x_j-y_j)^2} )^p} &lt;/equation&gt;&lt;p&gt;而正则化是表示到哪儿的距离呢？到0点，也就是和完全不改变原函数的区别（图我懒得找了，随便搜图搜个L2norm都能搜出来）。而由于是到0点，所以可以只保留一个参数，那么上面哪个&lt;equation&gt;L(x,y) = \sum_{j=1}^{m}{(\sqrt{(x_j-y_j)^2} )^p} &lt;/equation&gt;就变成了&lt;equation&gt;L(\theta,0) = \sum_{j=1}^{m}{(\sqrt{(\theta _j-0)^2} )^p} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;啊，差不多就糊弄完了……&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22505062&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Mon, 19 Sep 2016 12:24:44 GMT</pubDate></item><item><title>正规方程的推导过程</title><link>https://zhuanlan.zhihu.com/p/22474562</link><description>那啥，之前笔记里这部分是略过的。这里整理一下吧。有兴趣的可以对照看看和你推倒的过程一样不。&lt;p&gt;我们先回顾一下，我们定义观测结果y和预测结果y'之间的差别为Rss:&lt;/p&gt;&lt;equation&gt;Rss = \sum_{i=1}^{n}({y_i-y_i'} )^2= \sum_{i=1}^{n}({y_i-h(x_i)} )^2 = (y-h(X))^T*(y-h(X))&lt;/equation&gt;&lt;p&gt;设若参数的矩阵为&lt;equation&gt;\theta&lt;/equation&gt;,则&lt;equation&gt;h(X)=\theta*X&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;那么&lt;equation&gt;Rss  = (y-h(X))^T*(y-h(X)) =  (y-X*\theta)^T*(y-X*\theta) &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;按照我们的定义，这个Rss的意思是y和y'之间的差，那么当Rss无限趋近于0的时候，则y≈y'，即我们求得的预测结果就等于实际结果。&lt;/p&gt;&lt;p&gt;于是，令Rss等于某一极小值&lt;equation&gt;\delta &lt;/equation&gt;，则&lt;equation&gt;(y-X*\theta)^T*(y-X*\theta) ==\delta &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;对参数&lt;equation&gt;\theta&lt;/equation&gt;求导，得：&lt;/p&gt;&lt;equation&gt;\frac{d}{d(\theta)}(y-X*\theta)^T*(y-X*\theta)== 2X^T*(y-X*\theta)==0&lt;/equation&gt;&lt;p&gt;展开，得&lt;equation&gt; 2X^T*y==2*X^T*X*\theta&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;进而就可以得到&lt;equation&gt;\theta ==(X^T*X)^{-1}*X^T*y&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;于是我们就得到正规方程了。&lt;/p&gt;&lt;p&gt;再讲一个推导方式：&lt;/p&gt;&lt;p&gt;我们可以用矩阵乘法：&lt;/p&gt;&lt;equation&gt;Y=X\theta &lt;/equation&gt;&lt;p&gt;两边同时乘以&lt;equation&gt;X^T&lt;/equation&gt;&lt;/p&gt;&lt;equation&gt;X^TY=X^TX\theta &lt;/equation&gt;&lt;p&gt;然后再乘以&lt;equation&gt;(X^TX)^{-1}&lt;/equation&gt;&lt;/p&gt;&lt;equation&gt;(X^TX)^{-1}X^TY=(X^TX)^{-1}X^TX\theta &lt;/equation&gt;&lt;p&gt;就得到&lt;equation&gt;\theta = (X^TX)^{-1}X^TY&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;……不过这第二种方法是在知道了正规方程是什么以后再推导的。虽然看起来很快，然而并没有告诉你为什么。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22474562&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Sat, 17 Sep 2016 08:44:47 GMT</pubDate></item><item><title>小结及接下来的打算：</title><link>https://zhuanlan.zhihu.com/p/22440031</link><description>目前主干内容已经补充完了。还有大量内容没补充完，今后会继续补充。（比如正规方程怎么推倒啊不对推导的）&lt;p&gt;接下来我有个想法，个人学了几套不同的机器学习课程，感觉光是课堂布置的练习还是不够的，打算自己给自己布置个简单的结业项目。目前的想法是做这两个：&lt;/p&gt;&lt;p&gt;1、做一个简单的房价预测网站。&lt;/p&gt;&lt;p&gt;2、做一个知乎的答案推荐系统。&lt;/p&gt;&lt;p&gt;先做第一个，第一个的计划是这样的，分为3部分：&lt;/p&gt;&lt;p&gt;A、房地产数据获取及数据清洗：嘛，有现成的api最好，找不到的话，就自己爬虫爬，爬来了以后做一个协同过滤系统补完缺失数据。&lt;/p&gt;&lt;p&gt;B、通过数据预测房价。第一个项目就不用神经网络了，用线性回归就行了……做完再慢慢完善嘛。而且就算是个线性回归，不用别人的库，从头开始撸代码要写好了也不是一件容易的事……&lt;/p&gt;&lt;p&gt;C、做个简单的网站通过输入数据可以输出房价。就是做个网站而已。因为光是做了学习系统，以后万一跟别人吹牛逼也没好吹的，放到网站里，以后不管是找工作还是吹牛逼都可以拿出来说了……&lt;/p&gt;&lt;p&gt;目前的想法是，根据这3部分，建立一个学习小组，如果你有兴趣加入，至少得对其中一个部分能有帮助，能编写你的代码并教会小组内其他小伙伴。不接受其他方式加入，做完以后会征求组内意见把学习过程及内容放出来大家同意放出来的部分。&lt;/p&gt;&lt;p&gt;嘛，有人来一起做边做边学最好。没人来的话，反正我这水货的编程能力，用python爬虫也能编，很丑的学习系统也能编，用flask做个小网站也不是不会。只是我自己一个人的话边做边学就会很久咯。&lt;/p&gt;&lt;p&gt;有兴趣的可以联系我。两周内开始。&lt;/p&gt;&lt;p&gt;想要加入请私信我告诉我你能在3部分中，在&lt;u&gt;哪一部分帮助到其他同学&lt;/u&gt;（例：你可以发私信的时候告诉我说你爬虫爬的很不错，会爬数据并整理到sql里云云的都可以。总之门槛很低，但是你必须得能分享知识给别人。除此之外不接受任何加入方式。）。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22440031&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Wed, 14 Sep 2016 08:03:57 GMT</pubDate></item><item><title>第十周笔记：大量数据算法</title><link>https://zhuanlan.zhihu.com/p/22168288</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/ad95d41a014fb95c79712a446925d4d0_r.jpg"&gt;&lt;/p&gt;（第九周笔记还有推荐系统没补充完，不过要补充好推荐系统还得查很多资料，所以暂时没有补充的欲望。）&lt;p&gt;首先一个问题，是否需要用大数据？&lt;/p&gt;&lt;p&gt;记得之前笔记（第六周笔记）中，我们分析过一种情况，就是当样本数量m达到一定的程度时，增加样本数量，对于交叉验证集和训练集之间得出的误差的差距，并没有太多减少的空间时，就不再需要增加样本量。&lt;/p&gt;&lt;p&gt;一句话，当分别计算J_cv和J_train在当前样本数量下的误差时，差距极大时，应当使用更多数据，差距极小时，就不应当使用大量数据了。&lt;/p&gt;&lt;p&gt;那么，大量数据时，用什么算法呢？&lt;/p&gt;&lt;p&gt;数据太多，如果还按照批量梯度下降来玩，估计一个因子都得算个三五天，整个公式出出来，改一改，一个月就过去了……&lt;/p&gt;&lt;p&gt;那就不用所有因子来玩嘛。一步一步局部下降（贪婪算法）也不错嘛！但是贪婪算法也有问题，那就是，如果你的数据凹凸不平的，有很多个局部最优，那就会发生如下情况：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/d0b5aa7bdd8bcab2ff1bcaa1c31e5d7e.png" data-rawwidth="471" data-rawheight="324"&gt;&lt;p&gt;那咋办呢？是的，你可以多重复随机选初始点迭代几次，然后在交叉集上比较，但是万一这种坑坑洼洼的地方很多，那你得重复很多次，运气不好兴许一辈子就过去了~&lt;/p&gt;&lt;p&gt;那我们就不用局部最优，还是得用全部数据，那不就慢了么，怎么办呢？那就每次就一个数据就拿去改变参数，凑合凑合得了。由于很随便，所以就叫它随便，啊不对随机梯度下降……&lt;/p&gt;&lt;p&gt;&lt;b&gt;随机梯度下降：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们先看看以前的批量梯度下降的公式：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\theta_{j}=\theta_{j}-\alpha *\frac{d}{d\theta_{j} } J_{\theta} &lt;/equation&gt;=&lt;equation&gt;\theta_{j}=\theta_{j}-\alpha /m*\sum_{i}^{m}{} (h(x_i) -y_{i}  )*x_{i,j}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;那么，现在我们改成了每次随便挑一个数据，于是公式就成了：&lt;/p&gt;&lt;equation&gt;\theta_{j}=\theta_{j}-\alpha /m*{} (h(x_i) -y_{i}  )*x_{j}&lt;/equation&gt;&lt;p&gt;也就是说，other than 把所有的数据拿来加一遍再改变参数值，我们现在变成了急不可耐地看到一个数据就改变一次参数值……&lt;/p&gt;&lt;p&gt;啥，你说这不就是贪婪算法，并不随机么……？&lt;/p&gt;&lt;p&gt;那好，那就……执行之前，把数据打乱，python的话就randshaffle一下，于是就随机了呗~~~&lt;/p&gt;&lt;p&gt;那么最后还有一个问题，我们以前的批量梯度下降，是一遍又一遍地迭代，直到收敛到某一个范围内。现在，你每个数据就下降一次，需要重复多少次呢？&lt;/p&gt;&lt;p&gt;不重复啊。说了这是给大量数据准备的，我们就假吧意思大量数据来说下降一次就够了。于是我们就总共只下降一次……只是对于其中，每个数据迭代的时候，就直接改变参数值&lt;equation&gt;\theta&lt;/equation&gt;了。所以你也看到了……由于只下降一次，数据不够多时就别用了。误差会很大的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;迷你批量梯度下降（略拗口）：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们也可以换个思路，还是用批量梯度下降，只是这时候不用所有数据，而是和刚才哪个随机梯度下降的思路结合一下。批量一度下降是所有数据一起算一次，改变一次参数值对吧？随机梯度下降时每个观测数据都拿来改变一次参数值对吧？咱中国人讲究中庸嘛。就既不拿所有的，也不光拿一个，而是一小批一小批地拿来下降，比如每次10个数据下降一次啦，每次20个数据下降一次啦。之类的。&lt;/p&gt;&lt;p&gt;那么我们怎么选呢？&lt;/p&gt;&lt;p&gt;把数据随机分成x份，每份里面有10~100个数据。以每份b个数据举例，我们要迭代的参数公式就成了这样：&lt;/p&gt;&lt;p&gt;i=1,b=10(假设b为10)&lt;/p&gt;&lt;p&gt;while i &amp;lt; m&lt;/p&gt;&lt;equation&gt;\theta_{j}=\theta_{j}-\alpha /m*\sum_{i}^{i+b-1}{} (h(x_i) -y_{i}  )*x_{i,j}&lt;/equation&gt;&lt;p&gt;i += b&lt;/p&gt;&lt;p&gt;差不多就是这样……意思就是从每个拿来改变一次参数&lt;equation&gt;\theta&lt;/equation&gt;，变成了每b个拿来改一次参数&lt;equation&gt;\theta&lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;检查收敛：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;那么现在又有一个问题来了。我们数据多了就变懒了，算一次凑合凑合过了，万一不收敛怎么办？所以还得检查收敛……&lt;/p&gt;&lt;p&gt;怎么检查收敛呢？&lt;/p&gt;&lt;p&gt;拿出我们的代价函数：&lt;equation&gt;cost=1/2*(h(x_{i} )-y_{i}  )^2&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;每迭代k次，就计算一下。最后把图画出来，看看收敛不收敛。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/e04f380c74c1942eaeca162e3d2009ff.png" data-rawwidth="720" data-rawheight="419"&gt;蓝色是1000次看一下，红色是5000次看一下。横坐标是看的次数，纵坐标是当前看的时候cost的值。&lt;/p&gt;&lt;p&gt;可以看到，第一幅图是在收敛的表现，到达最低点后开始震荡。第二幅图意思是你看的范围越大，看到的振幅越小（批量梯度下降看起来就很光滑了。相反，k=1的时候，看起来就心电图了……）第三附图表示的意思是，如果你看着震荡不确定到最小没有，增加k，就可以知道到底是应该优化算法还是只是杂音造成的震荡。第四附图当然就是发散了，这时候可以试试小一点的&lt;equation&gt;\alpha &lt;/equation&gt;值。&lt;/p&gt;&lt;p&gt;接下来又有一个问题来了。大量数据一般都不是一次就给完的，很多时候，是连续给你赛数据，算法也要与时俱进嘛。当然，这时候你就可以直接用上面说的两种算法，新数据直接分成几份，算了就马上拿去改变参数值。这里可以让客户端完成一部分简单的数据处理再交给服务器，比如服务器每天计算一次参数，客户端通过这些参数，把缺失数据补充完整，再返回给客户端。又例如如果网络恨不好，而数据量足够客户端依据数据计算出一个代价函数，返回这个代价函数而不是数据，也许能提高一定的效率。嘛……再多的就要学分布式处理了。这里就不多扯淡了、&lt;/p&gt;&lt;p&gt;第11周的笔记内容很少，大致就是讲一下如何涉及一个机器学习的解决计划，以及感谢收看云云。在这里一并写了吧。基本思路就是，把一个复杂的机器学习问题分解成n个小问题（当然你也可以让神经网络自己分解去……）例如你要让机器自己看书，你就可以这样分解问题，让机器识别出书，让机器通过书识别出段落，语句，让机器通过段落语句识别其中的文字关联，进而识别出其抽象化的意思。让机器通过抽象化的意思重新组合合成你需要的信息。这种，把一个复杂的问题拆分成n个细小的问题一个一个解决。&lt;/p&gt;&lt;p&gt;常见的，例如我们要自动分析一个地区菜价，拿上来的数据不可能100%完整，那么就可以分为两套来做，第一个系统用来补充不完整信息（天气，道路交通，政府政策等），第二个系统用第一个系统已经加工好的信息来分析出想要的数据，第三个系统根据这些数据来预测未来的菜价。&lt;/p&gt;&lt;p&gt;实际过程中，由于每次系统都有偏差，最后总准确率会不断下降。例如上面说的哪个分析菜价的，也许总准确率为80%，然后如果是给完善的信息，不考虑第1个系统的误差，准确率是90%，如果给完善的数据，直接用来预测，准确率是98%。那么就可以简单的理解为，系统1可改善上限为10%，系统2可改善上限为8%，系统3可改善上限为2%。假如时间有限，也许就应该把宝贵的时间主要用以改善系统1和系统2。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22168288&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Tue, 13 Sep 2016 08:46:36 GMT</pubDate></item><item><title>第九周笔记：密度估计</title><link>https://zhuanlan.zhihu.com/p/21898453</link><description>考虑一个产品，每个工厂生产线都有一定的概率产生次品。假设在用户退货之前，我们没法知道一样东西是否是次品。那么我们只能通过产品的各项指标估计（重量，硬度，曲率，发热量等，不同产品指标不一样）将合格的某项指标画在图上，也许会得到这样一个图（左）：&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/606054110fbd2fbdcc26ea5e5aaf8356.png" data-rawwidth="500" data-rawheight="250"&gt;把图按照浓度梯度描红，得到右边的这个区域。按照聚类的思考方式，大概就可以理解为，越接近高浓度区域，其正品概率越高。而越不太可能产生落点的区域，其次品的概率越高。&lt;/p&gt;&lt;p&gt;用数学语言描述就是：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;p(x)&amp;lt;\xi &lt;/equation&gt;时，产品判断为次品。&lt;equation&gt;\xi &lt;/equation&gt;为异常判断参数，&lt;equation&gt;p(x)&lt;/equation&gt;为当前特征的产品产生概率。&lt;/p&gt;&lt;p&gt;&lt;equation&gt;p(x)=\prod_{j=1}^{n} p(x_j,\mu _j,\sigma _j)&lt;/equation&gt;，其中&lt;equation&gt;\mu _i = \frac{1}{m} \sum_{i=1}^{m}{x_j^{(i)}} &lt;/equation&gt;，&lt;equation&gt;\sigma _j^2=\frac{1}{m} \sum_{1}^{m}{(x_j^{(i)}-\mu _j)^2}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;当然，你可以简单地把正常和异常记作y=1和y=0.&lt;/p&gt;&lt;p&gt;选&lt;equation&gt;\xi &lt;/equation&gt;的方法，就是之前（第六周笔记）里计算F值，取F值最大时的&lt;equation&gt;\xi &lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;那么问题来了，既然可以用y=1和y=0来标记出正常和异常，为什么不用监督学习，而要用密度估计呢？&lt;/p&gt;&lt;p&gt;因为大多数情况下，生产线上的异常值相对正常值来说，是极其少量的。在这种情况下，如果用监督学习，那么就没有足够的负样本用于训练集。此时，为了准确，我们就可以只把负样本用于交叉验证集与测验集，而训练集不用负样本。既然不用负样本，那么训练的时候也只要用非监督学习了……&lt;/p&gt;&lt;p&gt;还有一种可能，负样本相对与数据来说，非常奇怪，每一个负样本都不一样，或者未来可能产生的负样本具有不确定性，那么这种情况下，负样本无法建模，没法判断，只能通过和正样本的差异来判断是否为负样本。这种情况下，显然训练时，只用正样本的非监督学习是优于监督学习的……&lt;/p&gt;&lt;p&gt;反之，如果样本特征明显，可预测，未来样本可确定，且正负样本均足够多的情况下，用监督学习更优。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21898453&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Wed, 10 Aug 2016 18:46:02 GMT</pubDate></item></channel></rss>