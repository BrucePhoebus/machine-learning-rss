<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>智能单元 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/intelligentunit</link><description>面向通用人工智能和机器人学习，聚焦深度增强学习，可微神经计算机和生成对抗模型。</description><lastBuildDate>Thu, 09 Mar 2017 06:15:10 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>深度增强学习前沿算法思想</title><link>https://zhuanlan.zhihu.com/p/25302079</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-3ce09841f5ec27b18afa2f741512b78e_r.jpg"&gt;&lt;/p&gt;&lt;blockquote&gt;本文原载于《程序员》杂志2017年1月刊&lt;/blockquote&gt;&lt;p&gt;2016年AlphaGo计算机围棋系统战胜顶尖职业棋手李世石，引起了全世界的广泛关注，人工智能进一步被推到了风口浪尖。而其中的深度增强学习算法是AlphaGo的核心，也是通用人工智能的实现关键。本文将带领大家了解深度增强学习的前沿算法思想，领略人工智能的核心奥秘。&lt;/p&gt;&lt;h2&gt;前言&lt;/h2&gt;&lt;p&gt;深度增强学习（Deep Reinforcement Learning，DRL）是近两年来深度学习领域迅猛发展起来的一个分支，目的是解决计算机从感知到决策控制的问题，从而实现通用人工智能。以Google DeepMind公司为首，基于深度增强学习的算法已经在视频游戏、围棋、机器人等领域取得了突破性进展。2016年Google DeepMind推出的AlphaGo围棋系统，使用蒙特卡洛树搜索和深度学习结合的方式使计算机的围棋水平达到甚至超过了顶尖职业棋手的水平，引起了世界性的轰动。AlphaGo的核心就在于使用了深度增强学习算法，使得计算机能够通过自对弈的方式不断提升棋力。深度增强学习算法由于能够基于深度神经网络实现从感知到决策控制的端到端自学习，具有非常广阔的应用前景，它的发展也将进一步推动人工智能的革命。&lt;/p&gt;&lt;h2&gt;深度增强学习与通用人工智能&lt;/h2&gt;&lt;p&gt;当前深度学习已经在计算机视觉、语音识别、自然语言理解等领域取得了突破，相关技术也已经逐渐成熟并落地进入到我们的生活当中。然而，这些领域研究的问题都只是为了让计算机能够感知和理解这个世界。以此同时，决策控制才是人工智能领域要解决的核心问题。计算机视觉等感知问题要求输入感知信息到计算机，计算机能够理解，而决策控制问题则要求计算机能够根据感知信息进行判断思考，输出正确的行为。要使计算机能够很好地决策控制，要求计算机具备一定的“思考”能力，使计算机能够通过学习来掌握解决各种问题的能力，而这正是通用人工智能（Artificial General Intelligence，AGI）（即强人工智能）的研究目标。通用人工智能是要创造出一种无需人工编程自己学会解决各种问题的智能体，最终目标是实现类人级别甚至超人级别的智能。&lt;/p&gt;&lt;p&gt;通用人工智能的基本框架即是增强学习（Reinforcement Learning，RL）的框架，如图1所示。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-a770339f0fb4817e4de1cac83c53fb28.jpg" data-rawwidth="317" data-rawheight="202"&gt;&lt;p&gt;                                                  图1 通用人工智能基本框架&lt;/p&gt;&lt;p&gt;智能体的行为都可以归结为与世界的交互。智能体观察这个世界，然后根据观察及自身的状态输出动作，这个世界会因此而发生改变，从而形成回馈返回给智能体。所以核心问题就是如何构建出这样一个能够与世界交互的智能体。深度增强学习将深度学习（Deep Learning）和增强学习（Reinforcement Learning）结合起来，深度学习用来提供学习的机制，而增强学习为深度学习提供学习的目标。这使得深度增强学习具备构建出复杂智能体的潜力，也因此，AlphaGo的第一作者David Silver认为深度增强学习等价于通用人工智能DRL=DL+RL=Universal AI。&lt;/p&gt;&lt;h2&gt;深度增强学习的Actor-Critic框架&lt;/h2&gt;&lt;p&gt;目前深度增强学习的算法都可以包含在Actor-Critic框架下，如图2所示。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-b8063b6f418ac88c71fc4118bb965002.jpg" data-rawwidth="503" data-rawheight="275"&gt;&lt;p&gt;                                                         图2 Actor-Critic框架&lt;/p&gt;&lt;p&gt;把深度增强学习的算法认为是智能体的大脑，那么这个大脑包含了两个部分：Actor行动模块和Critic评判模块。其中Actor行动模块是大脑的执行机构，输入外部的状态s，然后输出动作a。而Critic评判模块则可认为是大脑的价值观，根据历史信息及回馈r进行自我调整，然后影响整个Actor行动模块。这种Actor-Critic的方法非常类似于人类自身的行为方式。我们人类也是在自身价值观和本能的指导下进行行为，并且价值观受经验的影响不断改变。在Actor-Critic框架下，Google DeepMind相继提出了DQN，A3C和UNREAL等深度增强学习算法，其中UNREAL是目前最好的深度增强学习算法。下面我们将介绍这三个算法的基本思想。&lt;/p&gt;&lt;h2&gt;DQN（Deep Q Network）算法&lt;/h2&gt;&lt;p&gt;DQN是Google DeepMind于2013年提出的第一个深度增强学习算法，并在2015年进一步完善，发表在2015年的《Nature》上。DeepMind将DQN应用在计算机玩Atari游戏上，不同于以往的做法，仅使用视频信息作为输入，和人类玩游戏一样。在这种情况下，基于DQN的程序在多种Atari游戏上取得了超越人类水平的成绩。这是深度增强学习概念的第一次提出，并由此开始快速发展。&lt;/p&gt;&lt;p&gt;DQN算法面向相对简单的离散输出，即输出的动作仅有少数有限的个数。在这种情况下，DQN算法在Actor-Critic框架下仅使用Critic评判模块，而没有使用Actor行动模块，因为使用Critic评判模块即可以选择并执行最优的动作，如图3所示。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-76645f7b80f658ead8feba17e3236611.jpg" data-rawwidth="496" data-rawheight="230"&gt;&lt;p&gt;                                                            图3 DQN基本结构&lt;/p&gt;&lt;p&gt;在DQN中，用一个价值网络（Value Network）来表示Critic评判模块，价值网络输出Q(s,a)，即状态s和动作a下的价值。基于价值网络，我们可以遍历某个状态s下各种动作的价值，然后选择价值最大的一个动作输出。所以，主要问题是如何通过深度学习的随机梯度下降方法来更新价值网络。为了使用梯度下降方法，我们必须为价值网络构造一个损失函数。由于价值网络输出的是Q值，因此如果能够构造出一个目标Q值，就能够通过平方差MSE的方式来得到损失函数。但对于价值网络来说，输入的信息仅有状态s，动作a及回馈r。因此，如何计算出目标Q值是DQN算法的关键，而这正是增强学习能够解决的问题。基于增强学习的Bellman公式，我们能够基于输入信息特别是回馈r构造出目标Q值，从而得到损失函数，对价值网络进行更新。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-56dfb96ae6ff9cdac50833288075a57c.jpg" data-rawwidth="619" data-rawheight="373"&gt;&lt;p&gt;                                                    图4 UNREAL算法框图&lt;/p&gt;&lt;p&gt;在实际使用中，价值网络可以根据具体的问题构造不同的网络形式。比如Atari有些输入的是图像信息，就可以构造一个卷积神经网络（Convolutional Neural Network，CNN）来作为价值网络。为了增加对历史信息的记忆，还可以在CNN之后加上LSTM长短记忆模型。在DQN训练的时候，先采集历史的输入输出信息作为样本放在经验池（Replay Memory）里面，然后通过随机采样的方式采样多个样本进行minibatch的随机梯度下降训练。&lt;/p&gt;&lt;p&gt;DQN算法作为第一个深度增强学习算法，仅使用价值网络，训练效率较低，需要大量的时间训练，并且只能面向低维的离散控制问题，通用性有限。但由于DQN算法第一次成功结合了深度学习和增强学习，解决了高维数据输入问题，并且在Atari游戏上取得突破，具有开创性的意义。&lt;/p&gt;&lt;h2&gt;A3C（Asynchronous Advantage Actor Critic）算法&lt;/h2&gt;&lt;p&gt;A3C算法是2015年DeepMind提出的相比DQN更好更通用的一个深度增强学习算法。A3C算法完全使用了Actor-Critic框架，并且引入了异步训练的思想，在提升性能的同时也大大加快了训练速度。A3C算法的基本思想，即Actor-Critic的基本思想，是对输出的动作进行好坏评估，如果动作被认为是好的，那么就调整行动网络（Actor Network）使该动作出现的可能性增加。反之如果动作被认为是坏的，则使该动作出现的可能性减少。通过反复的训练，不断调整行动网络找到最优的动作。AlphaGo的自我学习也是基于这样的思想。&lt;/p&gt;&lt;p&gt;基于Actor-Critic的基本思想，Critic评判模块的价值网络（Value Network）可以采用DQN的方法进行更新，那么如何构造行动网络的损失函数，实现对网络的训练是算法的关键。一般行动网络的输出有两种方式：一种是概率的方式，即输出某一个动作的概率；另一种是确定性的方式，即输出具体的某一个动作。A3C采用的是概率输出的方式。因此，我们从Critic评判模块，即价值网络中得到对动作的好坏评价，然后用输出动作的对数似然值（Log Likelihood）乘以动作的评价，作为行动网络的损失函数。行动网络的目标是最大化这个损失函数，即如果动作评价为正，就增加其概率，反之减少，符合Actor-Critic的基本思想。有了行动网络的损失函数，也就可以通过随机梯度下降的方式进行参数的更新。&lt;/p&gt;&lt;p&gt;为了使算法取得更好的效果，如何准确地评价动作的好坏也是算法的关键。A3C在动作价值Q的基础上，使用优势A（Advantage）作为动作的评价。优势A是指动作a在状态s下相对其他动作的优势。假设状态s的价值是V，那么A=Q-V。这里的动作价值Q是指状态s下a的价值，与V的含义不同。直观上看，采用优势A来评估动作更为准确。举个例子来说，假设在状态s下，动作1的Q值是3，动作2的Q值是1，状态s的价值V是2。如果使用Q作为动作的评价，那么动作1和2的出现概率都会增加，但是实际上我们知道唯一要增加出现概率的是动作1。这时如果采用优势A，我们可以计算出动作1的优势是1，动作2的优势是-1。基于优势A来更新网络，动作1的出现概率增加，动作2的出现概率减少，更符合我们的目标。因此，A3C算法调整了Critic评判模块的价值网络，让其输出V值，然后使用多步的历史信息来计算动作的Q值，从而得到优势A，进而计算出损失函数，对行动网络进行更新。&lt;/p&gt;&lt;p&gt;A3C算法为了提升训练速度还采用异步训练的思想，即同时启动多个训练环境，同时进行采样，并直接使用采集的样本进行训练。相比DQN算法，A3C算法不需要使用经验池来存储历史样本，节约了存储空间，并且采用异步训练，大大加倍了数据的采样速度，也因此提升了训练速度。与此同时，采用多个不同训练环境采集样本，样本的分布更加均匀，更有利于神经网络的训练。&lt;/p&gt;&lt;p&gt;A3C算法在以上多个环节上做出了改进，使得其在Atari游戏上的平均成绩是DQN算法的4倍，取得了巨大的提升，并且训练速度也成倍的增加。因此，A3C算法取代了DQN成为了更好的深度增强学习算法。&lt;/p&gt;&lt;h2&gt;UNREAL（UNsupervised REinforcement and Auxiliary Learning）算法&lt;/h2&gt;&lt;p&gt;UNREAL算法是2016年11月DeepMind提出的最新深度增强学习算法，在A3C算法的基础上对性能和速度进行进一步提升，在Atari游戏上取得了人类水平8.8倍的成绩，并且在第一视角的3D迷宫环境Labyrinth上也达到了87%的人类水平，成为当前最好的深度增强学习算法。&lt;/p&gt;&lt;p&gt;A3C算法充分使用了Actor-Critic框架，是一套完善的算法，因此，我们很难通过改变算法框架的方式来对算法做出改进。UNREAL算法在A3C算法的基础上，另辟蹊径，通过在训练A3C的同时，训练多个辅助任务来改进算法。UNREAL算法的基本思想来源于我们人类的学习方式。人要完成一个任务，往往通过完成其他多种辅助任务来实现。比如说我们要收集邮票，可以自己去买，也可以让朋友帮忙获取，或者和其他人交换的方式得到。UNREAL算法通过设置多个辅助任务，同时训练同一个A3C网络，从而加快学习的速度，并进一步提升性能。&lt;/p&gt;&lt;p&gt;在UNREAL算法中，包含了两类辅助任务：第一种是控制任务，包括像素控制和隐藏层激活控制。像素控制是指控制输入图像的变化，使得图像的变化最大。因为图像变化大往往说明智能体在执行重要的环节，通过控制图像的变化能够改善动作的选择。隐藏层激活控制则是控制隐藏层神经元的激活数量，目的是使其激活量越多越好。这类似于人类大脑细胞的开发，神经元使用得越多，可能越聪明，也因此能够做出更好的选择。另一种辅助任务是回馈预测任务。因为在很多场景下，回馈r并不是每时每刻都能获取的（比如在Labyrinth中吃到苹果才能得1分），所以让神经网络能够预测回馈值会使其具有更好的表达能力。在UNREAL算法中，使用历史连续多帧的图像输入来预测下一步的回馈值作为训练目标。除了以上两种回馈预测任务外，UNREAL算法还使用历史信息额外增加了价值迭代任务，即DQN的更新方法，进一步提升算法的训练速度。&lt;/p&gt;&lt;p&gt;UNREAL算法本质上是通过训练多个面向同一个最终目标的任务来提升行动网络的表达能力和水平，符合人类的学习方式。值得注意的是，UNREAL虽然增加了训练任务，但并没有通过其他途径获取别的样本，是在保持原有样本数据不变的情况下对算法进行提升，这使得UNREAL算法被认为是一种无监督学习的方法。基于UNREAL算法的思想，可以根据不同任务的特点针对性地设计辅助任务，来改进算法。&lt;/p&gt;&lt;h2&gt;小结&lt;/h2&gt;&lt;p&gt;深度增强学习经过近两年的发展，在算法层面上取得了越来越好的效果。从DQN，A3C到UNREAL，精妙的算法设计无不闪耀着人类智慧的光芒。在未来，除了算法本身的改进，深度增强学习作为能够解决从感知到决策控制的通用型学习算法，将能够在现实生活中的各种领域得到广泛的应用。AlphaGo的成功只是通用人工智能爆发的前夜。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/25302079&amp;pixel&amp;useReferer"/&gt;</description><author>Flood Sung</author><pubDate>Sat, 18 Feb 2017 13:10:26 GMT</pubDate></item><item><title>吴恩达对于增强学习的形象论述（下）</title><link>https://zhuanlan.zhihu.com/p/24996278</link><description>&lt;b&gt;版权声明：本文&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" class="" data-editable="true" data-title="智能单元"&gt;智能单元&lt;/a&gt;首发，本人原创，禁止未授权转载。&lt;/b&gt;&lt;p&gt;&lt;b&gt;前言：&lt;/b&gt;吴恩达在2003年为完成博士学位要求做了专题论文：&lt;a href="https://link.zhihu.com/?target=http%3A//rll.berkeley.edu/deeprlcourse/docs/ng-thesis.pdf" class="" data-editable="true" data-title="Shaping and policy search in Reinforcement learning"&gt;Shaping and policy search in Reinforcement learning&lt;/a&gt;，其第一、二章被&lt;a href="https://zhuanlan.zhihu.com/p/24721292?refer=intelligentunit" class="" data-editable="true" data-title="伯克利CS294：深度增强学习课程"&gt;伯克利CS294：深度增强学习课程&lt;/a&gt;作为推荐材料。本文基于笔者的理解，对第二章做有选择的编译与注释。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;第二章 增强学习和马尔科夫决策过程&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在本章中，我们将描述增强学习问题，并从形式上介绍马尔科夫决策（MDP）和部分可观测的马尔科夫决策过程（POMDP）。同时，我们会介绍本专题论文其他部分将会用到的各种符号。我们还会描述增强学习中的一些关键思想和算法，并展示在这个框架中进行决策和控制会有多少重要问题。&lt;/p&gt;&lt;p&gt;如果读者还需要更加细节地学习增强学习和马尔科夫决策过程，可以阅读参考文献[99，17，85]或者Kaelbling的调查报告[48]。&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：本文就不一一列出参考材料了，有需求的知友可到原文中按图索骥。坦率地说，&lt;b&gt;没有必要过于深究&lt;/b&gt;，即便是本文看完后有一些不太理解的，也不必深究，带着疑问继续CS294的学习就是了。当然也欢迎评论留言，大家讨论。&lt;/blockquote&gt;&lt;h2&gt;2.1 马尔科夫决策过程&lt;/h2&gt;&lt;p&gt;在面对不确定性时，马尔科夫决策过程为我们提供了一种对规划和行动进行推理的形式。要定义马尔科夫决策过程有很多种方法，其中很多的定义方式实际上是等效的，只不过是问题的形式进行了一些小小的变换。其中一种比较常用的定义是某个马尔科夫决策过程M是一个元组，符号为&lt;equation&gt;(S,D,A,\{P_{sa}(\cdot) \},\gamma,R)&lt;/equation&gt;，其中各个符号的含义为：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;equation&gt;S&lt;/equation&gt;：所有可能&lt;b&gt;状态（state）&lt;/b&gt;的集合。&lt;/li&gt;&lt;li&gt;&lt;equation&gt;D&lt;/equation&gt;：一个&lt;b&gt;初始的状态分布（initial state distribution）&lt;/b&gt;（即&lt;equation&gt;S&lt;/equation&gt;的概率分布）。&lt;/li&gt;&lt;li&gt;&lt;equation&gt;A&lt;/equation&gt;：在每个时间点我们都会选择动作，所有可能的动作的集合就是&lt;equation&gt;A&lt;/equation&gt;，需要注意的是集合的元素数量要求（&lt;equation&gt;|A|&amp;gt;=2&lt;/equation&gt;）。&lt;/li&gt;&lt;li&gt;&lt;equation&gt;P_{sa}(\cdot)&lt;/equation&gt;：&lt;b&gt;状态转换分布（state transition distribution）&lt;/b&gt;。对于每个属于集合&lt;equation&gt;S&lt;/equation&gt;的状态&lt;equation&gt;s&lt;/equation&gt;和每个属于集合&lt;equation&gt;A&lt;/equation&gt;的动作&lt;equation&gt;a&lt;/equation&gt;，如果我们在状态&lt;equation&gt;s&lt;/equation&gt;中采取了动作&lt;equation&gt;s&lt;/equation&gt;，那么我们就会转换到一个新的状态中，而&lt;b&gt;状态转换分布&lt;/b&gt;就给出了我们会随机转换到哪个状态的概率分布。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：如果不是很明白&lt;b&gt;状态转换分布&lt;/b&gt;，请结合后文&lt;b&gt;图2.1的机器人走方格的例子&lt;/b&gt;进行理解。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;equation&gt;\gamma&lt;/equation&gt;：一个在[0，1]之间的实数，被称为&lt;b&gt;折扣因子&lt;/b&gt;。&lt;/li&gt;&lt;li&gt;&lt;equation&gt;R:S\to \mathbb{R}&lt;/equation&gt;：&lt;b&gt;回馈函数（reward function）&lt;/b&gt;。回馈函数被&lt;equation&gt;R_{max}&lt;/equation&gt;约束（&lt;equation&gt;|R(s)&amp;lt;=R_{max}|&lt;/equation&gt;）。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：&lt;b&gt;回馈函数是重要的核心概念&lt;/b&gt;，后文将有更多讨论。经过和&lt;a href="https://www.zhihu.com/people/23deec836a24f295500a6d740011359c" data-hash="23deec836a24f295500a6d740011359c" class="member_mention" data-editable="true" data-title="@Flood Sung" data-hovercard="p$b$23deec836a24f295500a6d740011359c"&gt;@Flood Sung&lt;/a&gt;的讨论，在我们的创作中将统一把&lt;b&gt;Reward Function&lt;/b&gt;称为&lt;b&gt;回馈函数&lt;/b&gt;，将&lt;b&gt;Return Function&lt;/b&gt;称为&lt;b&gt;回报函数&lt;/b&gt;，请知友注意。之所以这么翻译，是因为reward function给出的是某种规则，行动后马上得到的环境对于你采取的行动的&lt;b&gt;反馈&lt;/b&gt;。而return function则是长期来看在某种策略下的你得到的&lt;b&gt;最终回报&lt;/b&gt;。&lt;/blockquote&gt;在马尔科夫决策过程中，事件是像这样一步步推进的：根据分布&lt;equation&gt;D&lt;/equation&gt;，我们得到了初始状态&lt;equation&gt;s_{0}&lt;/equation&gt;，那么就从&lt;equation&gt;s_{0}&lt;/equation&gt;开始。在每个时间点&lt;equation&gt;t&lt;/equation&gt;，我们都必须选择一个动作&lt;equation&gt;a_{t}&lt;/equation&gt;，由于我们采取了动作，所以根据状态转换分布状态&lt;equation&gt;P_{s_{t}a_{t}}(\cdot)&lt;/equation&gt;，状态就转换到了&lt;equation&gt;s_{t+1}&lt;/equation&gt;。通过不断地选择动作，我们就逐渐穿过了状态组成的的序列&lt;equation&gt;s_0,s_1,...&lt;/equation&gt;，而我们最终的结果是对这整个状态序列的折扣回馈求和：&lt;equation&gt;R(s_0)+\gamma R(s_1)+ \gamma^{2}R(s_2)+... \qquad \qquad (2.1)&lt;/equation&gt;&lt;p&gt;这里的折扣因子&lt;equation&gt;\gamma&lt;/equation&gt;是小于1的，这就使得序列中较远的回馈得到的权重更小。在经济学中，&lt;equation&gt;\gamma&lt;/equation&gt;有一个天然的解释，这就是无风险折现率（risk-free interest rate），意思就是说能马上得到的钱（回馈）比未来得到钱更有价值。如果我们令&lt;equation&gt;\gamma&lt;/equation&gt;的值接近1，那么就可以看做是近似了一个非折扣问题。&lt;/p&gt;&lt;p&gt;回馈可以是状态的确定函数，也可以是随机函数，而且通常是被写成略有差异的形式，比如&lt;equation&gt;R(s_t,a_t)&lt;/equation&gt;是基于当前状态和动作，或&lt;equation&gt;R(s_t,a_t,s_{t+1})&lt;/equation&gt;是基于当前状态、动作和下一个状态；我们会偶尔使用这些回馈函数的形式。在不需要考虑初始状态分布的时候，我们可以将马尔科夫决策写作&lt;equation&gt;(S,A,\{P_{sa}(\cdot) \},\gamma,R)&lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;在增强学习中，我们的目标是找到一种方法，使得我们能够随着时间不断选择动作&lt;equation&gt;a_0,a_1,...&lt;/equation&gt;，并能够最大化公式（2.1）中的回馈的期望值。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-cf8066ba7b0e9ca0a7ada931e2ae1a3d.png" data-rawwidth="1166" data-rawheight="478"&gt;&lt;i&gt;图 2.1：5x5的网格世界，机器人从初始位置S出发自己走到目标位置G。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;图2.1展示了一个可以被建模为MDP的比较标准的简单问题。想象一个机器人居住在这个5x5的网格世界中。25个网格就组成了它的状态集合&lt;equation&gt;S&lt;/equation&gt;。而它的动作集合&lt;equation&gt;A&lt;/equation&gt;则由4个动作组成，这4个就是机器人向着自己相邻的4个方向移动。但是由于轮子可能会打滑，机器人并不总是能够想要去哪个方向就成功地去了哪个方向，会有一定概率地随机地走到并不是自己想要去的网格（状态）中，这种情况就可以编码为状态转换概率（state transition probability）。（举例来说，&lt;equation&gt;P_{(3,3),north}((3,4))=0.85&lt;/equation&gt;，而&lt;equation&gt;P_{(3,3),north}((3,2))= P_{(3,3),north}((2,3))= P_{(3,3),north}((4,3))=0.05&lt;/equation&gt;。）&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：意思就是机器人在（3，3）点，采取行动“向北走一步”后，到（3，4）点的概率是0.85，而在“向北走一步”行动后，机器人还有一定小概率出现在其他点，原因可能是轮子打滑或者其他什么故障，机器人出现在（3，2），（2，3）和（4，3）点的概率均为0.05。&lt;/blockquote&gt;&lt;p&gt;机器人是从左下角的状态开始的，所以&lt;equation&gt;D&lt;/equation&gt;分配给（1，1）状态的概率为1，而当机器人到达目标点G时，它会得到一个正整数1作为回馈（即&lt;equation&gt;R((5,5))=1&lt;/equation&gt;）。如果我们使用的折扣因子&lt;equation&gt;\gamma &amp;lt;1&lt;/equation&gt;，那么我们实际上是更倾向于更快地到达目标点并获得回馈。&lt;/p&gt;&lt;p&gt;在马尔科夫决策过程中，我们的机器人在每一步都会观察当前的状态&lt;equation&gt;s_t&lt;/equation&gt;，然后选择下一个动作&lt;equation&gt;a_t&lt;/equation&gt;，它选择动作的方式可以看做是基于之前和当前状态&lt;equation&gt;s_0,...,s_t&lt;/equation&gt;的一个函数。但是由于MDP的&lt;b&gt;马尔科夫性质&lt;/b&gt;（不那么正式地来说，就是给出当前状态后，未来在某种条件下是独立于之前的状态的），要获得最优的回馈之和的期望值，它的行为选择是一个基于当前状态&lt;equation&gt;s_t&lt;/equation&gt;的函数就足够了[19，99]。这就是说，增强学习可以看做是在寻找一个好的&lt;b&gt;策略&lt;equation&gt;\pi :S\to A&lt;/equation&gt;，&lt;/b&gt;这样如果在每个状态&lt;equation&gt;s&lt;/equation&gt;中我们采取动作&lt;equation&gt;\pi (s)&lt;/equation&gt;，那么将会得到一个很大的回馈的期望和：&lt;/p&gt;&lt;equation&gt;E_{\pi}[R(s_0)+\gamma R(s_1)+\gamma^2 R(s_2)+...]\qquad \qquad (2.2)&lt;/equation&gt;&lt;p&gt;（此处使用“&lt;equation&gt;E_{\pi}&lt;/equation&gt;”，首先表示所有的动作都是根据策略&lt;equation&gt;\pi&lt;/equation&gt;选择的，其次表示这是一个根据动作得到的回报的期望）。&lt;/p&gt;&lt;p&gt;反馈函数&lt;equation&gt;R&lt;/equation&gt;实际上是“任务描述”，指明了我们想要最优化的目标。有时候，我们在选择回馈函数&lt;equation&gt;R&lt;/equation&gt;这个问题上有很大的自由。比如在上面的网格世界的例子中，如果不采用在机器人到达目的地时给它+1分的奖励的方式，也可以采取每走一步给-1分的惩罚的方式，知道机器人到达目的地。还可以每当机器人向着目的地前进的时候给它0.1分的奖励的方式。选择某些回馈函数可以使得机器人的学习速度呈数量级地加快；而其他选择很可能导致机器人只能找到次优的解决方案。回馈函数的选择对于算法性能有巨大影响，在第3章中，我们将探索在回馈函数选择上的自由度，试着理解如何选择&lt;equation&gt;R&lt;/equation&gt;来加速学习过程，但是需要注意的是，这样做并不能保证学习到的解决方法的质量。&lt;/p&gt;&lt;p&gt;有时候，&lt;equation&gt;\gamma =1&lt;/equation&gt;的非折扣形式也很有意思。在这种情况下，我们需要确认公式（2.1）中的回馈的期望仍然是被良好地定义的。当&lt;equation&gt;S&lt;/equation&gt;是有限的时候，一个能够保证这一点的假定是：假设存在一个不同的“吸收状态”&lt;equation&gt;s_{send}&lt;/equation&gt;，这样MDP一旦进入到这个状态，就会“停止”。进一步的假设转换概率是这样的：无论机器人选择什么动作，进入到&lt;equation&gt;s_{send}&lt;/equation&gt;的概率均为1。（有时候，“&lt;equation&gt;s_0&lt;/equation&gt;”被用来表示吸收状态，而不是初始状态）。如果这些假定都成立，我们称转换概率&lt;equation&gt;P_{sa}(\cdot)&lt;/equation&gt;是&lt;b&gt;专有&lt;/b&gt;（&lt;b&gt;proper）&lt;/b&gt;的。（&lt;b&gt;译者注&lt;/b&gt;：此处proper翻译请求批评。）&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：对于“吸收状态”，可以参考《Reinforcement Learning: An Introduction》一书的第55页的例图，如下截图所示：&lt;/blockquote&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-981efcd757166808f480a69e23828184.png" data-rawwidth="1268" data-rawheight="192"&gt;&lt;p&gt;最后，在我们需要考虑的应用场景中，可测量性并不是什么大问题，不需要过于担心，我们只需要假设任何需要被测量的东西都是可测量的就行了，并且在表达我们的思想的时候忽略掉可测量性这个问题。如果读者对于在马尔科夫链或马尔科夫决策过程中可测量性的细节处理感兴趣，可以查阅资料[31，45]。&lt;/p&gt;&lt;h2&gt;2.2 一些MDP性质和算法&lt;/h2&gt;&lt;p&gt;我们现在复习一些MDP的标准定义和结论，其中一些在后续章节中将会用到。本章中介绍的内容对于状态和动作空间大小有限的有折扣的MDP是适用的，如果进行一些小小的修改，能够同样适用于无限的MDP，或者适用于无折扣的专有（&lt;b&gt;proper）的&lt;/b&gt;MDP。如果想要了解本章中陈述结论的证明过程，可以参考材料[85，19，98，15，16，14]。&lt;/p&gt;&lt;p&gt;在本章中，我们使用“&lt;equation&gt;R(s,a)&lt;/equation&gt;”来表示回馈。给出策略&lt;equation&gt;\pi&lt;/equation&gt;，定义它的价值函数（&lt;b&gt;value function&lt;/b&gt;）&lt;equation&gt;V^{\pi}:S \to \mathbb{R}&lt;/equation&gt;是回报（Return）的期望（即有折扣的回馈的总和），这个价值函数是从状态&lt;equation&gt;s&lt;/equation&gt;开始，根据&lt;equation&gt;\pi&lt;/equation&gt;来采取行动的：&lt;/p&gt;&lt;equation&gt;V^{\pi}(s)=E_{\pi}[R(s_0,a_0)+\gamma R(s_1,a_1)+\gamma^2 R(s_2,a_2)+...|s_0=s]\qquad \qquad (2.3)&lt;/equation&gt;&lt;p&gt;和价值函数非常相近的就是Q函数（&lt;b&gt;Q-function&lt;/b&gt;），给出策略&lt;equation&gt;\pi&lt;/equation&gt;，它的Q函数是&lt;equation&gt;Q^{\pi}:S \times A \to \mathbb{R}&lt;/equation&gt;，Q函数对于从一个给定的状态开始，首先采取一个指定的动作，然后根据策略&lt;equation&gt;\pi&lt;/equation&gt;采取后续动作的情况给出回报的期望：&lt;/p&gt;&lt;equation&gt;Q^{\pi}(s,a)=E_{\pi}[R(s_0,a_0)+\gamma R(s_1,a_1)+\gamma^2 R(s_2,a_2)+...|s_0=s,a_0=a,\forall t&amp;gt;0\ a_t=\pi (s_t)]\qquad \qquad (2.4)&lt;/equation&gt;&lt;p&gt;我们同样定义了&lt;b&gt;最优价值函数&lt;/b&gt;（&lt;b&gt;optimal value function&lt;/b&gt;）&lt;equation&gt;V^*:S \to \mathbb{R}&lt;/equation&gt;，最优价值函数是从状态&lt;equation&gt;s&lt;/equation&gt;开始的最优的回馈期望总和：&lt;/p&gt;&lt;equation&gt;V^*(s)=max_{\pi}V^{\pi}(s)\qquad \qquad (2.5)&lt;/equation&gt;&lt;p&gt;类似的，&lt;b&gt;最优Q函数&lt;/b&gt;（&lt;b&gt;optimal Q-function&lt;/b&gt;）被定义为：&lt;/p&gt;&lt;equation&gt;Q^*(s,a)=max_{\pi}Q^{\pi}(s,a)\qquad \qquad (2.6)&lt;/equation&gt;&lt;p&gt;当我们需要表达多个MDP的时候，就是用下标&lt;equation&gt;M&lt;/equation&gt;来指明某个具体的MDP&lt;equation&gt;M&lt;/equation&gt;，比如&lt;equation&gt;V^*_M(s),\pi_M&lt;/equation&gt;等。&lt;/p&gt;&lt;p&gt;&lt;equation&gt;V^*&lt;/equation&gt;和&lt;equation&gt;V^{\pi}&lt;/equation&gt;同样满足下面两个方程：&lt;/p&gt;&lt;equation&gt;V^*(s)=max_aR(s,a)+\gamma \sum_{s'\in S}P_{sa}(s')V^*(s') \qquad\qquad (2.7)&lt;/equation&gt;&lt;equation&gt;V^{\pi}(s)=R(s,\pi (s))+\gamma \sum_{s'\in S}P_{s \pi(s)}(s')V^{\pi}(s') \qquad\qquad (2.8)&lt;/equation&gt;&lt;p&gt;上面的&lt;b&gt;贝尔曼方程&lt;/b&gt;（&lt;b&gt;Bellman Equation&lt;/b&gt;）给出了&lt;equation&gt;V^*&lt;/equation&gt;和&lt;equation&gt;V^{\pi}&lt;/equation&gt;的递归定义形式。比如在公式（2.7）中，&lt;equation&gt;max&lt;/equation&gt;后面的式子的含义是：如果我们采取了动作&lt;equation&gt;a&lt;/equation&gt;（并且马上得到了一个回馈&lt;equation&gt;R(s,a)&lt;/equation&gt;），并且从此以后都进行最优的动作（即从后续状态&lt;equation&gt;s'&lt;/equation&gt;得到期望回报&lt;equation&gt;\gamma V^*(s')&lt;/equation&gt;），最后求所有期望回报的总和。公式（2.7）就说明了通过选择最优的动作&lt;equation&gt;a&lt;/equation&gt;，并在后续中持续保持最优动作，就能够得到&lt;equation&gt;V^*(s)&lt;/equation&gt;。公式（2.8）也有类似的含义：如果我们持续地根据&lt;equation&gt;\pi&lt;/equation&gt;来进行动作，那么采取策略&lt;equation&gt;\pi&lt;/equation&gt;的期望回报就是当前的回馈加上未来的期望回报。&lt;/p&gt;&lt;p&gt;另一个事实是，根据给出的策略&lt;equation&gt;\pi&lt;/equation&gt;，&lt;equation&gt;V^*&lt;/equation&gt;和&lt;equation&gt;V^{\pi}&lt;/equation&gt;不仅满足方程（2.7-2.8），而且是这两个方程的&lt;b&gt;唯一&lt;/b&gt;解法。&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;原注&lt;/b&gt;：在无限的MDP的情况下，它们是这些方程的唯一受限（bounded）解法。当然可能存在其他一些不受限的解法，但是它们既不是&lt;equation&gt;V^*&lt;/equation&gt;也不是&lt;equation&gt;V^{\pi}&lt;/equation&gt;。&lt;/blockquote&gt;&lt;p&gt;有很多量都可以被写成Q函数或者价值函数的形式，我们通常选择比较方便的那个形式来进行书写表达。实际上，通过下面两个等式它们可以相关起来：&lt;/p&gt;&lt;equation&gt;V^*(s)=max_{a\in A}Q^*(s,a)\qquad\qquad (2.9)&lt;/equation&gt;&lt;equation&gt;V^{\pi}(s)=Q^{\pi}(s,\pi(s))\qquad\qquad (2.10)&lt;/equation&gt;&lt;p&gt;类似的，将方程（2.7-2.8）中的价值函数用Q函数替换可以得到：&lt;/p&gt;&lt;equation&gt;Q^*(s,a)=R(s,a)+\gamma \sum_{s'\in S}P_{sa}(s')max_{a'\in A}Q^*(s',a')\qquad\qquad (2.11)&lt;/equation&gt;&lt;equation&gt;Q^{\pi}(s,a)=R(s,a)+\gamma \sum_{s'\in S}P_{sa}(s')Q^{\pi}(s',\pi(s'))\qquad\qquad (2.12)&lt;/equation&gt;&lt;p&gt;根据初始状态分布&lt;equation&gt;D&lt;/equation&gt;和我们起始的那个状态，你可能会想是否存在这么一种情况，即对于某些初始状态用某些策略会比较好，而对于其他一些初始状态用其他策略会比较好。对于多个MDP，一个非常棒的事实就是存在一个最优的策略&lt;equation&gt;\pi^*:S \to A&lt;/equation&gt;，使得从任何状态开始，&lt;equation&gt;\pi^*&lt;/equation&gt;都能够得到最优的期望回报：&lt;/p&gt;&lt;equation&gt;V^{\pi^*}(s)=V^*(s)\ for\ all \ s \qquad\qquad (2.13)&lt;/equation&gt;&lt;p&gt;（&lt;b&gt;非常遗憾的是&lt;/b&gt;，在后续内容中我们将看到对于部分可观察的马尔科夫决策过程（POMDP）这个结论是不适用的，所谓POMDP，就是机器人不总能够知道当前状态&lt;equation&gt;s_t&lt;/equation&gt;）。还有，&lt;equation&gt;\pi^*&lt;/equation&gt;可以由下面两个等效的公式来定义：&lt;/p&gt;&lt;equation&gt;\pi^*(s)=arg \max_{a\in A}Q^*(s,a)\qquad\qquad(2.14)&lt;/equation&gt;&lt;equation&gt;\pi^*(s)=arg \max_{a\in A}R(s,a)+\gamma \sum_{s'\in S}P_{sa}(s')V^*(s') \qquad\qquad(2.15)&lt;/equation&gt;&lt;p&gt;注意：如果一个机器人知道&lt;equation&gt;Q^*&lt;/equation&gt;，那么它是可以很轻松地计算出最优策略的。而从&lt;equation&gt;V^*&lt;/equation&gt;得到最优策略则相对复杂，而且必须要知道&lt;equation&gt;P_{sa}(\cdot)&lt;/equation&gt;。&lt;/p&gt;&lt;h2&gt;2.3 MDP算法&lt;/h2&gt;&lt;p&gt;我们现在简单复习一些MDP中用于寻找策略的标准算法。和之前一样，对于细节感兴趣的读者可以阅读相关参考文献。&lt;/p&gt;&lt;p&gt;因为价值函数（或者Q函数）定义了一个最优策略，很多算法尝试要找到&lt;equation&gt;Q^*&lt;/equation&gt;或者&lt;equation&gt;V^*&lt;/equation&gt;。比如，如果转换概率&lt;equation&gt;P{sa}(\cdot)&lt;/equation&gt;一致，则公式（2.7）就定义了一个方程系统，它的解就能够得到&lt;equation&gt;V^*&lt;/equation&gt;。这些方程可以通过相关的线性程序来求解（比如文献[27，41，65]），也可以通过迭代更新下面的方程&lt;/p&gt;&lt;equation&gt;V(s):=\max_a R(s,a)+\gamma \sum_{s'\in S}P_{sa}(s')V(s') \qquad\qquad (2.16)&lt;/equation&gt;&lt;p&gt;直到它收敛。后面这种方法，是基于动态编程（dynamic programming based）的算法，被称为&lt;b&gt;价值迭代&lt;/b&gt;（&lt;b&gt;value iteration&lt;/b&gt;）。&lt;/p&gt;&lt;p&gt;另一种标准算法是&lt;b&gt;策略迭代&lt;/b&gt;（&lt;b&gt;policy iteration&lt;/b&gt;），它在内存中保持了策略&lt;equation&gt;\pi&lt;/equation&gt;和一个估计的价值函数&lt;equation&gt;V&lt;/equation&gt;，然后根据公式（2.15）对策略进行迭代更新：&lt;/p&gt;&lt;equation&gt;\pi(s):=arg \max_{a\in A}R(s,a)+\gamma \sum_{s'\in S}P_{sa}(s')V(s') \qquad\qquad(2.17)&lt;/equation&gt;&lt;p&gt;然后通过求解线性方程组公式（2.8）来根据&lt;equation&gt;\pi&lt;/equation&gt;将价值函数更新为&lt;equation&gt;V^{\pi}&lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;上述方法都有一个前提，那就是假设我们知道状态的转换概率&lt;equation&gt;P{sa}(\cdot)&lt;/equation&gt;。如果不知道转换概率，一般会先学习到它，然后使用估计的概率来跑算法。这种方法被称为“&lt;b&gt;基于模型的增强学习&lt;/b&gt;（&lt;b&gt;model-based reinforcement learning&lt;/b&gt;）”。相对的，我们也可以不去学习模型（即状态转换概率），直接去学习价值函数或者Q函数。而这种方法就被称为“&lt;b&gt;无模型的增强学习&lt;/b&gt;（&lt;b&gt;model-free reinforcement learning&lt;/b&gt;）”。比如，Q-learning算法在采取动作&lt;equation&gt;a&lt;/equation&gt;后如果看见状态从&lt;equation&gt;s&lt;/equation&gt;转换到了&lt;equation&gt;s'&lt;/equation&gt;，就会进行如下更新[106]：&lt;/p&gt;&lt;equation&gt;Q(s,a):=(1-\alpha)Q(s,a)+\alpha \left( R(s,a)+\gamma \max_{a'\in A}Q(s',a') \right) \qquad\qquad (2.18)&lt;/equation&gt;&lt;p&gt;（可以和公式（2.11）比较一下）。这里&lt;equation&gt;\alpha &lt;/equation&gt;被称为学习率参数。&lt;/p&gt;&lt;p&gt;在某些需要在线学习的应用中，可能会需要机器人在尝试并学习新动作和利用已知动作获取更多回馈这两方面进行权衡，这被称为“探索与利用（exploration vs exploitation）”问题。&lt;/p&gt;&lt;p&gt;我们讨论的大多数算法对于解决小型的MDP问题都比较有效，所谓小型MDP问题，就是指状态空间&lt;equation&gt;S&lt;/equation&gt;足够小，使得&lt;equation&gt;V:S\to \mathbb{R}&lt;/equation&gt;能够以一个列表的形式被存储起来，列表中每一项对应一个状态，对于&lt;equation&gt;\pi&lt;/equation&gt;也类似。对于更大型的MDP，这些方法要应用起来就比较棘手了。具体说来，就是在很多问题中，随着状态变量的增加，状态的数量呈指数级增长。例如，如果有状态空间&lt;equation&gt;S=\left\{ 0,1 \right\} ^n&lt;/equation&gt;包含了&lt;equation&gt;n&lt;/equation&gt;个二值的状态变量，那么状态的数量就是&lt;equation&gt;2^n&lt;/equation&gt;，那么用来表达&lt;equation&gt;V&lt;/equation&gt;或者&lt;equation&gt;\pi&lt;/equation&gt;的耗费就会远远高于&lt;equation&gt;n&lt;/equation&gt;。类似的，如果我们对n维连续状态空间使用基于网格的离散化，以此将问题变成一个状态数量有限的问题，那么仍然会遇到离散的状态数量以n呈指数级增长的问题。贝尔曼称这个问题为“&lt;b&gt;维度诅咒&lt;/b&gt;（curse of dimensionality）”[13]，维度诅咒问题使得在简单增强学习问题中适用的算法无法适用于更大型的MDP问题。&lt;/p&gt;&lt;p&gt;因此，研究者提出了很多方法来寻找价值函数的近似。最近一些研究在不同的设定下提出了各自的方法[58，42，27，41，28]，而且这个领域依然是研究的热点。&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：注意论文成文时间。上面提出的方法实际上都比较老了，我们专注在深度增强学习即可。&lt;/blockquote&gt;&lt;p&gt;到目前为止的方法都是通过公式（2.14-2.14），使用（近似的）&lt;equation&gt;V^*&lt;/equation&gt;或者&lt;equation&gt;Q^*&lt;/equation&gt;来隐式地定义一个策略&lt;equation&gt;\pi&lt;/equation&gt;。另一个选择是直接对策略空间做工作。在第四章中，我们将更多地从细节上探索策略搜索方法，并通过和基于价值函数的方法的比较，来讨论它的优劣。一个广为人知的策略搜素算法是William的Reinforce算法[109]。（其他一些相关的有[57，9，12]）。假设有一个简单的MDP只有2个状态，我们从状态&lt;equation&gt;s_0&lt;/equation&gt;开始，选择动作&lt;equation&gt;a_i&lt;/equation&gt;，得到回馈&lt;equation&gt;R(s_0,a_i)&lt;/equation&gt;，然后转换到一个回馈始终为0的吸收状态&lt;equation&gt;s_{send}&lt;/equation&gt;。进一步假设有一个由参数&lt;equation&gt;\theta&lt;/equation&gt;平滑了的动作概率分布&lt;equation&gt;p(a_i;\theta)&lt;/equation&gt;，该分布是用于定义我们的策略&lt;equation&gt;\pi&lt;/equation&gt;的。具体来说，就是&lt;equation&gt;p(a_i;\theta)&lt;/equation&gt;我们在状态&lt;equation&gt;s_0&lt;/equation&gt;中采取动作&lt;equation&gt;a_i&lt;/equation&gt;的概率。比如可以使用带参数的&lt;equation&gt;softmax&lt;/equation&gt;来定义它：&lt;equation&gt;p(a_i;\theta)=exp(\theta_i)/\sum_j exp(\theta_i)&lt;/equation&gt;。现在假设我们对于&lt;equation&gt;\theta&lt;/equation&gt;有一些选择，然后提高我们期望的结果，我们就会像登山一样向上行走：&lt;equation&gt;\theta_i:=\theta_i+\alpha \frac{\partial}{\partial\theta_i}V^{\pi}(s_0)&lt;/equation&gt;，其中&lt;equation&gt;\alpha&lt;/equation&gt;是学习率参数。公式如下：&lt;/p&gt;&lt;equation&gt;\frac{\partial}{\partial\theta_i}V^{\pi}(s_0)=\frac{\partial}{\partial\theta_i}\sum_i R(s_0,a_i)p(a_i;\theta)
&lt;/equation&gt;&lt;equation&gt;=\sum_i R(s_0,a_i)\frac{\partial}{\partial\theta_i}p(a_i;\theta)&lt;/equation&gt;&lt;equation&gt;=\sum_i R(s_0,a_i)p(a_i;\theta)\frac{\partial}{\partial\theta_i}\ln p(a_i;\theta)&lt;/equation&gt;&lt;equation&gt;=E_{a_i\sim p(\cdot;\theta)}\left[  R(s_0,a_i)\frac{\partial}{\partial\theta_i}\ln p(a_i;\theta) \right] &lt;/equation&gt;&lt;p&gt;这样，如果我们根据当前的策略&lt;equation&gt;p(a_i;\theta)&lt;/equation&gt;来采取动作，对于随机选择的动作&lt;equation&gt;a_i&lt;/equation&gt;，向着&lt;equation&gt;R(s_0,a_i)\frac{d}{d\theta}\ln p(a_i;\theta)&lt;/equation&gt;方向前进一小步，我们采取的就是&lt;b&gt;随机梯度上升&lt;/b&gt;（&lt;b&gt;stochastic gradient ascent&lt;/b&gt;）的一步。这样在期望上，我们就能向着目标函数最大化的山顶前进了。&lt;/p&gt;&lt;p&gt;上述例子是一个小的单步的MDP，但Reinforce算法（本质上）对一般问题都能适用。可证明，Reinforce算法在不同的条件下都能够收敛到局部最优，虽然从实际经验证明这个过程会比较耗时。的确，为了让算法有所进展，Reinforce算法可能需要的步数或者时间会随着状态的数量呈指数级增长（在第4章中有更细节的描述）[49]。更严重的问题是，Reinforce算法依然受到&lt;i&gt;随机&lt;/i&gt;策略的限制。除了我们可以考虑的策略种类的限制，在安全性优先的应用中（比如在第5章中介绍的直升机），通过强迫策略随机选取动作，以此来增加随机性应对一个已经足够随机的问题，其效果是很不理想的。&lt;/p&gt;&lt;p&gt;在第4章中，我们将从细节上探索策略搜索方法，并介绍一个不被上述问题困扰的策略搜索方法。Reinforce算法的思路是，一旦开始向上迭代，就使用从MDP取样来的数据，用完就丢掉。和Reinforce算法不同，新算法核心思路是：重用（reusing）我们从MDP中获得的数据。这会使得我们得到一个更高效的算法，性能也很不错。&lt;/p&gt;&lt;h2&gt;2.4 部分可观测的马尔科夫决策过程&lt;/h2&gt;&lt;p&gt;到目前为止，我们的讨论还是针对的可完全观察的马尔科夫决策过程的，在这种情况下，机器人在每个时间点&lt;equation&gt;t&lt;/equation&gt;都能够观察到它当前的状态&lt;equation&gt;s_t&lt;/equation&gt;。在部分可观测的马尔科夫决策过程（Partially observable MDP，简写为POMDP）中，机器人有一个观测集&lt;equation&gt;O&lt;/equation&gt;，在每个时间点，它只能够看到一个观测&lt;equation&gt;o_t=o(s_t)&lt;/equation&gt;，其中&lt;equation&gt;o:S\to O&lt;/equation&gt;是一个固定的观测函数（随机观测函数也类似）。例如，&lt;equation&gt;s&lt;/equation&gt;可以是一个装着状态变量的向量，而我们的传感器可能只能探测到这些状态变量的一个子集，在第5章中有一个具体例子。另一个情况是，根据传感器探测到的信息，某些状态可能很难同另一些状态区分开来，在这种情况下观察函数&lt;equation&gt;o(s_t)&lt;/equation&gt;可能就变成了一个包含相同类状态的列表。&lt;/p&gt;&lt;p&gt;在POMDP中，找到最优策略的难度显著增加。确实，取决于具体的假设，即使是找一个近似最优的策略通常都是NP-困难的。和MDP的情况不同，在POMDP中机器人即使是知道&lt;equation&gt;Q^*&lt;/equation&gt;也不能最优地行动，因为它不能总是知道当前的状态&lt;equation&gt;s&lt;/equation&gt;是什么样，因此不能够持续地选取&lt;equation&gt;arg\max_aQ^*(s,a)&lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;一个在POMDP中能够行动的方法就是引入&lt;b&gt;置信状态跟踪&lt;/b&gt;（&lt;b&gt;belief state tracking&lt;/b&gt;）。在这个方法中，概率分布&lt;equation&gt;p(s)&lt;/equation&gt;是对于当前处于什么状态的置信程度，加上之前的观测结果和采取的动作，我们将使用这三者来进行工作。具体说来，就是根据这个&lt;b&gt;置信状态&lt;/b&gt;（即&lt;equation&gt;S&lt;/equation&gt;的分布），我们可以定义和计算最优Q函数或者最优价值函数，并根据他们来采取最优动作。由于置信状态是可以被完全观测到的，所有这样就把POMDP问题降解为了一个MDP问题。需要注意的是，即使是假设&lt;equation&gt;S&lt;/equation&gt;是有限的，这个方法需要根据&lt;equation&gt;S&lt;/equation&gt;的概率分布来计算价值函数，这个计算是连续的并且复杂度为&lt;equation&gt;(|S|-1)&lt;/equation&gt;维。如此，该方法就把一个状态有限的MDP问题变成了一个高维且连续的，状态无限的MDP问题。使用上述思路来寻找最优或者近似最优策略的方法包括[111，60，23，107，62，61，24]。&lt;/p&gt;&lt;p&gt;这些基于置信状态的方法虽然对于某些问题效果不错，但是对于大型的POMDP问题效果还是不佳，比如状态空间是高维度切连续，而且状态数无限的问题。确实，如果&lt;equation&gt;S&lt;/equation&gt;是连续的，那么根据置信状态&lt;equation&gt;p(s)&lt;/equation&gt;学习价值函数和解决问题就会比较有挑战性，因为&lt;equation&gt;p(s)&lt;/equation&gt;对于状态空间的分布会比较复杂（这些置信状态也可以用不同的方法来近似，如[68，21，51]）。&lt;/p&gt;&lt;p&gt;多种用于设计POMDP控制器的启发方法被提了出来，比如我们可能会找到最有可能处于的状态&lt;equation&gt;\hat{s}=arg\max_sp(s)&lt;/equation&gt;，然后根据&lt;equation&gt;arg\max_aQ^*(\hat{s},a)&lt;/equation&gt;来选择动作。这些启发方法的效果随着具体问题不同而变化很大，缺乏一般性上的保证。（可以查看文献[22]）&lt;/p&gt;&lt;p&gt;目前来看，解决POMDP问题比较有前景的方法是使用基于策略搜索的方法。关于这个思路，我们在第四章中有一个讨论。在讨论中我们将看到，为什么在很多情况下策略搜索方法相较于动态编程或价值函数方法，能够更好地对POMDP问题一般性收敛。&lt;/p&gt;&lt;h2&gt;译者反馈&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;看完这篇材料后，不用刨根问底，记下不太明白的地方，开始你的CS294之旅吧；&lt;/li&gt;&lt;li&gt;关于CS231n和CS294的资源，请关注下我们的微信公众号（搜索“&lt;b&gt;智能单元&lt;/b&gt;”即可），然后回复cs231n或cs294。涨个关注，谢谢大家：）&lt;/li&gt;&lt;li&gt;利用个人时间学习并翻译，有不当之处欢迎留言批评指正！&lt;/li&gt;&lt;/ol&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24996278&amp;pixel&amp;useReferer"/&gt;</description><author>杜客</author><pubDate>Fri, 10 Feb 2017 17:15:36 GMT</pubDate></item><item><title>逻辑与神经之间的桥 (2.0版)</title><link>https://zhuanlan.zhihu.com/p/25027944</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-fc658b76b2f0ed0c863953573cd5f462_r.jpg"&gt;&lt;/p&gt;之前发表过了，但这次大修改后，有更实质的结论。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-faa076553f9d1165ee2239f6e4ae5de0.jpg" data-rawwidth="885" data-rawheight="1317"&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-e433ed5ad33031e754a2cd86bbc3d910.jpg" data-rawwidth="884" data-rawheight="1308"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-cf81c8b8ba907af107235a2d66f3dafe.jpg" data-rawwidth="879" data-rawheight="1315"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-a1d33a3100e2b4be6ace2893dd9f55be.jpg" data-rawwidth="886" data-rawheight="1316"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-9914470ca2ee5981d7dde8f38906aa6d.jpg" data-rawwidth="884" data-rawheight="1308"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-7822293996b1a46b538f946dec03cc4c.jpg" data-rawwidth="897" data-rawheight="1310"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-82f947cfd79b0941364281cfc4360d0c.jpg" data-rawwidth="882" data-rawheight="1320"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-b092c59c16f87e9f8840e526a305754a.jpg" data-rawwidth="886" data-rawheight="1309"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-032a241ccae593595041d0b54b8c47d5.jpg" data-rawwidth="903" data-rawheight="1309"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-d668c4e7e328423feaf773fb4c56d10c.jpg" data-rawwidth="891" data-rawheight="1306"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-a1970406157b4e136e6ffdb75651de09.jpg" data-rawwidth="894" data-rawheight="1306"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-ad8961a77167a18eff21d2b76eb4ed4e.jpg" data-rawwidth="891" data-rawheight="749"&gt;（注： 这篇的内容建立在&lt;a href="https://zhuanlan.zhihu.com/p/23978763"&gt;《游荡在思考的迷宫中》&lt;/a&gt;的架构上）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/25027944&amp;pixel&amp;useReferer"/&gt;</description><author>甄景贤</author><pubDate>Thu, 26 Jan 2017 17:26:58 GMT</pubDate></item><item><title>最前沿：基于GAN和RL的思想来训练对话生成，通过图灵测试可期！</title><link>https://zhuanlan.zhihu.com/p/25027693</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-3ce09841f5ec27b18afa2f741512b78e_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;PS：本文分析略深，需要一定的RL和GAN的基础。&lt;/p&gt;&lt;p&gt;前两天，Stanford的NLP小组出了一篇神经网络对话生成的论文：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a5577184d4a57d19befb57d33c1afecd.png" data-rawwidth="2006" data-rawheight="734"&gt;原文链接：&lt;a href="https://arxiv.org/pdf/1701.06547.pdf" data-editable="true" data-title="arxiv.org 的页面"&gt;https://arxiv.org/pdf/1701.06547.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;标题就是使用对抗学习来做神经对话生成。&lt;/p&gt;&lt;p&gt;这个idea非常的赞！在我看来是通往图灵测试的正确一步。&lt;/p&gt;&lt;p&gt;以前的对话生成，我们使用Seq2Seq的监督学习，其实也就是模仿学习。但是模仿学习的问题是神经网络的理解能力有限，训练样本有限，只能生成一定程度的对话。&lt;/p&gt;&lt;p&gt;那么，有没有可能让计算机真正理解对话的意思，然后自己学会对话呢？&lt;/p&gt;&lt;p&gt;有了深度增强学习，有了AlphaGo大家可以知道这是可能的。事实上这篇论文的作者Jiwei Li之前的一篇文章就是用深度增强学习来做对话生成。&lt;/p&gt;&lt;p&gt;但是使用深度增强学习最大的问题就是需要有reward。没有reward没法训练。&lt;/p&gt;&lt;p&gt;但是怎么定义一个对话的reward呢？好困难，有太多评价标准。但是有一个标准是绝对的，就是图灵测试的标准。只要这个对话看起来像人说的就行了。&lt;/p&gt;&lt;p&gt;这就不得不联系到了GAN生成对抗网络。把GAN中的分类器用来对对话做分类就行了。这样训练出来的分类器可以一定程度上判断计算机生成的对话与人的对话的差距。而这个&lt;strong&gt;差距就是reward&lt;/strong&gt;！&lt;/p&gt;&lt;p&gt;这篇文章可以说把DRL和GAN的思想很好的结合起来并应用在对话生成问题上，也取得了比较好的效果。相信在这个方法的基础上进一步发展，比如改进网络结构，将对话拓展到段落，更多的训练等等。&lt;strong&gt;也许3-5年图灵测试就真正通过了，而这一次，是机器自己真正学会了交流！&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;PS：本文同步发于“智能单元”微信公众号，欢迎大家关注，第一时间获取通用人工智能原创资讯！&lt;/strong&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/25027693&amp;pixel&amp;useReferer"/&gt;</description><author>Flood Sung</author><pubDate>Thu, 26 Jan 2017 16:44:01 GMT</pubDate></item><item><title>汉字生成模型的那些坑</title><link>https://zhuanlan.zhihu.com/p/24805121</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-56db09cd10a86d006257d0ff386dc2f7_r.jpg"&gt;&lt;/p&gt;&lt;h2&gt;引言&lt;/h2&gt;&lt;p&gt;人工智能技术目前仍处于技术积累期，人们真正希望看到的人工智能技术是要能够融入生活中的，真正为人们带来便利的技术，但目前能真正做到“实用化”的人工智能产品并不多。除了大家熟悉的，如机器翻译、聊天机器人、语音识别与生成、图像分类、路径规划、智能跟踪等，我认为汉字生成是最具有潜力、最早一批进入市场的人工智能产品，因为其领域相对较小（与Robot、NLP等比较），而且成本硬件较低，这两年汉字生成问题也确实受到了越来越多的关注。&lt;/p&gt;目前汉字生成模型主要应用场景包括&lt;b&gt;生成手写字体&lt;/b&gt;以及生&lt;b&gt;成手写汉字&lt;/b&gt;两个方面。生成手写字体主要是想解决中文字太多，设计一款新的字体需要巨大工程量的问题，如果设计师仅手工设计少量的字体文字，然后机器就能根据这些问题提取出其中的风格然后自动设计生成剩余文字，则可以极大减轻设计师的负担。生成手写汉字是指采集某人的书写笔迹，让计算机学会模仿他的风格进行书写，我认为这是汉字生成的最终目标，具体而言就是要让机械臂拿起笔帮我们处理手写任务，比如代替我们写亲笔信，或帮我们做手写试卷等。&lt;p&gt;汉字生成模型在技术上主要可以分为&lt;b&gt;基于笔迹的生成模型&lt;/b&gt;以及&lt;b&gt;基于图像的生成模型&lt;/b&gt;。二者的区别来源于应用场景的不同，若以设计一个会写字的机械臂为目标，则必须得到具体的笔画；若以生成新的中文字体为目标，则可以仅依赖文字的图像。&lt;/p&gt;&lt;p&gt;目前研究手写问题的文章并不是很多，但手写问题涉及到的领域非常广，不仅需要RNN而且若想真正做好还需要加入GAN、RL等人工智能技术。&lt;/p&gt;&lt;h2&gt;一、样本的表示方法&lt;/h2&gt;&lt;p&gt;样本的表示是一个值得探究的问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;如果样本是文字图像：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;大家很容易理解，但这种方法会损失书写的笔画顺序、文字起点与终点、每一个笔画的起笔与落笔等信息。&lt;/li&gt;&lt;li&gt;该方法也是有优点的，比如容易用CNN处理，比较容易提取风格特征，生成速度较快（一次生成整张图片）等。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;如果样本是文字笔迹：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;首先考虑是连续问题还是离散问题~&lt;/li&gt;&lt;li&gt;离散问题具体而言就是将文字投影到一个离散的画布上如100x100像素的画布，其中的每一个笔画由若干点组成，每一个点对应一个整数型坐标。更进一步，可以将每一个笔画分解为若干个关键点，关键点直接用直线连接，这样一条直线上就可以仅用起点与终点表示，可以省略掉中间的点。离散问题在生成文字时（每次生成一个点）可以看作一个&lt;b&gt;分类问题&lt;/b&gt;，因此比较容易训练，但其最大问题就是会损失一定精度。&lt;/li&gt;&lt;li&gt;连续问题就是将文字投影到（-1,1）的连续空间，用浮点数来精确表示每个点的坐标，这也是目前主流的方法，其本质上是一个&lt;b&gt;回归问题&lt;/b&gt;，也因此训练难度较高。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;样本是笔迹，在连续空间上，如何具体表示呢？&lt;/li&gt;&lt;ul&gt;&lt;li&gt;如果样本是笔迹，笔迹是由点组成的，每一个点&lt;equation&gt;p&lt;/equation&gt;可以用&lt;equation&gt;\{(x,y),(a,b,c)\}&lt;/equation&gt;这种形式来表示，其中&lt;equation&gt;(x,y)&lt;/equation&gt;是坐标，&lt;equation&gt;(a,b,c)&lt;/equation&gt;是一个分类向量，可以设&lt;equation&gt;a&lt;/equation&gt;是一个笔画的结束，&lt;equation&gt;b&lt;/equation&gt;是一般的点，&lt;equation&gt;c&lt;/equation&gt;是整个文字的结束。因此一个文字就可以用点的序列表示：&lt;equation&gt;S=(p_0,p_1,\cdots,p_n)&lt;/equation&gt;。&lt;/li&gt;&lt;li&gt;还有一个问题需要考虑，就是应该采用绝对坐标还是相对坐标呢？绝对坐标很容易理解，而相对坐标就是下一个点的&lt;equation&gt;(x,y)&lt;/equation&gt;并不是绝对位置，而是相对于上一个点的偏移。个人以为，两种方法皆可，但是相对表示表现更好，因为误差不会跃层向下传播。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;h2&gt;二、用RNN完成最基本的生成任务&lt;/h2&gt;&lt;p&gt;我们首先考虑一个最简单的生成模型：给若干个汉字的训练样本（笔迹样本），然后训练神经网络来生成风格类似的文字。&lt;/p&gt;&lt;p&gt;具体而言，就是用一个RNN训练，RNN的输入是要书写文字的&lt;b&gt;高维特征向量&lt;/b&gt;和一个&lt;b&gt;开始标识&lt;/b&gt;。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;高维特征向量&lt;/b&gt;代表我需要生成哪一个文字，其实也可以用一个one-hot向量表示，但由于汉字的数量很多，用一太长的向量表示比不上用一个更有意义的维度较低的向量表示，这也是借鉴了NLP中的word2vec的思想，这个更有意义的向量应该可以反映出要生成的汉字的固有特征。另外高维特征向量需要预先训练，可以采用自编码技术，或截取一个已训练神经网络的特征提取部分。&lt;/li&gt;&lt;li&gt;&lt;b&gt;开始标&lt;/b&gt;&lt;b&gt;识&lt;/b&gt;类似于NLP中的一段话的开始标志，如&amp;lt;begin&amp;gt;Hello world&amp;lt;end&amp;gt;，中的&amp;lt;begin&amp;gt;，具体到文字生成中，开始标志是一个特殊的点如&lt;equation&gt;\{(0,0),(0,1,0)\}&lt;/equation&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;具体的生成方法采用一次生成一个点的方式进行，直到生成结束点为止，举例而言就是给出一个文字比如“宋”，然后一笔一划地生成一个手写的“宋”，生成流程如图所示：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-c226c2a454f216e8a4f0e803812e1fca.png" data-rawwidth="704" data-rawheight="381"&gt;&lt;p&gt;在训练中，Loss的定义至关重要，在连续空间中采用传统的L2损失函数可能会导致无法拟合多值函数的问题，因此建议采用Mixture Density Loss，具体可以参考&lt;a href="http://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/" data-title="这篇文" class="" data-editable="true"&gt;这里&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;总体而言，做到这一步就算实现了一个简单的demo了，但是还有很多地方需要优化，离实用还有很多工作要做，我们将继续往前探索。&lt;/p&gt;&lt;h2&gt;三、风格提取与迁移&lt;/h2&gt;&lt;p&gt;风格提取与迁移问题最近比较热，比如生成风格化的图片，生成某人的声音等。风格提取主要是为了解决样本少的问题，我们希望我们设计的手写机器人能够仅训练某个用户的少量手写文字就能自动分析出该用户的手写风格，然后再根据先验知识（所有中文文字的书写规则）就可以生成训练集中未出现过的文字。&lt;/p&gt;&lt;p&gt;风格迁移就是强制神经网络学习到目标字体与参考字体间风格的异同，具体来说就是求出风格转移函数&lt;equation&gt;font_2=f(font_1)&lt;/equation&gt;，比如输入一个宋体的“我”字，输出楷体的“我”。最早的基于图像的风格迁移算法采用了传统的CNN模型，在训练时，输入参考字体（如宋体），输出目标字体（如楷体），训练完后在生成时输入新的参考字体，生成对应的目标字体，网络结构和生成示例如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-bf1bf0f9c01fec243cb94c814e9c5dab.png" data-rawwidth="960" data-rawheight="540"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-f239324798c56f061f1c2d8f6b2b1854.png" data-rawwidth="960" data-rawheight="540"&gt;&lt;p&gt;&lt;i&gt;（图片来源：&lt;/i&gt;Rewrite: Neural Style Transfer For Chinese Fonts）&lt;/p&gt;&lt;p&gt;这种仅采用CNN的基于图像的风格迁移算法整体表现并不是很好，在差异比较大的字体之间的迁移效果欠佳，这说明只通过字体的图像，神经网络难以学习到汉字风格的深度特征。&lt;/p&gt;&lt;p&gt;所以最近又有研究者将样本由图片表示替换为笔迹表示，现已较好地解决了风格迁移问题，风格迁移效果如下图所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-4febf45a4a0b1dda8ea3662eaaa64271.png" data-rawwidth="385" data-rawheight="335"&gt;&lt;i&gt;（图片来源： Automatic Generation of Large-scale Handwriting Fonts via Style Learning）&lt;/i&gt;&lt;/p&gt;&lt;p&gt;这种算法首先对参考文字进行笔迹（点）的提取，然后再对目标字体也提取相等数量的点，然后求二者的误差，作为最终的Loss。但现有的这种基于笔迹的算法仍有改进空间，因为该算法在提取笔迹（关键点的个数）和loss的定义上都还是人为设定的，在这两个方面应该还有较大优化的空间，真正的人工智能应该尽可能地减少人为干预。&lt;/p&gt;&lt;p&gt;总之风格提取在文字生成算法中还有一段较长的路要走，其主要研究点应该包括如何更好地定义风格的异同，如何设计一种尽量少依赖人工干预的，鲁棒性更强的表示方法；以及如何优化存储风格的神经网络结构。&lt;/p&gt;&lt;h2&gt;四、更真实的文字&lt;/h2&gt;&lt;p&gt;我们人在书写的过程中，哪怕写100个“我”字，也不会出现两个完全相同的。但是传统的神经网络无法解决差异化输出的问题，我们用一个训练好的神经网络，输出的文字总是一模一样的，因此这样的手写机器人仍不能代替人来写字。&lt;/p&gt;&lt;p&gt;目前，尚未看到有关手写汉字领域的差异化输出相关研究。但是在图像生成领域有一些相关的解决方案，那就是对抗网络生成模型GAN。该模型包括生成器（G）和鉴别器（D），前者用一个随机的100维向量作为输入，目的是为了生成一个与训练集中的图像类似（风格相似）的一张图像，后者是一个二元分类器，用于鉴别该图像是否是在训练集中，每次训练时将生成的图像与训练集中的图像各取50%，进行有监督的学习，标签就是该图像是真实的还是伪造的。生成器需要尽可能生成具有类似风格的图片，而鉴别器要尽可能区分伪造的和真实的，具体公式可以表示为：&lt;/p&gt;&lt;equation&gt;\min \limits_G \max \limits_D D(x)-D(G(z)) &lt;/equation&gt;&lt;p&gt;该方法有些类似RL中的policy方法，两个网络同时训练，同时优化。训练完毕后，生成器可以看成是将训练样本的高维特征提取出来了，其输入的向量就是高维特征向量，根据该向量生成一张图片。&lt;/p&gt;&lt;p&gt;在手写问题上，我们可以采用该方法提取同一个文字的高维表示，然后在其中添加伪随机因子，由此生成各对应文字的不同副本。&lt;/p&gt;&lt;h2&gt;五、数据来源问题&lt;/h2&gt;&lt;p&gt;汉字生成模型由Demo到实用化还有较长的一段路要走，其中一个重要的问题就数据样本采集问题。目前表现最佳的算法主要采用具备更丰富信息的基于笔迹的样本数据，但是通常采集这种具有笔迹信息的样本是比较困难的，要么需要特制的手写笔，要么需要手写板，最经济的也需要装一个手机APP（用手写笔在手机屏幕上写）。但是实际上每个人都有大量的现成的写在纸上的字迹（图片样本），如果能够利用这些手写数据则可以获得更好的用户体验，同时也节约了人力物力。有些论文中提出了用人工的方法或一些特定的算法来提取笔迹但这显然不是良策，更好的方法应该是采用&lt;b&gt;增强学习&lt;/b&gt;的方法，让机器自己学习如何由图片样本提取笔迹，然后在此基础上进行进一步的训练，目前与这个点相关的研究较少，笔者也正在探索之中。&lt;/p&gt;&lt;p&gt;另外，若要尽一步减小人为干预，在增强学习之前还需要进行预处理，要有一个定位器和一个识别器，用于定位和识别已有图片样本中的每一个文字。（这些都是坑啊~~~）&lt;/p&gt;&lt;h2&gt;六、小结&lt;/h2&gt;&lt;p&gt;最后总结一下汉字生成模型的那些坑以及每个坑需要用什么土来填：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;如何表示文字：建议采用基于点的序列来表示，每个点可表示为&lt;equation&gt;\{(x,y),(a,b,c)\}&lt;/equation&gt;的形式。&lt;/li&gt;&lt;li&gt;如何生成文字：采用RNN网络每次生成一个点，直到生成具有结束标志的点。&lt;/li&gt;&lt;li&gt;如何提取文字风格：用已有的数据训练一个风格迁移网络，保存风格信息。&lt;/li&gt;&lt;li&gt;如何让生成的文字更真实（各不相同）：用对抗网络（GAN）提取每个文字的高维特征，并在此基础上加入伪随机因子。&lt;/li&gt;&lt;li&gt;如何充分利用已有的图片样本数据：采用增强学习（RL）的方法，让机器自己学习如何由图片样本提取笔迹。&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;参考文献：&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;Generating Sequences with Recurrent Neural Networks.&lt;/li&gt;&lt;li&gt;Generating online fake Chinese characters with LSTM-RNN,” 2015. [Online]. Available: &lt;a href="http://blog.otoro.net/2015/12/28/recurrent-net-dreams-up-fake-chinese-characters-in-vector-format-with-tensorflow/" data-editable="true" data-title="Recurrent Net Dreams Up Fake Chinese Characters in Vector Format with TensorFlow" class=""&gt;Recurrent Net Dreams Up Fake Chinese Characters in Vector Format with TensorFlow&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Automatic Generation of Personal Chinese Handwriting by Capturing the Characteristics of Personal Handwriting.&lt;/li&gt;&lt;li&gt;Drawing and Recognizing Chinese Characters with Recurrent Neural Network.&lt;/li&gt;&lt;li&gt;Automatic Generation of Large-scale Handwriting Fonts via Style Learning.&lt;/li&gt;&lt;li&gt;Learning Typographic Style.&lt;/li&gt;&lt;li&gt;Rewrite: Neural Style Transfer For Chinese Fonts [Online]. Available:&lt;a href="https://github.com/kaonashi-tyc/Rewrite" data-editable="true" data-title="kaonashi-tyc/Rewrite" class=""&gt;kaonashi-tyc/Rewrite&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Generative Adversarial Nets.&lt;/li&gt;&lt;/ol&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24805121&amp;pixel&amp;useReferer"/&gt;</description><author>Lonely.wm</author><pubDate>Mon, 23 Jan 2017 12:21:15 GMT</pubDate></item><item><title>Q&amp;A 知乎Live：从AlphaGo看人工智能前沿技术</title><link>https://zhuanlan.zhihu.com/p/24977176</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-3ce09841f5ec27b18afa2f741512b78e_r.jpg"&gt;&lt;/p&gt;&lt;h2&gt;1 前言&lt;/h2&gt;&lt;p&gt;在昨晚的 &lt;a href="https://www.zhihu.com/lives/802155571712253952?utm_campaign=zhihulive&amp;amp;utm_source=zhihucolumn&amp;amp;utm_medium=Livecolumn" data-title="知乎Live：从AlphaGo看人工智能前沿技术" class="" data-editable="true"&gt;知乎Live：从AlphaGo看人工智能前沿技术&lt;/a&gt; 中，大家提了很多问题，由于Live时间的限制，来不及一一回复。这里将一些在Live中没有回答到的问题回答在这里。&lt;/p&gt;&lt;h2&gt;2 Q&amp;amp;A&lt;/h2&gt;&lt;p&gt;（1）深度增强学习和深度学习有什么联系和区别？&lt;/p&gt;&lt;p&gt;答：深度增强学习是深度学习+增强学习，可以说是深度学习的拓展分支。深度增强学习的核心是在增强学习的框架下使用多层神经网络来表示策略网络Policy Network或价值网络Value Network&lt;b&gt;。增强学习为深度学习提供学习目标，深度学习则提供学习的机制。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;（2）深度增强学习这个技术在以后无法替代的人工领域或技能可能是哪些？&lt;/p&gt;&lt;p&gt;答：由于深度增强学习是一个面向通用人工智能的算法，这个算法的不断发展将会不断替代一些之前认为机器不可能做的人类工作。现在我们可见的是翻译，自动驾驶，监控等领域，接下来很多金融决策，医疗诊断，语音助理（语音客服）都会被取代。可能现阶段一些涉及人类感性因素的工作比如传媒行业，艺术创作等比较不好被取代，但是从深度增强学习的理论上来看，机器也有可能通过训练来理解人类的感性因素，比如什么是幽默，什么好笑，什么是爱。一旦机器也能解决这样的问题，恐怕就没有什么是不能被取代的了。当然，这还需要好多年。&lt;/p&gt;&lt;p&gt;（3）深度增强学习本身的逻辑是什么？&lt;/p&gt;&lt;p&gt;答：深度增强学习最基本的逻辑有两个：一是不断试错，二是根据试错的经验评估行为的好坏，调整不同行为输出的可能性。&lt;/p&gt;&lt;p&gt;（4）这个人工智能理论可以反过来对人类学习有借鉴作用吗？比如二语学习。&lt;/p&gt;&lt;p&gt;答：对深度增强学习的研究确实是可以反过来更好的理解人类的学习方式，从而反过来提升我们人类自身的学习水平。从AlphaGo看，人类可以根据AlphaGo的棋谱来研究新的思路。从深度增强学习的算法本身，我们会更清楚的知道，要勇于试错，并且不断调整自己，要认识到自身必然存在的固执的思维。&lt;/p&gt;&lt;p&gt;（5）想请教下是否深度学习能在金融市场这个多人参与的博弈市场发挥功效？&lt;/p&gt;&lt;p&gt;答：显然是可以的，深度增强学习是一套面向决策控制的算法，而金融博弈正是这样的一种决策问题。但是金融市场毕竟存在大量的外部信息，要使一个人工智能程序能处理所有这些信息并给出正确的决策需要巨量的训练，存在较大的挑战。目前深度增强学习的研究正在从简单到复杂，从最基本的游戏到复杂的真实场景。&lt;/p&gt;&lt;p&gt;（6）目前，深度增强学习在每个人的日常生活中的应用达到了什么程度，使用它需要的硬件的门槛高吗？硬件的门槛短期内能下降很多吗？&lt;/p&gt;&lt;p&gt;答：深度增强学习是近2-3年来才发展起来的最前沿人工智能技术，因此还没有达到能在日常生活中广泛应用的程度。它所需要的硬件门槛和深度学习的研究是一样的，都需要较高性能的计算，GPU是目前深度学习研究必备的硬件设备。硬件门槛恐怕短期内很难下降，神经网络芯片从推出到成熟还需要很长时间。而且由于数据越来越大，所需的硬件性能越来越高，一定程度上加大了门槛的降低。&lt;/p&gt;&lt;p&gt;（7）计算机&amp;amp;统计学双修本科在读，业余五段，希望走这条发展路线，想知道以后的高年级计算机选修课应该选哪个方向，谢谢！&lt;/p&gt;&lt;p&gt;答：多选人工智能方向的课，比如计算机视觉，机器学习等课程。当然，最佳的建议是学习网上名校名师的课程。比如Stanford的CS231n计算机视觉课程，UC Berkerley的CS294 深度增强学习课程，还有Coursera，Udacity上都有不错的人工智能课程。&lt;/p&gt;&lt;p&gt;（8）机器学习如何入门，有哪些基础知识，整体的学习流程应该是怎么样的？&lt;/p&gt;&lt;p&gt;答：机器学习的入门建议学习Andrew Ng的机器学习课程。主要需要一定的数学基础，包括概率论和矩阵，也就是大学的数学课程。学习了机器学习入门课之后，可以直接学习深度学习相关的课程，建议都学习网上名校的公开课程，见上一个问题。&lt;/p&gt;&lt;p&gt;（9）本科医疗专业想向人工智能方向转行有什么建议（知识技能储备，路线规划，就业前景等）。要考研究人工智能研究生，能否谈谈大学和专业。&lt;/p&gt;&lt;p&gt;答：学习详见前两个问题。就业前景是非常大的，未来多年人工智能算法工程师都会是稀缺的职位。考研的话国内推荐清华，南大，上交，中科院及国防科大。&lt;/p&gt;&lt;p&gt;（10）人们普遍认为高低电平的计算机不可能实现真正的人工智能，现在所谓的人工智能也只能在给定模式下运行。请问您认为普适的人工智能可能存在吗？&lt;/p&gt;&lt;p&gt;答：当然是存在的，人类的大脑的智能可能也就是神经元的数据传输。现在的通用人工智能才刚刚开始，未来的发展必然会出现能处理多种任务的人工智能程序。&lt;/p&gt;&lt;p&gt;（11）电算能力和感情会同时以高水平(计算机和普通人的情感)存在吗？&lt;/p&gt;&lt;p&gt;答：由于目前的人工智能水平还无法模拟人类的情感，所以很难回答这个问题。从个人理解上看，人类的情感也是能被模拟出来的。&lt;/p&gt;&lt;p&gt;（12）深度学习的ai在计算性能比较强的计算机上完成学习后，能否移植到性能较差的计算机上应用？&lt;/p&gt;&lt;p&gt;答：学习完成后，只要使用神经网络进行前向传播，所需的计算量相对来说少很多，是可以移植的。现在比如Tensorflow训练好的神经网络，可以移植到手机上运行。&lt;/p&gt;&lt;p&gt;（13）讲讲go怎么用的先验？&lt;/p&gt;&lt;p&gt;答：使用人类专家的数据进行监督学习（模仿学习）。&lt;/p&gt;&lt;p&gt;（14）除了replay.targat network还有哪些技巧呢？&lt;/p&gt;&lt;p&gt;答：还有Dueling Network，Prioritised Replay等。当然这些都是DQN发展出来的，现在最好的算法是A3C及其之后的发展算法，比如UNREAL。&lt;/p&gt;&lt;p&gt;（15）如何自己做一个初始的AlphaGo，有放出的源码吗？&lt;/p&gt;&lt;p&gt;答：可以按照AlphaGo的论文方法去复现，github有一些非官方的代码。但是最大的问题在于除非在大公司否则很难有那么高性能的分布式计算机器支持，还要收集大量数据，一个人很难做出来。&lt;/p&gt;&lt;p&gt;（16）对DRL应用到imperfect information games中比较感兴趣。&lt;/p&gt;&lt;p&gt;回答：对于DRL应用到不完全信息游戏的问题，2016年David Silver组出了一篇文章名为：Deep Reinforcement Learning from
Self-Play in Imperfect-Information Games。那么这篇文章面向多人零和游戏，比如扑克。那么idea比较简单，就是综合自身的判断和面对其他人的判断。稍微解释一下。自己的判断就是完全不管对手的策略，自己根据当前的状态（牌面）给出平均最佳选择。而面对其他人的判断，是指考虑当前对手的策略，根据当前情况作出一个最佳反应。Idea就是综合两者的选择给出一个结果，最后能够趋近于纳什均衡。比只用DQN效果好很多。 但是如果是对于星际争霸这样的环境，涉及的问题就多很多，仅仅用这个策略显然也很难达到好的效果。&lt;/p&gt;&lt;p&gt;（17）目前的drl通用框架，以及目前的implement用到的工具推荐～&lt;/p&gt;&lt;p&gt;答：DQN，A3C，UNREAL。推荐使用tensorflow。&lt;/p&gt;&lt;p&gt;（18）rl在机械臂、机器人平台上应用的潜在优势是什么？学术上rl算法往往以小游戏做算法验证，用到真实机械臂上问题很多，我没有想清楚随着rl发展，它在机械臂机器人上应用的潜在优势。请教一下。&lt;/p&gt;&lt;p&gt;答：rl在机器人上的最大优势是可以通过学习来掌握控制，并且是使用从感知到控制的端到端神经网络。Google之前已经放出了一些他们使用DRL做机械臂的进展，可以关注。我认为使用DRL是真正实现机器人灵活控制的方法。这个领域很新，很前沿，值得研究。&lt;/p&gt;&lt;p&gt;（19）求深度讲解算法，推荐一些相关论文，想编程实现一下这个算法？&lt;/p&gt;&lt;p&gt;答：&lt;a href="https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap" data-editable="true" data-title="songrotek/Deep-Learning-Papers-Reading-Roadmap"&gt;songrotek/Deep-Learning-Papers-Reading-Roadmap&lt;/a&gt;&lt;/p&gt;&lt;p&gt;（20）读了好几遍论文，有些不太明白，想问些细节问题：&lt;/p&gt;1. rollout网络就是局部特征的线性组合么？这儿根本没用卷积网络吧？&lt;p&gt;2. 在做数据集时，相同局面下的相同走法要合并吗？&lt;/p&gt;&lt;p&gt;3. sutton 书里 REINFORCE算法里同时调整了值网络的参数，感觉alphago的在做强化学习的时候并没有，而是用生成对弈数据集再用监督学习做的。是这样理解的吗？&lt;/p&gt;&lt;p&gt;4. self-play时RLpolicy是每次初始化为SLpolicy，还是在前一个RLpolicy的基础上继续？&lt;/p&gt;&lt;p&gt;答：1. rollout也是一个神经网络，只不过这个神经网络比较小，使用的局部特征作为输入，但是很快。2. 一般很难出现完全相同的局面的，如果出现，也不需要合并，直接作为训练数据。3. AlphaGo的价值网络是单独训练的，没错。4. 是在之前的RLpolicy基础上继续，也只有这样才会不断进步。自学习时是不断用新版本取代旧版本，把旧版本作为对手进行训练。&lt;/p&gt;&lt;p&gt;（21）请问机器学习，深度学习包括深度增强学习等最近进展巨大的技术，对于机器人技术的影响和应用在什么地方？&lt;/p&gt;&lt;p&gt;答：有巨大潜力。欢迎阅读：&lt;a href="https://zhuanlan.zhihu.com/p/22758556?refer=intelligentunit" data-editable="true" data-title="知乎专栏" class=""&gt;最前沿之谷歌的机械臂&lt;/a&gt;&lt;/p&gt;&lt;p&gt;（22）从数据挖掘到机器学习再到深度学习，其中最关键的知识内核是什么？在该怎么深入学习？用tensorflow教程跑完一遍mnist，然后呢？&lt;/p&gt;&lt;p&gt;答：深度学习说白了就是三个部分：模型，数据及训练方法。要深入学习最好还是学习网上名校的公开课，然后自己尝试去复现代码。&lt;/p&gt;&lt;p&gt;最后，感谢所有参加本次Live的知友们！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24977176&amp;pixel&amp;useReferer"/&gt;</description><author>Flood Sung</author><pubDate>Sat, 21 Jan 2017 11:37:47 GMT</pubDate></item><item><title>Flood Sung的Live--从AlphaGo看人工智能前沿技术</title><link>https://zhuanlan.zhihu.com/p/24816290</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-3ce09841f5ec27b18afa2f741512b78e_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;大家好！我是 Flood Sung ，研究通用人工智能与机器人学习，&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" class="" data-editable="true" data-title="「智能单元」知乎专栏"&gt;「智能单元」知乎专栏&lt;/a&gt;作者之一。&lt;/p&gt;&lt;p&gt;        深度增强学习（ Deep Reinforcement Learning ） 2013 年就被 DeepMind 提出，然而被关注度非常低。直到去年的 AlphaGo ，才使得深度增强学习真正火起来。深度增强学习是 AlphaGo 的核心技术，是 AlphaGo 能够实现自我学习的关键。&lt;/p&gt;&lt;p&gt;        当前，深度增强学习已成为人工智能领域最重要的研究分支之一，该技术能够使计算机通过&lt;b&gt;深度神经网络&lt;/b&gt;处理&lt;b&gt;从感知到决策控制&lt;/b&gt;的问题，无论是&lt;b&gt;下棋&lt;/b&gt;、&lt;b&gt;金融决策&lt;/b&gt;、&lt;b&gt;医疗诊断&lt;/b&gt;还是&lt;b&gt;机器人控制&lt;/b&gt;，都能派上用场，具有超强的应用前景，是未来的&lt;b&gt;革命性技术&lt;/b&gt;，值得我们每一个人关注！&lt;/p&gt;&lt;h2&gt;2 Live主题与内容&lt;/h2&gt;&lt;p&gt;&lt;b&gt;&lt;a href="https://www.zhihu.com/lives/802155571712253952?utm_campaign=zhihulive&amp;amp;utm_source=zhihucolumn&amp;amp;utm_medium=Livecolumn" class="" data-editable="true" data-title="Live入口：请点击进入"&gt;Live入口：请点击进入&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;        Live时间：&lt;/b&gt;&lt;b&gt;2017-01-20 20:00&lt;/b&gt;&lt;/p&gt;&lt;p&gt;        本次Live的主题是 &lt;b&gt;AlphaGo&lt;/b&gt; 与&lt;b&gt;深度增强学习&lt;/b&gt;，想讲给对人工智能前沿技术感兴趣，对 AlphaGo 的核心技术感兴趣的朋友！&lt;/p&gt;&lt;p&gt;        在Live中我将从 AlphaGo 出发，为大家介绍 AlphaGo 的核心技术—深度增强学习的基本原理与方法，分析 AlphaGo 能够在自对弈中不断学习的原因，让大家了解深度增强学习这个方法的通用性和革命性！ &lt;/p&gt;&lt;p&gt;&lt;b&gt; 本次 Live 主要包括以下问题：&lt;/b&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;＊ AlphaGo 为什么能在围棋上取得如此重大的突破？ &lt;/p&gt;&lt;p&gt;＊ 深度增强学习是什么，有怎样的发展历史？&lt;/p&gt;&lt;p&gt; ＊ 深度增强学习与通用人工智能有什么关系？ &lt;/p&gt;&lt;p&gt;＊ 深度增强学习拥有怎样的核心思想？ &lt;/p&gt;&lt;p&gt;＊ AlphaGo 自对弈中不断学习的方法是什么？ &lt;/p&gt;&lt;p&gt;＊ 深度增强学习未来的应用和发展前景是什么？&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;期待大家的参与！&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24816290&amp;pixel&amp;useReferer"/&gt;</description><author>Flood Sung</author><pubDate>Tue, 10 Jan 2017 18:26:59 GMT</pubDate></item><item><title>吴恩达对于增强学习的形象论述（上）</title><link>https://zhuanlan.zhihu.com/p/24761972</link><description>&lt;b&gt;版权声明：本文&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" data-editable="true" data-title="智能单元"&gt;智能单元&lt;/a&gt;首发，本人原创，禁止未授权转载。&lt;/b&gt;&lt;p&gt;&lt;b&gt;前言：&lt;/b&gt;吴恩达在2003年为完成博士学位要求做了专题论文：&lt;a href="http://rll.berkeley.edu/deeprlcourse/docs/ng-thesis.pdf" data-title="Shaping and policy search in Reinforcement learning" class="" data-editable="true"&gt;Shaping and policy search in Reinforcement learning&lt;/a&gt;，其第一、二章被&lt;a href="https://zhuanlan.zhihu.com/p/24721292?refer=intelligentunit" data-title="伯克利CS294：深度增强学习课程" class="" data-editable="true"&gt;伯克利CS294：深度增强学习课程&lt;/a&gt;作为推荐材料。本文基于笔者的理解，对第一章做有选择的编译与注释。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;第一章 简介&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在本章中，我们将对本论文中需要考虑的增强学习框架给出一个非正式的，不涉及数学形式的综述。同时还将描述一些增强学习中的问题，这些问题是我们需要尝试解决的。最后，给出整个专题论文的概要。&lt;/p&gt;&lt;h2&gt;1.1 对增强学习的介绍&lt;/h2&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-5e4cda0cb4d5455d8a410d10ed04b8fb.png" data-rawwidth="994" data-rawheight="678"&gt;&lt;i&gt;图 1.1 伯克利大学的无人直升机&lt;/i&gt;&lt;/p&gt;&lt;p&gt;给一个像图1.1中那样的直升机，我们如何才能学习，或者说自动地设计一个控制器，使得直升机能够正常飞行呢？&lt;/p&gt;&lt;p&gt;人工智能和控制中的一个基础问题就是在随机系统中进行序列决策。飞行中的直升机就是随机系统的一个很好例子，因为它展现出随机和不可预测的行为，而大风和其他类似的干扰可能导致它的运动偏离预期。直升机的控制也是一个序列决策问题，控制直升机需要连续地决策向着哪个方向推操纵杆。比起那些只需要针对一个情况及时作出一个正确决策的问题，本问题展现出了所谓的“&lt;i&gt;&lt;b&gt;延迟后果（delayed consequences）&lt;/b&gt;&lt;/i&gt;”性质，解决问题的难度可谓是大为增加。所谓延迟后果，就是说直升机的自动控制器的水平好坏是根据它的长期表现来决定的，比如假设它现在做出了一个错误的操作，直升机并不会马上坠毁，可能依然能够飞行很多秒。导致直升机控制问题难度增大的另一个方面是它的&lt;b&gt;局部可观测性&lt;/b&gt;。具体来说，就是我们不能够精确地观测到直升机的位置/状态；但是，即便是面对系统状态的不确定性，我们仍然在每一秒都需要计算出正确的控制指令，使得直升机能够在空中正常飞行。&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：吴恩达擅长将一个问题通过比喻和举例的方式讲得通俗易懂，这是教学者的金钥匙。无人机控制这个例子里面包含了好几个马尔科夫决策过程和增强学习里面的用数学公式表达起来比较抽象的性质，待看到公式的时候，回头想这个例子，可以很好地帮助理解记忆。&lt;/blockquote&gt;&lt;p&gt;我们将马尔科夫决策过程（MDP）框架的公式表达推迟到第二章再讲。简单地来说，有的系统（就好比无人直升机）的控制是在每个时间点都会处于某种“状态”，我们一般对于这种系统比较感兴趣，而马尔科夫决策过程就是对这种系统进行建模。例如直升机的状态也许就可以用它的位置和方向来表示。我们的任务就是选择动作，使得系统能够倾向于保持在“好”的状态中，比如保持悬停，并且能够避免“坏”的状态，比如坠机。数量巨大且不同的问题都可以用马尔科夫决策过程形式来进行建模。比如规划和机器人导航，库存管理，机器维护，网络路由，电梯控制和搭建推荐系统。&lt;/p&gt;&lt;p&gt;增强学习针对解决MDP形式的问题给出了一系列的工具。虽然它获得了巨大成功，但是在解决很多问题时仍面临困难，还存在很多问题和挑战。我们简要地描述其中一些问题，这些问题会让某些增强学习问题具有挑战性：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;首先，存在&lt;b&gt;高维度问题&lt;/b&gt;。具体说来，就是基于离散的简单增强学习算法，经常会遇到状态变量的数量成指数增长的情况。这个问题就是所谓的“维度诅咒”，我们将在第二章中更详细地讨论。我们能够设计出一个即能可证地有效运作，又能更好地扩展到搞维度问题的实用算法吗？&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：现在高维度问题对深度增强学习已经基本上不是问题。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;同时，如何选择“&lt;b&gt;回馈函数（reward function）&lt;/b&gt;”也是一个问题。在增强学习中，设计者必须指明一个函数，该函数能够告诉我们直升飞机什么时候飞得好，什么时候是飞得不好。我们在选择这个函数的时候有很大的自由度，在第三章中还会看到，如果选择得当，某些函数能够成数量级地加速学习过程。当然同时也存在一些看起来不错，但是实际上让控制器的表现非常糟糕的函数。我们能够合理地选择回馈函数，既避免这种问题，又能让增强学习算法学习得又快又好吗？&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：在后续的学习实践中会常常接触，现在不明白不用担心。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;局部可观测性&lt;/b&gt;（&lt;b&gt;Partial Observability&lt;/b&gt;）是指被控制的系统的状态不能被精确观察的情况，比如直升机上的传感器只能是近似地测量直升机的位置。局部可观测性让问题的解决更加困难了，许多标准的增强学习算法不能解决这种情况，有的即使能够解决，也非常艰难。那么，如果只能近似地观察系统在做什么，我们该如何选择正确的控制呢？&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在本论文中，我们提出了一些方法，尝试解决前两个问题。我们的最终算法在局部可观测的情况下也能很好地工作，算法被运用到了图1.1中的无人直升机控制中。在实践的过程中，我们还涉及了经典控制理论中的一些专题，比如系统识别与验证等。&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：看这篇论文的&lt;b&gt;目的是帮助我们熟悉增强学习和马尔科夫决策过程&lt;/b&gt;，为CS294的学习&lt;b&gt;做知识预习&lt;/b&gt;的，没必要看完整个论文（150多页）来搞明白当年吴恩达的算法是啥情况。&lt;/blockquote&gt;&lt;h2&gt;1.2 与有监督学习的比较&lt;/h2&gt;&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：其实个人不太愿意翻译这段，因为对于学习深度增强学习，这个学术和理论味儿比较浓的小节没有太大意义。类似的情况在增强学习的著作&lt;a href="http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html" data-title="Reinforcement Learning: An Introduction" class="" data-editable="true"&gt;Reinforcement Learning: An Introduction&lt;/a&gt;中也有，作者花了很多篇幅去论证增强学习与有监督学习、无监督学习是不同的，应该与之并列什么的。&lt;b&gt;个人意见：虽然翻译了，可略过&lt;/b&gt;。&lt;/blockquote&gt;&lt;p&gt;有监督学习是人工智能领域中另一类标准问题。它可以看成是增强学习的某种特殊形式，在这种形式下，只需要对系统进行一次控制，因此我们只需要一次决策，而不是连续的序列决策。虽然看起来差别不是很大，但是实际上这让有监督学习变成了一个非常简单的问题。&lt;/p&gt;&lt;p&gt;举个具体例子，考虑根据给出的病人的多种检查数据或“特征”（比如心率、体温、多种医学检查结果），用有监督学习算法来预测一个病人是否患有心脏病。这里假设我们拥有一个训练集，其中包含有一些病人特征的样本，以及指明其中哪些病人是否患有心脏病的信息。我们可以用有监督学习算法来让一些函数（线性映射或者神经网络）来对数据进行拟合。当一个新的病人来就诊时，我们可以根据病人的特征，使用这个拟合的函数来预测他是否患有心脏病。而当这个一眼的诊断出来后，我们的病人就要去面对他的命运了。如果我们预测出错（比如我们决定赶紧对病人进行手术，而接下来的操作发现病人实际上没有任何问题，手术完全没有必要），那么我们也能马上观察到结果，并从这个结果中继续学习。&lt;/p&gt;&lt;p&gt;然而，在增强学习中，我们动作的结果通常是有延迟的，因此想要识别并从动作的长期效果中学习变得更加困难了。例如下棋，如果我们在第63手的时候输了（或者赢了），可能非常重要的一点是需要认识到我们在第17手的时候下的一记妙手奠定了胜局。这个“可信度分配（credit assignment）”问题让算法从过去的失误中吸取教训或从过去的成功中学习经验都更加困难了。&lt;/p&gt;&lt;p&gt;其次，在增强学习问题中的连续环境让算法重用（reuse）数据变得更加困难。在有监督学习中，如果我们预先收集并且存储了一些病人的样本数据集，并且想要测试一个新的神经网络在心脏病预测方面的性能，那么我们可以很容易地在这些数据集上进行测试，并分析结果与真实情况之间的差异，从而得出新模型的性能好坏。然而在增强学习中，假设我们也预先测试了一个控制直升机翻转飞行的控制器（或者举个更自然的例子，控制直升机稍微向右倾斜的控制器），在测试期间收集的数据是可以让我们知道直升机是如何翻转飞行的，但是如何使用这些数据来评价一个控制直升机稳定平飞的新控制器呢？目前还不清楚。因此，如果，新控制器控制直升机飞行的方式与之前的不同，那么对于每个新的控制器，我们都需要收集新的数据来进行测试。这个性质使得增强学习相较于有监督学习需要更多的数据。本论文的目标之一就是探索如何在增强学习中高效地重用数据，并且尝试在实用的学习算法中利用这些思想。&lt;/p&gt;&lt;p&gt;最后，增强学习一个常见主题是“不可知论学习（agnostic learning）”（在人工智能领域中，这和界限最优化（bounded optimality）联系紧密）。这个主题是指限制可能的分类器集合的思想，这一思想通常被采用。以上文的心脏病举例来说，相较于考虑所有的将病人的特征映射到{患病，没患病}的函数（这将会是一个巨大的函数空间），我们可以将注意力集中在更小一点的函数集中，比如所有的阈值线性函数，或者所有中等尺寸的神经网络。这样就可以显著地降低我们需要考虑的分类器的数量。如果分辨病人是否患病的“真实”的决策边界极端复杂，以至于没有神经网络能够准确预测分类，那么既然我们已经是将注意力限制在了用神经网络表达的函数上，这就说明我们没法找到一个好的分类器。但是如果决策边界不是那么复杂，那么我们限制的的分类器集合就允许一个分类器展示出只需要少量的训练数据就能够很好地拟合一个神经网络。具体来说，就是训练数据量的需求是由神经网络中“自由参数（free parameter）”的数量决定的，而不是以输入的病人和心脏病的发生的复杂度来决定的。基于在有监督学习中数据可以重用的事实，这些结果是可以证明的。在第四章中，我们将把这些结论一般化到增强学习中，观察针对增强学习问题，通过将我们的注意力限制到一个较小的控制器集合中，我们依旧可以得到类似的结论，即想要算法较好地学习，需要对样本尺寸做出限制。&lt;/p&gt;&lt;h2&gt;1.3 论文概要及贡献&lt;/h2&gt;&lt;blockquote&gt;译者注：意义不大，略过。感兴趣的知友请自行阅读。&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;第一章原文翻译完毕。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;小结&lt;/h2&gt;&lt;p&gt;翻译本章，主要面向对增强学习没有概念的知友。在第一章中，吴恩达使用无人直升机的例子比较形象直观地介绍了增强学习问题。&lt;b&gt;读者主要理解该例，其他部分适当了解即可&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;在下篇中&lt;/b&gt;，我将编译论文的第二章，其内容主要是用数学公式严谨地对马尔科夫决策过程和增强学习做出定义和讲解。&lt;/p&gt;&lt;h2&gt;读者反馈&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;持续有知友问如何获取cs231n的资源，逐个回复太累，我在“智能单元”微信公众号上做了自动回复，请有需求的知友对公众号回复cs231n或者CS231n即可，也算是给公众号涨点关注。&lt;/li&gt;&lt;li&gt;欢迎想要学习CS294：深度增强学习的知友根据课程推荐材料学习的同时，进行翻译或者编译，本专栏接受投稿。&lt;/li&gt;&lt;li&gt;一如既往地欢迎大家对内容进行批评指正，贡献者我都会在文末更新感谢：）&lt;/li&gt;&lt;/ul&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24761972&amp;pixel&amp;useReferer"/&gt;</description><author>杜客</author><pubDate>Mon, 09 Jan 2017 11:24:14 GMT</pubDate></item><item><title>CS 294：深度增强学习，2017年春季学期</title><link>https://zhuanlan.zhihu.com/p/24721292</link><description>&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：本文编译自&lt;a href="http://rll.berkeley.edu/deeprlcourse/#syllabus" data-title="伯克利大学2017年春季学期课程CS294介绍页面" class="" data-editable="true"&gt;伯克利大学2017年春季学期课程CS294介绍页面&lt;/a&gt;。该课程主题选择深度增强学习，即紧跟当前人工智能研究的热点，又可作为深度学习的后续方向，&lt;b&gt;值得推荐&lt;/b&gt;。&lt;/blockquote&gt;&lt;h2&gt;课程时间&lt;/h2&gt;&lt;p&gt;2017年1月18日至5月3日。&lt;/p&gt;&lt;h2&gt;课程前置要求&lt;/h2&gt;&lt;p&gt;学习该课程，会假设学员对于增强学习，最优化方法和机器学习这些知识背景比较熟悉。要是学员对于这些内容不太了解，就需要根据提供的参考资料补习以下的知识点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;增强学习和马尔科夫决策过程（MDPs）&lt;/li&gt;&lt;ul&gt;&lt;li&gt;MDPs的定义&lt;/li&gt;&lt;li&gt;具体算法：策略迭代和价值迭代&lt;/li&gt;&lt;li&gt;搜索算法&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;数值最优化方法&lt;/li&gt;&lt;ul&gt;&lt;li&gt;梯度下降和随机梯度下降&lt;/li&gt;&lt;li&gt;反向传播算法&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;机器学习&lt;/li&gt;&lt;ul&gt;&lt;li&gt;分类和回归问题：用什么样的损失函数，如何拟合线性或非线性模型&lt;/li&gt;&lt;li&gt;训练/测试误差，过拟合&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：在2016年跟着专栏翻译的CS231n课程笔记学习的知友可以发现，缺少的知识点只有增强学习和马尔科夫决策过程，学习这门课的难度降低。&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;对于增强学习和MDPs的介绍材料有&lt;/b&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://ai.berkeley.edu/" data-editable="true" data-title="CS188 EdX course"&gt;CS188 EdX course&lt;/a&gt;，从马尔科夫决策过程第一部分开始。&lt;/li&gt;&lt;li&gt;&lt;a href="http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html" data-editable="true" data-title="Sutton &amp;amp; Barto" class=""&gt;Sutton &amp;amp; Barto&lt;/a&gt;的著作，学习第3章和第4章。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：该著作&lt;b&gt;重要&lt;/b&gt;，建议中文母语学习者打印该书，方便查阅、学习与装逼：）&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;对MDPs的简洁介绍，可以参考&lt;a href="http://rll.berkeley.edu/deeprlcourse/docs/ng-thesis.pdf" data-editable="true" data-title="吴恩达这篇论文" class=""&gt;吴恩达这篇论文&lt;/a&gt;的第1章和第2章。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：不想看大部头的著作就看这篇，简明扼要方便理解。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;David Silver的课程，下文有链接。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：要是看完并基本掌握了David Silver的课程，这门课也就是看看了。&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;对于机器学习和神经网络的介绍材料有&lt;/b&gt;：&lt;/p&gt;&lt;li&gt;&lt;a href="http://cs231n.github.io/" data-editable="true" data-title="Andrej Karpathy的课程" class=""&gt;Andrej Karpathy的课程&lt;/a&gt;。&lt;/li&gt;&lt;blockquote&gt;译者注：也就是CS231n了，算成是AK的也不太妥当，毕竟老板是李大姐，讲师还有Justin 。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://www.coursera.org/course/neuralnets" data-editable="true" data-title="Coursera上Hinton大爷的课程"&gt;Coursera上&lt;b&gt;Hinton&lt;/b&gt;大爷的课程&lt;/a&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：拜拜亨大爷总是安心些。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://www.coursera.org/learn/machine-learning/" data-editable="true" data-title="Coursera上吴恩达的课程"&gt;Coursera上吴恩达的课程&lt;/a&gt;。&lt;/li&gt;&lt;li&gt;&lt;a href="https://work.caltech.edu/telecourse.html" data-editable="true" data-title="Yaser Abu-Mostafa’s course" class=""&gt;Yaser Abu-Mostafa的课程&lt;/a&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：本人没有看过这个课程，有学习过的知友可以在评论中简单谈谈感受。&lt;/blockquote&gt;&lt;h2&gt;课程安排&lt;/h2&gt;&lt;p&gt;如下图所示，课件和参考材料会随着课程进度发布。这篇翻译的内容也会同步更新。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-1c31c9c163da64f7d29f14d48965abbe.jpg" data-rawwidth="1938" data-rawheight="1406"&gt;&lt;h2&gt;课程视频&lt;/h2&gt;&lt;p&gt;原文展示了2015年的4个课程视频，在Youtube上，清晰度很低，这里就不放出了，感兴趣的知友自行查看原文。随着课程进展，本部分也更新2017年的课程视频链接。&lt;/p&gt;&lt;h2&gt;相关课程&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" data-editable="true" data-title="David Silver关于增强学习的课程及课程视频" class=""&gt;David Silver关于增强学习的课程及课程视频&lt;/a&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：&lt;b&gt;重要&lt;/b&gt;。&lt;/blockquote&gt;&lt;li&gt;&lt;a href="https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/" data-editable="true" data-title="Nando de Freitas关于机器学习的课程" class=""&gt;Nando de Freitas关于机器学习的课程&lt;/a&gt;&lt;/li&gt;&lt;blockquote&gt;译者注：没必要，看吴恩达的课程。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://cs231n.github.io/" data-editable="true" data-title="Andrej Karpathy的课程" class=""&gt;Andrej Karpathy的课程&lt;/a&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：去年我们一直在推荐，不赘述了。&lt;/blockquote&gt;&lt;h2&gt;相关书籍&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html" data-editable="true" data-title="Sutton和Barto的《Reinforcement Learning: An Introduction》" class=""&gt;Sutton和Barto的Reinforcement Learning: An Introduction&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：&lt;b&gt;重要&lt;/b&gt;，系统学习的话就打印吧。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://www.ualberta.ca/~szepesva/RLBook.html" data-editable="true" data-title="Szepesvari, Algorithms for Reinforcement Learning" class=""&gt;Szepesvari, Algorithms for Reinforcement Learning&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://www.athenasc.com/dpbook.html" data-editable="true" data-title="Bertsekas, Dynamic Programming and Optimal Control, Vols I and II" class=""&gt;Bertsekas, Dynamic Programming and Optimal Control, Vols I and II&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471727822.html" data-editable="true" data-title="Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming" class=""&gt;Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://adp.princeton.edu/" data-editable="true" data-title="Powell, Approximate Dynamic Programming" class=""&gt;Powell, Approximate Dynamic Programming&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;泛读链接&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://rll.berkeley.edu/deeprlcourse/#syllabus" data-editable="true" data-title="深度学习资源的集锦"&gt;深度学习资源的集锦&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：收藏了你也不会看的，但是可以装逼：）&lt;/blockquote&gt;&lt;h2&gt;之前课程&lt;/h2&gt;&lt;p&gt;2015年秋季开过同名课程，介绍链接&lt;a href="http://rll.berkeley.edu/deeprlcourse-fa15/" data-title="点击这里" class="" data-editable="true"&gt;点击这里&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;课程简介翻译完毕&lt;/b&gt;。&lt;/p&gt;&lt;h2&gt;学习建议&lt;/h2&gt;&lt;p&gt;想要学习深度增强学习的知友，在我个人的看来，可能是以下几种情况：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;看到了深度增强学习的前景，想要提前布局，提高自身价值；&lt;/li&gt;&lt;li&gt;学完了深度学习，想要继续学习人工智能领域其他内容；&lt;/li&gt;&lt;li&gt;大公司或者拿了投资的创业公司的项目组想要搞相关应用，比如BetaGo或者GammaGo：）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;都挺好的，学吧！专栏成立的初心之一，就是促进这个领域的学习和交流。个人也会以这次课程为抓手，再系统性地把深度增强学习给学扎实。后续会以该课程学习为主题，在专栏进行相关内容的写作，也欢迎大家针对课程学习内容进行讨论。&lt;/p&gt;&lt;p&gt;如果是因为近几年人工智能突然火热起来想要学习的知友，需要知道人工智能领域历史上起起落落好几次，入坑前想好自己是不是真的感兴趣。&lt;/p&gt;&lt;p&gt;如果上文中的课程前置要求中三方面知识点都不太熟悉，也不建议直接学习该课程。建议&lt;b&gt;先看吴恩达或者Hinton的课程，然后看CS231n，然后再来学习这门课程&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;最后照例&lt;b&gt;打鸡血：&lt;/b&gt;虽说人工智能起起落落好几次，但是这次确实是解决了之前没有解决的问题，突破了之前没有突破的水平，不是吗？相较于“呵呵，根据历史规律，人工智能也就还能火几年，步子太大要扯到蛋”的态度，我个人更倾向：&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;I am looking at the future with concern, but with good hope&lt;/b&gt;. &lt;b&gt;-Albert Schweitzer&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;----------------------------------------------------------------------------------------&lt;/p&gt;&lt;p&gt;PS：我们开通了&lt;b&gt;智能单元微信公众号&lt;/b&gt;，搜索“&lt;b&gt;智能单元&lt;/b&gt;”就能找到，内容上会和专栏差异化。可以的话请大家关注支持一个，算是对我的鼓励，先谢过啦：）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24721292&amp;pixel&amp;useReferer"/&gt;</description><author>杜客</author><pubDate>Thu, 05 Jan 2017 10:26:25 GMT</pubDate></item><item><title>Master 横扫围棋各路高手，是时候全面研究通用人工智能了！</title><link>https://zhuanlan.zhihu.com/p/24709235</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-eab9cfb4a3d814feb1e92cc1b69db9aa_r.png"&gt;&lt;/p&gt;在写这篇文章的时候，聂卫平将挑战Master，不出意外，老聂也只能扑街。&lt;p&gt;有的人说，汽车早就比人跑得快，人也不会惊慌，围棋AI战胜人类，也是迟早的事，同样不必惊慌。&lt;/p&gt;&lt;p&gt;是这样吗？&lt;/p&gt;&lt;p&gt;不是的。&lt;/p&gt;&lt;p&gt;猎豹本来就比人类跑得快，但人类仍然是地球的主宰。为什么？因为人类引以为傲的智慧！&lt;/p&gt;&lt;p&gt;因为人类拥有地球生物中最高的智能，所以虽然人的肉体机能很有限，但是不妨碍人类去创造各种辅助人类的工具。&lt;/p&gt;&lt;p&gt;然而，人工智能，却是一个和汽车，飞机完全不同的东西！我们人类在试图制造超越人类智慧的东西！&lt;/p&gt;&lt;p&gt;当AlphaGo战胜李世石，当Master豪取50连胜的时候，很多人会产生一种对智能的恐惧。相信那些真正和Master战斗的棋手们会更深有体会。&lt;/p&gt;&lt;p&gt;为什么？我们的智慧被超越！我们以前的认识被完全打破了。以至于当今围棋第一人柯洁说出“我们对围棋的理解都是错的”这样的话。&lt;/p&gt;&lt;p&gt;人们也很容易的想到《三体》中水滴入侵人类舰队的那一刻。&lt;/p&gt;&lt;p&gt;那种感觉是何曾的相同。&lt;/p&gt;&lt;p&gt;让人绝望而恐惧！&lt;/p&gt;&lt;p&gt;有人会说，n年前国际象棋就被攻克了，没什么大不了。&lt;/p&gt;&lt;p&gt;但是，&lt;b&gt;这次真的不一样！这次真的不一样！这次真的不一样！&lt;/b&gt;（重要的事情说三篇）&lt;/p&gt;&lt;p&gt;如果你懂深度学习，如果你懂AlphaGo，你就会明白，AlphaGo不再是依靠蛮力计算，而主要是靠深度神经网络，靠增强学习自我学习。&lt;/p&gt;&lt;p&gt;深度神经网络是个黑箱。输入棋局，通过神经网络，输出对棋局的判断。AlphaGo只要这么简单，智能就在神经网络当中。&lt;/p&gt;&lt;p&gt;这次是围棋，完全可观察，如果下次就是&lt;b&gt;星际争霸&lt;/b&gt;，那么大家会作何感想呢？&lt;/p&gt;&lt;p&gt;深度学习正在变革所有行业，AlphaGo则开启了通用人工智能的大门！&lt;/p&gt;&lt;h2&gt;通用人工智能是什么？&lt;/h2&gt;&lt;p&gt;通用人工智能（General Artificial Intelligence），是指能通过自我学习解决各种问题的智能算法。人类的大脑就是一种通用智能，因为人既可以学游泳，也可以学下棋。开发AlphaGo的DeepMind就是这么一家公司，以实现通用人工智能为目标。&lt;/p&gt;&lt;p&gt;通用人工智能并不是等价于类人智能。但解决了通用人工智能，类人智能也必然能够达到。&lt;/p&gt;&lt;p&gt;AlphaGo的算法就是典型的通用人工智能算法，核心使用了深度学习（Deep Learning），增强学习（Reinforcement Learning）。而深度增强学习（Deep Reinforcement Learning），就是通用人工智能算法的具体表现形式。什么叫通用？就是这个算法既可以训练用来下围棋，也可以训练用来开车，还可以训练用来股票交易。&lt;/p&gt;&lt;p&gt;有人说把围棋的路数从19路变成21路，AlphaGo就没辙了。这是没错。但是DeepMind很容易就可以训练一个适应多个路数的AlphaGo。都只是时间问题。&lt;/p&gt;&lt;p&gt;通用人工智能算法就是倚天剑，屠龙刀！算法在手，任何问题都可以尝试去解决！&lt;/p&gt;&lt;p&gt;目前国内的研究还相当少，但是，真的&lt;/p&gt;&lt;p&gt;&lt;b&gt;是时候全面研究通用人工智能了！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;DeepMind和OpenAI正在大力发展，这个才是真正掀起人工智能革命的关键！中国在这一块如果落后的话会非常致命！必须庆幸DeepMind和OpenAI还一直公开他们的论文！&lt;/p&gt;&lt;p&gt;我们从DeepMind和OpenAI研究的方向就知道应该做什么了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1 Deep Reinforcement Learning深度增强学习，用于构造学习机制&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2 Deep Generative Model深度生成模型，用于理解信息，可以用于预测规划&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;3 Neural Memory神经网络记忆，用于存储信息和推理&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;4 One Shot Learning 一眼学习，用于快速学习&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;5 Deep Transfer Learning 深度迁移学习，用于移植知识&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;以上多点的综合运用，必将可以制造更强大的通用人工智能算法！而这些方向的研究，都越来越接近人类大脑的本质，或者说智能的本质！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;要不畏惧人工智能，那只有理解并掌握人工智能！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;（本文算作一种呼吁，呼吁更多的人工智能研究人员投入到通用人工智能当中。）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;---------------------------------------&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本专栏将全面聚焦通用人工智能算法！&lt;/p&gt;&lt;p&gt;最后，加一个推广！为了使关注通用人工智能的朋友们能够更及时获取通用人工智能的前沿资讯，我们开通了 &lt;b&gt;智能单元 微信公众号&lt;/b&gt;！第一时间为大家推送最新资讯，并且不定期推送纯原创最新通用人工智能进展的解读！&lt;/p&gt;&lt;p&gt;与大家一起学习，一起进步！&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-864ef735513a261c8140410e43436050.png" data-rawwidth="180" data-rawheight="320"&gt;欢迎在微信上搜索“智能单元” 公众号关注！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24709235&amp;pixel&amp;useReferer"/&gt;</description><author>Flood Sung</author><pubDate>Wed, 04 Jan 2017 15:05:39 GMT</pubDate></item></channel></rss>