<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>智能单元 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/intelligentunit</link><description>面向通用人工智能和机器人学习，聚焦深度增强学习，可微神经计算机和生成对抗模型。</description><lastBuildDate>Mon, 16 Jan 2017 06:15:09 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Flood Sung的Live--从AlphaGo看人工智能前沿技术</title><link>https://zhuanlan.zhihu.com/p/24816290</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-3ce09841f5ec27b18afa2f741512b78e_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;大家好！我是 Flood Sung ，研究通用人工智能与机器人学习，&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" class="" data-editable="true" data-title="「智能单元」知乎专栏"&gt;「智能单元」知乎专栏&lt;/a&gt;作者之一。&lt;/p&gt;&lt;p&gt;        深度增强学习（ Deep Reinforcement Learning ） 2013 年就被 DeepMind 提出，然而被关注度非常低。直到去年的 AlphaGo ，才使得深度增强学习真正火起来。深度增强学习是 AlphaGo 的核心技术，是 AlphaGo 能够实现自我学习的关键。&lt;/p&gt;&lt;p&gt;        当前，深度增强学习已成为人工智能领域最重要的研究分支之一，该技术能够使计算机通过&lt;b&gt;深度神经网络&lt;/b&gt;处理&lt;b&gt;从感知到决策控制&lt;/b&gt;的问题，无论是&lt;b&gt;下棋&lt;/b&gt;、&lt;b&gt;金融决策&lt;/b&gt;、&lt;b&gt;医疗诊断&lt;/b&gt;还是&lt;b&gt;机器人控制&lt;/b&gt;，都能派上用场，具有超强的应用前景，是未来的&lt;b&gt;革命性技术&lt;/b&gt;，值得我们每一个人关注！&lt;/p&gt;&lt;h2&gt;2 Live主题与内容&lt;/h2&gt;&lt;p&gt;&lt;b&gt;&lt;a href="https://www.zhihu.com/lives/802155571712253952?utm_campaign=zhihulive&amp;amp;utm_source=zhihucolumn&amp;amp;utm_medium=Livecolumn" class="" data-editable="true" data-title="Live入口：请点击进入"&gt;Live入口：请点击进入&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;        Live时间：&lt;/b&gt;&lt;b&gt;2017-01-20 20:00&lt;/b&gt;&lt;/p&gt;&lt;p&gt;        本次Live的主题是 &lt;b&gt;AlphaGo&lt;/b&gt; 与&lt;b&gt;深度增强学习&lt;/b&gt;，想讲给对人工智能前沿技术感兴趣，对 AlphaGo 的核心技术感兴趣的朋友！&lt;/p&gt;&lt;p&gt;        在Live中我将从 AlphaGo 出发，为大家介绍 AlphaGo 的核心技术—深度增强学习的基本原理与方法，分析 AlphaGo 能够在自对弈中不断学习的原因，让大家了解深度增强学习这个方法的通用性和革命性！ &lt;/p&gt;&lt;p&gt;&lt;b&gt; 本次 Live 主要包括以下问题：&lt;/b&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;＊ AlphaGo 为什么能在围棋上取得如此重大的突破？ &lt;/p&gt;&lt;p&gt;＊ 深度增强学习是什么，有怎样的发展历史？&lt;/p&gt;&lt;p&gt; ＊ 深度增强学习与通用人工智能有什么关系？ &lt;/p&gt;&lt;p&gt;＊ 深度增强学习拥有怎样的核心思想？ &lt;/p&gt;&lt;p&gt;＊ AlphaGo 自对弈中不断学习的方法是什么？ &lt;/p&gt;&lt;p&gt;＊ 深度增强学习未来的应用和发展前景是什么？&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;期待大家的参与！&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24816290&amp;pixel&amp;useReferer"/&gt;</description><author>Flood Sung</author><pubDate>Tue, 10 Jan 2017 18:26:59 GMT</pubDate></item><item><title>吴恩达对于增强学习的形象论述（上）</title><link>https://zhuanlan.zhihu.com/p/24761972</link><description>&lt;b&gt;版权声明：本文&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" data-editable="true" data-title="智能单元"&gt;智能单元&lt;/a&gt;首发，本人原创，禁止未授权转载。&lt;/b&gt;&lt;p&gt;&lt;b&gt;前言：&lt;/b&gt;吴恩达在2003年为完成博士学位要求做了专题论文：&lt;a href="http://rll.berkeley.edu/deeprlcourse/docs/ng-thesis.pdf" data-title="Shaping and policy search in Reinforcement learning" class="" data-editable="true"&gt;Shaping and policy search in Reinforcement learning&lt;/a&gt;，其第一、二章被&lt;a href="https://zhuanlan.zhihu.com/p/24721292?refer=intelligentunit" data-title="伯克利CS294：深度增强学习课程" class="" data-editable="true"&gt;伯克利CS294：深度增强学习课程&lt;/a&gt;作为推荐材料。本文基于笔者的理解，对第一章做有选择的编译与注释。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;第一章 简介&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在本章中，我们将对本论文中需要考虑的增强学习框架给出一个非正式的，不涉及数学形式的综述。同时还将描述一些增强学习中的问题，这些问题是我们需要尝试解决的。最后，给出整个专题论文的概要。&lt;/p&gt;&lt;h2&gt;1.1 对增强学习的介绍&lt;/h2&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-5e4cda0cb4d5455d8a410d10ed04b8fb.png" data-rawwidth="994" data-rawheight="678"&gt;&lt;i&gt;图 1.1 伯克利大学的无人直升机&lt;/i&gt;&lt;/p&gt;&lt;p&gt;给一个像图1.1中那样的直升机，我们如何才能学习，或者说自动地设计一个控制器，使得直升机能够正常飞行呢？&lt;/p&gt;&lt;p&gt;人工智能和控制中的一个基础问题就是在随机系统中进行序列决策。飞行中的直升机就是随机系统的一个很好例子，因为它展现出随机和不可预测的行为，而大风和其他类似的干扰可能导致它的运动偏离预期。直升机的控制也是一个序列决策问题，控制直升机需要连续地决策向着哪个方向推操纵杆。比起那些只需要针对一个情况及时作出一个正确决策的问题，本问题展现出了所谓的“&lt;i&gt;&lt;b&gt;延迟后果（delayed consequences）&lt;/b&gt;&lt;/i&gt;”性质，解决问题的难度可谓是大为增加。所谓延迟后果，就是说直升机的自动控制器的水平好坏是根据它的长期表现来决定的，比如假设它现在做出了一个错误的操作，直升机并不会马上坠毁，可能依然能够飞行很多秒。导致直升机控制问题难度增大的另一个方面是它的&lt;b&gt;局部可观测性&lt;/b&gt;。具体来说，就是我们不能够精确地观测到直升机的位置/状态；但是，即便是面对系统状态的不确定性，我们仍然在每一秒都需要计算出正确的控制指令，使得直升机能够在空中正常飞行。&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：吴恩达擅长将一个问题通过比喻和举例的方式讲得通俗易懂，这是教学者的金钥匙。无人机控制这个例子里面包含了好几个马尔科夫决策过程和增强学习里面的用数学公式表达起来比较抽象的性质，待看到公式的时候，回头想这个例子，可以很好地帮助理解记忆。&lt;/blockquote&gt;&lt;p&gt;我们将马尔科夫决策过程（MDP）框架的公式表达推迟到第二章再讲。简单地来说，有的系统（就好比无人直升机）的控制是在每个时间点都会处于某种“状态”，我们一般对于这种系统比较感兴趣，而马尔科夫决策过程就是对这种系统进行建模。例如直升机的状态也许就可以用它的位置和方向来表示。我们的任务就是选择动作，使得系统能够倾向于保持在“好”的状态中，比如保持悬停，并且能够避免“坏”的状态，比如坠机。数量巨大且不同的问题都可以用马尔科夫决策过程形式来进行建模。比如规划和机器人导航，库存管理，机器维护，网络路由，电梯控制和搭建推荐系统。&lt;/p&gt;&lt;p&gt;增强学习针对解决MDP形式的问题给出了一系列的工具。虽然它获得了巨大成功，但是在解决很多问题时仍面临困难，还存在很多问题和挑战。我们简要地描述其中一些问题，这些问题会让某些增强学习问题具有挑战性：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;首先，存在&lt;b&gt;高维度问题&lt;/b&gt;。具体说来，就是基于离散的简单增强学习算法，经常会遇到状态变量的数量成指数增长的情况。这个问题就是所谓的“维度诅咒”，我们将在第二章中更详细地讨论。我们能够设计出一个即能可证地有效运作，又能更好地扩展到搞维度问题的实用算法吗？&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：现在高维度问题对深度增强学习已经基本上不是问题。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;同时，如何选择“&lt;b&gt;回馈函数（reward function）&lt;/b&gt;”也是一个问题。在增强学习中，设计者必须指明一个函数，该函数能够告诉我们直升飞机什么时候飞得好，什么时候是飞得不好。我们在选择这个函数的时候有很大的自由度，在第三章中还会看到，如果选择得当，某些函数能够成数量级地加速学习过程。当然同时也存在一些看起来不错，但是实际上让控制器的表现非常糟糕的函数。我们能够合理地选择回馈函数，既避免这种问题，又能让增强学习算法学习得又快又好吗？&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：在后续的学习实践中会常常接触，现在不明白不用担心。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;局部可观测性&lt;/b&gt;（&lt;b&gt;Partial Observability&lt;/b&gt;）是指被控制的系统的状态不能被精确观察的情况，比如直升机上的传感器只能是近似地测量直升机的位置。局部可观测性让问题的解决更加困难了，许多标准的增强学习算法不能解决这种情况，有的即使能够解决，也非常艰难。那么，如果只能近似地观察系统在做什么，我们该如何选择正确的控制呢？&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在本论文中，我们提出了一些方法，尝试解决前两个问题。我们的最终算法在局部可观测的情况下也能很好地工作，算法被运用到了图1.1中的无人直升机控制中。在实践的过程中，我们还涉及了经典控制理论中的一些专题，比如系统识别与验证等。&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：看这篇论文的&lt;b&gt;目的是帮助我们熟悉增强学习和马尔科夫决策过程&lt;/b&gt;，为CS294的学习&lt;b&gt;做知识预习&lt;/b&gt;的，没必要看完整个论文（150多页）来搞明白当年吴恩达的算法是啥情况。&lt;/blockquote&gt;&lt;h2&gt;1.2 与有监督学习的比较&lt;/h2&gt;&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：其实个人不太愿意翻译这段，因为对于学习深度增强学习，这个学术和理论味儿比较浓的小节没有太大意义。类似的情况在增强学习的著作&lt;a href="http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html" data-title="Reinforcement Learning: An Introduction" class="" data-editable="true"&gt;Reinforcement Learning: An Introduction&lt;/a&gt;中也有，作者花了很多篇幅去论证增强学习与有监督学习、无监督学习是不同的，应该与之并列什么的。&lt;b&gt;个人意见：虽然翻译了，可略过&lt;/b&gt;。&lt;/blockquote&gt;&lt;p&gt;有监督学习是人工智能领域中另一类标准问题。它可以看成是增强学习的某种特殊形式，在这种形式下，只需要对系统进行一次控制，因此我们只需要一次决策，而不是连续的序列决策。虽然看起来差别不是很大，但是实际上这让有监督学习变成了一个非常简单的问题。&lt;/p&gt;&lt;p&gt;举个具体例子，考虑根据给出的病人的多种检查数据或“特征”（比如心率、体温、多种医学检查结果），用有监督学习算法来预测一个病人是否患有心脏病。这里假设我们拥有一个训练集，其中包含有一些病人特征的样本，以及指明其中哪些病人是否患有心脏病的信息。我们可以用有监督学习算法来让一些函数（线性映射或者神经网络）来对数据进行拟合。当一个新的病人来就诊时，我们可以根据病人的特征，使用这个拟合的函数来预测他是否患有心脏病。而当这个一眼的诊断出来后，我们的病人就要去面对他的命运了。如果我们预测出错（比如我们决定赶紧对病人进行手术，而接下来的操作发现病人实际上没有任何问题，手术完全没有必要），那么我们也能马上观察到结果，并从这个结果中继续学习。&lt;/p&gt;&lt;p&gt;然而，在增强学习中，我们动作的结果通常是有延迟的，因此想要识别并从动作的长期效果中学习变得更加困难了。例如下棋，如果我们在第63手的时候输了（或者赢了），可能非常重要的一点是需要认识到我们在第17手的时候下的一记妙手奠定了胜局。这个“可信度分配（credit assignment）”问题让算法从过去的失误中吸取教训或从过去的成功中学习经验都更加困难了。&lt;/p&gt;&lt;p&gt;其次，在增强学习问题中的连续环境让算法重用（reuse）数据变得更加困难。在有监督学习中，如果我们预先收集并且存储了一些病人的样本数据集，并且想要测试一个新的神经网络在心脏病预测方面的性能，那么我们可以很容易地在这些数据集上进行测试，并分析结果与真实情况之间的差异，从而得出新模型的性能好坏。然而在增强学习中，假设我们也预先测试了一个控制直升机翻转飞行的控制器（或者举个更自然的例子，控制直升机稍微向右倾斜的控制器），在测试期间收集的数据是可以让我们知道直升机是如何翻转飞行的，但是如何使用这些数据来评价一个控制直升机稳定平飞的新控制器呢？目前还不清楚。因此，如果，新控制器控制直升机飞行的方式与之前的不同，那么对于每个新的控制器，我们都需要收集新的数据来进行测试。这个性质使得增强学习相较于有监督学习需要更多的数据。本论文的目标之一就是探索如何在增强学习中高效地重用数据，并且尝试在实用的学习算法中利用这些思想。&lt;/p&gt;&lt;p&gt;最后，增强学习一个常见主题是“不可知论学习（agnostic learning）”（在人工智能领域中，这和界限最优化（bounded optimality）联系紧密）。这个主题是指限制可能的分类器集合的思想，这一思想通常被采用。以上文的心脏病举例来说，相较于考虑所有的将病人的特征映射到{患病，没患病}的函数（这将会是一个巨大的函数空间），我们可以将注意力集中在更小一点的函数集中，比如所有的阈值线性函数，或者所有中等尺寸的神经网络。这样就可以显著地降低我们需要考虑的分类器的数量。如果分辨病人是否患病的“真实”的决策边界极端复杂，以至于没有神经网络能够准确预测分类，那么既然我们已经是将注意力限制在了用神经网络表达的函数上，这就说明我们没法找到一个好的分类器。但是如果决策边界不是那么复杂，那么我们限制的的分类器集合就允许一个分类器展示出只需要少量的训练数据就能够很好地拟合一个神经网络。具体来说，就是训练数据量的需求是由神经网络中“自由参数（free parameter）”的数量决定的，而不是以输入的病人和心脏病的发生的复杂度来决定的。基于在有监督学习中数据可以重用的事实，这些结果是可以证明的。在第四章中，我们将把这些结论一般化到增强学习中，观察针对增强学习问题，通过将我们的注意力限制到一个较小的控制器集合中，我们依旧可以得到类似的结论，即想要算法较好地学习，需要对样本尺寸做出限制。&lt;/p&gt;&lt;h2&gt;1.3 论文概要及贡献&lt;/h2&gt;&lt;blockquote&gt;译者注：意义不大，略过。感兴趣的知友请自行阅读。&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;第一章原文翻译完毕。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;小结&lt;/h2&gt;&lt;p&gt;翻译本章，主要面向对增强学习没有概念的知友。在第一章中，吴恩达使用无人直升机的例子比较形象直观地介绍了增强学习问题。&lt;b&gt;读者主要理解该例，其他部分适当了解即可&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;在下篇中&lt;/b&gt;，我将编译论文的第二章，其内容主要是用数学公式严谨地对马尔科夫决策过程和增强学习做出定义和讲解。&lt;/p&gt;&lt;h2&gt;读者反馈&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;持续有知友问如何获取cs231n的资源，逐个回复太累，我在“智能单元”微信公众号上做了自动回复，请有需求的知友对公众号回复cs231n或者CS231n即可，也算是给公众号涨点关注。&lt;/li&gt;&lt;li&gt;欢迎想要学习CS294：深度增强学习的知友根据课程推荐材料学习的同时，进行翻译或者编译，本专栏接受投稿。&lt;/li&gt;&lt;li&gt;一如既往地欢迎大家对内容进行批评指正，贡献者我都会在文末更新感谢：）&lt;/li&gt;&lt;/ul&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24761972&amp;pixel&amp;useReferer"/&gt;</description><author>杜客</author><pubDate>Mon, 09 Jan 2017 11:24:14 GMT</pubDate></item><item><title>CS 294：深度增强学习，2017年春季学期</title><link>https://zhuanlan.zhihu.com/p/24721292</link><description>&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：本文编译自&lt;a href="http://rll.berkeley.edu/deeprlcourse/#syllabus" data-title="伯克利大学2017年春季学期课程CS294介绍页面" class="" data-editable="true"&gt;伯克利大学2017年春季学期课程CS294介绍页面&lt;/a&gt;。该课程主题选择深度增强学习，即紧跟当前人工智能研究的热点，又可作为深度学习的后续方向，&lt;b&gt;值得推荐&lt;/b&gt;。&lt;/blockquote&gt;&lt;h2&gt;课程时间&lt;/h2&gt;&lt;p&gt;2017年1月18日至5月3日。&lt;/p&gt;&lt;h2&gt;课程前置要求&lt;/h2&gt;&lt;p&gt;学习该课程，会假设学员对于增强学习，最优化方法和机器学习这些知识背景比较熟悉。要是学员对于这些内容不太了解，就需要根据提供的参考资料补习以下的知识点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;增强学习和马尔科夫决策过程（MDPs）&lt;/li&gt;&lt;ul&gt;&lt;li&gt;MDPs的定义&lt;/li&gt;&lt;li&gt;具体算法：策略迭代和价值迭代&lt;/li&gt;&lt;li&gt;搜索算法&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;数值最优化方法&lt;/li&gt;&lt;ul&gt;&lt;li&gt;梯度下降和随机梯度下降&lt;/li&gt;&lt;li&gt;反向传播算法&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;机器学习&lt;/li&gt;&lt;ul&gt;&lt;li&gt;分类和回归问题：用什么样的损失函数，如何拟合线性或非线性模型&lt;/li&gt;&lt;li&gt;训练/测试误差，过拟合&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：在2016年跟着专栏翻译的CS231n课程笔记学习的知友可以发现，缺少的知识点只有增强学习和马尔科夫决策过程，学习这门课的难度降低。&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;对于增强学习和MDPs的介绍材料有&lt;/b&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://ai.berkeley.edu/" data-editable="true" data-title="CS188 EdX course"&gt;CS188 EdX course&lt;/a&gt;，从马尔科夫决策过程第一部分开始。&lt;/li&gt;&lt;li&gt;&lt;a href="http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html" data-editable="true" data-title="Sutton &amp;amp; Barto" class=""&gt;Sutton &amp;amp; Barto&lt;/a&gt;的著作，学习第3章和第4章。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：该著作&lt;b&gt;重要&lt;/b&gt;，建议中文母语学习者打印该书，方便查阅、学习与装逼：）&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;对MDPs的简洁介绍，可以参考&lt;a href="http://rll.berkeley.edu/deeprlcourse/docs/ng-thesis.pdf" data-editable="true" data-title="吴恩达这篇论文" class=""&gt;吴恩达这篇论文&lt;/a&gt;的第1章和第2章。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：不想看大部头的著作就看这篇，简明扼要方便理解。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;David Silver的课程，下文有链接。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：要是看完并基本掌握了David Silver的课程，这门课也就是看看了。&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;对于机器学习和神经网络的介绍材料有&lt;/b&gt;：&lt;/p&gt;&lt;li&gt;&lt;a href="http://cs231n.github.io/" data-editable="true" data-title="Andrej Karpathy的课程" class=""&gt;Andrej Karpathy的课程&lt;/a&gt;。&lt;/li&gt;&lt;blockquote&gt;译者注：也就是CS231n了，算成是AK的也不太妥当，毕竟老板是李大姐，讲师还有Justin 。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://www.coursera.org/course/neuralnets" data-editable="true" data-title="Coursera上Hinton大爷的课程"&gt;Coursera上&lt;b&gt;Hinton&lt;/b&gt;大爷的课程&lt;/a&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：拜拜亨大爷总是安心些。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://www.coursera.org/learn/machine-learning/" data-editable="true" data-title="Coursera上吴恩达的课程"&gt;Coursera上吴恩达的课程&lt;/a&gt;。&lt;/li&gt;&lt;li&gt;&lt;a href="https://work.caltech.edu/telecourse.html" data-editable="true" data-title="Yaser Abu-Mostafa’s course" class=""&gt;Yaser Abu-Mostafa的课程&lt;/a&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：本人没有看过这个课程，有学习过的知友可以在评论中简单谈谈感受。&lt;/blockquote&gt;&lt;h2&gt;课程安排&lt;/h2&gt;&lt;p&gt;如下图所示，课件和参考材料会随着课程进度发布。这篇翻译的内容也会同步更新。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-1c31c9c163da64f7d29f14d48965abbe.jpg" data-rawwidth="1938" data-rawheight="1406"&gt;&lt;h2&gt;课程视频&lt;/h2&gt;&lt;p&gt;原文展示了2015年的4个课程视频，在Youtube上，清晰度很低，这里就不放出了，感兴趣的知友自行查看原文。随着课程进展，本部分也更新2017年的课程视频链接。&lt;/p&gt;&lt;h2&gt;相关课程&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" data-editable="true" data-title="David Silver关于增强学习的课程及课程视频" class=""&gt;David Silver关于增强学习的课程及课程视频&lt;/a&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：&lt;b&gt;重要&lt;/b&gt;。&lt;/blockquote&gt;&lt;li&gt;&lt;a href="https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/" data-editable="true" data-title="Nando de Freitas关于机器学习的课程" class=""&gt;Nando de Freitas关于机器学习的课程&lt;/a&gt;&lt;/li&gt;&lt;blockquote&gt;译者注：没必要，看吴恩达的课程。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://cs231n.github.io/" data-editable="true" data-title="Andrej Karpathy的课程" class=""&gt;Andrej Karpathy的课程&lt;/a&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：去年我们一直在推荐，不赘述了。&lt;/blockquote&gt;&lt;h2&gt;相关书籍&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html" data-editable="true" data-title="Sutton和Barto的《Reinforcement Learning: An Introduction》" class=""&gt;Sutton和Barto的Reinforcement Learning: An Introduction&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：&lt;b&gt;重要&lt;/b&gt;，系统学习的话就打印吧。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://www.ualberta.ca/~szepesva/RLBook.html" data-editable="true" data-title="Szepesvari, Algorithms for Reinforcement Learning" class=""&gt;Szepesvari, Algorithms for Reinforcement Learning&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://www.athenasc.com/dpbook.html" data-editable="true" data-title="Bertsekas, Dynamic Programming and Optimal Control, Vols I and II" class=""&gt;Bertsekas, Dynamic Programming and Optimal Control, Vols I and II&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471727822.html" data-editable="true" data-title="Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming" class=""&gt;Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://adp.princeton.edu/" data-editable="true" data-title="Powell, Approximate Dynamic Programming" class=""&gt;Powell, Approximate Dynamic Programming&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;泛读链接&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://rll.berkeley.edu/deeprlcourse/#syllabus" data-editable="true" data-title="深度学习资源的集锦"&gt;深度学习资源的集锦&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：收藏了你也不会看的，但是可以装逼：）&lt;/blockquote&gt;&lt;h2&gt;之前课程&lt;/h2&gt;&lt;p&gt;2015年秋季开过同名课程，介绍链接&lt;a href="http://rll.berkeley.edu/deeprlcourse-fa15/" data-title="点击这里" class="" data-editable="true"&gt;点击这里&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;课程简介翻译完毕&lt;/b&gt;。&lt;/p&gt;&lt;h2&gt;学习建议&lt;/h2&gt;&lt;p&gt;想要学习深度增强学习的知友，在我个人的看来，可能是以下几种情况：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;看到了深度增强学习的前景，想要提前布局，提高自身价值；&lt;/li&gt;&lt;li&gt;学完了深度学习，想要继续学习人工智能领域其他内容；&lt;/li&gt;&lt;li&gt;大公司或者拿了投资的创业公司的项目组想要搞相关应用，比如BetaGo或者GammaGo：）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;都挺好的，学吧！专栏成立的初心之一，就是促进这个领域的学习和交流。个人也会以这次课程为抓手，再系统性地把深度增强学习给学扎实。后续会以该课程学习为主题，在专栏进行相关内容的写作，也欢迎大家针对课程学习内容进行讨论。&lt;/p&gt;&lt;p&gt;如果是因为近几年人工智能突然火热起来想要学习的知友，需要知道人工智能领域历史上起起落落好几次，入坑前想好自己是不是真的感兴趣。&lt;/p&gt;&lt;p&gt;如果上文中的课程前置要求中三方面知识点都不太熟悉，也不建议直接学习该课程。建议&lt;b&gt;先看吴恩达或者Hinton的课程，然后看CS231n，然后再来学习这门课程&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;最后照例&lt;b&gt;打鸡血：&lt;/b&gt;虽说人工智能起起落落好几次，但是这次确实是解决了之前没有解决的问题，突破了之前没有突破的水平，不是吗？相较于“呵呵，根据历史规律，人工智能也就还能火几年，步子太大要扯到蛋”的态度，我个人更倾向：&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;I am looking at the future with concern, but with good hope&lt;/b&gt;. &lt;b&gt;-Albert Schweitzer&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;----------------------------------------------------------------------------------------&lt;/p&gt;&lt;p&gt;PS：我们开通了&lt;b&gt;智能单元微信公众号&lt;/b&gt;，搜索“&lt;b&gt;智能单元&lt;/b&gt;”就能找到，内容上会和专栏差异化。可以的话请大家关注支持一个，算是对我的鼓励，先谢过啦：）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24721292&amp;pixel&amp;useReferer"/&gt;</description><author>杜客</author><pubDate>Thu, 05 Jan 2017 10:26:25 GMT</pubDate></item><item><title>Master 横扫围棋各路高手，是时候全面研究通用人工智能了！</title><link>https://zhuanlan.zhihu.com/p/24709235</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-eab9cfb4a3d814feb1e92cc1b69db9aa_r.png"&gt;&lt;/p&gt;在写这篇文章的时候，聂卫平将挑战Master，不出意外，老聂也只能扑街。&lt;p&gt;有的人说，汽车早就比人跑得快，人也不会惊慌，围棋AI战胜人类，也是迟早的事，同样不必惊慌。&lt;/p&gt;&lt;p&gt;是这样吗？&lt;/p&gt;&lt;p&gt;不是的。&lt;/p&gt;&lt;p&gt;猎豹本来就比人类跑得快，但人类仍然是地球的主宰。为什么？因为人类引以为傲的智慧！&lt;/p&gt;&lt;p&gt;因为人类拥有地球生物中最高的智能，所以虽然人的肉体机能很有限，但是不妨碍人类去创造各种辅助人类的工具。&lt;/p&gt;&lt;p&gt;然而，人工智能，却是一个和汽车，飞机完全不同的东西！我们人类在试图制造超越人类智慧的东西！&lt;/p&gt;&lt;p&gt;当AlphaGo战胜李世石，当Master豪取50连胜的时候，很多人会产生一种对智能的恐惧。相信那些真正和Master战斗的棋手们会更深有体会。&lt;/p&gt;&lt;p&gt;为什么？我们的智慧被超越！我们以前的认识被完全打破了。以至于当今围棋第一人柯洁说出“我们对围棋的理解都是错的”这样的话。&lt;/p&gt;&lt;p&gt;人们也很容易的想到《三体》中水滴入侵人类舰队的那一刻。&lt;/p&gt;&lt;p&gt;那种感觉是何曾的相同。&lt;/p&gt;&lt;p&gt;让人绝望而恐惧！&lt;/p&gt;&lt;p&gt;有人会说，n年前国际象棋就被攻克了，没什么大不了。&lt;/p&gt;&lt;p&gt;但是，&lt;b&gt;这次真的不一样！这次真的不一样！这次真的不一样！&lt;/b&gt;（重要的事情说三篇）&lt;/p&gt;&lt;p&gt;如果你懂深度学习，如果你懂AlphaGo，你就会明白，AlphaGo不再是依靠蛮力计算，而主要是靠深度神经网络，靠增强学习自我学习。&lt;/p&gt;&lt;p&gt;深度神经网络是个黑箱。输入棋局，通过神经网络，输出对棋局的判断。AlphaGo只要这么简单，智能就在神经网络当中。&lt;/p&gt;&lt;p&gt;这次是围棋，完全可观察，如果下次就是&lt;b&gt;星际争霸&lt;/b&gt;，那么大家会作何感想呢？&lt;/p&gt;&lt;p&gt;深度学习正在变革所有行业，AlphaGo则开启了通用人工智能的大门！&lt;/p&gt;&lt;h2&gt;通用人工智能是什么？&lt;/h2&gt;&lt;p&gt;通用人工智能（General Artificial Intelligence），是指能通过自我学习解决各种问题的智能算法。人类的大脑就是一种通用智能，因为人既可以学游泳，也可以学下棋。开发AlphaGo的DeepMind就是这么一家公司，以实现通用人工智能为目标。&lt;/p&gt;&lt;p&gt;通用人工智能并不是等价于类人智能。但解决了通用人工智能，类人智能也必然能够达到。&lt;/p&gt;&lt;p&gt;AlphaGo的算法就是典型的通用人工智能算法，核心使用了深度学习（Deep Learning），增强学习（Reinforcement Learning）。而深度增强学习（Deep Reinforcement Learning），就是通用人工智能算法的具体表现形式。什么叫通用？就是这个算法既可以训练用来下围棋，也可以训练用来开车，还可以训练用来股票交易。&lt;/p&gt;&lt;p&gt;有人说把围棋的路数从19路变成21路，AlphaGo就没辙了。这是没错。但是DeepMind很容易就可以训练一个适应多个路数的AlphaGo。都只是时间问题。&lt;/p&gt;&lt;p&gt;通用人工智能算法就是倚天剑，屠龙刀！算法在手，任何问题都可以尝试去解决！&lt;/p&gt;&lt;p&gt;目前国内的研究还相当少，但是，真的&lt;/p&gt;&lt;p&gt;&lt;b&gt;是时候全面研究通用人工智能了！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;DeepMind和OpenAI正在大力发展，这个才是真正掀起人工智能革命的关键！中国在这一块如果落后的话会非常致命！必须庆幸DeepMind和OpenAI还一直公开他们的论文！&lt;/p&gt;&lt;p&gt;我们从DeepMind和OpenAI研究的方向就知道应该做什么了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1 Deep Reinforcement Learning深度增强学习，用于构造学习机制&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2 Deep Generative Model深度生成模型，用于理解信息，可以用于预测规划&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;3 Neural Memory神经网络记忆，用于存储信息和推理&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;4 One Shot Learning 一眼学习，用于快速学习&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;5 Deep Transfer Learning 深度迁移学习，用于移植知识&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;以上多点的综合运用，必将可以制造更强大的通用人工智能算法！而这些方向的研究，都越来越接近人类大脑的本质，或者说智能的本质！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;要不畏惧人工智能，那只有理解并掌握人工智能！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;（本文算作一种呼吁，呼吁更多的人工智能研究人员投入到通用人工智能当中。）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;---------------------------------------&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本专栏将全面聚焦通用人工智能算法！&lt;/p&gt;&lt;p&gt;最后，加一个推广！为了使关注通用人工智能的朋友们能够更及时获取通用人工智能的前沿资讯，我们开通了 &lt;b&gt;智能单元 微信公众号&lt;/b&gt;！第一时间为大家推送最新资讯，并且不定期推送纯原创最新通用人工智能进展的解读！&lt;/p&gt;&lt;p&gt;与大家一起学习，一起进步！&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-864ef735513a261c8140410e43436050.png" data-rawwidth="180" data-rawheight="320"&gt;欢迎在微信上搜索“智能单元” 公众号关注！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24709235&amp;pixel&amp;useReferer"/&gt;</description><author>Flood Sung</author><pubDate>Wed, 04 Jan 2017 15:05:39 GMT</pubDate></item><item><title>ICLR 2017 DRL相关论文</title><link>https://zhuanlan.zhihu.com/p/23807875</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-a583638767b7ef5d44ddc68d748af3a5_r.png"&gt;&lt;/p&gt;&lt;h2&gt;1 前言&lt;/h2&gt;&lt;p&gt;ICLR 2017中和Deep Reinforcement Learning相关的论文我这边收集了一下，一共有30篇（可能有漏），大部分来自于DeepMind和OpenAI，可见DRL依然主要由DeepMind和OpenAI把持。由于论文太多，时间有限，先把论文列出来。之后根据情况做一定分析。也欢迎大家一起补充。&lt;/p&gt;&lt;h2&gt;2 DeepMind的论文分析&lt;/h2&gt;&lt;p&gt;&lt;b&gt;[1] LEARNING TO COMPOSE WORDS INTO SENTENCES
WITH REINFORCEMENT LEARNING&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;推荐阅读指数：&lt;/b&gt;⭐️⭐️&lt;/p&gt;&lt;p&gt;&lt;b&gt;论文类型&lt;/b&gt;：应用型&lt;/p&gt;&lt;p&gt;&lt;b&gt;应用方向&lt;/b&gt;：自然语言理解&lt;/p&gt;&lt;p&gt;&lt;b&gt;论文基本内容介绍&lt;/b&gt;：&lt;/p&gt;&lt;p&gt;这篇文章面向一类自然语言理解问题，就是句子的理解，如何更好的输入一个句子的每一个词语，然后输出一个句子的表达。一种方式就是不管句子的结构，一个一个输入到RNN中，然后输出一个向量来表示这个句子的含义，一种就是探索句子的组成树结构（Tree Structure），基于树结构输入词语，然后输出句子的表达。比如“我喜欢打篮球”这句话，很显然，“我，喜欢，打，篮球”可以作为特定的结构输入，就类似于句子的分析，我们显然不会按照“我喜，欢打篮，球”来理解。下图就是两个树结构的范例，要按照不同的顺序组合词语然后输入到RNN中。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-c713955a4285122aece12058ac0acadc.png" data-rawwidth="1862" data-rawheight="514"&gt;&lt;p&gt;那么输入一个句子的每一个词语，输出句子的表达之后，就有用了，可以利用这个输出做很多事：比如判断这个句子是电影的正面评论还是负面平台，判断两个句子是否意思相近，判断两个句子是矛盾，中立还是相同立场，还可以基于句子预测下一个句子。所以句子的理解是自然语言理解的基础，这篇文章的目的也就是希望能够得到更好的句子的理解。&lt;/p&gt;&lt;p&gt;接下来这篇文章做的事情就是使用增强学习来探索句子的树形结构组成，就是说如何构建树结构的问题。如果构造了一个堆栈，然后有一个句子等待输入，那么每一次有两个动作，插入S和合并R。插入就是将一个词语word插入到堆栈，合并就是将两个词语合并。如下图所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-1a21383e6e32c818b9c56985d077a171.png" data-rawwidth="1966" data-rawheight="472"&gt;那么本文使用REINFORCE算法（策略梯度算法）来训练这个树结构的动作选择网络。Reward是在每次生成完整个句子后获取（类似AlphaGo）。具体算法这里不详细介绍。效果是显然的，肯定能够学习到一定的组成句子的方法，从而比那种随便输入的句子理解更好。&lt;/p&gt;&lt;p&gt;&lt;b&gt;论文评价&lt;/b&gt;：一篇中规中矩的Paper，在NLP领域找到了一个不错的应用增强学习的问题，并且取得了一定的效果。但是这种效果并不具有大幅度的提高，也说明其实没必要采用树结构输入，一个一个词语输入其实也可以，只要神经网络能够理解就好。人类就可以做到，虽然人类也分析句子的结构。&lt;/p&gt;&lt;p&gt;&lt;b&gt;[2] LEARNING TO NAVIGATE IN COMPLEX ENVIRONMENTS&lt;/b&gt;&lt;/p&gt;&lt;p&gt;推荐阅读指数：⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;&lt;p&gt;&lt;b&gt;[3] LEARNING TO PERFORM PHYSICS EXPERIMENTS VIA
DEEP REINFORCEMENT LEARNING &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[4] PGQ: COMBINING POLICY GRADIENT AND Q-
LEARNING&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[5] Q-PROP: SAMPLE-EFFICIENT POLICY GRADIENT
WITH AN OFF-POLICY CRITIC&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[6] REINFORCEMENT LEARNING WITH UNSUPERVISED
AUXILIARY TASKS&lt;/b&gt;&lt;/p&gt;&lt;p&gt;推荐阅读指数：⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;&lt;p&gt;&lt;b&gt;[7] SAMPLE EFFICIENT ACTOR-CRITIC WITH
EXPERIENCE REPLAY&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[8] THE PREDICTRON: END-TO-END LEARNING AND PLANNING &lt;/b&gt;&lt;/p&gt;&lt;h2&gt;3 OpenAI的论文分析（包含Sergey Levine的论文）&lt;/h2&gt;&lt;p&gt;&lt;b&gt;[9] #EXPLORATION: A STUDY OF COUNT-BASED EXPLORATION
FOR DEEP REINFORCEMENT LEARNING &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[10] GENERALIZING SKILLS WITH SEMI-SUPERVISED
REINFORCEMENT LEARNING  &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[11] LEARNING INVARIANT FEATURE SPACES TO TRANS-
FER SKILLS WITH REINFORCEMENT LEARNING &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[12] LEARNING VISUAL SERVOING WITH DEEP FEATURES
AND TRUST REGION FITTED Q-ITERATION &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[13] MODULAR MULTITASK REINFORCEMENT
LEARNING WITH POLICY SKETCHES &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[14] STOCHASTIC NEURAL NETWORKS FOR
HIERARCHICAL REINFORCEMENT LEARNING &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[15] THIRD PERSON IMITATION LEARNING &lt;/b&gt;&lt;/p&gt;&lt;p&gt;推荐阅读指数：⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;&lt;p&gt;&lt;b&gt;[16] UNSUPERVISED PERCEPTUAL REWARDS
FOR IMITATION LEARNING &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[17] EPOPT: LEARNING ROBUST NEURAL NETWORK POLICIES USING MODEL ENSEMBLES &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[18] RL2: FAST REINFORCEMENT LEARNING VIA SLOW REINFORCEMENT LEARNING &lt;/b&gt;&lt;/p&gt;&lt;h2&gt;4 其他论文&lt;/h2&gt;&lt;p&gt;&lt;b&gt;[19] COMBATING DEEP REINFORCEMENT LEARNING’S
SISYPHEAN CURSE WITH INTRINSIC FEAR &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[20] COMMUNICATING HIERARCHICAL NEURAL
CONTROLLERS FOR LEARNINGZERO-SHOT TASK GENERALIZATION&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[21] DESIGNING NEURAL NETWORK ARCHITECTURES
USING REINFORCEMENT LEARNING &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[22] LEARNING TO PLAY IN A DAY: FASTER DEEP REIN-
FORCEMENT LEARNING BY OPTIMALITY TIGHTENING&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[23] LEARNING TO REPEAT: FINE GRAINED ACTION REPETITION FOR
DEEP REINFORCEMENT LEARNING &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[24] MULTI-TASK LEARNING WITH DEEP MODEL BASED
REINFORCEMENT LEARNING&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[25] NEURAL ARCHITECTURE SEARCH WITH
REINFORCEMENT LEARNING&lt;/b&gt;&lt;/p&gt;&lt;p&gt;推荐阅读指数：⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;&lt;p&gt;&lt;b&gt;[26] OPTIONS DISCOVERY WITH BUDGETED REINFORCE-
MENT LEARNING  &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[27] REINFORCEMENT LEARNING THROUGH ASYNCHRONOUS ADVANTAGE ACTOR-CRITIC ON A GPU &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[28] SPATIO-TEMPORAL ABSTRACTIONS IN
REINFORCEMENT LEARNING THROUGH
NEURAL ENCODING &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[29] SURPRISE-BASED INTRINSIC MOTIVATION FOR DEEP
REINFORCEMENT LEARNING&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[30] TUNING RECURRENT NEURAL NETWORKS WITH REINFORCEMENT LEARNING&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;最后，我们开通了智能单元微信公众号，第一时间推送最前沿技术和资讯，欢迎在微信搜索“智能单元”关注。&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23807875&amp;pixel&amp;useReferer"/&gt;</description><author>Flood Sung</author><pubDate>Tue, 03 Jan 2017 11:54:32 GMT</pubDate></item><item><title>干货和原创的智能单元微信公众号开启</title><link>https://zhuanlan.zhihu.com/p/24682204</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-8b8c855409438a124291382551cc0cac_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;各位知友2017新年快乐！在新的一年，我们&lt;b&gt;推出智能单元微信公众号，提供有别于知乎专栏的差异化优质内容。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;智能单元微信公众号&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;智能单元微信公众号&lt;/b&gt;是一个致力于&lt;b&gt;推动通用人工智能（Artificial General Intelligence）发展&lt;/b&gt;的原创独立媒体。通过这个平台，我们会给大家：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;分享最有价值的实践：&lt;b&gt;会&lt;u&gt;分享我们的开源代码与软件；&lt;/u&gt;&lt;/b&gt;&lt;/li&gt;&lt;li&gt;分享最前沿的研究成果：会&lt;b&gt;广读领域内新论文并评估分量&lt;/b&gt;，&lt;b&gt;对高价值论文给出第一时间的有态度解读&lt;/b&gt;。详细解读和复现工作还是交给专栏吧！&lt;/li&gt;&lt;li&gt;分享最新鲜的资讯：会推送经我们挑选的有调性的领域内资讯，节约知友们的信息搜索时间。&lt;/li&gt;&lt;/ul&gt;智能单元微信公众号&lt;b&gt;&lt;u&gt;聚焦通用人工智能&lt;/u&gt;，将涉及当前最前沿的机器学习算法&lt;/b&gt;：包&lt;b&gt;括深度学习Deep Learning，增强学习Reinforcement Learning，迁移学习Transfer Learning，神经网络记忆Neural Memory（神经图灵机，DNC），无监督学习（主要是生成式对抗网络GAN）及一眼学习（One Shot Learning）&lt;/b&gt;等。将这些技术应用到机器人当中，将使机器人技术实现变革，使机器人具备学习能力，因此&lt;b&gt;机器人学习&lt;/b&gt;是通用人工智能核心应用方向，也是智能单元关注的核心。&lt;h2&gt;&lt;b&gt;公众号与专栏&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;智能单元专栏重在提供学习材料、展现深度思考&lt;/b&gt;，将专注于：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;通用人工智能相关技术原创教程，主要涉及深度增强学习，生成式对抗网络及神经网络记忆相关技术；&lt;/li&gt;&lt;li&gt;最前沿领域内论文的详细解读；&lt;/li&gt;&lt;li&gt;最前沿领域内技术的分析与总结。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;智能单元微信公众号重在时效性、分享优质内容&lt;/b&gt;，专注方向参见前文，这里不赘述。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;一个小目标&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;2017年，我们期望&lt;b&gt;智能单元知乎专栏&lt;/b&gt;和&lt;b&gt;智能单元微信公众号&lt;/b&gt;既一脉相承，又各有侧重，&lt;b&gt;能够给在人工智能领域的奋斗的知友们提供更多帮助&lt;/b&gt;！&lt;/p&gt;&lt;p&gt;PS：&lt;b&gt;欢迎知友们在评论中留下自己2017年在人工智能领域的小目标&lt;/b&gt;，这将帮助我们更好地提供优质内容。况且，大家相互祝福（&lt;b&gt;吐槽&lt;/b&gt;）难倒不是一件欢乐的事儿吗？&lt;/p&gt;PS2：&lt;b&gt;题图就是二维码，欢迎扫码关注！也可在微信中直接搜索“智能单元”关注我们&lt;/b&gt;！&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24682204&amp;pixel&amp;useReferer"/&gt;</description><author>杜客</author><pubDate>Tue, 03 Jan 2017 11:27:11 GMT</pubDate></item><item><title>深度学习论文阅读路线图 Deep Learning Papers Reading Roadmap</title><link>https://zhuanlan.zhihu.com/p/23080129</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-6f589df38509d14f839737645322a011_r.jpg"&gt;&lt;/p&gt;&lt;h2&gt;1 前言&lt;/h2&gt;&lt;p&gt;相信很多想入门深度学习的朋友都会遇到这个问题，就是应该看哪些论文。包括我自己，也是花费了大量的时间在寻找文章上。另一方面，对于一些已经入门的朋友，常常也需要了解一些和自己研究方向不同的方向的文章。&lt;/p&gt;&lt;p&gt;因此，这里做了一个深度学习论文阅读路线图，也就是paper list，希望能够帮助大家对深度学习的全貌和具体的方向有一个深入的理解。&lt;/p&gt;&lt;h2&gt;2 路线图的构建原则&lt;/h2&gt;&lt;p&gt;有以下四个原则：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;从整体到局部。即从Survey的文章，影响大局的文章到具体子问题子领域的文章。&lt;/li&gt;&lt;li&gt;从过去到最前沿。即每个topic的文章是按照时间顺序排列的，这样大家就可以清楚的看到这个方向的研究发展脉络。&lt;/li&gt;&lt;li&gt;从通用到应用。即有些深度学习的文章是面向深度学习通用理论，比如Resnet，可以用在任意的神经网络中，而有些文章则是具体应用，比如Image Caption。&lt;/li&gt;&lt;li&gt;面向最前沿。收集的文章会有很多是最新的，甚至就是几天前出来的，这样能保证路线图是最新的。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;每一种topic只选择最有代表性的几篇文章，比如深度增强学习（Deep Reinforcement Learning），这个领域现在有几十篇文章，但只选择几篇，要深入了解甚至做为自己的研究方向，还需要进一步的阅读该领域的文章。&lt;/p&gt;&lt;h2&gt;3 说明&lt;/h2&gt;&lt;p&gt;这个论文阅读路线图选择的文章除了文章本身的影响力和重要性之外，也依赖于本人对文章的理解。因此会有一定的主观性，即我觉得这篇文章好，值得读，所以推荐。这方面需要大家的理解。也欢迎大家提出批评意见以改进。&lt;/p&gt;&lt;p&gt;这个路线图还在完善，会不断更新。&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家pull requests！（基本要求：一个topic不超过10篇，并且包含当前最前沿和最有影响力的文章，也欢迎增加新的topic）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;路线图在Github上，地址是：&lt;/p&gt;&lt;p&gt;&lt;a href="https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap" data-editable="true" data-title="GitHub - songrotek/Deep-Learning-Papers-Reading-Roadmap: Deep Learning papers reading roadmap for anyone who are eager to learn this amazing tech!" class=""&gt;GitHub - songrotek/Deep-Learning-Papers-Reading-Roadmap: Deep Learning papers reading roadmap for anyone who are eager to learn this amazing tech!&lt;/a&gt;&lt;/p&gt;&lt;p&gt;以下是截图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-eef22d5319df25855f056b2683df8ff3.png" data-rawwidth="2012" data-rawheight="98"&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-cccfd935d9e3f50f963046c3892b61ab.png" data-rawwidth="1880" data-rawheight="1366"&gt;希望对大家有所帮助！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23080129&amp;pixel&amp;useReferer"/&gt;</description><author>Flood Sung</author><pubDate>Thu, 20 Oct 2016 12:12:51 GMT</pubDate></item><item><title>深层学习为何要“Deep”（上）</title><link>https://zhuanlan.zhihu.com/p/22888385</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-b1c917b1f2616bc51c7d833fdcc0c05d_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2016年11月22日更新：&lt;a href="https://link.zhihu.com/?target=https%3A//yjango.gitbooks.io/superorganism/content/shen_ceng_wang_luo.html" class="" data-editable="true" data-title="深层神经网络为什么要"&gt;深层神经网络为什么要&lt;/a&gt;deep（下）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;结合&lt;a href="https://yjango.gitbooks.io/superorganism/content/xue_xi.html" class="" data-editable="true" data-title="生物学习"&gt;生物学习&lt;/a&gt;与&lt;a href="https://yjango.gitbooks.io/superorganism/content/tong_ji_xue_xi.html" class="" data-title="机器学习" data-editable="true"&gt;机器学习&lt;/a&gt;一起来看&lt;/b&gt;&lt;/p&gt;&lt;p&gt;深层学习开启了人工智能的新时代。不论任何行业都害怕错过这一时代浪潮，因而大批资金和人才争相涌入。但深层学习却以“黑箱”而闻名，不仅调参难，训练难，“新型”网络结构的论文又如雨后春笋般地涌现，使得对所有结构的掌握变成了不现实。我们缺少一个对深层学习合理的认识。&lt;/p&gt;&lt;p&gt;本文就是通过对深层神经网络惊人表现&lt;b&gt;背后原因&lt;/b&gt;的思考，揭示&lt;b&gt;设计一个神经网络的本质&lt;/b&gt;，从而获得一个对“&lt;b&gt;如何设计&lt;/b&gt;网络”的全局指导。由于问题本身过于庞大，我们先把问题拆分成几部分加以思考。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1、神经网络为什么可以用于识别 （已回答）2、神经网络变深后我们获得了什么 &lt;/b&gt;（已回答）3、“过深”的网络的效果又变差的原因 4、“深浅”会影响神经网络表现的背后原因 5、RNN、CNN以及各种不同网络结构的共性是什么 6、设计神经网络的本质是什么 &lt;/p&gt;&lt;p&gt;文章分为&lt;b&gt;上下&lt;/b&gt;两部分。 &lt;b&gt;上篇&lt;/b&gt;涉及的内容是1,2两个问题，是为了理解“深层”神经网络的&lt;b&gt;预备知识&lt;/b&gt;。描述的是&lt;strong&gt;为何能识别&lt;/strong&gt;和&lt;strong&gt;如何训练&lt;/strong&gt;两部分。看完后能明白的是：1、为什么神经网络&lt;strong&gt;能够&lt;/strong&gt;识别，2、训练网络&lt;strong&gt;基本流程&lt;/strong&gt;，以及深层神经网络大家族中其他技术&lt;strong&gt;想要解决的问题&lt;/strong&gt;（并不需要知道具体的解决步骤）。 &lt;/p&gt;&lt;p&gt;文章的理解需要线性代数基础知识，数学零基础的请看&lt;/p&gt;&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/23361299?refer=YJango" class="" data-editable="true" data-title="串讲 线性代数、概率、熵 - 超有机体 - 知乎专栏"&gt;串讲 线性代数、概率、熵 - 超有机体 - 知乎专栏&lt;/a&gt;&lt;/p&gt;&lt;p&gt;对神经网络有了大致了解后，《深层学习为何要“Deep”（下）》会进一步围绕“深层”二字再次讨论深层学习为何要“Deep”，会讨论CNN、RNN、Transfer learning、distillation training等技术的共性，并解释&lt;strong&gt;设计网络结构的本质&lt;/strong&gt;是什么。&lt;/p&gt;目录&lt;ul&gt;&lt;li&gt;&lt;a href="http://blog.csdn.net/u010751535/article/details/52739803#%E4%B8%80%E5%9F%BA%E6%9C%AC%E5%8F%98%E6%8D%A2%E5%B1%82" data-editable="true" data-title="一基本变换层"&gt;一基本变换层&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://blog.csdn.net/u010751535/article/details/52739803#%E4%BA%8C%E7%90%86%E8%A7%A3%E8%A7%86%E8%A7%92" data-editable="true" data-title="二理解视角"&gt;二理解视角&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://blog.csdn.net/u010751535/article/details/52739803#%E6%95%B0%E5%AD%A6%E8%A7%86%E8%A7%92%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86" data-editable="true" data-title="数学视角线性可分"&gt;数学视角线性可分&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://blog.csdn.net/u010751535/article/details/52739803#%E7%89%A9%E7%90%86%E8%A7%86%E8%A7%92%E7%89%A9%E8%B4%A8%E7%BB%84%E6%88%90" data-editable="true" data-title="物理视角物质组成"&gt;物理视角物质组成&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://blog.csdn.net/u010751535/article/details/52739803#%E4%B8%89%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%AE%AD%E7%BB%83" data-editable="true" data-title="三神经网络的训练"&gt;三神经网络的训练&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://blog.csdn.net/u010751535/article/details/52739803#%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83" data-editable="true" data-title="如何训练"&gt;如何训练&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://blog.csdn.net/u010751535/article/details/52739803#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E9%97%AE%E9%A2%98" data-editable="true" data-title="梯度下降的问题"&gt;梯度下降的问题&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://blog.csdn.net/u010751535/article/details/52739803#1%E5%B1%80%E9%83%A8%E6%9E%81%E5%B0%8F%E5%80%BC" data-editable="true" data-title="1局部极小值"&gt;1局部极小值&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://blog.csdn.net/u010751535/article/details/52739803#2%E6%A2%AF%E5%BA%A6%E7%9A%84%E8%AE%A1%E7%AE%97" data-editable="true" data-title="2梯度的计算" class=""&gt;2梯度的计算&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://blog.csdn.net/u010751535/article/details/52739803#%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B%E5%9B%BE" data-editable="true" data-title="基本流程图"&gt;基本流程图&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://blog.csdn.net/u010751535/article/details/52739803#%E5%9B%9B%E6%B7%B1%E5%B1%82%E7%9A%84%E6%80%9D%E8%80%83%E7%9C%9F%E7%9A%84%E5%8F%AA%E6%9C%89%E8%BF%99%E4%BA%9B%E5%8E%9F%E5%9B%A0%E5%90%97" data-editable="true" data-title="四深层的思考真的只有这些原因吗"&gt;四深层的思考真的只有这些原因吗&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;一、基本变换：层&lt;/h2&gt;&lt;p&gt;神经网络是由一层一层构建的，那么每&lt;strong&gt;层&lt;/strong&gt;究竟在做什么？&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;数学式子&lt;/strong&gt;：&lt;equation&gt;\vec{y}= a(W\cdot\vec{x} + {b})&lt;/equation&gt;，其中&lt;equation&gt;\vec{x}&lt;/equation&gt;是输入向量，&lt;equation&gt;\vec{y}&lt;/equation&gt;是输出向量，&lt;equation&gt;\vec{b}&lt;/equation&gt;是偏移向量，&lt;equation&gt;W&lt;/equation&gt;是权重矩阵，&lt;equation&gt;a()&lt;/equation&gt;是激活函数。每一层仅仅是把输入&lt;equation&gt;\vec x&lt;/equation&gt;经过如此简单的操作得到&lt;equation&gt;\vec y&lt;/equation&gt;。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;数学理解&lt;/strong&gt;：通过如下5种对输入空间（输入向量的集合）的操作，完成 &lt;strong&gt;输入空间 ——&amp;gt; 输出空间&lt;/strong&gt; 的变换 (矩阵的行空间到列空间)。 注：用“空间”二字的原因是被分类的并不是单个事物，而是&lt;strong&gt;一类&lt;/strong&gt;事物。空间是指这类事物所有个体的集合。&lt;ul&gt;&lt;li&gt;&lt;strong&gt;1.&lt;/strong&gt; 升维/降维&lt;/li&gt;&lt;li&gt;&lt;strong&gt;2.&lt;/strong&gt; 放大/缩小&lt;/li&gt;&lt;li&gt;&lt;strong&gt;3.&lt;/strong&gt; 旋转&lt;/li&gt;&lt;li&gt;&lt;strong&gt;4.&lt;/strong&gt; 平移&lt;/li&gt;&lt;li&gt;&lt;strong&gt;5.&lt;/strong&gt; “弯曲” 这5种操作中，1,2,3的操作由&lt;equation&gt;W\cdot\vec{x}&lt;/equation&gt;完成，4的操作是由&lt;equation&gt;+\vec{b}&lt;/equation&gt;完成，5的操作则是由&lt;equation&gt;a()&lt;/equation&gt;来实现。 (此处有动态图&lt;a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/1layer.gif" data-editable="true" data-title="5种空间操作" class=""&gt;5种空间操作&lt;/a&gt;，帮助理解)&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-1ebee9a3fb36a6d1502d517b24bfb5c3.jpg" data-rawwidth="239" data-rawheight="233"&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;p&gt;每层神经网络的数学理解：&lt;strong&gt;用线性变换跟随着非线性变化，将输入空间投向另一个空间&lt;/strong&gt;。&lt;/p&gt;&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;物理理解&lt;/strong&gt;：对 &lt;equation&gt;W\cdot\vec{x}&lt;/equation&gt; 的理解就是&lt;strong&gt;通过组合形成新物质&lt;/strong&gt;。&lt;equation&gt;
a()&lt;/equation&gt;又符合了我们所处的世界都是非线性的特点。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;情景：&lt;/strong&gt;&lt;equation&gt;\vec{x}&lt;/equation&gt;是二维向量，维度是碳原子和氧原子的数量&lt;equation&gt; [C;O]&lt;/equation&gt;，数值且定为&lt;equation&gt;[1;1]&lt;/equation&gt;，若确定&lt;equation&gt;\vec{y}&lt;/equation&gt;是三维向量，就会形成如下网络的形状 (神经网络的每个节点表示一个维度)。通过改变权重的值，可以获得若干个不同物质。右侧的节点数决定了想要获得多少种不同的新物质。（矩阵的行数） &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-69d03cf2b3677ad7dc3b0d9af58841b4.jpg" data-rawwidth="144" data-rawheight="164"&gt;&lt;strong&gt;1.&lt;/strong&gt;如果权重W的数值如（1），那么网络的输出y⃗ 就会是三个新物质，[二氧化碳，臭氧，一氧化碳]。 &lt;equation&gt;\left[
 \begin{matrix}
   CO_{2}\\
   O_{3}\\
   CO
  \end{matrix}
  \right]=
 \left[
 \begin{matrix}
   1 &amp;amp; 2 \\
   0 &amp;amp; 3\\
   1 &amp;amp; 1
  \end{matrix}
  \right] \cdot \left[
 \begin{matrix}
   C \\
   O \\
  \end{matrix}
  \right]&lt;/equation&gt; （1）&lt;/li&gt;&lt;li&gt;&lt;strong&gt;2.&lt;/strong&gt;也可以减少右侧的一个节点，并改变权重W至（2），那么输出&lt;equation&gt;\vec{y}&lt;/equation&gt; 就会是两个新物质，&lt;equation&gt;[ O_{0.3};CO_{1.5}]&lt;/equation&gt;。  &lt;equation&gt;\left[
 \begin{matrix}
    O_{0.3}\\
   CO_{1.5}\\
  \end{matrix}
  \right]=
 \left[
 \begin{matrix}
   0&amp;amp; 0.3 \\
   1 &amp;amp; 1.5\\
  \end{matrix}
  \right] \cdot \left[
 \begin{matrix}
   C \\
   O \\
  \end{matrix}
  \right]&lt;/equation&gt;（2）&lt;strong&gt;3.&lt;/strong&gt;如果希望通过层网络能够从[C, O]空间转变到&lt;equation&gt;[CO_{2};O_{3};CO]&lt;/equation&gt;空间的话，那么网络的学习过程就是将W的数值变成尽可能接近(1)的过程 。如果再加一层，就是通过组合&lt;equation&gt;[CO_{2};O_{3};CO]&lt;/equation&gt;这三种基础物质，形成若干更高层的物质。 &lt;strong&gt;4.&lt;/strong&gt;重要的是这种组合思想，组合成的东西在神经网络中并不需要有物理意义。 &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;p&gt;每层神经网络的物理理解：&lt;strong&gt;通过现有的不同物质的组合形成新物质&lt;/strong&gt;。&lt;/p&gt;&lt;/blockquote&gt;&lt;h1&gt;二、理解视角：&lt;/h1&gt;&lt;p&gt;现在我们知道了每一层的行为，但这种行为又是如何完成识别任务的呢？&lt;/p&gt;&lt;h2&gt;数学视角：“线性可分”&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;一维情景&lt;/strong&gt;：以分类为例，当要分类正数、负数、零，三类的时候，一维空间的直线可以找到两个超平面（比当前空间低一维的子空间。当前空间是直线的话，超平面就是点）分割这三类。但面对像分类奇数和偶数无法找到可以区分它们的点的时候，我们借助 x % 2（取余）的转变，把x变换到另一个空间下来比较，从而分割。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-92ff4b847ac5fa41d91d1e76a910c483.jpg" data-rawwidth="370" data-rawheight="63"&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;二维情景&lt;/strong&gt;：平面的四个象限也是线性可分。但下图的红蓝两条线就无法找到一超平面去分割。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-b1bd0f75b46ed27daf27910f2a6b6e3f.jpg" data-rawwidth="197" data-rawheight="204"&gt;神经网络的解决方法依旧是转换到另外一个空间下，用的是所说的&lt;strong&gt;5种空间变换操作&lt;/strong&gt;。比如下图就是经过放大、平移、旋转、扭曲原二维空间后，在三维空间下就可以成功找到一个超平面分割红蓝两线 (同SVM的思路一样)。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-8d7d1ef957ebbf8ba9bb9cf8ff2d87ff.jpg" data-rawwidth="197" data-rawheight="198"&gt;上面是一层神经网络可以做到的，如果把&lt;equation&gt;\vec{y}&lt;/equation&gt; 当做新的输入再次用这5种操作进行第二遍空间变换的话，网络也就变为了二层。最终输出是&lt;equation&gt;\vec{y}= a_{2}(W_{2}\cdot(a_{1}(W_{1}\cdot\vec{x} + {b}_{1})) + {b}_{2})&lt;/equation&gt;。 设想网络拥有很多层时，对原始输入空间的“扭曲力”会大幅增加，如下图，最终我们可以轻松找到一个超平面分割空间。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-b7d47097d8f10e6baeb329e88e59b563.jpg" data-rawwidth="239" data-rawheight="233"&gt;当然也有如下图失败的时候，关键在于“如何扭曲空间”。所谓监督学习就是给予神经网络网络大量的训练例子，让网络从训练例子中学会如何变换空间。每一层的权重W就&lt;strong&gt;控制着如何变换空间&lt;/strong&gt;，我们最终需要的也就是训练好的神经网络的所有层的权重矩阵。。这里有非常棒的&lt;a href="http://cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html" data-editable="true" data-title="可视化空间变换demo"&gt;可视化空间变换demo&lt;/a&gt;，&lt;strong&gt;一定要&lt;/strong&gt;打开尝试并感受这种扭曲过程。更多内容请看&lt;a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" data-editable="true" data-title="Neural Networks, Manifolds, and Topology"&gt;Neural Networks, Manifolds, and Topology&lt;/a&gt;。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-664c152ffc58a28a7f900f9a723cbb83.jpg" data-rawwidth="239" data-rawheight="233"&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;上面的内容有三张动态图，对于理解这种空间变化非常有帮助。可以在gitbook&lt;a href="https://yjango.gitbooks.io/-deep/content/wei_he_you_yong.html" class="" data-editable="true" data-title="深层学习为何要“deep”"&gt;深层学习为何要“deep”&lt;/a&gt;上感受那三张图。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;线性可分视角：神经网络的学习就是&lt;strong&gt;学习如何利用矩阵的线性变换加激活函数的非线性变换，将原始输入空间投向线性可分/稀疏的空间去分类/回归。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;增加节点数：增加维度，即增加线性转换能力。&lt;/strong&gt;&lt;strong&gt;增加层数：增加激活函数的次数，即增加非线性转换次数。&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;h2&gt;物理视角：“物质组成”&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;类比&lt;/strong&gt;：回想上文由碳氧原子通过不同组合形成若干分子的例子。从分子层面继续迭代这种组合思想，可以形成DNA，细胞，组织，器官，最终可以形成一个完整的人。继续迭代还会有家庭，公司，国家等。这种现象在身边随处可见。并且原子的内部结构与太阳系又惊人的相似。不同层级之间都是以类似的几种规则再不断形成新物质。你也可能听过&lt;strong&gt;分形学&lt;/strong&gt;这三个字。可通过观看&lt;a href="http://www.tudou.com/programs/view/o41zy0SeSS0" data-editable="true" data-title="从1米到150亿光年" class=""&gt;从1米到150亿光年&lt;/a&gt;来感受自然界这种层级现象的普遍性。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-3ec7216f7ab84dac089836b166c0ae28.jpg" data-rawwidth="488" data-rawheight="340"&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;人脸识别情景&lt;/strong&gt;：我们可以模拟这种思想并应用在画面识别上。由像素组成菱角再组成五官最后到不同的人脸。每一层代表不同的不同的物质层面 (如分子层)。而每层的W&lt;strong&gt;存储着如何组合上一层的物质从而形成新物质&lt;/strong&gt;。 如果我们完全掌握一架飞机是如何从分子开始一层一层形成的，拿到一堆分子后，我们就可以判断他们是否可以以此形成方式，形成一架飞机。 附：&lt;a href="http://playground.tensorflow.org/" data-editable="true" data-title="Tensorflow playground"&gt;Tensorflow playground&lt;/a&gt;展示了数据是如何“流动”的。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-82f05552fd2ddde28a0ef20814d7acbb.png" data-rawwidth="624" data-rawheight="218"&gt;&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;p&gt;物质组成视角：神经网络的学习过程就是&lt;strong&gt;学习物质组成方式的过程。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;增加节点数：增加同一层物质的种类，比如118个元素的原子层就有118个节点。&lt;/strong&gt;&lt;strong&gt;增加层数：增加更多层级，比如分子层，原子层，器官层，并通过判断更抽象的概念来识别物体。&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;h1&gt;三、神经网络的训练&lt;/h1&gt;&lt;p&gt;知道了神经网络的学习过程就是&lt;strong&gt;学习&lt;/strong&gt;控制着空间变换方式（物质组成方式）的&lt;strong&gt;权重矩阵&lt;/strong&gt;后，接下来的问题就是&lt;strong&gt;如何学习&lt;/strong&gt;每一层的权重矩阵W。&lt;/p&gt;&lt;h2&gt;如何训练：&lt;/h2&gt;&lt;p&gt;既然我们希望网络的输出尽可能的接近真正想要预测的值。那么就可以通过&lt;strong&gt;比较&lt;/strong&gt;当前网络的&lt;strong&gt;预测值&lt;/strong&gt;和我们真正想要的&lt;strong&gt;目标值&lt;/strong&gt;，再根据两者的差异情况来更新每一层的权重矩阵（比如，如果网络的预测值高了，就调整权重让它预测低一些，不断调整，直到能够预测出目标值）。因此就需要先&lt;strong&gt;定义“如何比较&lt;/strong&gt;预测值和目标值的&lt;strong&gt;差异&lt;/strong&gt;”，这便是&lt;strong&gt;损失函数或目标函数（loss function or objective function）&lt;/strong&gt;，用于衡量预测值和目标值的差异的方程。loss function的输出值（loss）越高表示差异性越大。那神经网络的训练就变成了尽可能的缩小loss的过程。 所用的方法是&lt;strong&gt;梯度下降（Gradient descent）&lt;/strong&gt;：通过使loss值向当前点对应梯度的反方向不断移动，来降低loss。一次移动多少是由&lt;strong&gt;学习速率（learning rate）&lt;/strong&gt;来控制的。&lt;/p&gt;&lt;h2&gt;梯度下降的问题：&lt;/h2&gt;&lt;p&gt;然而使用梯度下降训练神经网络拥有两个主要难题。&lt;/p&gt;&lt;h3&gt;1、局部极小值&lt;/h3&gt;&lt;p&gt;梯度下降寻找的是loss function的局部极小值，而我们想要全局最小值。如下图所示，我们希望loss值可以降低到右侧深蓝色的最低点，但loss有可能“卡”在左侧的局部极小值中。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-00fe10bf8877137bc5957cf0cd7f9219.png" data-rawwidth="420" data-rawheight="250"&gt;试图解决“卡在局部极小值”问题的方法分两大类：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;调节步伐：&lt;/strong&gt;调节学习速率，使每一次的更新“步伐”不同。常用方法有：&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;随机梯度下降（Stochastic Gradient Descent (SGD)：每次只更新一个样本所计算的梯度&lt;/p&gt;&lt;/li&gt;&lt;li&gt;小批量梯度下降（Mini-batch gradient descent）：每次更新若干样本所计算的梯度的平均值&lt;/li&gt;&lt;li&gt;动量（Momentum）：不仅仅考虑当前样本所计算的梯度；Nesterov动量（Nesterov Momentum）：Momentum的改进&lt;/li&gt;&lt;li&gt;&lt;p&gt;Adagrad、RMSProp、Adadelta、Adam：这些方法都是训练过程中依照规则降低学习速率，部分也综合动量&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;优化起点&lt;/strong&gt;：合理初始化权重（weights initialization）、预训练网络（pre-train），使网络获得一个较好的“起始点”，如最右侧的起始点就比最左侧的起始点要好。常用方法有：高斯分布初始权重（Gaussian distribution）、均匀分布初始权重（Uniform distribution）、Glorot 初始权重、He初始权、稀疏矩阵初始权重（sparse matrix）&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;2、梯度的计算&lt;/h3&gt;&lt;p&gt;机器学习所处理的数据都是高维数据，该&lt;strong&gt;如何快速计算梯度&lt;/strong&gt;、而不是以年来计算。 其次如何更新&lt;strong&gt;隐藏层&lt;/strong&gt;的权重？ 解决方法是：计算图：&lt;strong&gt;反向传播算法&lt;/strong&gt;这里的解释留给非常棒的&lt;a href="http://colah.github.io/posts/2015-08-Backprop/" data-editable="true" data-title="Computational Graphs: Backpropagation" class=""&gt;Computational Graphs: Backpropagation&lt;/a&gt;需要知道的是，&lt;strong&gt;反向传播算法是求梯度的一种方法&lt;/strong&gt;。如同快速傅里叶变换（FFT）的贡献。 而计算图的概念又使梯度的计算更加合理方便。&lt;/p&gt;&lt;h3&gt;基本流程图：&lt;/h3&gt;&lt;p&gt;下面就结合图简单浏览一下训练和识别过程，并描述各个部分的作用。要&lt;b&gt;结合图解阅读以下内容。但手机显示的图过小，最好用电脑打开&lt;/b&gt;。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-af6c22793412bfc37e1428369a0e36e0.jpg" data-rawwidth="743" data-rawheight="345"&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;收集训练集（train data）：&lt;/strong&gt;也就是同时有input以及对应label的数据。每个数据叫做训练样本（sample）。label也叫target，也是机器学习中最贵的部分。上图表示的是我的数据库。假设input本别是x的维度是39，label的维度是48。&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;设计网络结构（architecture）：&lt;/strong&gt;确定层数、每一隐藏层的节点数和激活函数，以及输出层的激活函数和损失函数。上图用的是两层隐藏层（最后一层是输出层）。隐藏层所用激活函数a( )是ReLu，输出层的激活函数是线性linear（也可看成是没有激活函数）。隐藏层都是1000节点。损失函数L( )是用于比较距离MSE：mean((output - target)^2)。MSE越小表示预测效果越好。训练过程就是不断减小MSE的过程。到此所有数据的维度都已确定：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;训练数据：&lt;equation&gt;input \in R^{39} ;label \in R^{48}&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;权重矩阵：&lt;equation&gt;W_{h1}\in R^{1000x39};W_{h2}\in R^{1000x1000} ;W_{o}\in R^{48x1000}&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;偏移向量：&lt;equation&gt;b_{h1}\in R^{1000};b_{h2}\in R^{1000} ;b_{o}\in R^{48}&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;网络输出：&lt;equation&gt;output \in R^{48}&lt;/equation&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;数据预处理（preprocessing）：&lt;/strong&gt;将所有样本的input和label处理成能够使用神经网络的数据，label的值域符合激活函数的值域。并简单优化数据以便让训练易于收敛。比如中心化（mean subtraction）、归一化（normlization）、主成分分析（PCA）、白化（whitening）。假设上图的input和output全都经过了中心化和归一化。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;权重初始化（weights initialization）&lt;/strong&gt;：&lt;equation&gt;W_{h1},W_{h2},W_{0}&lt;/equation&gt;在训练前不能为空，要初始化才能够计算loss从而来降低。&lt;equation&gt;W_{h1},W_{h2},W_{0}&lt;/equation&gt;初始化决定了loss在loss function中从哪个点开始作为起点训练网络。上图用均匀分布初始权重（Uniform distribution）。&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;训练网络（training）&lt;/strong&gt;：训练过程就是用训练数据的input经过网络计算出output，再和label计算出loss，再计算出gradients来更新weights的过程。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;正向传递：，算当前网络的预测值&lt;equation&gt;output =linear (W_{o} \cdot Relu(W_{h2}\cdot Relu(W_{h1}\cdot input+b_{h1})+b_{h2}) +b_{o})&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;计算loss：&lt;equation&gt;loss = mean((output - target)^2)&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;计算梯度：从loss开始反向传播计算每个参数（parameters）对应的梯度（gradients）。这里用Stochastic Gradient Descent (SGD) 来计算梯度，即每次更新所计算的梯度都是从一个样本计算出来的。传统的方法Gradient Descent是正向传递所有样本来计算梯度。SGD的方法来计算梯度的话，loss function的形状如下图所示会有变化，这样在更新中就有可能“跳出”局部最小值。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-0c0e7f5ffa98c2c1eb87763dd5d1d9a3.png" data-rawwidth="469" data-rawheight="227"&gt;&lt;/li&gt;&lt;li&gt;更新权重：这里用最简单的方法来更新，即所有参数都 &lt;equation&gt;W = W - learningrate * gradient&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;预测新值：训练过所有样本后，打乱样本顺序再次训练若干次。训练完毕后，当再来新的数据input，就可以利用训练的网络来预测了。这时的output就是效果很好的预测值了。下图是一张&lt;b&gt;实际值&lt;/b&gt;和&lt;b&gt;预测值&lt;/b&gt;的三组对比图。输出数据是48维，这里只取1个维度来画图。蓝色的是实际值，绿色的是实际值。最上方的是训练数据的对比图，而下方的两行是神经网络模型&lt;b&gt;从未见过&lt;/b&gt;的数据预测对比图。（不过这里用的是RNN，主要是为了让大家感受一下效果）&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-a675e692f7f7755d91bcdba5e988e910.jpg" data-rawwidth="2000" data-rawheight="1600"&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;注：此部分内容&lt;strong&gt;不是&lt;/strong&gt;这篇文章的&lt;strong&gt;重点&lt;/strong&gt;，但为了理解&lt;strong&gt;深层&lt;/strong&gt;神经网络，需要明白最基本的训练过程。 若能理解训练过程是通过梯度下降尽可能缩小loss的过程即可。 若有理解障碍，可以用python实践一下&lt;a href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/" data-editable="true" data-title="从零开始训练一个神经网络" class=""&gt;从零开始训练一个神经网络&lt;/a&gt;，体会整个训练过程。若有时间则可以再体会一下计算图自动求梯度的方便&lt;a href="https://www.tensorflow.org/versions/r0.11/tutorials/mnist/beginners/index.html#mnist-for-ml-beginners" data-editable="true" data-title="利用TensorFlow" class=""&gt;利用TensorFlow&lt;/a&gt;。&lt;/p&gt;&lt;h2&gt;结合&lt;a href="https://link.zhihu.com/?target=http%3A//playground.tensorflow.org/" class="" data-editable="true" data-title="Tensorflow playground"&gt;Tensorflow playground&lt;/a&gt;理解&lt;b&gt;5种空间操作&lt;/b&gt;和&lt;b&gt;物质组成视角&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;打开网页后，总体来说，蓝色代表正值，黄色代表负值。拿&lt;b&gt;分类&lt;/b&gt;任务来分析。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;数据：在二维平面内，若干点被标记成了两种颜色。黄色，蓝色，表示想要区分的两类。你可以把平面内的任意点标记成任意颜色。网页给你提供了4种规律。神经网络会根据你给的数据训练，再分类相同规律的点。&lt;/li&gt;&lt;/ul&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-6d17d1fb77d3ae1838f5253193456317.png" data-rawwidth="173" data-rawheight="169"&gt;&lt;ul&gt;&lt;li&gt;输入：在二维平面内，你想给网络多少关于“点”的信息。从颜色就可以看出来，&lt;equation&gt;x_{1}&lt;/equation&gt;左边是负，右边是正，&lt;equation&gt;x_{1}&lt;/equation&gt;表示此点的横坐标值。同理，&lt;equation&gt;x_{2}&lt;/equation&gt;表示此点的纵坐标值。&lt;equation&gt;x_{1}^{2}&lt;/equation&gt;是关于横坐标值的“抛物线”信息。你也可以给更多关于这个点的信息。给的越多，越容易被分开。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-111e37e8479aa57bedbfb2dbcd8e5b63.png" data-rawwidth="92" data-rawheight="228"&gt;&lt;/li&gt;&lt;li&gt;连接线：表示权重，蓝色表示用神经元的原始输出，黄色表示用负输出。深浅表示权重的绝对值大小。鼠标放在线上可以看到具体值。也可以更改。在（1）中，当把&lt;equation&gt;x_{2}&lt;/equation&gt;输出的一个权重改为-1时，&lt;equation&gt;x_{2}&lt;/equation&gt;的形状直接倒置了。不过还需要考虑激活函数。（1）中用的是linear。在（2）中，当换成sigmoid时，你会发现没有黄色区域了。因为sigmoid的值域是(0,1)&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-2820b0562c8fcd7a49d57c4deb1e4f3c.png" data-rawwidth="315" data-rawheight="136"&gt;（1）&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-83fd9f01e2ea38c7f6b8aeaa308cf040.png" data-rawwidth="294" data-rawheight="123"&gt;（2）&lt;/li&gt;&lt;li&gt;输出：黄色背景颜色都被归为黄点类，蓝色背景颜色都被归为蓝点类。深浅表示可能性的强弱。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-dff3f6e72881ebd222414eabb9504671.png" data-rawwidth="1116" data-rawheight="363"&gt;上图中所有在黄色背景颜色的点都会被分类为“黄点“，同理，蓝色区域被分成蓝点。在上面的分类分布图中你可以看到每一层通过上一层信息的组合所形成的。权重（那些连接线）控制了“如何组合”。神经网络的学习也就是从数据中学习那些权重。Tensorflow playground所表现出来的现象就是“在我文章里所写的“物质组成思想”，这也是为什么我把&lt;a href="https://link.zhihu.com/?target=http%3A//playground.tensorflow.org/" class="" data-editable="true" data-title="Tensorflow playground"&gt;Tensorflow playground&lt;/a&gt;放在了那一部分。&lt;/li&gt;&lt;/ul&gt;不过你要是把Tensorflow的个名字拆开来看的话，是tensor（张量）的flow（流动）。Tensorflow playground的作者想要阐述的侧重点是“&lt;b&gt;张量如何流动&lt;/b&gt;”的。&lt;b&gt;5种空间变换的理解&lt;/b&gt;：Tensorflow playground下没有体现5种空间变换的理解。需要打开这个网站尝试：&lt;a href="http://cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html" data-editable="true" data-title="ConvNetJS demo: Classify toy 2D data" class=""&gt;ConvNetJS demo: Classify toy 2D data&lt;/a&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-55811ac3d91f56f19543714b1b5abe49.png" data-rawwidth="841" data-rawheight="425"&gt;左侧是原始输入空间下的分类图，右侧是转换后的高维空间下的扭曲图。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a81a10592b96a1d2b067e1d4ae3951e7.png" data-rawwidth="848" data-rawheight="417"&gt;最终的扭曲效果是所有绿点都被扭曲到了一侧，而所有红点都被扭曲到了另一侧。这样就可以线性分割（用超平面（这里是一个平面）在中间分开两类）&lt;h1&gt;四、“深层”的思考：真的只有这些原因吗？&lt;/h1&gt;&lt;p&gt;文章的最后稍微提一下深层神经网络。深层神经网络就是拥有更多层数的神经网络。&lt;/p&gt;&lt;p&gt;按照上文在理解视角中所述的观点，可以想出下面两条理由关于为什么更深的网络会更加容易识别，增加容纳变异体（variation）（红苹果、绿苹果）的能力、鲁棒性（robust）。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;数学视角&lt;/strong&gt;：变异体（variation）很多的分类的任务需要高度非线性的分割曲线。不断的利用那5种空间变换操作将原始输入空间像“捏橡皮泥一样”在高维空间下捏成更为线性可分/稀疏的形状。 &lt;strong&gt;物理视角&lt;/strong&gt;：通过对“&lt;strong&gt;抽象概念&lt;/strong&gt;”的判断来识别物体，而非细节。比如对“飞机”的判断，即便人类自己也无法用语言或者若干条规则来解释自己如何判断一个飞机。因为人脑中真正判断的不是是否“有机翼”、“能飞行”等细节现象，而是一个抽象概念。层数越深，这种概念就越抽象，所能&lt;strong&gt;涵盖的变异体&lt;/strong&gt;就越多，就可以容纳战斗机，客机等很多种不同种类的飞机。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;然而深层神经网络的惊人表现真的只有这些原因吗？&lt;/strong&gt;&lt;strong&gt;为什么神经网络过深后，预测的表现又变差？ 而且这时变差的原因是由于“过深”吗？&lt;/strong&gt;&lt;strong&gt;接下来要写的《深层学习为何要“Deep”（下）》是关于“深层”二字的进一步思考，找出所有网络结构的共性，并解释设计神经网络的本质是什么。&lt;/strong&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22888385&amp;pixel&amp;useReferer"/&gt;</description><author>YJango</author><pubDate>Wed, 12 Oct 2016 02:33:48 GMT</pubDate></item><item><title>逻辑与神经之间的桥</title><link>https://zhuanlan.zhihu.com/p/22457562</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/fc658b76b2f0ed0c863953573cd5f462_r.jpg"&gt;&lt;/p&gt;这是我迄今为止最有成果的论文，欢迎讨论！&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-2860c7305704f8a9f47e3dec8c3f7cb0.jpg" data-rawwidth="963" data-rawheight="1509"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-1ab416079097690e31fa00c95c1b97ea.jpg" data-rawwidth="955" data-rawheight="1483"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-3eeb1d2483a14d86f38a4e750d623d08.jpg" data-rawwidth="955" data-rawheight="1495"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-68eb7652697f1969b0b4cf13f001d5e8.jpg" data-rawwidth="953" data-rawheight="1491"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-977f44c4f962b2f4b5bf630153ea2a3d.jpg" data-rawwidth="956" data-rawheight="1485"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-c2a24fb3f29db902b505acc436480404.jpg" data-rawwidth="954" data-rawheight="1489"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-1ba6dc1c99e604b25733b52d620ef12a.jpg" data-rawwidth="956" data-rawheight="1491"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-109ad8458276933f1195ccde55a638c4.jpg" data-rawwidth="951" data-rawheight="1488"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-f6451264e7924fd973676972a7f6e49b.jpg" data-rawwidth="962" data-rawheight="349"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22457562&amp;pixel&amp;useReferer"/&gt;</description><author>甄景贤</author><pubDate>Thu, 15 Sep 2016 12:55:35 GMT</pubDate></item><item><title>最前沿 之 谷歌的协作机械臂</title><link>https://zhuanlan.zhihu.com/p/22758556</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-6f589df38509d14f839737645322a011_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;本文由我和&lt;a href="https://www.zhihu.com/people/li-yi-ying-73" data-editable="true" data-title="李艺颖" class=""&gt;李艺颖&lt;/a&gt;共同撰写。&lt;/p&gt;&lt;h2&gt;1 前言&lt;/h2&gt;&lt;p&gt;前几天也就是2016年10月3号，Google Research Blog上发表了最新的Blog，介绍他们在机器人上的工作：&lt;a href="https://research.googleblog.com/2016/10/how-robots-can-acquire-new-skills-from.html" data-editable="true" data-title="googleblog.com 的页面" class=""&gt;https://research.googleblog.com/2016/10/how-robots-can-acquire-new-skills-from.html&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-6bbe8af2dbc2e7e7036c00aac03a5917.png" data-rawwidth="1898" data-rawheight="1228"&gt;现在Google们都喜欢发文章+写博客的套路，似乎不太需要我们做面向大众的解读了。因此，本文决定不介绍他们基本的工作了，我们来研究一下他们的具体工作，来点干货。&lt;/p&gt;&lt;p&gt;这次Google联合了Google Brain和DeepMind一起搞，一次发了四篇文章（够狠，链接转自官方博客）：&lt;/p&gt;&lt;p&gt;【1】&lt;a href="https://arxiv.org/abs/1610.00633" data-editable="true" data-title="Deep Reinforcement Learning for Robotic Manipulation" class=""&gt;Deep Reinforcement Learning for Robotic Manipulation&lt;/a&gt;. &lt;i&gt;Shixiang Gu, Ethan Holly, Timothy Lillicrap, Sergey Levine.&lt;/i&gt; [&lt;a href="https://sites.google.com/site/deeproboticmanipulation/" data-editable="true" data-title="video" class=""&gt;video&lt;/a&gt;]【2】&lt;a href="https://arxiv.org/abs/1610.00696" data-editable="true" data-title="Deep Visual Foresight for Planning Robot Motion" class=""&gt;Deep Visual Foresight for Planning Robot Motion&lt;/a&gt;. &lt;i&gt;Chelsea Finn, Sergey Levine.&lt;/i&gt; [&lt;a href="https://www.youtube.com/watch?v=CKRWJEVSXMI" data-editable="true" data-title="video"&gt;video&lt;/a&gt;] [&lt;a href="https://sites.google.com/site/brainrobotdata/home" data-editable="true" data-title="data"&gt;data&lt;/a&gt;]【3】&lt;a href="https://arxiv.org/abs/1610.00673" data-editable="true" data-title="Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search" class=""&gt;Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search&lt;/a&gt;. &lt;i&gt;Ali Yahya, Adrian Li, Mrinal Kalakrishnan, Yevgen Chebotar, Sergey Levine.&lt;/i&gt;  [&lt;a href="https://youtu.be/ZBFwe1gF0FU" data-editable="true" data-title="video"&gt;video&lt;/a&gt;]【4】&lt;a href="https://arxiv.org/abs/1610.00529" data-editable="true" data-title="Path Integral Guided Policy Search" class=""&gt;Path Integral Guided Policy Search&lt;/a&gt;. &lt;i&gt;Yevgen Chebotar, Mrinal Kalakrishnan, Ali Yahya, Adrian Li, Stefan Schaal, Sergey Levine. &lt;/i&gt;[&lt;a href="https://www.youtube.com/watch?v=ncp1kY5JV90" data-editable="true" data-title="video"&gt;video&lt;/a&gt;]&lt;/p&gt;&lt;p&gt;所以，今天我们来以快速的分析一下这四篇文章。&lt;/p&gt;&lt;h2&gt;2 Deep Reinforcement Learning for Robotic Manipulation &lt;/h2&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-4a8ecedc83c7b64c450d760978316c0b.png" data-rawwidth="1234" data-rawheight="660"&gt;&lt;p&gt;这篇文章的题目弄得很大，用深度增强学习来解决机器人的操纵问题。具体一点就是让机器人从零开始学会开门。&lt;/p&gt;&lt;p&gt;文章中表示实现让机械臂自己学会开门就是他们的贡献，他们是第一个做出这个demonstration的。当然，这么说也是ok的。&lt;/p&gt;&lt;p&gt;对于理论上的创新，他们表示他们拓展了NAF算法，变成异步NAF。我表示异步的思想早就有了，而且他们的异步NAF的实现方式实在是太太太简单了。就是&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;用多个线程收集不同机器人的数据，然后用一个线程去训练，并且训练线程在服务器上，训练后不断把最新的神经网络参数传递给每一个机器人用于新的采样&lt;/b&gt;。&lt;/blockquote&gt;&lt;p&gt;可能让机器人自己开门这个任务在我看来是本来就能实现的，所以其实并没有太多震撼的地方。而且，在这篇文章中，并&lt;b&gt;不使用视觉输入！&lt;/b&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;不使用视觉输入，声称能让机器人学会开门有多大意义呢?&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;从文章中可以看到，对于开门这个任务，门把手的位置的给定的：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;In addition, we append a target position to the
state, which depends on the task: for the door
opening, this is the handle position when the door is closed and the quaternion measurement of the sensor attached to
the door frame.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;大家看到了吧，不但门把手的位置，连门的姿态也就是四元数quaternion也是有传感器来测量的。那么，这样号称做了一个很屌的Demonstration真的有意思吗？比较&lt;b&gt;质疑这篇文章的贡献&lt;/b&gt;。这里使用的神经网络也只是两个200的全连接神经网络。某种程度上讲，其实这个任务的状态输入是低维而不是高维的。我觉得如果这篇文章能够完全使用图像输入来实现端到端自学习的话，那么就很厉害！虽然该团队之前有&lt;a href="https://arxiv.org/abs/1603.02199" data-editable="true" data-title="一篇文章"&gt;一篇文章&lt;/a&gt;就是用视觉输入，但是用了十几台机器人，并且训练几个月来收集数据。现在这个任务只要几小时，不过没有视觉，意义不够大。&lt;/p&gt;&lt;p&gt;关于NAF算法，可以参考本专栏的：&lt;a href="https://zhuanlan.zhihu.com/p/21609472?refer=intelligentunit" data-editable="true" data-title="DQN从入门到放弃7 连续控制DQN算法-NAF - 智能单元 - 知乎专栏" class=""&gt;DQN从入门到放弃7  连续控制DQN算法-NAF - 智能单元 - 知乎专栏&lt;/a&gt;&lt;/p&gt;&lt;p&gt;关于DDPG算法，可以参考本人的CSDN blog：&lt;a href="http://blog.csdn.net/songrotek/article/details/50917337" data-editable="true" data-title="Paper Reading 3:Continuous control with Deep Reinforcement Learning" class=""&gt;Paper Reading 3:Continuous control with Deep Reinforcement Learning&lt;/a&gt;&lt;/p&gt;&lt;p&gt;关于DDPG的源码复现，可以参考本人的github：&lt;a href="https://github.com/songrotek/DDPG" data-editable="true" data-title="GitHub - songrotek/DDPG: Reimplementation of DDPG(Continuous Control with Deep Reinforcement Learning) based on OpenAI Gym + Tensorflow" class=""&gt;GitHub - songrotek/DDPG: Reimplementation of DDPG(Continuous Control with Deep Reinforcement Learning) based on OpenAI Gym + Tensorflow&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;3 Deep Visual Foresight for Planning Robot Motion&lt;/h2&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-644e1a2ea1c79ff7492365e10044eb5a.png" data-rawwidth="1052" data-rawheight="744"&gt;这篇文章和深度增强学习没有直接关系，完全另外一个思路。为什么研究这个，我们先来说说&lt;b&gt;机器人学习之难难在哪？&lt;/b&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;机器人学习之难难在环境不完全可见，难在没有Model！&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;在控制领域，有一种任务相对比较好做，比如火箭发射！（当然也是很难的大工程），但是火箭发射上个世纪就解决了，人类甚至可以发射探测器到很远的地方，火星探测器也上了好几次了。Elon Musk前不久才提出他的火星登陆计划：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-df90f1f33f6539d183c3619d9a4f733e.png" data-rawwidth="2428" data-rawheight="1078"&gt;&lt;blockquote&gt;&lt;b&gt;为什么人类早早就能把探测器发送到那么远的地方，却连“简单”的让机器人开个门都那么难？&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;因为对于航天工程，我们可以精确的计算探测器，火箭的运动模型（运动方程），我们可以精确的计算出火箭在怎样的推力下会达到的轨迹，因此我们可以精确的控制。我们有足够的人力，足够的资源来进行数学计算，我们也就能够实现很好的控制。但是&lt;b&gt;对于机器人开门这种事，我们没法算&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;为什么？&lt;/p&gt;&lt;ol&gt;&lt;li&gt;每个门都可能不一样，门把手也不一样&lt;/li&gt;&lt;li&gt;机器人的位置不固定，门的位置也不固定。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们可以针对某个特定的门精确计算来实现控制，但是我们不可能遍历所有的门，更何况对于一个新的门怎么办？大家看到了，这里没有所谓的Model模型，我们无法建立模型，特别是如果我们只使用摄像头，类似人的第一视角，那么我们得到的信息更是有限。每时每刻的控制都将不一样。这就是机器人学习难的原因。&lt;/p&gt;&lt;p&gt;所以，深度增强学习的很多方式都是所谓的Model-Free的方法，也就是不需要模型，通过trial-and-error来学会整个过程。&lt;/p&gt;&lt;p&gt;可是，人类并不仅仅是通过trial-and-error来学习的。我们人类其实在大脑里能够构建一些基本的模型的，也就是比如门把手的位置，很多东西的位置，在我们大脑中是有概念的，我们也能够预测他们的位移。特别是足球的守门员，就需要掌握一项技能，那就是预判球的位置。&lt;/p&gt;&lt;p&gt;所以，问题就这么来了：我们能不能来预测一下物体的位置，从而帮助机器人抓取物品？&lt;/p&gt;&lt;p&gt;这篇文章也就做了这个事，弄了一个物体预测模型来预测物体的位置。本质上是未来研究model-based的方法。&lt;/p&gt;&lt;p&gt;个人看法：model-based和model-free方法结合起来用能使机器人学习发挥出更大威力。&lt;/p&gt;&lt;p&gt;关于预测模型，其实这篇文章也不新鲜了，之前就有文章研究预测atari游戏的画面的，也有文章预测汽车的运行轨迹的。只是这篇文章比较具体，面向机器人控制的具体问题，把预测模型和机器人控制MPC直接结合在一起，从而形成了一个确实的demo。&lt;/p&gt;&lt;p&gt;所以，重复一下，&lt;b&gt;这篇文章的关键不是弄成一个预测模型，而是真正把模型给用在机器人控制上。在文章中，作者也是说的很明确，没有夸大的成分：&lt;/b&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;The primary contribution of our paper is to demonstrate
that deep predictive models of video can be used by real
physical robotic systems to manipulate previously unseen
objects.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我们稍微来说一下这个深度预测模型：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-a61c2f5dd46954bdd64d884fa7c1753a.png" data-rawwidth="2378" data-rawheight="842"&gt;结构上搞的比较复杂。先说输入输出。输入有三个，一个是当前帧，当前的状态state和动作action，然后输出下一帧图像（预测）。&lt;/p&gt;&lt;p&gt;中间的结构大致可以分成三部分：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;CNN+LSTM部分，用于提取图像特征信息，不过这里的输出很特别，并没有使用反卷积直接生成图像，而是输出一个图像的mask蒙版。这个蒙版可以认为是计算出里面的物体的移动流也就是pixel flow。&lt;/li&gt;&lt;li&gt;状态和动作输入部分，将状态和动作从卷积层的中间插入。&lt;/li&gt;&lt;li&gt;图像生成部分。首先是利用中间的卷积层抽取多个卷积核然后与原始图像做卷积，得到变化后的图像transformed images，然后和mask一起生成pixel flow map F，然后F与原始图像一起生成下一帧的图像。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;所以这个神经网络本质上是要学习出物体移动的像素流pixel flow，类似光流，从而计算出下一帧的图像。作者虽然限定了神经网络的结构，但是在学习训练时只使用视频，动作，状态数据做监督学习，也就是端到端的学习，中间的mask蒙版和像素流并没有单独的监督学习。&lt;b&gt;然而文章中并没有对mask和flow的具体表现形式做分析，是否就是产生出对物体运动的捕捉我比较怀疑。&lt;/b&gt;只能说通过训练，神经网络确实学习了捕捉物体的运动并能够根据输入加以预测。&lt;/p&gt;&lt;p&gt;接下来是这篇文章的区别其他文章的工作，直接使用这个预测模型与经典的MPC控制结合，来实现机械臂的控制。基本是思路就是利用预测模型预测不同动作未来的移动情况，从而选择最优的移动方式。这种方式取得成功说明利用预测模型进行机器人控制的可行性。&lt;b&gt;下一步进一步拓展深度预测模型将成为可能，&lt;/b&gt;这也是这篇文章最大的意义。&lt;/p&gt;&lt;h2&gt;4 Path Integral Guided Policy Search&lt;/h2&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-81ffccb38f7e3c8ca417bdb6949f3adc.png" data-rawwidth="1164" data-rawheight="488"&gt;&lt;p&gt;这篇文章以及下一篇文章是对Sergey Levine提出的GPS算法的拓展和改进。关于GPS（Guided Policy Search）这个算法，我一直觉得不是一个好的算法，至少未来这个算法我认为没有必要（为什么之后说），但是这个算法展示性比深度增强学习算法强，能够直接应用到真实的机器人上。先说一下深度增强学习应用到机器人上最大的困难，就是&lt;b&gt;采样！&lt;/b&gt;我们可以在仿真环境中千百万次的训练机器人，但是我们没办法在真实环境中这么做。时间不允许，机器人也不允许。以此同时，完全的高维输入，高维输出做机器人控制目前仍是困难很大的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;那么GPS是什么呢？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;它的想法非常简单，就是把问题先分解成不同的初始条件，比如开门，有的是这个角度，有的是那个角度。然后，针对不同的条件单独训练一个局部策略Local Policy。那么这个训练方式他这里&lt;b&gt;不管了！！！也就是你想用什么传统的控制算法都可以！&lt;/b&gt;然后有了这些可以用的策略之后，就利用策略采集样本，只是把输入变了，比如变成视觉输入，然后利用样本训练一个网络来代替这些局部策略，也就是模仿学习Imitation Learning，通过这种方式实现视觉伺服。我表示&lt;b&gt;GPS很没意思。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;为什么没意思？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;就说机器人开门这件事吧。我们要实现视觉控制，也就是让机器人看着打开门，和人一样。我们希望即使门放在不同的角度位置机器人也可以开。按照深度增强学习做法，那必须是End-to-End啊，输入视觉信息，输出控制，然后训练。从Deep Reinforcement Learning for Robot Manipulation这篇我们大概可以猜测出，这样做失败了，所以那篇文章并没有使用视觉输入。但我现在就要视觉输入这么办？&lt;/p&gt;&lt;p&gt;&lt;b&gt;Imitation Learning！模仿学习&lt;/b&gt;&lt;/p&gt;&lt;p&gt;神经网络啥都能学习，因此只要我们能够收集到好的输入输出样本，我们就可以训练。模仿学习就是这么干。我们可以利用人类获取样本。比如人拿着机械臂做几百次开门动作，然后记录这些动作作为样本进行训练。但是只是用人比较麻烦，不用人就用机器人控制的算法，比如LQR，我们就有model怎么啦。我们先利用机械臂的model信息和门的精确信息来优化出一条最优轨迹，然后这不就是样本了吗？为了实现神经网络的通用性，我们面对不同的门的角度位置弄多条对应的最优轨迹，然后收集所有样本进行训练。在拓展一下，就是反过来利用训练的神经网络生成样本，然后反过来让控制算法进行优化。&lt;/p&gt;&lt;p&gt;&lt;b&gt;所以，虽然GPS能够实现视觉伺服控制，但是其中间过程一点也不单纯。使用了太多额外的信息来做训练。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在我看来，训练时也只使用视觉信息，不使用额外信息才是有用的算法。GPS最致命的地方也在这里，这个开门可以精确建模，可以有额外数据，但是很多其他任务可没有那么多额外数据可以弄。&lt;b&gt;GPS本质上不具备自学习能力，而只是传统方法的神经网络化。&lt;/b&gt;监督学习无法使机器人超越传统方法。&lt;/p&gt;&lt;p&gt;那么这篇文章又做了什么事呢？将LQR用一个model free的RL算法替代。PI2这个算法之前不是很了解，看了文章感觉就是一种进化算法。基本思想就是采用多种路径，然后让路径概率向着损失较小的方向靠。和CEM（交叉熵方法）也差不多。就是对于Cost，采样概率还有参数更新方式不一样。&lt;/p&gt;&lt;p&gt;有了PI2算法，GPS就可以做到model free了。但是训练过程还是一样。PI2算法的使用过程中并不使用视觉信息。只是因此采用的方法不一样（比起LQR），能够通过训练来解决开门这种间断连续控制问题（要先让机械臂移动到门把手那里，然后抓住，旋转，打开）。在我看来，直接用深度增强学习算法比如DDPG甚至REINFORCE来训练local policy不就完了，最后再综合所有样本监督学习一个，效果肯定好。&lt;/p&gt;&lt;p&gt;总的来说，GPS看似有用，实则鸡肋，改进它意义不大，还不如研究如何实现少样本学习。&lt;/p&gt;&lt;h2&gt;5 Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search&lt;/h2&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-324517c941a6e509a94a369ad86dc97e.png" data-rawwidth="1654" data-rawheight="534"&gt;这篇文章其实和Deep Reinforcement Learning for Robot Manipulation很类似，只是针对的算法不一样，说白了就将Guided Policy Search拓展成并行异步的版本，从而可以实现多机器人协同训练。&lt;/p&gt;&lt;p&gt;----------------------&lt;/p&gt;&lt;p&gt;下面的分析来自&lt;a href="https://www.zhihu.com/people/li-yi-ying-73" data-editable="true" data-title="李艺颖" class=""&gt;李艺颖&lt;/a&gt;：&lt;/p&gt;&lt;p&gt;从架构方面，这十分契合云机器人的主旨，机器人可以将它们各自的经验通过网络传递给其它机器人。斯坦福人工智能百年报告之《人工智能与2030年的生活》中也指出以家用机器人为例，多机器人协同能够使机器人“共享更多家庭内收集的数据集，反过来能提供给云端进行机器学习，进一步改进已经部署的机器人。”本文就是鉴于考虑到真实世界中环境具有多样性和复杂性，所以想到让机器人将自身经历传递给彼此，使它们在环境中相互配合来学习技能，同时机器人也基于自身任务的特定样本改进局部的策略。实验采用4个机器人，任务是基于视觉学习开门，4个机器人对应的门的姿态和外观也都有所不同，采用的算法是GPS，在它们反复尝试和共享经历中不断提高任务执行水平。多机器人提高了样本的多样性，提高了学习的泛化能力和可使用能力。&lt;/p&gt;-----------------------&lt;p&gt;算法的做法依然是很简单。一句话就可以概括。就是几个机器人分别有一个local policy来优化，每个机器人面对的场景都不一样。然后训练，将样本上传服务器，在服务器上监督学习一个神经网络，然后用这个神经网络Global Policy来辅助采样优化local policy。&lt;/p&gt;&lt;p&gt;只能说多机器人协助必然能够提升学习训练速度，但是这种idea非常简单，算法的改进也是非常简单。当然，我们也不得不承认，Google的整个实验难度很大，要训练好很难，甚至这个训练用的机械臂都是Google自己造的（话说Google高层不让卖）。&lt;/p&gt;&lt;h2&gt;6 小结&lt;/h2&gt;&lt;p&gt;Google似乎想惊艳一下大家，一次发四篇文章说他们的机器人进展。但是很可惜，从具体文章的内容和贡献来看，并没有太多惊艳的思想和效果。多机器人协作是必然，核心还在于算法的改进。当然，我们也必须承认，Google能够实现让机器人完全使用视觉来开门是一个不错的Demo，只是这个Demo是在当前算法框架下必然可以实现的，不过也只有土豪的Google能这么做。&lt;/p&gt;&lt;p&gt;最后，大家也看到了，&lt;b&gt;让机器人学会开门竟然是21世纪的今天人类最前沿科技都还没很好解决的问题&lt;/b&gt;，可见人类的文明程度是有多低。但，这也就是我们研究机器人实现机器人革命的机会！&lt;/p&gt;&lt;h2&gt;声明：本文为原创文章，未经允许不得转载。另外本文的图片都来自于本文介绍的四篇paper和网络。&lt;/h2&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22758556&amp;pixel&amp;useReferer"/&gt;</description><author>Flood Sung</author><pubDate>Fri, 07 Oct 2016 16:39:58 GMT</pubDate></item></channel></rss>