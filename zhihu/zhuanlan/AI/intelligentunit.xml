<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>智能单元 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/intelligentunit</link><description>面向通用人工智能和机器人学习，聚焦深度增强学习，可微神经计算机和生成对抗模型。</description><lastBuildDate>Thu, 09 Feb 2017 07:15:11 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>逻辑与神经之间的桥 (2.0版)</title><link>https://zhuanlan.zhihu.com/p/25027944</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-fc658b76b2f0ed0c863953573cd5f462_r.jpg"&gt;&lt;/p&gt;之前发表过了，但这次大修改后，有更实质的结论。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-faa076553f9d1165ee2239f6e4ae5de0.jpg" data-rawwidth="885" data-rawheight="1317"&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-e433ed5ad33031e754a2cd86bbc3d910.jpg" data-rawwidth="884" data-rawheight="1308"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-cf81c8b8ba907af107235a2d66f3dafe.jpg" data-rawwidth="879" data-rawheight="1315"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-a1d33a3100e2b4be6ace2893dd9f55be.jpg" data-rawwidth="886" data-rawheight="1316"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-9914470ca2ee5981d7dde8f38906aa6d.jpg" data-rawwidth="884" data-rawheight="1308"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-7822293996b1a46b538f946dec03cc4c.jpg" data-rawwidth="897" data-rawheight="1310"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-82f947cfd79b0941364281cfc4360d0c.jpg" data-rawwidth="882" data-rawheight="1320"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-b092c59c16f87e9f8840e526a305754a.jpg" data-rawwidth="886" data-rawheight="1309"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-032a241ccae593595041d0b54b8c47d5.jpg" data-rawwidth="903" data-rawheight="1309"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-d668c4e7e328423feaf773fb4c56d10c.jpg" data-rawwidth="891" data-rawheight="1306"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a1970406157b4e136e6ffdb75651de09.jpg" data-rawwidth="894" data-rawheight="1306"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-ad8961a77167a18eff21d2b76eb4ed4e.jpg" data-rawwidth="891" data-rawheight="749"&gt;（注： 这篇的内容建立在&lt;a href="https://zhuanlan.zhihu.com/p/23978763"&gt;《游荡在思考的迷宫中》&lt;/a&gt;的架构上）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/25027944&amp;pixel&amp;useReferer"/&gt;</description><author>甄景贤</author><pubDate>Thu, 26 Jan 2017 17:26:58 GMT</pubDate></item><item><title>最前沿：基于GAN和RL的思想来训练对话生成，通过图灵测试可期！</title><link>https://zhuanlan.zhihu.com/p/25027693</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-3ce09841f5ec27b18afa2f741512b78e_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;PS：本文分析略深，需要一定的RL和GAN的基础。&lt;/p&gt;&lt;p&gt;前两天，Stanford的NLP小组出了一篇神经网络对话生成的论文：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-a5577184d4a57d19befb57d33c1afecd.png" data-rawwidth="2006" data-rawheight="734"&gt;原文链接：&lt;a href="https://arxiv.org/pdf/1701.06547.pdf" data-editable="true" data-title="arxiv.org 的页面"&gt;https://arxiv.org/pdf/1701.06547.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;标题就是使用对抗学习来做神经对话生成。&lt;/p&gt;&lt;p&gt;这个idea非常的赞！在我看来是通往图灵测试的正确一步。&lt;/p&gt;&lt;p&gt;以前的对话生成，我们使用Seq2Seq的监督学习，其实也就是模仿学习。但是模仿学习的问题是神经网络的理解能力有限，训练样本有限，只能生成一定程度的对话。&lt;/p&gt;&lt;p&gt;那么，有没有可能让计算机真正理解对话的意思，然后自己学会对话呢？&lt;/p&gt;&lt;p&gt;有了深度增强学习，有了AlphaGo大家可以知道这是可能的。事实上这篇论文的作者Jiwei Li之前的一篇文章就是用深度增强学习来做对话生成。&lt;/p&gt;&lt;p&gt;但是使用深度增强学习最大的问题就是需要有reward。没有reward没法训练。&lt;/p&gt;&lt;p&gt;但是怎么定义一个对话的reward呢？好困难，有太多评价标准。但是有一个标准是绝对的，就是图灵测试的标准。只要这个对话看起来像人说的就行了。&lt;/p&gt;&lt;p&gt;这就不得不联系到了GAN生成对抗网络。把GAN中的分类器用来对对话做分类就行了。这样训练出来的分类器可以一定程度上判断计算机生成的对话与人的对话的差距。而这个&lt;strong&gt;差距就是reward&lt;/strong&gt;！&lt;/p&gt;&lt;p&gt;这篇文章可以说把DRL和GAN的思想很好的结合起来并应用在对话生成问题上，也取得了比较好的效果。相信在这个方法的基础上进一步发展，比如改进网络结构，将对话拓展到段落，更多的训练等等。&lt;strong&gt;也许3-5年图灵测试就真正通过了，而这一次，是机器自己真正学会了交流！&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;PS：本文同步发于“智能单元”微信公众号，欢迎大家关注，第一时间获取通用人工智能原创资讯！&lt;/strong&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/25027693&amp;pixel&amp;useReferer"/&gt;</description><author>Flood Sung</author><pubDate>Thu, 26 Jan 2017 16:44:01 GMT</pubDate></item><item><title>汉字生成模型的那些坑</title><link>https://zhuanlan.zhihu.com/p/24805121</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-56db09cd10a86d006257d0ff386dc2f7_r.jpg"&gt;&lt;/p&gt;&lt;h2&gt;引言&lt;/h2&gt;&lt;p&gt;人工智能技术目前仍处于技术积累期，人们真正希望看到的人工智能技术是要能够融入生活中的，真正为人们带来便利的技术，但目前能真正做到“实用化”的人工智能产品并不多。除了大家熟悉的，如机器翻译、聊天机器人、语音识别与生成、图像分类、路径规划、智能跟踪等，我认为汉字生成是最具有潜力、最早一批进入市场的人工智能产品，因为其领域相对较小（与Robot、NLP等比较），而且成本硬件较低，这两年汉字生成问题也确实受到了越来越多的关注。&lt;/p&gt;目前汉字生成模型主要应用场景包括&lt;b&gt;生成手写字体&lt;/b&gt;以及生&lt;b&gt;成手写汉字&lt;/b&gt;两个方面。生成手写字体主要是想解决中文字太多，设计一款新的字体需要巨大工程量的问题，如果设计师仅手工设计少量的字体文字，然后机器就能根据这些问题提取出其中的风格然后自动设计生成剩余文字，则可以极大减轻设计师的负担。生成手写汉字是指采集某人的书写笔迹，让计算机学会模仿他的风格进行书写，我认为这是汉字生成的最终目标，具体而言就是要让机械臂拿起笔帮我们处理手写任务，比如代替我们写亲笔信，或帮我们做手写试卷等。&lt;p&gt;汉字生成模型在技术上主要可以分为&lt;b&gt;基于笔迹的生成模型&lt;/b&gt;以及&lt;b&gt;基于图像的生成模型&lt;/b&gt;。二者的区别来源于应用场景的不同，若以设计一个会写字的机械臂为目标，则必须得到具体的笔画；若以生成新的中文字体为目标，则可以仅依赖文字的图像。&lt;/p&gt;&lt;p&gt;目前研究手写问题的文章并不是很多，但手写问题涉及到的领域非常广，不仅需要RNN而且若想真正做好还需要加入GAN、RL等人工智能技术。&lt;/p&gt;&lt;h2&gt;一、样本的表示方法&lt;/h2&gt;&lt;p&gt;样本的表示是一个值得探究的问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;如果样本是文字图像：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;大家很容易理解，但这种方法会损失书写的笔画顺序、文字起点与终点、每一个笔画的起笔与落笔等信息。&lt;/li&gt;&lt;li&gt;该方法也是有优点的，比如容易用CNN处理，比较容易提取风格特征，生成速度较快（一次生成整张图片）等。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;如果样本是文字笔迹：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;首先考虑是连续问题还是离散问题~&lt;/li&gt;&lt;li&gt;离散问题具体而言就是将文字投影到一个离散的画布上如100x100像素的画布，其中的每一个笔画由若干点组成，每一个点对应一个整数型坐标。更进一步，可以将每一个笔画分解为若干个关键点，关键点直接用直线连接，这样一条直线上就可以仅用起点与终点表示，可以省略掉中间的点。离散问题在生成文字时（每次生成一个点）可以看作一个&lt;b&gt;分类问题&lt;/b&gt;，因此比较容易训练，但其最大问题就是会损失一定精度。&lt;/li&gt;&lt;li&gt;连续问题就是将文字投影到（-1,1）的连续空间，用浮点数来精确表示每个点的坐标，这也是目前主流的方法，其本质上是一个&lt;b&gt;回归问题&lt;/b&gt;，也因此训练难度较高。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;样本是笔迹，在连续空间上，如何具体表示呢？&lt;/li&gt;&lt;ul&gt;&lt;li&gt;如果样本是笔迹，笔迹是由点组成的，每一个点&lt;equation&gt;p&lt;/equation&gt;可以用&lt;equation&gt;\{(x,y),(a,b,c)\}&lt;/equation&gt;这种形式来表示，其中&lt;equation&gt;(x,y)&lt;/equation&gt;是坐标，&lt;equation&gt;(a,b,c)&lt;/equation&gt;是一个分类向量，可以设&lt;equation&gt;a&lt;/equation&gt;是一个笔画的结束，&lt;equation&gt;b&lt;/equation&gt;是一般的点，&lt;equation&gt;c&lt;/equation&gt;是整个文字的结束。因此一个文字就可以用点的序列表示：&lt;equation&gt;S=(p_0,p_1,\cdots,p_n)&lt;/equation&gt;。&lt;/li&gt;&lt;li&gt;还有一个问题需要考虑，就是应该采用绝对坐标还是相对坐标呢？绝对坐标很容易理解，而相对坐标就是下一个点的&lt;equation&gt;(x,y)&lt;/equation&gt;并不是绝对位置，而是相对于上一个点的偏移。个人以为，两种方法皆可，但是相对表示表现更好，因为误差不会跃层向下传播。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;h2&gt;二、用RNN完成最基本的生成任务&lt;/h2&gt;&lt;p&gt;我们首先考虑一个最简单的生成模型：给若干个汉字的训练样本（笔迹样本），然后训练神经网络来生成风格类似的文字。&lt;/p&gt;&lt;p&gt;具体而言，就是用一个RNN训练，RNN的输入是要书写文字的&lt;b&gt;高维特征向量&lt;/b&gt;和一个&lt;b&gt;开始标识&lt;/b&gt;。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;高维特征向量&lt;/b&gt;代表我需要生成哪一个文字，其实也可以用一个one-hot向量表示，但由于汉字的数量很多，用一太长的向量表示比不上用一个更有意义的维度较低的向量表示，这也是借鉴了NLP中的word2vec的思想，这个更有意义的向量应该可以反映出要生成的汉字的固有特征。另外高维特征向量需要预先训练，可以采用自编码技术，或截取一个已训练神经网络的特征提取部分。&lt;/li&gt;&lt;li&gt;&lt;b&gt;开始标&lt;/b&gt;&lt;b&gt;识&lt;/b&gt;类似于NLP中的一段话的开始标志，如&amp;lt;begin&amp;gt;Hello world&amp;lt;end&amp;gt;，中的&amp;lt;begin&amp;gt;，具体到文字生成中，开始标志是一个特殊的点如&lt;equation&gt;\{(0,0),(0,1,0)\}&lt;/equation&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;具体的生成方法采用一次生成一个点的方式进行，直到生成结束点为止，举例而言就是给出一个文字比如“宋”，然后一笔一划地生成一个手写的“宋”，生成流程如图所示：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-c226c2a454f216e8a4f0e803812e1fca.png" data-rawwidth="704" data-rawheight="381"&gt;&lt;p&gt;在训练中，Loss的定义至关重要，在连续空间中采用传统的L2损失函数可能会导致无法拟合多值函数的问题，因此建议采用Mixture Density Loss，具体可以参考&lt;a href="http://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/" data-title="这篇文" class="" data-editable="true"&gt;这里&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;总体而言，做到这一步就算实现了一个简单的demo了，但是还有很多地方需要优化，离实用还有很多工作要做，我们将继续往前探索。&lt;/p&gt;&lt;h2&gt;三、风格提取与迁移&lt;/h2&gt;&lt;p&gt;风格提取与迁移问题最近比较热，比如生成风格化的图片，生成某人的声音等。风格提取主要是为了解决样本少的问题，我们希望我们设计的手写机器人能够仅训练某个用户的少量手写文字就能自动分析出该用户的手写风格，然后再根据先验知识（所有中文文字的书写规则）就可以生成训练集中未出现过的文字。&lt;/p&gt;&lt;p&gt;风格迁移就是强制神经网络学习到目标字体与参考字体间风格的异同，具体来说就是求出风格转移函数&lt;equation&gt;font_2=f(font_1)&lt;/equation&gt;，比如输入一个宋体的“我”字，输出楷体的“我”。最早的基于图像的风格迁移算法采用了传统的CNN模型，在训练时，输入参考字体（如宋体），输出目标字体（如楷体），训练完后在生成时输入新的参考字体，生成对应的目标字体，网络结构和生成示例如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-bf1bf0f9c01fec243cb94c814e9c5dab.png" data-rawwidth="960" data-rawheight="540"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-f239324798c56f061f1c2d8f6b2b1854.png" data-rawwidth="960" data-rawheight="540"&gt;&lt;p&gt;&lt;i&gt;（图片来源：&lt;/i&gt;Rewrite: Neural Style Transfer For Chinese Fonts）&lt;/p&gt;&lt;p&gt;这种仅采用CNN的基于图像的风格迁移算法整体表现并不是很好，在差异比较大的字体之间的迁移效果欠佳，这说明只通过字体的图像，神经网络难以学习到汉字风格的深度特征。&lt;/p&gt;&lt;p&gt;所以最近又有研究者将样本由图片表示替换为笔迹表示，现已较好地解决了风格迁移问题，风格迁移效果如下图所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-4febf45a4a0b1dda8ea3662eaaa64271.png" data-rawwidth="385" data-rawheight="335"&gt;&lt;i&gt;（图片来源： Automatic Generation of Large-scale Handwriting Fonts via Style Learning）&lt;/i&gt;&lt;/p&gt;&lt;p&gt;这种算法首先对参考文字进行笔迹（点）的提取，然后再对目标字体也提取相等数量的点，然后求二者的误差，作为最终的Loss。但现有的这种基于笔迹的算法仍有改进空间，因为该算法在提取笔迹（关键点的个数）和loss的定义上都还是人为设定的，在这两个方面应该还有较大优化的空间，真正的人工智能应该尽可能地减少人为干预。&lt;/p&gt;&lt;p&gt;总之风格提取在文字生成算法中还有一段较长的路要走，其主要研究点应该包括如何更好地定义风格的异同，如何设计一种尽量少依赖人工干预的，鲁棒性更强的表示方法；以及如何优化存储风格的神经网络结构。&lt;/p&gt;&lt;h2&gt;四、更真实的文字&lt;/h2&gt;&lt;p&gt;我们人在书写的过程中，哪怕写100个“我”字，也不会出现两个完全相同的。但是传统的神经网络无法解决差异化输出的问题，我们用一个训练好的神经网络，输出的文字总是一模一样的，因此这样的手写机器人仍不能代替人来写字。&lt;/p&gt;&lt;p&gt;目前，尚未看到有关手写汉字领域的差异化输出相关研究。但是在图像生成领域有一些相关的解决方案，那就是对抗网络生成模型GAN。该模型包括生成器（G）和鉴别器（D），前者用一个随机的100维向量作为输入，目的是为了生成一个与训练集中的图像类似（风格相似）的一张图像，后者是一个二元分类器，用于鉴别该图像是否是在训练集中，每次训练时将生成的图像与训练集中的图像各取50%，进行有监督的学习，标签就是该图像是真实的还是伪造的。生成器需要尽可能生成具有类似风格的图片，而鉴别器要尽可能区分伪造的和真实的，具体公式可以表示为：&lt;/p&gt;&lt;equation&gt;\min \limits_G \max \limits_D D(x)-D(G(z)) &lt;/equation&gt;&lt;p&gt;该方法有些类似RL中的policy方法，两个网络同时训练，同时优化。训练完毕后，生成器可以看成是将训练样本的高维特征提取出来了，其输入的向量就是高维特征向量，根据该向量生成一张图片。&lt;/p&gt;&lt;p&gt;在手写问题上，我们可以采用该方法提取同一个文字的高维表示，然后在其中添加伪随机因子，由此生成各对应文字的不同副本。&lt;/p&gt;&lt;h2&gt;五、数据来源问题&lt;/h2&gt;&lt;p&gt;汉字生成模型由Demo到实用化还有较长的一段路要走，其中一个重要的问题就数据样本采集问题。目前表现最佳的算法主要采用具备更丰富信息的基于笔迹的样本数据，但是通常采集这种具有笔迹信息的样本是比较困难的，要么需要特制的手写笔，要么需要手写板，最经济的也需要装一个手机APP（用手写笔在手机屏幕上写）。但是实际上每个人都有大量的现成的写在纸上的字迹（图片样本），如果能够利用这些手写数据则可以获得更好的用户体验，同时也节约了人力物力。有些论文中提出了用人工的方法或一些特定的算法来提取笔迹但这显然不是良策，更好的方法应该是采用&lt;b&gt;增强学习&lt;/b&gt;的方法，让机器自己学习如何由图片样本提取笔迹，然后在此基础上进行进一步的训练，目前与这个点相关的研究较少，笔者也正在探索之中。&lt;/p&gt;&lt;p&gt;另外，若要尽一步减小人为干预，在增强学习之前还需要进行预处理，要有一个定位器和一个识别器，用于定位和识别已有图片样本中的每一个文字。（这些都是坑啊~~~）&lt;/p&gt;&lt;h2&gt;六、小结&lt;/h2&gt;&lt;p&gt;最后总结一下汉字生成模型的那些坑以及每个坑需要用什么土来填：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;如何表示文字：建议采用基于点的序列来表示，每个点可表示为&lt;equation&gt;\{(x,y),(a,b,c)\}&lt;/equation&gt;的形式。&lt;/li&gt;&lt;li&gt;如何生成文字：采用RNN网络每次生成一个点，直到生成具有结束标志的点。&lt;/li&gt;&lt;li&gt;如何提取文字风格：用已有的数据训练一个风格迁移网络，保存风格信息。&lt;/li&gt;&lt;li&gt;如何让生成的文字更真实（各不相同）：用对抗网络（GAN）提取每个文字的高维特征，并在此基础上加入伪随机因子。&lt;/li&gt;&lt;li&gt;如何充分利用已有的图片样本数据：采用增强学习（RL）的方法，让机器自己学习如何由图片样本提取笔迹。&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;参考文献：&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;Generating Sequences with Recurrent Neural Networks.&lt;/li&gt;&lt;li&gt;Generating online fake Chinese characters with LSTM-RNN,” 2015. [Online]. Available: &lt;a href="http://blog.otoro.net/2015/12/28/recurrent-net-dreams-up-fake-chinese-characters-in-vector-format-with-tensorflow/" data-editable="true" data-title="Recurrent Net Dreams Up Fake Chinese Characters in Vector Format with TensorFlow" class=""&gt;Recurrent Net Dreams Up Fake Chinese Characters in Vector Format with TensorFlow&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Automatic Generation of Personal Chinese Handwriting by Capturing the Characteristics of Personal Handwriting.&lt;/li&gt;&lt;li&gt;Drawing and Recognizing Chinese Characters with Recurrent Neural Network.&lt;/li&gt;&lt;li&gt;Automatic Generation of Large-scale Handwriting Fonts via Style Learning.&lt;/li&gt;&lt;li&gt;Learning Typographic Style.&lt;/li&gt;&lt;li&gt;Rewrite: Neural Style Transfer For Chinese Fonts [Online]. Available:&lt;a href="https://github.com/kaonashi-tyc/Rewrite" data-editable="true" data-title="kaonashi-tyc/Rewrite" class=""&gt;kaonashi-tyc/Rewrite&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Generative Adversarial Nets.&lt;/li&gt;&lt;/ol&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24805121&amp;pixel&amp;useReferer"/&gt;</description><author>Lonely.wm</author><pubDate>Mon, 23 Jan 2017 12:21:15 GMT</pubDate></item><item><title>Q&amp;A 知乎Live：从AlphaGo看人工智能前沿技术</title><link>https://zhuanlan.zhihu.com/p/24977176</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-3ce09841f5ec27b18afa2f741512b78e_r.jpg"&gt;&lt;/p&gt;&lt;h2&gt;1 前言&lt;/h2&gt;&lt;p&gt;在昨晚的 &lt;a href="https://www.zhihu.com/lives/802155571712253952?utm_campaign=zhihulive&amp;amp;utm_source=zhihucolumn&amp;amp;utm_medium=Livecolumn" data-title="知乎Live：从AlphaGo看人工智能前沿技术" class="" data-editable="true"&gt;知乎Live：从AlphaGo看人工智能前沿技术&lt;/a&gt; 中，大家提了很多问题，由于Live时间的限制，来不及一一回复。这里将一些在Live中没有回答到的问题回答在这里。&lt;/p&gt;&lt;h2&gt;2 Q&amp;amp;A&lt;/h2&gt;&lt;p&gt;（1）深度增强学习和深度学习有什么联系和区别？&lt;/p&gt;&lt;p&gt;答：深度增强学习是深度学习+增强学习，可以说是深度学习的拓展分支。深度增强学习的核心是在增强学习的框架下使用多层神经网络来表示策略网络Policy Network或价值网络Value Network&lt;b&gt;。增强学习为深度学习提供学习目标，深度学习则提供学习的机制。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;（2）深度增强学习这个技术在以后无法替代的人工领域或技能可能是哪些？&lt;/p&gt;&lt;p&gt;答：由于深度增强学习是一个面向通用人工智能的算法，这个算法的不断发展将会不断替代一些之前认为机器不可能做的人类工作。现在我们可见的是翻译，自动驾驶，监控等领域，接下来很多金融决策，医疗诊断，语音助理（语音客服）都会被取代。可能现阶段一些涉及人类感性因素的工作比如传媒行业，艺术创作等比较不好被取代，但是从深度增强学习的理论上来看，机器也有可能通过训练来理解人类的感性因素，比如什么是幽默，什么好笑，什么是爱。一旦机器也能解决这样的问题，恐怕就没有什么是不能被取代的了。当然，这还需要好多年。&lt;/p&gt;&lt;p&gt;（3）深度增强学习本身的逻辑是什么？&lt;/p&gt;&lt;p&gt;答：深度增强学习最基本的逻辑有两个：一是不断试错，二是根据试错的经验评估行为的好坏，调整不同行为输出的可能性。&lt;/p&gt;&lt;p&gt;（4）这个人工智能理论可以反过来对人类学习有借鉴作用吗？比如二语学习。&lt;/p&gt;&lt;p&gt;答：对深度增强学习的研究确实是可以反过来更好的理解人类的学习方式，从而反过来提升我们人类自身的学习水平。从AlphaGo看，人类可以根据AlphaGo的棋谱来研究新的思路。从深度增强学习的算法本身，我们会更清楚的知道，要勇于试错，并且不断调整自己，要认识到自身必然存在的固执的思维。&lt;/p&gt;&lt;p&gt;（5）想请教下是否深度学习能在金融市场这个多人参与的博弈市场发挥功效？&lt;/p&gt;&lt;p&gt;答：显然是可以的，深度增强学习是一套面向决策控制的算法，而金融博弈正是这样的一种决策问题。但是金融市场毕竟存在大量的外部信息，要使一个人工智能程序能处理所有这些信息并给出正确的决策需要巨量的训练，存在较大的挑战。目前深度增强学习的研究正在从简单到复杂，从最基本的游戏到复杂的真实场景。&lt;/p&gt;&lt;p&gt;（6）目前，深度增强学习在每个人的日常生活中的应用达到了什么程度，使用它需要的硬件的门槛高吗？硬件的门槛短期内能下降很多吗？&lt;/p&gt;&lt;p&gt;答：深度增强学习是近2-3年来才发展起来的最前沿人工智能技术，因此还没有达到能在日常生活中广泛应用的程度。它所需要的硬件门槛和深度学习的研究是一样的，都需要较高性能的计算，GPU是目前深度学习研究必备的硬件设备。硬件门槛恐怕短期内很难下降，神经网络芯片从推出到成熟还需要很长时间。而且由于数据越来越大，所需的硬件性能越来越高，一定程度上加大了门槛的降低。&lt;/p&gt;&lt;p&gt;（7）计算机&amp;amp;统计学双修本科在读，业余五段，希望走这条发展路线，想知道以后的高年级计算机选修课应该选哪个方向，谢谢！&lt;/p&gt;&lt;p&gt;答：多选人工智能方向的课，比如计算机视觉，机器学习等课程。当然，最佳的建议是学习网上名校名师的课程。比如Stanford的CS231n计算机视觉课程，UC Berkerley的CS294 深度增强学习课程，还有Coursera，Udacity上都有不错的人工智能课程。&lt;/p&gt;&lt;p&gt;（8）机器学习如何入门，有哪些基础知识，整体的学习流程应该是怎么样的？&lt;/p&gt;&lt;p&gt;答：机器学习的入门建议学习Andrew Ng的机器学习课程。主要需要一定的数学基础，包括概率论和矩阵，也就是大学的数学课程。学习了机器学习入门课之后，可以直接学习深度学习相关的课程，建议都学习网上名校的公开课程，见上一个问题。&lt;/p&gt;&lt;p&gt;（9）本科医疗专业想向人工智能方向转行有什么建议（知识技能储备，路线规划，就业前景等）。要考研究人工智能研究生，能否谈谈大学和专业。&lt;/p&gt;&lt;p&gt;答：学习详见前两个问题。就业前景是非常大的，未来多年人工智能算法工程师都会是稀缺的职位。考研的话国内推荐清华，南大，上交，中科院及国防科大。&lt;/p&gt;&lt;p&gt;（10）人们普遍认为高低电平的计算机不可能实现真正的人工智能，现在所谓的人工智能也只能在给定模式下运行。请问您认为普适的人工智能可能存在吗？&lt;/p&gt;&lt;p&gt;答：当然是存在的，人类的大脑的智能可能也就是神经元的数据传输。现在的通用人工智能才刚刚开始，未来的发展必然会出现能处理多种任务的人工智能程序。&lt;/p&gt;&lt;p&gt;（11）电算能力和感情会同时以高水平(计算机和普通人的情感)存在吗？&lt;/p&gt;&lt;p&gt;答：由于目前的人工智能水平还无法模拟人类的情感，所以很难回答这个问题。从个人理解上看，人类的情感也是能被模拟出来的。&lt;/p&gt;&lt;p&gt;（12）深度学习的ai在计算性能比较强的计算机上完成学习后，能否移植到性能较差的计算机上应用？&lt;/p&gt;&lt;p&gt;答：学习完成后，只要使用神经网络进行前向传播，所需的计算量相对来说少很多，是可以移植的。现在比如Tensorflow训练好的神经网络，可以移植到手机上运行。&lt;/p&gt;&lt;p&gt;（13）讲讲go怎么用的先验？&lt;/p&gt;&lt;p&gt;答：使用人类专家的数据进行监督学习（模仿学习）。&lt;/p&gt;&lt;p&gt;（14）除了replay.targat network还有哪些技巧呢？&lt;/p&gt;&lt;p&gt;答：还有Dueling Network，Prioritised Replay等。当然这些都是DQN发展出来的，现在最好的算法是A3C及其之后的发展算法，比如UNREAL。&lt;/p&gt;&lt;p&gt;（15）如何自己做一个初始的AlphaGo，有放出的源码吗？&lt;/p&gt;&lt;p&gt;答：可以按照AlphaGo的论文方法去复现，github有一些非官方的代码。但是最大的问题在于除非在大公司否则很难有那么高性能的分布式计算机器支持，还要收集大量数据，一个人很难做出来。&lt;/p&gt;&lt;p&gt;（16）对DRL应用到imperfect information games中比较感兴趣。&lt;/p&gt;&lt;p&gt;回答：对于DRL应用到不完全信息游戏的问题，2016年David Silver组出了一篇文章名为：Deep Reinforcement Learning from
Self-Play in Imperfect-Information Games。那么这篇文章面向多人零和游戏，比如扑克。那么idea比较简单，就是综合自身的判断和面对其他人的判断。稍微解释一下。自己的判断就是完全不管对手的策略，自己根据当前的状态（牌面）给出平均最佳选择。而面对其他人的判断，是指考虑当前对手的策略，根据当前情况作出一个最佳反应。Idea就是综合两者的选择给出一个结果，最后能够趋近于纳什均衡。比只用DQN效果好很多。 但是如果是对于星际争霸这样的环境，涉及的问题就多很多，仅仅用这个策略显然也很难达到好的效果。&lt;/p&gt;&lt;p&gt;（17）目前的drl通用框架，以及目前的implement用到的工具推荐～&lt;/p&gt;&lt;p&gt;答：DQN，A3C，UNREAL。推荐使用tensorflow。&lt;/p&gt;&lt;p&gt;（18）rl在机械臂、机器人平台上应用的潜在优势是什么？学术上rl算法往往以小游戏做算法验证，用到真实机械臂上问题很多，我没有想清楚随着rl发展，它在机械臂机器人上应用的潜在优势。请教一下。&lt;/p&gt;&lt;p&gt;答：rl在机器人上的最大优势是可以通过学习来掌握控制，并且是使用从感知到控制的端到端神经网络。Google之前已经放出了一些他们使用DRL做机械臂的进展，可以关注。我认为使用DRL是真正实现机器人灵活控制的方法。这个领域很新，很前沿，值得研究。&lt;/p&gt;&lt;p&gt;（19）求深度讲解算法，推荐一些相关论文，想编程实现一下这个算法？&lt;/p&gt;&lt;p&gt;答：&lt;a href="https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap" data-editable="true" data-title="songrotek/Deep-Learning-Papers-Reading-Roadmap"&gt;songrotek/Deep-Learning-Papers-Reading-Roadmap&lt;/a&gt;&lt;/p&gt;&lt;p&gt;（20）读了好几遍论文，有些不太明白，想问些细节问题：&lt;/p&gt;1. rollout网络就是局部特征的线性组合么？这儿根本没用卷积网络吧？&lt;p&gt;2. 在做数据集时，相同局面下的相同走法要合并吗？&lt;/p&gt;&lt;p&gt;3. sutton 书里 REINFORCE算法里同时调整了值网络的参数，感觉alphago的在做强化学习的时候并没有，而是用生成对弈数据集再用监督学习做的。是这样理解的吗？&lt;/p&gt;&lt;p&gt;4. self-play时RLpolicy是每次初始化为SLpolicy，还是在前一个RLpolicy的基础上继续？&lt;/p&gt;&lt;p&gt;答：1. rollout也是一个神经网络，只不过这个神经网络比较小，使用的局部特征作为输入，但是很快。2. 一般很难出现完全相同的局面的，如果出现，也不需要合并，直接作为训练数据。3. AlphaGo的价值网络是单独训练的，没错。4. 是在之前的RLpolicy基础上继续，也只有这样才会不断进步。自学习时是不断用新版本取代旧版本，把旧版本作为对手进行训练。&lt;/p&gt;&lt;p&gt;（21）请问机器学习，深度学习包括深度增强学习等最近进展巨大的技术，对于机器人技术的影响和应用在什么地方？&lt;/p&gt;&lt;p&gt;答：有巨大潜力。欢迎阅读：&lt;a href="https://zhuanlan.zhihu.com/p/22758556?refer=intelligentunit" data-editable="true" data-title="知乎专栏" class=""&gt;最前沿之谷歌的机械臂&lt;/a&gt;&lt;/p&gt;&lt;p&gt;（22）从数据挖掘到机器学习再到深度学习，其中最关键的知识内核是什么？在该怎么深入学习？用tensorflow教程跑完一遍mnist，然后呢？&lt;/p&gt;&lt;p&gt;答：深度学习说白了就是三个部分：模型，数据及训练方法。要深入学习最好还是学习网上名校的公开课，然后自己尝试去复现代码。&lt;/p&gt;&lt;p&gt;最后，感谢所有参加本次Live的知友们！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24977176&amp;pixel&amp;useReferer"/&gt;</description><author>Flood Sung</author><pubDate>Sat, 21 Jan 2017 11:37:47 GMT</pubDate></item><item><title>Flood Sung的Live--从AlphaGo看人工智能前沿技术</title><link>https://zhuanlan.zhihu.com/p/24816290</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-3ce09841f5ec27b18afa2f741512b78e_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;大家好！我是 Flood Sung ，研究通用人工智能与机器人学习，&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" class="" data-editable="true" data-title="「智能单元」知乎专栏"&gt;「智能单元」知乎专栏&lt;/a&gt;作者之一。&lt;/p&gt;&lt;p&gt;        深度增强学习（ Deep Reinforcement Learning ） 2013 年就被 DeepMind 提出，然而被关注度非常低。直到去年的 AlphaGo ，才使得深度增强学习真正火起来。深度增强学习是 AlphaGo 的核心技术，是 AlphaGo 能够实现自我学习的关键。&lt;/p&gt;&lt;p&gt;        当前，深度增强学习已成为人工智能领域最重要的研究分支之一，该技术能够使计算机通过&lt;b&gt;深度神经网络&lt;/b&gt;处理&lt;b&gt;从感知到决策控制&lt;/b&gt;的问题，无论是&lt;b&gt;下棋&lt;/b&gt;、&lt;b&gt;金融决策&lt;/b&gt;、&lt;b&gt;医疗诊断&lt;/b&gt;还是&lt;b&gt;机器人控制&lt;/b&gt;，都能派上用场，具有超强的应用前景，是未来的&lt;b&gt;革命性技术&lt;/b&gt;，值得我们每一个人关注！&lt;/p&gt;&lt;h2&gt;2 Live主题与内容&lt;/h2&gt;&lt;p&gt;&lt;b&gt;&lt;a href="https://www.zhihu.com/lives/802155571712253952?utm_campaign=zhihulive&amp;amp;utm_source=zhihucolumn&amp;amp;utm_medium=Livecolumn" class="" data-editable="true" data-title="Live入口：请点击进入"&gt;Live入口：请点击进入&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;        Live时间：&lt;/b&gt;&lt;b&gt;2017-01-20 20:00&lt;/b&gt;&lt;/p&gt;&lt;p&gt;        本次Live的主题是 &lt;b&gt;AlphaGo&lt;/b&gt; 与&lt;b&gt;深度增强学习&lt;/b&gt;，想讲给对人工智能前沿技术感兴趣，对 AlphaGo 的核心技术感兴趣的朋友！&lt;/p&gt;&lt;p&gt;        在Live中我将从 AlphaGo 出发，为大家介绍 AlphaGo 的核心技术—深度增强学习的基本原理与方法，分析 AlphaGo 能够在自对弈中不断学习的原因，让大家了解深度增强学习这个方法的通用性和革命性！ &lt;/p&gt;&lt;p&gt;&lt;b&gt; 本次 Live 主要包括以下问题：&lt;/b&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;＊ AlphaGo 为什么能在围棋上取得如此重大的突破？ &lt;/p&gt;&lt;p&gt;＊ 深度增强学习是什么，有怎样的发展历史？&lt;/p&gt;&lt;p&gt; ＊ 深度增强学习与通用人工智能有什么关系？ &lt;/p&gt;&lt;p&gt;＊ 深度增强学习拥有怎样的核心思想？ &lt;/p&gt;&lt;p&gt;＊ AlphaGo 自对弈中不断学习的方法是什么？ &lt;/p&gt;&lt;p&gt;＊ 深度增强学习未来的应用和发展前景是什么？&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;期待大家的参与！&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24816290&amp;pixel&amp;useReferer"/&gt;</description><author>Flood Sung</author><pubDate>Tue, 10 Jan 2017 18:26:59 GMT</pubDate></item><item><title>吴恩达对于增强学习的形象论述（上）</title><link>https://zhuanlan.zhihu.com/p/24761972</link><description>&lt;b&gt;版权声明：本文&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" data-editable="true" data-title="智能单元"&gt;智能单元&lt;/a&gt;首发，本人原创，禁止未授权转载。&lt;/b&gt;&lt;p&gt;&lt;b&gt;前言：&lt;/b&gt;吴恩达在2003年为完成博士学位要求做了专题论文：&lt;a href="http://rll.berkeley.edu/deeprlcourse/docs/ng-thesis.pdf" data-title="Shaping and policy search in Reinforcement learning" class="" data-editable="true"&gt;Shaping and policy search in Reinforcement learning&lt;/a&gt;，其第一、二章被&lt;a href="https://zhuanlan.zhihu.com/p/24721292?refer=intelligentunit" data-title="伯克利CS294：深度增强学习课程" class="" data-editable="true"&gt;伯克利CS294：深度增强学习课程&lt;/a&gt;作为推荐材料。本文基于笔者的理解，对第一章做有选择的编译与注释。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;第一章 简介&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在本章中，我们将对本论文中需要考虑的增强学习框架给出一个非正式的，不涉及数学形式的综述。同时还将描述一些增强学习中的问题，这些问题是我们需要尝试解决的。最后，给出整个专题论文的概要。&lt;/p&gt;&lt;h2&gt;1.1 对增强学习的介绍&lt;/h2&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-5e4cda0cb4d5455d8a410d10ed04b8fb.png" data-rawwidth="994" data-rawheight="678"&gt;&lt;i&gt;图 1.1 伯克利大学的无人直升机&lt;/i&gt;&lt;/p&gt;&lt;p&gt;给一个像图1.1中那样的直升机，我们如何才能学习，或者说自动地设计一个控制器，使得直升机能够正常飞行呢？&lt;/p&gt;&lt;p&gt;人工智能和控制中的一个基础问题就是在随机系统中进行序列决策。飞行中的直升机就是随机系统的一个很好例子，因为它展现出随机和不可预测的行为，而大风和其他类似的干扰可能导致它的运动偏离预期。直升机的控制也是一个序列决策问题，控制直升机需要连续地决策向着哪个方向推操纵杆。比起那些只需要针对一个情况及时作出一个正确决策的问题，本问题展现出了所谓的“&lt;i&gt;&lt;b&gt;延迟后果（delayed consequences）&lt;/b&gt;&lt;/i&gt;”性质，解决问题的难度可谓是大为增加。所谓延迟后果，就是说直升机的自动控制器的水平好坏是根据它的长期表现来决定的，比如假设它现在做出了一个错误的操作，直升机并不会马上坠毁，可能依然能够飞行很多秒。导致直升机控制问题难度增大的另一个方面是它的&lt;b&gt;局部可观测性&lt;/b&gt;。具体来说，就是我们不能够精确地观测到直升机的位置/状态；但是，即便是面对系统状态的不确定性，我们仍然在每一秒都需要计算出正确的控制指令，使得直升机能够在空中正常飞行。&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：吴恩达擅长将一个问题通过比喻和举例的方式讲得通俗易懂，这是教学者的金钥匙。无人机控制这个例子里面包含了好几个马尔科夫决策过程和增强学习里面的用数学公式表达起来比较抽象的性质，待看到公式的时候，回头想这个例子，可以很好地帮助理解记忆。&lt;/blockquote&gt;&lt;p&gt;我们将马尔科夫决策过程（MDP）框架的公式表达推迟到第二章再讲。简单地来说，有的系统（就好比无人直升机）的控制是在每个时间点都会处于某种“状态”，我们一般对于这种系统比较感兴趣，而马尔科夫决策过程就是对这种系统进行建模。例如直升机的状态也许就可以用它的位置和方向来表示。我们的任务就是选择动作，使得系统能够倾向于保持在“好”的状态中，比如保持悬停，并且能够避免“坏”的状态，比如坠机。数量巨大且不同的问题都可以用马尔科夫决策过程形式来进行建模。比如规划和机器人导航，库存管理，机器维护，网络路由，电梯控制和搭建推荐系统。&lt;/p&gt;&lt;p&gt;增强学习针对解决MDP形式的问题给出了一系列的工具。虽然它获得了巨大成功，但是在解决很多问题时仍面临困难，还存在很多问题和挑战。我们简要地描述其中一些问题，这些问题会让某些增强学习问题具有挑战性：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;首先，存在&lt;b&gt;高维度问题&lt;/b&gt;。具体说来，就是基于离散的简单增强学习算法，经常会遇到状态变量的数量成指数增长的情况。这个问题就是所谓的“维度诅咒”，我们将在第二章中更详细地讨论。我们能够设计出一个即能可证地有效运作，又能更好地扩展到搞维度问题的实用算法吗？&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：现在高维度问题对深度增强学习已经基本上不是问题。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;同时，如何选择“&lt;b&gt;回馈函数（reward function）&lt;/b&gt;”也是一个问题。在增强学习中，设计者必须指明一个函数，该函数能够告诉我们直升飞机什么时候飞得好，什么时候是飞得不好。我们在选择这个函数的时候有很大的自由度，在第三章中还会看到，如果选择得当，某些函数能够成数量级地加速学习过程。当然同时也存在一些看起来不错，但是实际上让控制器的表现非常糟糕的函数。我们能够合理地选择回馈函数，既避免这种问题，又能让增强学习算法学习得又快又好吗？&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：在后续的学习实践中会常常接触，现在不明白不用担心。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;局部可观测性&lt;/b&gt;（&lt;b&gt;Partial Observability&lt;/b&gt;）是指被控制的系统的状态不能被精确观察的情况，比如直升机上的传感器只能是近似地测量直升机的位置。局部可观测性让问题的解决更加困难了，许多标准的增强学习算法不能解决这种情况，有的即使能够解决，也非常艰难。那么，如果只能近似地观察系统在做什么，我们该如何选择正确的控制呢？&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在本论文中，我们提出了一些方法，尝试解决前两个问题。我们的最终算法在局部可观测的情况下也能很好地工作，算法被运用到了图1.1中的无人直升机控制中。在实践的过程中，我们还涉及了经典控制理论中的一些专题，比如系统识别与验证等。&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：看这篇论文的&lt;b&gt;目的是帮助我们熟悉增强学习和马尔科夫决策过程&lt;/b&gt;，为CS294的学习&lt;b&gt;做知识预习&lt;/b&gt;的，没必要看完整个论文（150多页）来搞明白当年吴恩达的算法是啥情况。&lt;/blockquote&gt;&lt;h2&gt;1.2 与有监督学习的比较&lt;/h2&gt;&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：其实个人不太愿意翻译这段，因为对于学习深度增强学习，这个学术和理论味儿比较浓的小节没有太大意义。类似的情况在增强学习的著作&lt;a href="http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html" data-title="Reinforcement Learning: An Introduction" class="" data-editable="true"&gt;Reinforcement Learning: An Introduction&lt;/a&gt;中也有，作者花了很多篇幅去论证增强学习与有监督学习、无监督学习是不同的，应该与之并列什么的。&lt;b&gt;个人意见：虽然翻译了，可略过&lt;/b&gt;。&lt;/blockquote&gt;&lt;p&gt;有监督学习是人工智能领域中另一类标准问题。它可以看成是增强学习的某种特殊形式，在这种形式下，只需要对系统进行一次控制，因此我们只需要一次决策，而不是连续的序列决策。虽然看起来差别不是很大，但是实际上这让有监督学习变成了一个非常简单的问题。&lt;/p&gt;&lt;p&gt;举个具体例子，考虑根据给出的病人的多种检查数据或“特征”（比如心率、体温、多种医学检查结果），用有监督学习算法来预测一个病人是否患有心脏病。这里假设我们拥有一个训练集，其中包含有一些病人特征的样本，以及指明其中哪些病人是否患有心脏病的信息。我们可以用有监督学习算法来让一些函数（线性映射或者神经网络）来对数据进行拟合。当一个新的病人来就诊时，我们可以根据病人的特征，使用这个拟合的函数来预测他是否患有心脏病。而当这个一眼的诊断出来后，我们的病人就要去面对他的命运了。如果我们预测出错（比如我们决定赶紧对病人进行手术，而接下来的操作发现病人实际上没有任何问题，手术完全没有必要），那么我们也能马上观察到结果，并从这个结果中继续学习。&lt;/p&gt;&lt;p&gt;然而，在增强学习中，我们动作的结果通常是有延迟的，因此想要识别并从动作的长期效果中学习变得更加困难了。例如下棋，如果我们在第63手的时候输了（或者赢了），可能非常重要的一点是需要认识到我们在第17手的时候下的一记妙手奠定了胜局。这个“可信度分配（credit assignment）”问题让算法从过去的失误中吸取教训或从过去的成功中学习经验都更加困难了。&lt;/p&gt;&lt;p&gt;其次，在增强学习问题中的连续环境让算法重用（reuse）数据变得更加困难。在有监督学习中，如果我们预先收集并且存储了一些病人的样本数据集，并且想要测试一个新的神经网络在心脏病预测方面的性能，那么我们可以很容易地在这些数据集上进行测试，并分析结果与真实情况之间的差异，从而得出新模型的性能好坏。然而在增强学习中，假设我们也预先测试了一个控制直升机翻转飞行的控制器（或者举个更自然的例子，控制直升机稍微向右倾斜的控制器），在测试期间收集的数据是可以让我们知道直升机是如何翻转飞行的，但是如何使用这些数据来评价一个控制直升机稳定平飞的新控制器呢？目前还不清楚。因此，如果，新控制器控制直升机飞行的方式与之前的不同，那么对于每个新的控制器，我们都需要收集新的数据来进行测试。这个性质使得增强学习相较于有监督学习需要更多的数据。本论文的目标之一就是探索如何在增强学习中高效地重用数据，并且尝试在实用的学习算法中利用这些思想。&lt;/p&gt;&lt;p&gt;最后，增强学习一个常见主题是“不可知论学习（agnostic learning）”（在人工智能领域中，这和界限最优化（bounded optimality）联系紧密）。这个主题是指限制可能的分类器集合的思想，这一思想通常被采用。以上文的心脏病举例来说，相较于考虑所有的将病人的特征映射到{患病，没患病}的函数（这将会是一个巨大的函数空间），我们可以将注意力集中在更小一点的函数集中，比如所有的阈值线性函数，或者所有中等尺寸的神经网络。这样就可以显著地降低我们需要考虑的分类器的数量。如果分辨病人是否患病的“真实”的决策边界极端复杂，以至于没有神经网络能够准确预测分类，那么既然我们已经是将注意力限制在了用神经网络表达的函数上，这就说明我们没法找到一个好的分类器。但是如果决策边界不是那么复杂，那么我们限制的的分类器集合就允许一个分类器展示出只需要少量的训练数据就能够很好地拟合一个神经网络。具体来说，就是训练数据量的需求是由神经网络中“自由参数（free parameter）”的数量决定的，而不是以输入的病人和心脏病的发生的复杂度来决定的。基于在有监督学习中数据可以重用的事实，这些结果是可以证明的。在第四章中，我们将把这些结论一般化到增强学习中，观察针对增强学习问题，通过将我们的注意力限制到一个较小的控制器集合中，我们依旧可以得到类似的结论，即想要算法较好地学习，需要对样本尺寸做出限制。&lt;/p&gt;&lt;h2&gt;1.3 论文概要及贡献&lt;/h2&gt;&lt;blockquote&gt;译者注：意义不大，略过。感兴趣的知友请自行阅读。&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;第一章原文翻译完毕。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;小结&lt;/h2&gt;&lt;p&gt;翻译本章，主要面向对增强学习没有概念的知友。在第一章中，吴恩达使用无人直升机的例子比较形象直观地介绍了增强学习问题。&lt;b&gt;读者主要理解该例，其他部分适当了解即可&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;在下篇中&lt;/b&gt;，我将编译论文的第二章，其内容主要是用数学公式严谨地对马尔科夫决策过程和增强学习做出定义和讲解。&lt;/p&gt;&lt;h2&gt;读者反馈&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;持续有知友问如何获取cs231n的资源，逐个回复太累，我在“智能单元”微信公众号上做了自动回复，请有需求的知友对公众号回复cs231n或者CS231n即可，也算是给公众号涨点关注。&lt;/li&gt;&lt;li&gt;欢迎想要学习CS294：深度增强学习的知友根据课程推荐材料学习的同时，进行翻译或者编译，本专栏接受投稿。&lt;/li&gt;&lt;li&gt;一如既往地欢迎大家对内容进行批评指正，贡献者我都会在文末更新感谢：）&lt;/li&gt;&lt;/ul&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24761972&amp;pixel&amp;useReferer"/&gt;</description><author>杜客</author><pubDate>Mon, 09 Jan 2017 11:24:14 GMT</pubDate></item><item><title>CS 294：深度增强学习，2017年春季学期</title><link>https://zhuanlan.zhihu.com/p/24721292</link><description>&lt;blockquote&gt;&lt;b&gt;译者注&lt;/b&gt;：本文编译自&lt;a href="http://rll.berkeley.edu/deeprlcourse/#syllabus" data-title="伯克利大学2017年春季学期课程CS294介绍页面" class="" data-editable="true"&gt;伯克利大学2017年春季学期课程CS294介绍页面&lt;/a&gt;。该课程主题选择深度增强学习，即紧跟当前人工智能研究的热点，又可作为深度学习的后续方向，&lt;b&gt;值得推荐&lt;/b&gt;。&lt;/blockquote&gt;&lt;h2&gt;课程时间&lt;/h2&gt;&lt;p&gt;2017年1月18日至5月3日。&lt;/p&gt;&lt;h2&gt;课程前置要求&lt;/h2&gt;&lt;p&gt;学习该课程，会假设学员对于增强学习，最优化方法和机器学习这些知识背景比较熟悉。要是学员对于这些内容不太了解，就需要根据提供的参考资料补习以下的知识点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;增强学习和马尔科夫决策过程（MDPs）&lt;/li&gt;&lt;ul&gt;&lt;li&gt;MDPs的定义&lt;/li&gt;&lt;li&gt;具体算法：策略迭代和价值迭代&lt;/li&gt;&lt;li&gt;搜索算法&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;数值最优化方法&lt;/li&gt;&lt;ul&gt;&lt;li&gt;梯度下降和随机梯度下降&lt;/li&gt;&lt;li&gt;反向传播算法&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;机器学习&lt;/li&gt;&lt;ul&gt;&lt;li&gt;分类和回归问题：用什么样的损失函数，如何拟合线性或非线性模型&lt;/li&gt;&lt;li&gt;训练/测试误差，过拟合&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：在2016年跟着专栏翻译的CS231n课程笔记学习的知友可以发现，缺少的知识点只有增强学习和马尔科夫决策过程，学习这门课的难度降低。&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;对于增强学习和MDPs的介绍材料有&lt;/b&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://ai.berkeley.edu/" data-editable="true" data-title="CS188 EdX course"&gt;CS188 EdX course&lt;/a&gt;，从马尔科夫决策过程第一部分开始。&lt;/li&gt;&lt;li&gt;&lt;a href="http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html" data-editable="true" data-title="Sutton &amp;amp; Barto" class=""&gt;Sutton &amp;amp; Barto&lt;/a&gt;的著作，学习第3章和第4章。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：该著作&lt;b&gt;重要&lt;/b&gt;，建议中文母语学习者打印该书，方便查阅、学习与装逼：）&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;对MDPs的简洁介绍，可以参考&lt;a href="http://rll.berkeley.edu/deeprlcourse/docs/ng-thesis.pdf" data-editable="true" data-title="吴恩达这篇论文" class=""&gt;吴恩达这篇论文&lt;/a&gt;的第1章和第2章。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：不想看大部头的著作就看这篇，简明扼要方便理解。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;David Silver的课程，下文有链接。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：要是看完并基本掌握了David Silver的课程，这门课也就是看看了。&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;对于机器学习和神经网络的介绍材料有&lt;/b&gt;：&lt;/p&gt;&lt;li&gt;&lt;a href="http://cs231n.github.io/" data-editable="true" data-title="Andrej Karpathy的课程" class=""&gt;Andrej Karpathy的课程&lt;/a&gt;。&lt;/li&gt;&lt;blockquote&gt;译者注：也就是CS231n了，算成是AK的也不太妥当，毕竟老板是李大姐，讲师还有Justin 。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://www.coursera.org/course/neuralnets" data-editable="true" data-title="Coursera上Hinton大爷的课程"&gt;Coursera上&lt;b&gt;Hinton&lt;/b&gt;大爷的课程&lt;/a&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：拜拜亨大爷总是安心些。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://www.coursera.org/learn/machine-learning/" data-editable="true" data-title="Coursera上吴恩达的课程"&gt;Coursera上吴恩达的课程&lt;/a&gt;。&lt;/li&gt;&lt;li&gt;&lt;a href="https://work.caltech.edu/telecourse.html" data-editable="true" data-title="Yaser Abu-Mostafa’s course" class=""&gt;Yaser Abu-Mostafa的课程&lt;/a&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：本人没有看过这个课程，有学习过的知友可以在评论中简单谈谈感受。&lt;/blockquote&gt;&lt;h2&gt;课程安排&lt;/h2&gt;&lt;p&gt;如下图所示，课件和参考材料会随着课程进度发布。这篇翻译的内容也会同步更新。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-1c31c9c163da64f7d29f14d48965abbe.jpg" data-rawwidth="1938" data-rawheight="1406"&gt;&lt;h2&gt;课程视频&lt;/h2&gt;&lt;p&gt;原文展示了2015年的4个课程视频，在Youtube上，清晰度很低，这里就不放出了，感兴趣的知友自行查看原文。随着课程进展，本部分也更新2017年的课程视频链接。&lt;/p&gt;&lt;h2&gt;相关课程&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" data-editable="true" data-title="David Silver关于增强学习的课程及课程视频" class=""&gt;David Silver关于增强学习的课程及课程视频&lt;/a&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：&lt;b&gt;重要&lt;/b&gt;。&lt;/blockquote&gt;&lt;li&gt;&lt;a href="https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/" data-editable="true" data-title="Nando de Freitas关于机器学习的课程" class=""&gt;Nando de Freitas关于机器学习的课程&lt;/a&gt;&lt;/li&gt;&lt;blockquote&gt;译者注：没必要，看吴恩达的课程。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://cs231n.github.io/" data-editable="true" data-title="Andrej Karpathy的课程" class=""&gt;Andrej Karpathy的课程&lt;/a&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：去年我们一直在推荐，不赘述了。&lt;/blockquote&gt;&lt;h2&gt;相关书籍&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html" data-editable="true" data-title="Sutton和Barto的《Reinforcement Learning: An Introduction》" class=""&gt;Sutton和Barto的Reinforcement Learning: An Introduction&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：&lt;b&gt;重要&lt;/b&gt;，系统学习的话就打印吧。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://www.ualberta.ca/~szepesva/RLBook.html" data-editable="true" data-title="Szepesvari, Algorithms for Reinforcement Learning" class=""&gt;Szepesvari, Algorithms for Reinforcement Learning&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://www.athenasc.com/dpbook.html" data-editable="true" data-title="Bertsekas, Dynamic Programming and Optimal Control, Vols I and II" class=""&gt;Bertsekas, Dynamic Programming and Optimal Control, Vols I and II&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471727822.html" data-editable="true" data-title="Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming" class=""&gt;Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://adp.princeton.edu/" data-editable="true" data-title="Powell, Approximate Dynamic Programming" class=""&gt;Powell, Approximate Dynamic Programming&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;泛读链接&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://rll.berkeley.edu/deeprlcourse/#syllabus" data-editable="true" data-title="深度学习资源的集锦"&gt;深度学习资源的集锦&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;译者注：收藏了你也不会看的，但是可以装逼：）&lt;/blockquote&gt;&lt;h2&gt;之前课程&lt;/h2&gt;&lt;p&gt;2015年秋季开过同名课程，介绍链接&lt;a href="http://rll.berkeley.edu/deeprlcourse-fa15/" data-title="点击这里" class="" data-editable="true"&gt;点击这里&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;课程简介翻译完毕&lt;/b&gt;。&lt;/p&gt;&lt;h2&gt;学习建议&lt;/h2&gt;&lt;p&gt;想要学习深度增强学习的知友，在我个人的看来，可能是以下几种情况：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;看到了深度增强学习的前景，想要提前布局，提高自身价值；&lt;/li&gt;&lt;li&gt;学完了深度学习，想要继续学习人工智能领域其他内容；&lt;/li&gt;&lt;li&gt;大公司或者拿了投资的创业公司的项目组想要搞相关应用，比如BetaGo或者GammaGo：）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;都挺好的，学吧！专栏成立的初心之一，就是促进这个领域的学习和交流。个人也会以这次课程为抓手，再系统性地把深度增强学习给学扎实。后续会以该课程学习为主题，在专栏进行相关内容的写作，也欢迎大家针对课程学习内容进行讨论。&lt;/p&gt;&lt;p&gt;如果是因为近几年人工智能突然火热起来想要学习的知友，需要知道人工智能领域历史上起起落落好几次，入坑前想好自己是不是真的感兴趣。&lt;/p&gt;&lt;p&gt;如果上文中的课程前置要求中三方面知识点都不太熟悉，也不建议直接学习该课程。建议&lt;b&gt;先看吴恩达或者Hinton的课程，然后看CS231n，然后再来学习这门课程&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;最后照例&lt;b&gt;打鸡血：&lt;/b&gt;虽说人工智能起起落落好几次，但是这次确实是解决了之前没有解决的问题，突破了之前没有突破的水平，不是吗？相较于“呵呵，根据历史规律，人工智能也就还能火几年，步子太大要扯到蛋”的态度，我个人更倾向：&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;I am looking at the future with concern, but with good hope&lt;/b&gt;. &lt;b&gt;-Albert Schweitzer&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;----------------------------------------------------------------------------------------&lt;/p&gt;&lt;p&gt;PS：我们开通了&lt;b&gt;智能单元微信公众号&lt;/b&gt;，搜索“&lt;b&gt;智能单元&lt;/b&gt;”就能找到，内容上会和专栏差异化。可以的话请大家关注支持一个，算是对我的鼓励，先谢过啦：）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24721292&amp;pixel&amp;useReferer"/&gt;</description><author>杜客</author><pubDate>Thu, 05 Jan 2017 10:26:25 GMT</pubDate></item><item><title>Master 横扫围棋各路高手，是时候全面研究通用人工智能了！</title><link>https://zhuanlan.zhihu.com/p/24709235</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-eab9cfb4a3d814feb1e92cc1b69db9aa_r.png"&gt;&lt;/p&gt;在写这篇文章的时候，聂卫平将挑战Master，不出意外，老聂也只能扑街。&lt;p&gt;有的人说，汽车早就比人跑得快，人也不会惊慌，围棋AI战胜人类，也是迟早的事，同样不必惊慌。&lt;/p&gt;&lt;p&gt;是这样吗？&lt;/p&gt;&lt;p&gt;不是的。&lt;/p&gt;&lt;p&gt;猎豹本来就比人类跑得快，但人类仍然是地球的主宰。为什么？因为人类引以为傲的智慧！&lt;/p&gt;&lt;p&gt;因为人类拥有地球生物中最高的智能，所以虽然人的肉体机能很有限，但是不妨碍人类去创造各种辅助人类的工具。&lt;/p&gt;&lt;p&gt;然而，人工智能，却是一个和汽车，飞机完全不同的东西！我们人类在试图制造超越人类智慧的东西！&lt;/p&gt;&lt;p&gt;当AlphaGo战胜李世石，当Master豪取50连胜的时候，很多人会产生一种对智能的恐惧。相信那些真正和Master战斗的棋手们会更深有体会。&lt;/p&gt;&lt;p&gt;为什么？我们的智慧被超越！我们以前的认识被完全打破了。以至于当今围棋第一人柯洁说出“我们对围棋的理解都是错的”这样的话。&lt;/p&gt;&lt;p&gt;人们也很容易的想到《三体》中水滴入侵人类舰队的那一刻。&lt;/p&gt;&lt;p&gt;那种感觉是何曾的相同。&lt;/p&gt;&lt;p&gt;让人绝望而恐惧！&lt;/p&gt;&lt;p&gt;有人会说，n年前国际象棋就被攻克了，没什么大不了。&lt;/p&gt;&lt;p&gt;但是，&lt;b&gt;这次真的不一样！这次真的不一样！这次真的不一样！&lt;/b&gt;（重要的事情说三篇）&lt;/p&gt;&lt;p&gt;如果你懂深度学习，如果你懂AlphaGo，你就会明白，AlphaGo不再是依靠蛮力计算，而主要是靠深度神经网络，靠增强学习自我学习。&lt;/p&gt;&lt;p&gt;深度神经网络是个黑箱。输入棋局，通过神经网络，输出对棋局的判断。AlphaGo只要这么简单，智能就在神经网络当中。&lt;/p&gt;&lt;p&gt;这次是围棋，完全可观察，如果下次就是&lt;b&gt;星际争霸&lt;/b&gt;，那么大家会作何感想呢？&lt;/p&gt;&lt;p&gt;深度学习正在变革所有行业，AlphaGo则开启了通用人工智能的大门！&lt;/p&gt;&lt;h2&gt;通用人工智能是什么？&lt;/h2&gt;&lt;p&gt;通用人工智能（General Artificial Intelligence），是指能通过自我学习解决各种问题的智能算法。人类的大脑就是一种通用智能，因为人既可以学游泳，也可以学下棋。开发AlphaGo的DeepMind就是这么一家公司，以实现通用人工智能为目标。&lt;/p&gt;&lt;p&gt;通用人工智能并不是等价于类人智能。但解决了通用人工智能，类人智能也必然能够达到。&lt;/p&gt;&lt;p&gt;AlphaGo的算法就是典型的通用人工智能算法，核心使用了深度学习（Deep Learning），增强学习（Reinforcement Learning）。而深度增强学习（Deep Reinforcement Learning），就是通用人工智能算法的具体表现形式。什么叫通用？就是这个算法既可以训练用来下围棋，也可以训练用来开车，还可以训练用来股票交易。&lt;/p&gt;&lt;p&gt;有人说把围棋的路数从19路变成21路，AlphaGo就没辙了。这是没错。但是DeepMind很容易就可以训练一个适应多个路数的AlphaGo。都只是时间问题。&lt;/p&gt;&lt;p&gt;通用人工智能算法就是倚天剑，屠龙刀！算法在手，任何问题都可以尝试去解决！&lt;/p&gt;&lt;p&gt;目前国内的研究还相当少，但是，真的&lt;/p&gt;&lt;p&gt;&lt;b&gt;是时候全面研究通用人工智能了！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;DeepMind和OpenAI正在大力发展，这个才是真正掀起人工智能革命的关键！中国在这一块如果落后的话会非常致命！必须庆幸DeepMind和OpenAI还一直公开他们的论文！&lt;/p&gt;&lt;p&gt;我们从DeepMind和OpenAI研究的方向就知道应该做什么了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1 Deep Reinforcement Learning深度增强学习，用于构造学习机制&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2 Deep Generative Model深度生成模型，用于理解信息，可以用于预测规划&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;3 Neural Memory神经网络记忆，用于存储信息和推理&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;4 One Shot Learning 一眼学习，用于快速学习&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;5 Deep Transfer Learning 深度迁移学习，用于移植知识&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;以上多点的综合运用，必将可以制造更强大的通用人工智能算法！而这些方向的研究，都越来越接近人类大脑的本质，或者说智能的本质！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;要不畏惧人工智能，那只有理解并掌握人工智能！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;（本文算作一种呼吁，呼吁更多的人工智能研究人员投入到通用人工智能当中。）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;---------------------------------------&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本专栏将全面聚焦通用人工智能算法！&lt;/p&gt;&lt;p&gt;最后，加一个推广！为了使关注通用人工智能的朋友们能够更及时获取通用人工智能的前沿资讯，我们开通了 &lt;b&gt;智能单元 微信公众号&lt;/b&gt;！第一时间为大家推送最新资讯，并且不定期推送纯原创最新通用人工智能进展的解读！&lt;/p&gt;&lt;p&gt;与大家一起学习，一起进步！&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-864ef735513a261c8140410e43436050.png" data-rawwidth="180" data-rawheight="320"&gt;欢迎在微信上搜索“智能单元” 公众号关注！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24709235&amp;pixel&amp;useReferer"/&gt;</description><author>Flood Sung</author><pubDate>Wed, 04 Jan 2017 15:05:39 GMT</pubDate></item><item><title>ICLR 2017 DRL相关论文</title><link>https://zhuanlan.zhihu.com/p/23807875</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-a583638767b7ef5d44ddc68d748af3a5_r.png"&gt;&lt;/p&gt;&lt;h2&gt;1 前言&lt;/h2&gt;&lt;p&gt;ICLR 2017中和Deep Reinforcement Learning相关的论文我这边收集了一下，一共有30篇（可能有漏），大部分来自于DeepMind和OpenAI，可见DRL依然主要由DeepMind和OpenAI把持。由于论文太多，时间有限，先把论文列出来。之后根据情况做一定分析。也欢迎大家一起补充。&lt;/p&gt;&lt;h2&gt;2 DeepMind的论文分析&lt;/h2&gt;&lt;p&gt;&lt;b&gt;[1] LEARNING TO COMPOSE WORDS INTO SENTENCES
WITH REINFORCEMENT LEARNING&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;推荐阅读指数：&lt;/b&gt;⭐️⭐️&lt;/p&gt;&lt;p&gt;&lt;b&gt;论文类型&lt;/b&gt;：应用型&lt;/p&gt;&lt;p&gt;&lt;b&gt;应用方向&lt;/b&gt;：自然语言理解&lt;/p&gt;&lt;p&gt;&lt;b&gt;论文基本内容介绍&lt;/b&gt;：&lt;/p&gt;&lt;p&gt;这篇文章面向一类自然语言理解问题，就是句子的理解，如何更好的输入一个句子的每一个词语，然后输出一个句子的表达。一种方式就是不管句子的结构，一个一个输入到RNN中，然后输出一个向量来表示这个句子的含义，一种就是探索句子的组成树结构（Tree Structure），基于树结构输入词语，然后输出句子的表达。比如“我喜欢打篮球”这句话，很显然，“我，喜欢，打，篮球”可以作为特定的结构输入，就类似于句子的分析，我们显然不会按照“我喜，欢打篮，球”来理解。下图就是两个树结构的范例，要按照不同的顺序组合词语然后输入到RNN中。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-c713955a4285122aece12058ac0acadc.png" data-rawwidth="1862" data-rawheight="514"&gt;&lt;p&gt;那么输入一个句子的每一个词语，输出句子的表达之后，就有用了，可以利用这个输出做很多事：比如判断这个句子是电影的正面评论还是负面平台，判断两个句子是否意思相近，判断两个句子是矛盾，中立还是相同立场，还可以基于句子预测下一个句子。所以句子的理解是自然语言理解的基础，这篇文章的目的也就是希望能够得到更好的句子的理解。&lt;/p&gt;&lt;p&gt;接下来这篇文章做的事情就是使用增强学习来探索句子的树形结构组成，就是说如何构建树结构的问题。如果构造了一个堆栈，然后有一个句子等待输入，那么每一次有两个动作，插入S和合并R。插入就是将一个词语word插入到堆栈，合并就是将两个词语合并。如下图所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-1a21383e6e32c818b9c56985d077a171.png" data-rawwidth="1966" data-rawheight="472"&gt;那么本文使用REINFORCE算法（策略梯度算法）来训练这个树结构的动作选择网络。Reward是在每次生成完整个句子后获取（类似AlphaGo）。具体算法这里不详细介绍。效果是显然的，肯定能够学习到一定的组成句子的方法，从而比那种随便输入的句子理解更好。&lt;/p&gt;&lt;p&gt;&lt;b&gt;论文评价&lt;/b&gt;：一篇中规中矩的Paper，在NLP领域找到了一个不错的应用增强学习的问题，并且取得了一定的效果。但是这种效果并不具有大幅度的提高，也说明其实没必要采用树结构输入，一个一个词语输入其实也可以，只要神经网络能够理解就好。人类就可以做到，虽然人类也分析句子的结构。&lt;/p&gt;&lt;p&gt;&lt;b&gt;[2] LEARNING TO NAVIGATE IN COMPLEX ENVIRONMENTS&lt;/b&gt;&lt;/p&gt;&lt;p&gt;推荐阅读指数：⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;&lt;p&gt;&lt;b&gt;[3] LEARNING TO PERFORM PHYSICS EXPERIMENTS VIA
DEEP REINFORCEMENT LEARNING &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[4] PGQ: COMBINING POLICY GRADIENT AND Q-
LEARNING&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[5] Q-PROP: SAMPLE-EFFICIENT POLICY GRADIENT
WITH AN OFF-POLICY CRITIC&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[6] REINFORCEMENT LEARNING WITH UNSUPERVISED
AUXILIARY TASKS&lt;/b&gt;&lt;/p&gt;&lt;p&gt;推荐阅读指数：⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;&lt;p&gt;&lt;b&gt;[7] SAMPLE EFFICIENT ACTOR-CRITIC WITH
EXPERIENCE REPLAY&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[8] THE PREDICTRON: END-TO-END LEARNING AND PLANNING &lt;/b&gt;&lt;/p&gt;&lt;h2&gt;3 OpenAI的论文分析（包含Sergey Levine的论文）&lt;/h2&gt;&lt;p&gt;&lt;b&gt;[9] #EXPLORATION: A STUDY OF COUNT-BASED EXPLORATION
FOR DEEP REINFORCEMENT LEARNING &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[10] GENERALIZING SKILLS WITH SEMI-SUPERVISED
REINFORCEMENT LEARNING  &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[11] LEARNING INVARIANT FEATURE SPACES TO TRANS-
FER SKILLS WITH REINFORCEMENT LEARNING &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[12] LEARNING VISUAL SERVOING WITH DEEP FEATURES
AND TRUST REGION FITTED Q-ITERATION &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[13] MODULAR MULTITASK REINFORCEMENT
LEARNING WITH POLICY SKETCHES &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[14] STOCHASTIC NEURAL NETWORKS FOR
HIERARCHICAL REINFORCEMENT LEARNING &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[15] THIRD PERSON IMITATION LEARNING &lt;/b&gt;&lt;/p&gt;&lt;p&gt;推荐阅读指数：⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;&lt;p&gt;&lt;b&gt;[16] UNSUPERVISED PERCEPTUAL REWARDS
FOR IMITATION LEARNING &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[17] EPOPT: LEARNING ROBUST NEURAL NETWORK POLICIES USING MODEL ENSEMBLES &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[18] RL2: FAST REINFORCEMENT LEARNING VIA SLOW REINFORCEMENT LEARNING &lt;/b&gt;&lt;/p&gt;&lt;h2&gt;4 其他论文&lt;/h2&gt;&lt;p&gt;&lt;b&gt;[19] COMBATING DEEP REINFORCEMENT LEARNING’S
SISYPHEAN CURSE WITH INTRINSIC FEAR &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[20] COMMUNICATING HIERARCHICAL NEURAL
CONTROLLERS FOR LEARNINGZERO-SHOT TASK GENERALIZATION&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[21] DESIGNING NEURAL NETWORK ARCHITECTURES
USING REINFORCEMENT LEARNING &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[22] LEARNING TO PLAY IN A DAY: FASTER DEEP REIN-
FORCEMENT LEARNING BY OPTIMALITY TIGHTENING&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[23] LEARNING TO REPEAT: FINE GRAINED ACTION REPETITION FOR
DEEP REINFORCEMENT LEARNING &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[24] MULTI-TASK LEARNING WITH DEEP MODEL BASED
REINFORCEMENT LEARNING&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[25] NEURAL ARCHITECTURE SEARCH WITH
REINFORCEMENT LEARNING&lt;/b&gt;&lt;/p&gt;&lt;p&gt;推荐阅读指数：⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;&lt;p&gt;&lt;b&gt;[26] OPTIONS DISCOVERY WITH BUDGETED REINFORCE-
MENT LEARNING  &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[27] REINFORCEMENT LEARNING THROUGH ASYNCHRONOUS ADVANTAGE ACTOR-CRITIC ON A GPU &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[28] SPATIO-TEMPORAL ABSTRACTIONS IN
REINFORCEMENT LEARNING THROUGH
NEURAL ENCODING &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[29] SURPRISE-BASED INTRINSIC MOTIVATION FOR DEEP
REINFORCEMENT LEARNING&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[30] TUNING RECURRENT NEURAL NETWORKS WITH REINFORCEMENT LEARNING&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;最后，我们开通了智能单元微信公众号，第一时间推送最前沿技术和资讯，欢迎在微信搜索“智能单元”关注。&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23807875&amp;pixel&amp;useReferer"/&gt;</description><author>Flood Sung</author><pubDate>Tue, 03 Jan 2017 11:54:32 GMT</pubDate></item><item><title>干货和原创的智能单元微信公众号开启</title><link>https://zhuanlan.zhihu.com/p/24682204</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-8b8c855409438a124291382551cc0cac_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;各位知友2017新年快乐！在新的一年，我们&lt;b&gt;推出智能单元微信公众号，提供有别于知乎专栏的差异化优质内容。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;智能单元微信公众号&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;智能单元微信公众号&lt;/b&gt;是一个致力于&lt;b&gt;推动通用人工智能（Artificial General Intelligence）发展&lt;/b&gt;的原创独立媒体。通过这个平台，我们会给大家：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;分享最有价值的实践：&lt;b&gt;会&lt;u&gt;分享我们的开源代码与软件；&lt;/u&gt;&lt;/b&gt;&lt;/li&gt;&lt;li&gt;分享最前沿的研究成果：会&lt;b&gt;广读领域内新论文并评估分量&lt;/b&gt;，&lt;b&gt;对高价值论文给出第一时间的有态度解读&lt;/b&gt;。详细解读和复现工作还是交给专栏吧！&lt;/li&gt;&lt;li&gt;分享最新鲜的资讯：会推送经我们挑选的有调性的领域内资讯，节约知友们的信息搜索时间。&lt;/li&gt;&lt;/ul&gt;智能单元微信公众号&lt;b&gt;&lt;u&gt;聚焦通用人工智能&lt;/u&gt;，将涉及当前最前沿的机器学习算法&lt;/b&gt;：包&lt;b&gt;括深度学习Deep Learning，增强学习Reinforcement Learning，迁移学习Transfer Learning，神经网络记忆Neural Memory（神经图灵机，DNC），无监督学习（主要是生成式对抗网络GAN）及一眼学习（One Shot Learning）&lt;/b&gt;等。将这些技术应用到机器人当中，将使机器人技术实现变革，使机器人具备学习能力，因此&lt;b&gt;机器人学习&lt;/b&gt;是通用人工智能核心应用方向，也是智能单元关注的核心。&lt;h2&gt;&lt;b&gt;公众号与专栏&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;智能单元专栏重在提供学习材料、展现深度思考&lt;/b&gt;，将专注于：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;通用人工智能相关技术原创教程，主要涉及深度增强学习，生成式对抗网络及神经网络记忆相关技术；&lt;/li&gt;&lt;li&gt;最前沿领域内论文的详细解读；&lt;/li&gt;&lt;li&gt;最前沿领域内技术的分析与总结。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;智能单元微信公众号重在时效性、分享优质内容&lt;/b&gt;，专注方向参见前文，这里不赘述。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;一个小目标&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;2017年，我们期望&lt;b&gt;智能单元知乎专栏&lt;/b&gt;和&lt;b&gt;智能单元微信公众号&lt;/b&gt;既一脉相承，又各有侧重，&lt;b&gt;能够给在人工智能领域的奋斗的知友们提供更多帮助&lt;/b&gt;！&lt;/p&gt;&lt;p&gt;PS：&lt;b&gt;欢迎知友们在评论中留下自己2017年在人工智能领域的小目标&lt;/b&gt;，这将帮助我们更好地提供优质内容。况且，大家相互祝福（&lt;b&gt;吐槽&lt;/b&gt;）难倒不是一件欢乐的事儿吗？&lt;/p&gt;PS2：&lt;b&gt;题图就是二维码，欢迎扫码关注！也可在微信中直接搜索“智能单元”关注我们&lt;/b&gt;！&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24682204&amp;pixel&amp;useReferer"/&gt;</description><author>杜客</author><pubDate>Tue, 03 Jan 2017 11:27:11 GMT</pubDate></item></channel></rss>