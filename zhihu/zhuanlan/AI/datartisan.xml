<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Datartisan数据工匠 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/datartisan</link><description></description><lastBuildDate>Tue, 20 Dec 2016 11:16:43 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>iPhone 步行数据分析</title><link>https://zhuanlan.zhihu.com/p/23540254</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-bd031dbee54442ad40d8957abbb1108d_r.png"&gt;&lt;/p&gt;&lt;p&gt;本文中我将展示如何利用 pandas 和 ggplot 来分析 iPhone 的步行数据，我主要利用 &lt;a href="https://www.yhat.com/products/rodeo" data-editable="true" data-title="Rodeo"&gt;Rodeo&lt;/a&gt;(Yhat's 的 IDE)来进行数据分析。&lt;/p&gt;&lt;h2&gt;数据收集&lt;/h2&gt;&lt;p&gt;首先我想从我的 iPhone 中导出用于分析的步行数据，Quantified Self 实验室的工作人员开发了一个数据提取的便捷工具——&lt;a href="http://quantifiedself.com/access-app/app" data-editable="true" data-title="QS Access" class=""&gt;QS Access&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;以下是一组关于步行数据的截图： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-da27bcecce398c32037d2d65bdee248a.png" data-rawwidth="756" data-rawheight="768"&gt;&lt;p&gt;QS Access 应用可以提取出一个包含某个时期内步行数据的 CSV 文件，该文件中包含三列变量——开始时点、结束时点和步数。&lt;/p&gt;&lt;h2&gt;数据分析&lt;/h2&gt;&lt;p&gt;我主要利用 pandas 中的时间序列分析工具来分析数据，当 &lt;a href="https://github.com/wesm" data-editable="true" data-title="Wes McKinney"&gt;Wes McKinney&lt;/a&gt;开始处理 pandas 项目时，他就已经在一家投资管理公司工作，该行业广泛依赖于时间序列分析方法。因此，pandas 中包含非常多的时间序列分析函数。&lt;/p&gt;&lt;p&gt;首先，当我们拥有了时间序列数据后，我们可以定义参数 parse_dates 使得 pandas 可以正确地处理时间序列数据。对于我们来说，结束时点变量并没有包含额外的有价值的信息，所以我们在分析过程中将不考虑该变量的情况。&lt;/p&gt;&lt;p&gt;最后我们将开始时点变量设定为索引变量，这个设定有助于我们进一步的数据分析。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-cb823ae0b8c7cdb321cfb85559b6742a.png" data-rawwidth="469" data-rawheight="273"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-5cb0777e9d06b074bdbdd589d10fa078.png" data-rawwidth="350" data-rawheight="328"&gt;&lt;h2&gt;每小时步行数据&lt;/h2&gt;&lt;p&gt;如何快速地绘图分析现有的步行数据呢？&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-517322a3a783ba8287e5c23a71f3d4ad.png" data-rawwidth="848" data-rawheight="610"&gt;&lt;p&gt;很不幸的是，我们无法从上图中得到比较有价值的信息，我们应该如何提高可视化效果呢？我想到一个好主意——我们可以利用 pandas 中的 resample 函数来改变数据集的时间粒度。&lt;/p&gt;&lt;p&gt;更精确地说，我们可以利用 &lt;strong&gt;downsampling&lt;/strong&gt; 的方法来降低时间的频度。比如，我们可以采集每小时的数据，然后利用重抽样和汇总计算的方法获得日度数据、周度数据和月度数据。&lt;/p&gt;&lt;h2&gt;获取每天步行数据&lt;/h2&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-93958825cb92b597c443fae2990b2dfc.png" data-rawwidth="460" data-rawheight="260"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-eed90038c0587b437ecdbb691ab1feff.png" data-rawwidth="814" data-rawheight="600"&gt;&lt;p&gt;从上图中我们可以看出，每天的步数存在一个上升趋势，随着时间的推移，步行的路程越长。&lt;/p&gt;&lt;h2&gt;获取每周和每月步行数据&lt;/h2&gt;&lt;p&gt;和上述代码一样，只要将 W 和 M 传递到 resample 函数中就能得到每周和每月的步行数据。&lt;/p&gt;&lt;p&gt;由于我更关心每天的运动情况，所以我将利用平均函数来计算每周或每月中平均每天的步行情况。具体的代码如下所示：  &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-fb147340d71fc16050f7e75194e392b4.png" data-rawwidth="473" data-rawheight="92"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-6dfc710d95c16669e3f2a86b7cbeb18a.png" data-rawwidth="746" data-rawheight="1230"&gt;&lt;h2&gt;更深入的分析 &lt;/h2&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-bd031dbee54442ad40d8957abbb1108d.png" data-rawwidth="728" data-rawheight="414"&gt;我很好奇的一件事是：工作日的运动量是否大于周末的运动量？我们可以利用 weekday 和 weekday_name 两个方法来帮助分析。对于每个时间戳数据，前者可以得知它属于一周中的第几天，而后者可以得知该时间点对应的时间名字信息。添加完这两个新变量后，我们还可以添加一个布尔变量来表示某个时间点是否是周末。  &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-10b3ef4230e541c99d0787e512470263.png" data-rawwidth="443" data-rawheight="241"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-50f45e5106d0cd6481ea9a07fcfde141.png" data-rawwidth="782" data-rawheight="320"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-309ea9bbb6482aa8165b391aeab49314.png" data-rawwidth="444" data-rawheight="123"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-023a6be32e57867dbc7381f80a56febb.png" data-rawwidth="844" data-rawheight="620"&gt;此外，我们还可以根据变量 weekend_bool来做分类汇总处理，并对比两组数据的差异情况。  &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-2a7ad8bd2cc939d6b9b73b2e73196f19.png" data-rawwidth="361" data-rawheight="536"&gt;&lt;p&gt;从上述结果中可以看出，周末期间的每天平均步数是 11,621 步，中位数是 10,228，而工作日期间的每天平均步数是 10,146 步，中位数是 9,742，因此我们可以认为周末期间的运动量更大。&lt;/p&gt;&lt;h2&gt;趋势分析&lt;/h2&gt;&lt;p&gt;最后让我们来讨论下上文提到的上升趋势，四月初由于工作的原因，我从夏洛特搬到了纽约城，担任 &lt;a href="https://www.yhat.com/" data-editable="true" data-title="Yhat"&gt;Yhat&lt;/a&gt;的软件工程师。&lt;/p&gt;&lt;p&gt;我想知道经过这次搬家之后，我每天的步行情况有没有发生改变？我们可以利用上文分析周末与工作日步行情况的方法来分析这个问题。  &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-528af85dd333c65fdfbb211964b2a0bb.png" data-rawwidth="808" data-rawheight="1236"&gt;&lt;p&gt;从上图中我们可以轻易地看出自从搬到纽约城之后，每天的运动量确实增加了。但这是由多方面因素共同决定的，比如搬到纽约城后我的跑步次数增加了，这会增加每天的平均步数。如果想要进行更深入的分析，我们需要获取更多的数据支持，由于篇幅问题，我们将在之后的文章中继续分析。&lt;/p&gt;&lt;h2&gt;总结&lt;/h2&gt;&lt;p&gt;我希望这个分析可以让您开始关注自己每天的步行情况，并利用 &lt;a href="https://www.yhat.com/products/rodeo" data-editable="true" data-title="Rodeo"&gt;Rodeo&lt;/a&gt; 和 &lt;a href="http://pandas.pydata.org/" data-editable="true" data-title="pandas"&gt;pandas&lt;/a&gt; 来分析数据。如果你对这个项目感兴趣的话，可以参阅&lt;a href="https://github.com/rkipp1210/data-projects" data-editable="true" data-title="该链接"&gt;该链接&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;http://weixin.qq.com/r/WkMCGqvEoPbfre959xZI (二维码自动识别)&lt;/p&gt;&lt;p&gt;原文链接：&lt;a href="http://blog.yhat.com/posts/phone-steps-timeseries.html" class="" data-editable="true" data-title="Å·hat | Analyzing iPhone Step Data"&gt;Å·hat | Analyzing iPhone Step Data&lt;/a&gt;&lt;/p&gt;&lt;p&gt;原文作者：Ross&lt;/p&gt;&lt;p&gt;译者：Fibears&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23540254&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Thu, 15 Dec 2016 10:28:56 GMT</pubDate></item><item><title>机器学习与Dota2英雄属性</title><link>https://zhuanlan.zhihu.com/p/23173919</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-efc37ac60357b68b466046b1d1132d6b_r.png"&gt;&lt;/p&gt;本文使用自然语言处理（NLP）方法对DOTA2 英雄角色属性做了简要的分析。&lt;h2&gt;引言&lt;/h2&gt;&lt;p&gt;如果有人问你“兔子”是什么意思，你会怎么回答？你可能会跑出去，每次看到一只兔子就指给那个人看，说“喏，那就是兔子。”当然那个人得和你三观相近，你才能解释得通。那如果对方对世界的认知方式和你不同，你要怎么向他解释？ &lt;/p&gt;&lt;p&gt;这就是为什么要引入词向量（word vector）这个概念。词向量基于分布假定，即“语义相近的词语往往会出现在相似的内容中”。 &lt;/p&gt;&lt;p&gt;在语言学中，如果你发现“negotiate”和“bargain”总是出现在相似的内容里，它们的语义就可能也是相近的。通过将词语向量化，再去计算向量化后“negotiate”和“bargain”的余弦相似度（一种测量两个向量间相似性的方法），你会发现相似度的值会接近1。词向量也常用于利用相似性去解释一些东西，比如说“皇帝”是“男人”，“皇后”就是“女人”。&lt;/p&gt;&lt;h2&gt;从词向量到英雄向量：&lt;/h2&gt;&lt;p&gt;那么词向量怎么帮助我们去认识 Dota2 中的队伍呢？你可以想象每只Dota2 的队伍配置都是由不同英雄做单词从而组成的句子，比如说：蝙蝠骑士、干扰者、拉席克、娜迦海妖、编织者，又或者：龙骑士、编织者、水晶室女、风行者、树精卫士。 &lt;/p&gt;&lt;p&gt;正如上文提到的分布假定所说的那样，只是这次我们不通过每个词所在的句子去判断词语的意思，而是通过每个Dota2英雄所在的队伍去判断每个英雄的角色属性。具体来说，如果我们发现“巫医”和“莱恩”总是出现在相似的队伍里，那么他们可能就具有相似的角色属性。 &lt;/p&gt;&lt;p&gt;本文使用datdota（&lt;a href="http://www.datdota.com/" data-editable="true" data-title="datdota: Home"&gt;datdota: Home&lt;/a&gt;）作为数据来源，并使用gensim library（&lt;a href="https://radimrehurek.com/gensim/index.html" class="" data-editable="true" data-title="gensim: Topic modelling for humans"&gt;gensim: Topic modelling for humans&lt;/a&gt;）使我们的英雄向量化。&lt;/p&gt;&lt;p&gt;举例来说，通过对数据集的训练，“影魔”这个英雄向量化后大致如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-db3ab7b6a371d0bae87c173181c8a3c0.png" data-rawwidth="444" data-rawheight="224"&gt;&lt;p&gt;这样，“影魔”这个英雄现在变成了50维空间里的一个点，但是把一个词变成50个数字并没有什么用处。现在来做一些炫酷的事吧！还是拿“影魔”为例，我们找出和它相似的一些英雄向量： &lt;/p&gt;&lt;ol&gt;&lt;li&gt;痛苦女王：0.9340388774871826&lt;/li&gt;&lt;li&gt;风暴之灵：0.9170020818710327&lt;/li&gt;&lt;li&gt;冥界亚龙：0.9082884788513184&lt;/li&gt;&lt;li&gt;狙击手：0.8958033919334412&lt;/li&gt;&lt;li&gt;宙斯：0.8526902794837952&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;（数字代表这些向量化后的英雄和“影魔”的余弦相似性） 下图则更直观地将结果进行展示，图中每行代表一个向量化后的英雄。 &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-3aab2ea1a132949cd4a3ba546ee44a57.png" data-rawwidth="753" data-rawheight="188"&gt;&lt;p&gt;这就看起来有点感觉了，接下来再尝试一些更难的东西看看我们是不是能通过这些向量化后的英雄来解释它们之间的类比性，例如 “莱恩”相对于“敌法师”就相当于“巫医”相对于以下几个英雄：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;幽鬼：0.9638912677764893&lt;/li&gt;&lt;li&gt;幻影长矛手：0.9185065031051636&lt;/li&gt;&lt;li&gt;幻影刺客：0.9039324522018433&lt;/li&gt;&lt;li&gt;变体精灵：0.858444333076477&lt;/li&gt;&lt;li&gt;噬魂鬼：0.8570600748062134&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;虽然这些分析可能不能帮助你打赢PPD,但至少下次你就知道该挑选哪些英雄来组队。&lt;/p&gt;&lt;h2&gt;3. 全局视角：&lt;/h2&gt;&lt;p&gt;我们可以把所有英雄同时展示在一张图里，但这难免让人眼花缭乱。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-54b3ea27bf915ed75a218aaeb52d0da1.png" data-rawwidth="589" data-rawheight="1454"&gt;如上图所示，每一列对应一个英雄，并把行列以树状图的形式展示出来。不过除非你已经看惯了这些图，不然直观地分清哪些英雄是一类的或者判断出哪个英雄和别的相似还是挺难的。 我们将树状图精简展示出来，来看看那些英雄是相似的，结果如下图所示： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-cbbcb22d4a3e309f919f88ce7f9dfe2e.png" data-rawwidth="485" data-rawheight="1454"&gt;&lt;p&gt;截至目前，我们已经得到了一些有意义（虽然并不完美）的英雄分类群体了，比如“劣势路三号位”群体包括了末日使者、司夜刺客、黑暗贤者、钢背兽和半人马战行者，“后期核心”群体包括了卓尔游侠、斯拉克、幻影长矛手和噬魂鬼，而“暴力型辅助”群体则包括了冥魂大帝、巨牙海民和亚巴顿。 &lt;/p&gt;&lt;p&gt;当然有其他方式来展示分类的结果。我们可以使用 t-SNE （&lt;a href="https://lvdmaaten.github.io/tsne/" data-editable="true" data-title="t-SNE – Laurens van der Maaten" class=""&gt;t-SNE – Laurens van der Maaten&lt;/a&gt;） 将50维的数据以二维的形式展示出来。t-SNE 是一种在尽可能不破坏原有高维数据结构的基础上以低维形式展示数据的方法。 &lt;/p&gt;&lt;p&gt;在对数据进行处理并在二维空间绘图后，我们可以得到下图： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-c3eba5d155e8ae098c438a41afb2b491.png" data-rawwidth="866" data-rawheight="866"&gt;&lt;p&gt;图中英雄不同的颜色来自于上文的分类结果，直观地看这些离散分类还是具有一定意义的。 &lt;/p&gt;&lt;h2&gt;4. 向量到底代表什么？&lt;/h2&gt;&lt;p&gt;那么我们给每个英雄赋的值到底代表什么呢？“影魔”在第一维空间里的值为-0.06813218，这个值代表什么意思？通常情况下，我们会说这些是潜在的属性值，就不再深究了。但在这个实例里，我们可以再深入一点。 &lt;/p&gt;&lt;p&gt;Dota2 gamepedia（&lt;a href="http://dota2.gamepedia.com/Dota" data-editable="true" data-title="Defense of the Anc"&gt;Defense of the Anc&lt;/a&gt;&lt;em&gt;2&lt;/em&gt;Wiki）给每个英雄都赋予了一系列角色属性。虽然这个属性有待商榷，而且在打完补丁之后有的英雄可能会从辅助变成核心，但是这仍然可以作为一个不错的出发点。 &lt;/p&gt;&lt;p&gt;现在，我们已经为接下来的分析做好准备了。回想一下之前我们已经用50个数字来刻画每个英雄了，并且从dota2 gamepedia网站中也有了每个英雄对应的角色属性。 &lt;/p&gt;&lt;p&gt;我们想验证向量维度和英雄的角色属性之间是否有一定的联系。有很多方法可以实现，但可能最简单的方式就是使用Logistic回归了。 &lt;/p&gt;&lt;p&gt;为了能得到稀疏集，我们使用L1范数规则化。 结果如下图所示： &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-647cbeced80f614b15d1201379b3aae5.png" data-rawwidth="866" data-rawheight="347"&gt;这幅图又意味着什么呢？我们可以看到第46个向量和“辅助”有很强的相关性，第38个向量和“远程英雄”联系紧密。下图是所有英雄第38和第46个向量值的散点图：  &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-6ac33284adeb7e65da9365e835b51ca1.png" data-rawwidth="866" data-rawheight="866"&gt;&lt;p&gt;可以看出第46个向量值大的英雄很可能是辅助英雄。相似地，第38个向量值大的英雄很可能是具备远程攻击的能力。当然，你可能注意到这两个向量似乎有点相关（虽然相关性并不是那么强），不过这也正好与辅助英雄往往能够远程攻击的情况相符。上图也将不朽尸王、上古巨神、亚巴顿这些近战支援（辅助属性高，远程属性低）的英雄正确识别出来了，但卓尔游侠的向量值却和角色属性不符，小骷髅克林克兹也几乎被判断成了一个近战核心英雄（图中其第38和46个向量值都不高，与事实不太相符）。 &lt;/p&gt;&lt;h2&gt;5. 结论：&lt;/h2&gt;&lt;p&gt;一旦涉及到计算机领域，每次英雄的选派都可以看作是没有实际意义的随机序列，但是从这里出发，却能得到可以被较好解释的信息。因此，在训练这些向量值时，我们并没有使用任何从Dota2得到的英雄属性信息。 &lt;/p&gt;&lt;p&gt;我们可以将Dota2英雄的选派看作是我们不会说的一门外语。通过观察这些词的使用模式，找出词意相近的词语。 &lt;/p&gt;&lt;p&gt;进一步地，通过连接在无监督学习下得到的分类特征，我们能够解释这些潜在属性。 &lt;/p&gt;&lt;p&gt;&lt;p&gt;http://weixin.qq.com/r/WkMCGqvEoPbfre959xZI (二维码自动识别)&lt;/p&gt;原文链接：&lt;a href="http://federicov.github.io/category/machine-learning.html" data-editable="true" data-title="Federico's Blog"&gt;Federico's Blog&lt;/a&gt;原文作者：Federico Vaggi译者：Vector   &lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23173919&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Thu, 08 Dec 2016 10:51:56 GMT</pubDate></item><item><title>用Python进行梯度提升算法的参数调整</title><link>https://zhuanlan.zhihu.com/p/22528998</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-6d94db5b29cf111f5b93f338eebea9d2_r.jpeg"&gt;&lt;/p&gt;&lt;h3&gt;引言&lt;/h3&gt;&lt;p&gt;或许之前你都是把梯度提升算法(Gradient Boosting Model)作为一个“黑箱”来用，那么现在我们就要把这个黑箱打开来看，里面到底装着什么玩意儿。 &lt;/p&gt;&lt;p&gt;提升算法(Boosting)在处理偏差-方差权衡的问题上表现优越，和装袋算法(Bagging)仅仅注重控制方差不同，提升算法在控制偏差和方差的问题上往往更加有效。在这里，我们提供一个对梯度提升算法的透彻理解，希望他能让你在处理这一问题上更加胸有成竹。 &lt;/p&gt;&lt;p&gt;这篇文章我们将会用Python语言实践梯度提升算法，并通过调整参数来获得更加可信的结果。  &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-6d94db5b29cf111f5b93f338eebea9d2.jpeg" data-rawwidth="500" data-rawheight="280"&gt;&lt;h3&gt;提升算法的机制&lt;/h3&gt;&lt;p&gt;提升算法是一个序列型的集成学习方法，它通过把一系列弱学习器集成为强学习器来提升它的预测精度，对于第t次要训练的弱学习器，它会更加重视之前第t-1次预测错误的样本，相反给预测正确的样本更低的权重，我们用图来描述一下： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-e916374b5294045bd0a6cf3ef74f7f2e.png" data-rawwidth="536" data-rawheight="181"&gt;&lt;ul&gt;&lt;li&gt;图一：生成的第一个弱分类器&lt;ul&gt;&lt;li&gt;所有的样本拥有相同的权重(用大小表示)。&lt;/li&gt;&lt;li&gt;决策边界成功预测了2个+样本和5个-样本。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;图二：生成的第二个弱分类器&lt;ul&gt;&lt;li&gt;在图一中被正确分类的样本给予了一个更小的权重，而错分类样本权重更大。&lt;/li&gt;&lt;li&gt;这个分类器更加重视那些权重大的样本并把它们正确分类，但是会造成其他样本的错分类。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;图三也是一样的，这个过程会循环多次直到最后，然后把所有的弱学习器基于他们的准确性赋予权重，并最终集成为强学习器。 &lt;/p&gt;&lt;h2&gt;梯度提升算法的参数&lt;/h2&gt;&lt;p&gt;梯度提升算法的参数可以被分为三类： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;决策树参数：单独影响每个弱学习器(决策树)的参数&lt;/li&gt;&lt;li&gt;提升算法参数：影响提升算法运行的参数&lt;/li&gt;&lt;li&gt;其他参数：整个模型中的其他参数&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;决策树参数&lt;/h3&gt;&lt;p&gt;下面是对决策树参数的详细介绍，在这里我们用的是Python的scikit-learn包，或许和R语言的一些包不同，但是他们蕴含的思想是一致的。 &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;分支最小样本量&lt;/strong&gt;：一个节点想要继续分支所需要的最小样本数。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;叶节点最小样本量&lt;/strong&gt;：一个节点要划为叶节点所需最小样本数，与上一个参数相对应。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;最小叶节点相对权重&lt;/strong&gt;：和上一个参数类似，只不过按照权重的定义转变为分数的形式。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;树最大深度&lt;/strong&gt;：树的层次，树越深越有过拟合的风险。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;最大叶节点量&lt;/strong&gt;：叶节点的最大数目，和树最大深度可以相互替代。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;最大特征子集量&lt;/strong&gt;：选择最优特征进行分支的时候，特征子集的最大数目，可以根据这个数目在特征全集中随机抽样。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在定义下面两类参数之前，我们先来看一下一个二分类问题的梯度提升算法框架： &lt;/p&gt;&lt;ol&gt;&lt;li&gt;生成初始模型&lt;/li&gt;&lt;li&gt;从1开始循环迭代2.1 根据上一个运行的结果更新权重2.2 用调整过的样本子集重新拟合模型2.3 对样本全集做预测2.4 结合预测和学习率来更新输出结果&lt;/li&gt;&lt;li&gt;生成最终结果这是一个非常朴素的梯度提升算法框架，我们刚才讨论的哪些参数仅仅是影响2.2这一环节里的弱学习器模型拟合。 &lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;提升算法参数&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;学习率&lt;/strong&gt;：这个参数是2.4中针对预测的结果计算的学习率。梯度提升算法就是通过对初始模型进行一次次的调整来实现的，学习率就是衡量每次调整幅度的一个参数。这个参数值越小，迭代出的结果往往越好，但所需要的迭代次数越多，计算成本也越大。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;弱学习器数量&lt;/strong&gt;：就是生成的所有的弱学习器的数目，也就是第2步当中的迭代次数，当然不是越多越好，因为提升算法也会有过拟合的风险。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;样本子集所占比重&lt;/strong&gt;：用来训练弱学习器的样本子集占样本总体的比重，一般都是随机抽样以降低方差，默认是选择总体80%的样本来训练。 &lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;其他参数&lt;/h3&gt;&lt;p&gt;诸如&lt;strong&gt;损失函数(loss)&lt;/strong&gt;、&lt;strong&gt;随机数种子(random_state)&lt;/strong&gt;等参数，不在本文调整的参数范围内，大多是采用默认状态。&lt;/p&gt;&lt;h2&gt;模型拟合与参数调整&lt;/h2&gt;&lt;p&gt;我们用的是从Data Hackathon 3.x AV hackathon下载的数据，在预处理以后，我们在Python中载入要用的包并导入数据。  &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-a4b857f3e822ecf3efc38b91e034c1dc.png" data-rawwidth="442" data-rawheight="341"&gt;我们先定义一个函数来帮助我们创建梯度提升算法模型并实施交叉验证。 &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-dceb55cc1282e06d00dd89a5aca25464.png" data-rawwidth="492" data-rawheight="657"&gt;我们首先创建一个基准模型，在这里我们选择AUC作为预测标准，如果你有幸拟合了一个好的基准模型，那你就不用进行参数调整了。下图是拟合的结果：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-e313feceee9fa94f92e58843b7319d6f.png" data-rawwidth="1492" data-rawheight="1026"&gt;&lt;p&gt;所以平均下来的交叉验证得分是0.8319，我们要让模型表现得更好一点。 &lt;/p&gt;&lt;h3&gt;参数调整的典型方法&lt;/h3&gt;&lt;p&gt;事实上，我们很难找到一个最佳的学习率参数，因为往往小一点的学习率会训练更多的弱学习器从而使得集成起来的学习器表现优越，但是这样也会导致过度拟合的问题，而且对于个人用的电脑来说，计算成本太大。下面的参数调整的思路要能够谨记于心： &lt;/p&gt;&lt;ol&gt;&lt;li&gt;先选择一个相对较高的&lt;strong&gt;学习率&lt;/strong&gt;，通常就是默认值0.1但是一般0.05到0.2范围内的数值都是可以尝试使用的。&lt;/li&gt;&lt;li&gt;在学习率确定的情况下，进一步确定要训练的&lt;strong&gt;弱学习器数量&lt;/strong&gt;，应该在40到70棵决策树之间，当然选择的时候还要根据电脑的性能量力而行。&lt;/li&gt;&lt;li&gt;决定好学习率和弱学习器数目后，调整&lt;strong&gt;决策树参数&lt;/strong&gt;，我们可以选择不同的参数来定义每一棵决策树的形式，下面也会有范例。&lt;/li&gt;&lt;li&gt;如果这样训练的模型精度不够理想，降低当前的学习率、训练更多的弱学习器。&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;调整弱学习器数量&lt;/h3&gt;&lt;p&gt;首先先看一下Python默认的一些参数值：&lt;strong&gt;分支最小样本量=500&lt;/strong&gt;；&lt;strong&gt;叶节点最小样本量=50&lt;/strong&gt;；&lt;strong&gt;树最大深度=8&lt;/strong&gt;； &lt;strong&gt;样本子集所占比重=0.8&lt;/strong&gt;；&lt;strong&gt;最大特征子集量=特征总数平方根&lt;/strong&gt;。这些默认参数值我们要在接下来的步骤中调整。我们现在要做的是基于以上这些默认值和默认的0.1学习率来决定弱学习器数量，我们用&lt;strong&gt;网格搜索(grid search)&lt;/strong&gt;的方法，以10为步长，在20到80之间测试弱学习器的最优数量。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-4727b9010a648aa0c2caf0df72a30def.png" data-rawwidth="512" data-rawheight="226"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-abc045022a31eeacfe6e233ef8631c81.png" data-rawwidth="1050" data-rawheight="322"&gt;&lt;p&gt;输出结果显示，我们确定60个弱学习器时得分最高，这个结果恰巧比较合理。但是情况往往不都是如此：如果最终结果显示大概在20左右，那么我们应该降低学习率到0.05；如果显示超过80(在80的时候得分最高)，那么我们应该调高学习率。最后再调整弱学习器数量，直到进入合理区间。 &lt;/p&gt;&lt;h3&gt;调整决策树参数&lt;/h3&gt;&lt;p&gt;确定好弱学习器数量之后，现实情况下常用的调参思路为： &lt;/p&gt;&lt;ol&gt;&lt;li&gt;调整&lt;strong&gt;树最大深度&lt;/strong&gt;和&lt;strong&gt;分支最小样本量&lt;/strong&gt;。&lt;/li&gt;&lt;li&gt;调整&lt;strong&gt;叶节点最小样本量&lt;/strong&gt;。&lt;/li&gt;&lt;li&gt;调整&lt;strong&gt;最大特征子集量&lt;/strong&gt;。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;当然上述调参顺序是慎重决定的，应该先调整那些有更大影响的参数。&lt;strong&gt;&lt;em&gt;注意：&lt;/em&gt;&lt;/strong&gt;接下来的网格搜索可能每次会花费15~30分钟甚至更长的时间，在实战中，你可以根据你的计算机情况合理选择步长和范围。&lt;/p&gt;&lt;p&gt;首先我们以2为步长在5到15之间选择树最大深度，以200为步长在200到1000内选择分支最小样本量，这些都是基于我本人的经验和直觉，现实中你也可以选择更大的范围更小的步长。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-d759b2d8f1a6c2f0046fc010fd8c16d9.png" data-rawwidth="515" data-rawheight="204"&gt;从运行结果来看，选择深度为9、分支最小样本量为1000时得分最高，而1000是我们所选范围的上界，所以真实的最优值可能在1000以上，理论上应该扩大范围继续寻找最优值。我们以200为步长在大于1000的范围内确定分支最小样本量，在30到70的范围内以10为步长确定叶节点最小样本量。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-7002a737b6d70e8eee46675faa5297ca.png" data-rawwidth="519" data-rawheight="195"&gt;最终我们得到了分支最小样本量为1200，叶节点最小样本量为60。这个时候我们阶段性回顾一下，看之前的调参效果。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-e077e83c08102cf600fdb816ed9dbef6.png" data-rawwidth="516" data-rawheight="37"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-dc81ed08159f071e65e9585bca8997b9.png" data-rawwidth="1488" data-rawheight="1024"&gt;如果你对比了基准模型和新模型的特征重要程度，你会发现我们已经能够从更多的特征中获其价值，现在的模型已经学会把凝视在前几个特征的目光分散到后面的特征。现在我们再来调整最后的决策树参数--最大特征量。调整方式为以2为步长从7到19。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-1dcd2920d1db02a8381c049356b51c84.png" data-rawwidth="511" data-rawheight="193"&gt;最终结果显示最优值是7，这也是算法默认的平方根，所以这一参数的默认值就是最好的。当然，你也可以选择更小的值来测，毕竟7同时是我们所选的范围下界，但我选择安于现状。接下来我们调整子集所占比重，候选值为0.6、0.7、0.75、0.8、0.85、0.9。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-f35b0cd0d9ed3279a75107d54e58ee95.png" data-rawwidth="513" data-rawheight="197"&gt;&lt;p&gt;从结果来看，0.85是最优值。这样我们就获得了所有的调整后的决策树参数。最后看一下我们的调参结果： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;分支最小样本量&lt;/strong&gt;：1200&lt;/li&gt;&lt;li&gt;&lt;strong&gt;叶节点最小样本量&lt;/strong&gt;：60&lt;/li&gt;&lt;li&gt;&lt;strong&gt;树最大深度&lt;/strong&gt;：9&lt;/li&gt;&lt;li&gt;&lt;strong&gt;最大特征子集量&lt;/strong&gt;：7&lt;/li&gt;&lt;li&gt;&lt;strong&gt;样本子集所占比重&lt;/strong&gt;:85%&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;调整学习率&lt;/h3&gt;&lt;p&gt;现在我们的任务是重新降低学习率，寻找一个低于默认值0.1的学习率并成比例地增加弱学习器的数量，当然这个时候弱学习器的数目已经不再是一开始调整后那个最优值了，但是新的参数值会是一个很好的基准。当树增多的时候，交叉验证寻找最优值的计算成本会更大。为了让你对模型表现有个直观的把握，我计算了接下来每次调试后模型的&lt;strong&gt;private leaderboard得分&lt;/strong&gt;，这个数据是不开源的，所以你没有办法复制，但是它对你理解有帮助。首先我们降低学习率到0.05，弱学习器数量增加到120个:&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-adb1d1bd4074565f501703d1a2d88f7b.png" data-rawwidth="514" data-rawheight="146"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-39026001cad22e73168e50721a814854.png" data-rawwidth="1480" data-rawheight="1020"&gt;&lt;strong&gt;private leaderboard得分&lt;/strong&gt;:0.844139学习率降低到0.01，弱学习器数量增加到600个:  &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-82de156ed82beaa682cc38c8374756e2.png" data-rawwidth="1472" data-rawheight="1032"&gt;&lt;strong&gt;private leaderboard得分&lt;/strong&gt;:0.848145学习率降低到0.005，弱学习器数量增加到1200个:&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-4b806ab950f9512c0e57b5141fc022d0.png" data-rawwidth="1474" data-rawheight="986"&gt;&lt;strong&gt;private leaderboard得分&lt;/strong&gt;:0.848112可以看到得分降低了一点点，我们再做一次调整，只把弱学习器数量增加到1500个:&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-46399334f2dd9538b47e98f6310090af.png" data-rawwidth="1470" data-rawheight="992"&gt;&lt;strong&gt;private leaderboard得分&lt;/strong&gt;:0.848747到此为止，我们可以看到得分由0.844到0.849，可以视为是比较显著的变化。所以最终我们确定的学习率为0.005，弱学习器数量为1500，当然这个计算成本是很高的。 &lt;/p&gt;&lt;blockquote&gt;&lt;h2&gt;结语&lt;/h2&gt;&lt;p&gt;本文基于优化梯度提升算法模型，分为三个部分：首先介绍了提升算法的思想，接下来讨论了梯度提升算法的参数分类，最后是模型拟合和参数调整，并结合Python予以示例。关于详细的代码等资料可以去作者的&lt;a href="https://github.com/aarshayj/Analytics_Vidhya/tree/master/Articles/Parameter_Tuning_GBM_with_Example" data-editable="true" data-title="GitHub"&gt;GitHub&lt;/a&gt;(&lt;a href="https://github.com/aarshayj/Analytics" class="" data-editable="true" data-title="github.com 的页面"&gt;https://github.com/aarshayj/Analytics&lt;/a&gt;&lt;em&gt;Vidhya/tree/master/Articles/Parameter&lt;/em&gt;Tuning&lt;em&gt;GBM&lt;/em&gt;with_Example)上寻找。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;原文作者：Aarshay Jain&lt;/p&gt;&lt;p&gt;译者：Teacup原文链接：&lt;a href="https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/" data-editable="true" data-title="Complete Guide to Parameter Tuning in Gradient Boosting (GBM) in Python" class=""&gt;Complete Guide to Parameter Tuning in Gradient Boosting (GBM) in Python&lt;/a&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22528998&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Thu, 01 Dec 2016 10:57:39 GMT</pubDate></item><item><title>主题模型初学者指南[Python]</title><link>https://zhuanlan.zhihu.com/p/23034092</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-db1cb9e08d326d231d02ba3efa4ce0ff_r.png"&gt;&lt;/p&gt;&lt;h2&gt;引言&lt;/h2&gt;&lt;p&gt;近年来涌现出越来越多的非结构化数据，我们很难直接利用传统的分析方法从这些数据中获得信息。但是新技术的出现使得我们可以从这些轻易地解析非结构化数据，并提取出重要信息。&lt;/p&gt;&lt;p&gt;主题模型是处理非结构化数据的一种常用方法，从名字中就可以看出，该模型的主要功能就是从文本数据中提取潜在的主题信息。主题模型不同于其他的基于规则或字典的搜索方法，它是一种无监督学习的方法。&lt;/p&gt;&lt;p&gt;主题可以由语料库中的共现词项所定义，一个好的主题模型的拟合结果应该如下所示——“health”、“doctor”、“patient”、“hospital”构成医疗保健主题，而“farm”、“crops”、“wheat”则构成农业主题。&lt;/p&gt;&lt;p&gt;主题模型的适用领域有：文档聚类、信息提取和特征选择。比如，纽约时报利用主题模型的结果来提升文章推荐引擎的功能。许多专家将主题模型应用到招聘领域中，利用主题模型来提取工作要求中的潜在信息，并用模型的拟合结果来匹配候选人。此外，主题模型还被用于处理大规模的非结构化数据，如邮件、顾客评论和用户社交数据。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-db1cb9e08d326d231d02ba3efa4ce0ff.png" data-rawwidth="510" data-rawheight="273"&gt;如果你不熟悉主题模型的话，那么本文将告诉你主题模型的原理以及如何利用Python来构建主题模型。&lt;/p&gt;&lt;h2&gt;目录&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;LDA(Latent Dirichlet Allocation) 模型&lt;ul&gt;&lt;li&gt;LDA 模型的参数&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Python 实现过程&lt;ul&gt;&lt;li&gt;数据准备&lt;/li&gt;&lt;li&gt;数据清洗与预处理&lt;/li&gt;&lt;li&gt;计算文档词频矩阵&lt;/li&gt;&lt;li&gt;构建 LDA 模型&lt;/li&gt;&lt;li&gt;拟合结果&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;建议&lt;ul&gt;&lt;li&gt;频数过滤法&lt;/li&gt;&lt;li&gt;标记过滤法&lt;/li&gt;&lt;li&gt;Batch Wise LDA&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;特征选择&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;LDA 模型&lt;/h2&gt;&lt;p&gt;我们可以用多种方法来处理文本数据，比如 TF 和 IDF 方法。LDA模型是最流行的主题模型，我们接下来将详细介绍 LDA 模型。&lt;/p&gt;&lt;p&gt;LDA 模型假设文档是由一系列主题构成的，然后再从这些主题中依据相应的概率分布生成词语。给定一个文档数据集，LDA 模型主要用于识别文档中的主题分布情况。&lt;/p&gt;&lt;p&gt;LDA 模型是一种矩阵分解技术，在向量空间模型中，任何语料都能被表示成一个文档词频矩阵。如下所示，矩阵中包含 N 篇文档，M 个词语，矩阵中的数值表示词语在文档中出现的频率。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-324b8bb9a452479b1a2c486ec9746204.png" data-rawwidth="164" data-rawheight="126"&gt;LDA 模型将上述的文档词频矩阵转换成两个低维的矩阵—— M1 和 M2。其中 M1 表示文档主题矩阵，M2 表示主题词语矩阵，它们的维度分别是 N*K 和 K*M，K 表示文档中主题的个数，M 表示词语的数量。  &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-6d5233d715c1f8854535d847b6cb9d8b.png" data-rawwidth="182" data-rawheight="284"&gt;&lt;p&gt;需要注意的是，上述两个矩阵提供了文档主题和主题词语的初始分布情况，LDA 模型通过抽样的方法来更新这两个矩阵。该模型通过更新文档中每个词语的主题归属情况来调整模型的参数值 p1 和 p2，其中 $p&lt;em&gt;1 = p(\frac{topic&lt;/em&gt;t}{document&lt;em&gt;d})$，$p&lt;/em&gt;2 = p(\frac{word&lt;em&gt;w}{topic&lt;/em&gt;t})$。经过一系列的迭代计算后，LDA 模型达到收敛状态，此时我们即可得到一组最佳参数值。&lt;/p&gt;&lt;h3&gt;LDA 模型的参数&lt;/h3&gt;&lt;p&gt;超参数 alpha 和 beta —— alpha 表示文档—主题密度，beta 则表示主题—词语密度，其中 alpha 值越大表示文档中包含更多的主题，而更大的 beta 值则表示主题中包含更多的词语。&lt;/p&gt;&lt;p&gt;主题个数——我们可以利用 KL 散度得分来计算最佳的主题个数，由于这部分内容过于学术，我将不详细介绍这些内容，有兴趣的读者可以参阅&lt;a href="http://link.springer.com/chapter/10.1007%2F978-3-642-13657-3_43" data-editable="true" data-title="相关文献"&gt;相关文献&lt;/a&gt;(&lt;a href="http://link.springer.com/chapter/10.1007%2F978-3-642-13657-3_43" data-editable="true" data-title="On Finding the Natural Number of Topics with Latent Dirichlet Allocation: Some Observations"&gt;On Finding the Natural Number of Topics with Latent Dirichlet Allocation: Some Observations&lt;/a&gt;)。&lt;/p&gt;&lt;p&gt;主题中的词数——这个参数取决于你的真实需求，如果你的目标是提取主题信息，那么你最好选择较多的词语。如果你的目标是提取特征，那么你应该选择较少的词项。&lt;/p&gt;&lt;p&gt;迭代次数—— LDA 算法的迭代次数&lt;/p&gt;&lt;h2&gt;Python 实现&lt;/h2&gt;&lt;h3&gt;数据准备&lt;/h3&gt;&lt;p&gt;以下是一些示例数据：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-e6dc5c16bda933b2fec1ed9797c876ea.png" data-rawwidth="686" data-rawheight="222"&gt;&lt;h3&gt;数据清洗与预处理&lt;/h3&gt;&lt;p&gt;数据清洗是文本建模分析过程中的一个重要环节，在这个过程中我们将移除标点符号、停止词并规整数据集：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-c4c9bf500f5aa8ce2033ce348cacf438.png" data-rawwidth="643" data-rawheight="302"&gt;&lt;/p&gt;&lt;h3&gt;计算文档词频矩阵&lt;/h3&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-9f9eba396331e197727adea27adf233f.png" data-rawwidth="636" data-rawheight="114"&gt;&lt;h3&gt;构建 LDA 模型&lt;/h3&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-58086aed96a630c36d6f786bf31fdae2.png" data-rawwidth="643" data-rawheight="71"&gt;&lt;h3&gt;拟合结果&lt;/h3&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-09e468d38c2052e8bf580785da91552b.png" data-rawwidth="605" data-rawheight="174"&gt;&lt;h2&gt;建议&lt;/h2&gt;&lt;p&gt;主题模型的拟合结果完全取决于语料库中的特征项，而语料是由一个稀疏的文档词频矩阵所构成的。降低该矩阵的维度可以提升主题模型的拟合结果，根据我的个人经验，主要有以下几个降维方法：&lt;/p&gt;&lt;h3&gt;频数过滤法&lt;/h3&gt;&lt;p&gt;我们可以按照词语的频数进行排序，然后保留频数较高的词语并将频数较低的词语剔除掉。此外我们还可以借助探索性分析的方法来决定如何设置阈值。 &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-84e1b52cee284fc9f21927977cfd7235.png" data-rawwidth="395" data-rawheight="240"&gt;&lt;h3&gt;标记过滤法&lt;/h3&gt;&lt;p&gt;通常情况下，标记过滤法的效果优于频数过滤法。主题模型通过词语的共现情况来反映主题信息，然而在每个主题中并不是所有的词语都是同等重要的。我们可以将这些无关紧要的词语剔除掉，提升模型的拟合效果。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-2fbd06eea19883cac1a76341362da65a.png" data-rawwidth="468" data-rawheight="276"&gt;&lt;h3&gt;Batch Wise LDA&lt;/h3&gt;&lt;p&gt;为了提取出文档中最重要的主题信息，我们可以将语料库分割成一系列固定大小的子集。然后我们可以对每个子集数据构建多个 LDA 模型，出现次数最多的主题就是该文档中最重要的主题信息。&lt;/p&gt;&lt;h2&gt;特征选择&lt;/h2&gt;&lt;p&gt;有些时候，我们还可以利用 LDA 模型来选择特征。以文本分类问题为例，如果训练集中包含多个类别的文档，我们可以首先构建 LDA 模型，然后剔除掉不同类别文档中共同出现的主题信息，剩余的特征即为有助于提升文本分类模型的准确率。&lt;/p&gt;&lt;h2&gt;结语&lt;/h2&gt;&lt;p&gt;到此为止，我们已经介绍完主题模型了，我希望本文能够帮你了解如何处理文本数据。如果你想加深对主题模型的理解，那么我建议你最好亲自练习下本文的代码并检查模型的拟合结果。&lt;/p&gt;&lt;p&gt;如果你觉得本文对你有帮助的话，你可以将此文分享给你的朋友。 &lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;原文链接：&lt;a href="https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/" class=""&gt;https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;原文作者：Shivam Bansal&lt;/p&gt;&lt;p&gt;译者：Fibears&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23034092&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Thu, 24 Nov 2016 10:54:52 GMT</pubDate></item><item><title>机器学习系列-word2vec篇</title><link>https://zhuanlan.zhihu.com/p/23733638</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-922b676116eecb65136750c1f8b41173_r.png"&gt;&lt;/p&gt;作者：向日葵&lt;h3&gt;开篇&lt;/h3&gt;&lt;p&gt;深度学习方向当下如火如荼，就差跑进楼下大妈的聊天内容了。深度学习的宝藏很多，一个小领域的一段小代码，都可以发出璀璨的光芒。如果你也刚刚踏入这方向，一开始难免有一些彷徨，但慢慢会有，清晨入古寺 初日照高林，那种博大的体验。&lt;/p&gt;&lt;p&gt;word2vec就是这样的一小段代码，如果你对word2vec的代码了如指掌，那你可以直接return。这是一篇关于word2vec介绍的文章，读完以后你会欣喜的发现自己会灵活的使用word2vec，但你也可能会郁闷，因为还是会觉得像是盲人摸象一样，完全对深度学习没有一点头绪。没关系，谁不是这样一点一滴的积累起来的呢。&lt;/p&gt;&lt;h3&gt;从需求入门&lt;/h3&gt;&lt;p&gt;美国大选刚刚落幕，川普胜出。假设反过来想，给你一个川普的词，你会联想到哪些？正常的话，应该是美国、大选、希拉里、奥巴马；也就是相似词语的选取了。对于相识词的选取，算法非常的多。也有许多人用了很简单的办法就能求得两样东西的相似性，比如购物车里物品的相似度，最简单的办法就是看看同时买了这样东西的用户还同时买了什么，用简单的数据结构就很容易实现这样的一个算法。这种算法很简单也很方便，但就是这种简单而使他忽略了很多的问题。例如时间顺序，下面会有提到。&lt;/p&gt;&lt;p&gt;还是回归到相识度的问题。归结到数学问题上，最经常用的是把每个词都归结到一个坐标系下，再用距离公式（如：皮尔逊公式）可方便的求出各个词语之间的相识度。&lt;/p&gt;&lt;p&gt;这也是word2vec的方法，word2vec 通过训练，可以把对文本内容的处理简化为 K 维向量空间中的向量运算，而向量空间上的相似度可以用来表示文本语义上的相似度。 如图，下面是有五维向量空间的单词：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-0c7b4498252abb13418c59d22b08a1f2.png" data-rawwidth="976" data-rawheight="443"&gt;算法的关键步骤就是如何求出词语的向量空间。 &lt;/p&gt;&lt;h3&gt;word2vec算法介绍&lt;/h3&gt;&lt;p&gt;word2vec是2013年Google中开源的一款工具。2013年神经网络的各种算法都已经相当的成熟了，word2vec核心是神经网络的方法，采用 CBOW（Continuous Bag-Of-Words，即连续的词袋模型）和 Skip-Gram 两种模型，将词语映像到同一坐标系，得出数值向量的高效工具。&lt;/p&gt;&lt;p&gt;一般来说算法采用神经网络的话，要注意他的输入和输出。因为使用神经网络进行训练需要有输入和输出，输入通过神经网络后，通过和输入对比，进行神经网络的重新调整，达到训练网络的目的。抓住输入输出就能够很好的理解神经网络的算法过程。&lt;/p&gt;&lt;p&gt;语言模型采用神经网络，就要判断什么东西要作为输入，什么东西要作为输出。这是算法可以创新的地方，语言模型有许多种，大部分的原理也是采用根据上下文，来推测这个词的概率。&lt;/p&gt;&lt;p&gt;word2vec输入输出也算是鬼斧神功，算法跟哈夫曼树有关系。哈夫曼树可以比较准确的表达这边文章的结构。&lt;/p&gt;&lt;p&gt;a,b,c,d分别表示不同词，并附加找个词出现的频率，这些词就能有自己的路径和编码。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-922b676116eecb65136750c1f8b41173.png" data-rawwidth="517" data-rawheight="291"&gt;&lt;p&gt;关于哈夫曼树就不仔细详细说明了，他是一种压缩算法，能很好的保持一篇文章的特性。&lt;/p&gt;&lt;p&gt;训练的过程是，把每一段落取出来，每个词都通过哈夫曼树对应的路径和编码。编码是(0和1)，作为神经网络的输出，每个路径初始化一个给定维数的向量，跟自己段落中的每个词作为输入，进行反向的迭代，就可以训练出参数。&lt;/p&gt;&lt;p&gt;这就是算法的整个过程。&lt;/p&gt;&lt;h3&gt;快速入门&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;代码下载： &lt;a href="http://word2vec.googlecode.com/svn/trunk/" data-editable="true" data-title="googlecode.com 的页面" class=""&gt;http://word2vec.googlecode.com/svn/trunk/&lt;/a&gt;&lt;/li&gt;&lt;li&gt;针对个人需求修改 makefile 文件，比如作者使用的 linux 系统就需要把 makefile 编译选项中的-Ofast 要更改为-O2 或者-g（调试时用）， 同时删除编译器无法辨认的-march=native 和-Wno-unused-result 选项。 有些系统可能还需要修改相关的 c 语言头文件，具体网上搜搜应该可以解决。&lt;/li&gt;&lt;li&gt;&lt;p&gt;运行“ make”编译 word2vec 工具。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;运行 demo 脚本： ./demo-word.sh&lt;/p&gt;&lt;p&gt;demo-word.sh主要工作为：&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;1）编译（ make）&lt;/li&gt;&lt;li&gt;2）下载训练数据 text8，如果不存在。 text8 中为一些空格隔开的英文单词，但不含标点符号，一共有 1600 多万个单词。&lt;/li&gt;&lt;li&gt;3）训练，大概一个小时左右，取决于机器配置&lt;/li&gt;&lt;li&gt;4）调用distance，查找最近的词&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;python版本的命令如下：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Python的命令为python word2vec.py -train tx -model vb -cbow 0 -negative 0 -dim 5&lt;/p&gt;&lt;/blockquote&gt;&lt;h3&gt;应用&lt;/h3&gt;&lt;p&gt;word2vec是根据文章中每个词的上下关系，把每个词的关系映射到同一坐标系下，构成了一个大矩阵，矩阵下反映了每个词的关系。这些词的关系是通过上下文相关得出来的，它具有前后序列性，而Word2vec同时采用了哈夫曼的压缩算法，对是一些热门词进行了很好的降权处理。因此他在做一些相似词，或者词语的扩展都有很好的效果。&lt;/p&gt;&lt;p&gt;这种相识性还可以用在，物品的推荐上，根据用户购买物品的顺序，把每个物品当成一个单词，相当于一门外语了，谁也看不懂而已，但里面放映了上下文的关系，这个是很重要的，也是我们一开头那种普通算法无法做到的，同时对一些热门的物品自然有降权的处理，非常的方便。&lt;/p&gt;&lt;p&gt;word2vec自然规避了两大问题：词语的次序和热门词语的降权处理。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23733638&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Thu, 17 Nov 2016 12:59:58 GMT</pubDate></item><item><title>Github上Pandas,Numpy和 Scipy三个库中20个最常用的函数</title><link>https://zhuanlan.zhihu.com/p/22589899</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-798f51341c7d164bd44eaaf38393fc65_r.png"&gt;&lt;/p&gt;&lt;p&gt;几个月前，我看到一篇博客中列出了 Github 网站上 Python 常用库中使用频率最高的一些函数/模块。我在这个基础上做了可视化处理，并撰写了每个库中使用频率前十的函数示例。其中本文中只包含了部分示例，完整的示例可以参见我的 Github。 &lt;/p&gt;&lt;p&gt;首先我利用 requests 和 BeautifulSoup 从原始博客中爬取相关的数据，然后利用 matplotlib 和 seaborn 来绘制条形图，其中函数的排序由包含该函数的资源库(Repositories)数目所决定。比如，虽然 pd.Timestamp 的总频次特别高，但是该函数仅在少量的资源库中出现，所以它的排序相对靠后。&lt;/p&gt;&lt;p&gt;Pandas&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-40d2ea1b8cf1e0dfb2fb365f61a0f2cc.png" data-rawwidth="818" data-rawheight="494"&gt;&lt;h4&gt;DataFrame: 创建一个 dataframe 对象&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-d0a012915b1e7a78427ec3a77492c2c4.png" data-rawwidth="448" data-rawheight="500"&gt;&lt;h4&gt;merge：联结两个 dataframe&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-ee46064bfbfa818849118ef5ac9c7fa7.png" data-rawwidth="454" data-rawheight="403"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-35d684f66095fca3477f57010775b95c.png" data-rawwidth="785" data-rawheight="541"&gt;&lt;h4&gt;Numpy&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-f8b0281e4a7f166298981ddcbb7ae3ac.png" data-rawwidth="776" data-rawheight="526"&gt;&lt;/h4&gt;&lt;h4&gt;arange: 创建某个区间内等间距的序列数组&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-8cadd29490f8d0e1719d9df3e4d41f00.png" data-rawwidth="448" data-rawheight="175"&gt;&lt;h4&gt;mean: 沿着某个轴向计算列表/数组中所有数据的平均数&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-8d3760ef3bfe2c7a0fc9033bae5c67c7.png" data-rawwidth="451" data-rawheight="327"&gt;&lt;h4&gt;Scipy&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-00c7eb8d0f1991e3a245894f6cf34720.png" data-rawwidth="780" data-rawheight="523"&gt;&lt;h4&gt;stats: 常用的统计函数或分布函数&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-1fcddb0f0dadf7fe3db647724ab30f93.png" data-rawwidth="453" data-rawheight="355"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-3312fa73fcca36fd718e9825d0fbf162.png" data-rawwidth="381" data-rawheight="256"&gt;&lt;h4&gt;linalg: 常用的线性代数函数，如逆矩阵(linalg.inv)、行列式(linalg.det)&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-1d040fd9df660cd7f7339e0c5fdf423a.png" data-rawwidth="449" data-rawheight="521"&gt;&lt;h4&gt;interpolate: 样条函数和插值函数&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-7ee12e43107a108d56f9555bb2e898a7.png" data-rawwidth="451" data-rawheight="218"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-907edbd661c2d653b88acea7b80784b8.png" data-rawwidth="380" data-rawheight="256"&gt;&lt;/h4&gt;&lt;h4&gt;signal: 包含信号处理工具&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-01c14faa35a8e3013cb5251a05c91c6f.png" data-rawwidth="457" data-rawheight="541"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-b69e91802f6a38d8e15c648fb5fd731b.png" data-rawwidth="383" data-rawheight="266"&gt;&lt;/h4&gt;&lt;h4&gt;misc: misc.imread 和 misc.imsave 分别用于读取和保存图像数据&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-6301e3a3f640f18e6ebbed7a0c590b70.png" data-rawwidth="453" data-rawheight="236"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-798f51341c7d164bd44eaaf38393fc65.png" data-rawwidth="572" data-rawheight="204"&gt;&lt;p&gt;&lt;p&gt;http://weixin.qq.com/r/WkMCGqvEoPbfre959xZI (二维码自动识别)&lt;/p&gt;原文链接：&lt;a href="https://galeascience.wordpress.com/2016/08/10/top-10-pandas-numpy-and-scipy-functions-on-github/" class="" data-editable="true" data-title="Top 20 Pandas, NumPy and SciPy functions on GitHub"&gt;Top 20 Pandas, NumPy and SciPy functions on GitHub&lt;/a&gt;原文作者：Alexander Galea译者：Fibears&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22589899&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Thu, 10 Nov 2016 10:28:14 GMT</pubDate></item><item><title>美国大选Facebook舆情分析——基于R</title><link>https://zhuanlan.zhihu.com/p/22270222</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/adedd738064e6e8f128136380fccdb5d_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;2016年7月27日，希拉里·克林顿顺利成为民主党总统候选人，这也意味着她将和之前成为共和党总统候选人的唐纳德·特朗普在11月份进行最终角逐。两位候选人在之前的五个月的网络口水仗，也使得各大社交平台异彩纷呈。为了从社交媒体这一渠道对两位候选人的竞选表现和粉丝基础有一个更为具体的了解。我们分析了一年多以来克林顿（大约360万粉丝）和特朗普（大约800万粉丝）的Facebook流量。&lt;/p&gt;&lt;h3&gt;数据获取&lt;/h3&gt;&lt;p&gt;用R包“Rfacebook”爬取了2015年5月1日到2016年5月31日的候选人Facebook官方主页的内容，获得了这段时间内所有的帖子和相应的评论（特朗普大约140万评论而克林顿大约120万评论）。然后用R的文本挖掘包和文本数据定量分析方法，基于LIWC的词条目录对每个评论（不含非英文内容）单独解析它的情绪和心理结构。最后，我们汇总成为日度数据进行分析。&lt;/p&gt;&lt;h3&gt;情绪氛围&lt;/h3&gt;&lt;p&gt;特朗普评论区相较克林顿表现得更为积极：数据显示特朗普的积极评论占比69.46%，消极评论占比30.46%；克林顿的积极评论占比65.94%，消极评论占比33.94％。而第二张图显示特朗普评论区的情绪分化现象更为显著：这里用百分比的变异系数来表示情绪分化程度，特朗普评论的百分比变异系数是122.23，而克林顿的是115.31，这确实很容易理解，特朗普自竞选以来发表了种种言论，要么引来狂热的追捧，要么被人诟病为疯子。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/1d2e8019e3e72aa81dbc5ac903a7e90b.png" data-rawwidth="865" data-rawheight="508"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/15a02b9e169c971c733d554dd110da7c.png" data-rawwidth="865" data-rawheight="508"&gt;&lt;h3&gt;乐观程度&lt;/h3&gt;&lt;p&gt;克林顿的拥护者相较而言对竞选前景表现得更为乐观（克林顿31.13％而特朗普是29.57%），并但是可以发现特朗普的评论区有一种向上的积极发展趋势。同样，特朗普评论区的乐观程度分化也更为显著，拥有130.12的变异系数，而克林顿只有126.11。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/93fcfd6e0609e8ed99cf3524c3b902ad.png" data-rawwidth="865" data-rawheight="509"&gt;&lt;h3&gt;拥护者群体的包容度&lt;/h3&gt;&lt;p&gt;第四张图显示了评论区中，拥护者相互包容的的程度。比如说如果评论中更多的“我们”出现，则拥护者内部是更加包容和团结的；如果评论中更多的“我”出现，则拥护者内部是更加独立的。结果显示双方表达方式有很大不同，特朗普的拥护者表述的方式更加独立（36.12%），而克林顿的拥护者相对包容（30.38%），这可能意味着克林顿的拥护者能够更好地凝聚力量支持他们的选择。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/a99a686a5295b8d639d4432de9295a51.png" data-rawwidth="865" data-rawheight="508"&gt;七月底民调结果显示克林顿的支持率首次被特朗普反超，但是紧接着由于特朗普的不当言论，克林顿重获优势，但无论下一任世界领袖的归属如何，最终决定这一结果的，还是数据。 &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/ababd5743239a6482b089db6e9472764.png" data-rawwidth="640" data-rawheight="500"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22270222&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Thu, 03 Nov 2016 10:29:28 GMT</pubDate></item><item><title>跨境电商微信公众号推文热点解析</title><link>https://zhuanlan.zhihu.com/p/23110482</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-7abcf006901ac8a465dd12f9b5add773_r.png"&gt;&lt;/p&gt;&lt;h3&gt;作者：数据工匠－SD Cry!!&lt;/h3&gt;&lt;p&gt;近几年来，国内跨境电商发展势头良好，在习惯了由代购到海淘这类进口跨境电子商务模式之后，国内的电商卖家也将目光转向了广阔的全球市场。据中国商务部预测，2016年中国跨境电商进出口额预计增至6.5万亿元人民币，占总进出口贸易额的20%，年增长率将达到30%。毫无疑问，跨境电子商务正逐步成为我国经济新的增长动力，为外贸市场带来新的活力。在此背景下，各类与跨境电商相关的微信公众号也如雨后春笋般地走入了人们的视野，成为了国内跨境电商卖家了解跨境电商行业知识与实事情报的重要渠道。那么，在最近的一段时间内，这些公众号都在关注些什么呢？ &lt;/p&gt;&lt;p&gt;为了了解跨境电商微信公众号推文热点，我们采集了跨境电商相关的微信公众号2016年3月至8月发布的微信推文，总计612篇，各个公众号推文占比如下：   &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-7abcf006901ac8a465dd12f9b5add773.png" data-rawwidth="857" data-rawheight="569"&gt;在收集到的微信推文中，“跨境电商”、“电商平台”、“亚马逊平台”、“亚马逊卖家”以及“亚马逊运营”等词出现的次数位列高频词组榜前五，其中三个词组出现了“亚马逊”字样。在词云图中“亚马逊实操”、“什么FBA”也出现在了显眼的位置，由此可以推测亚马逊为现阶段微信公众号最关心的电商平台。 &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-b53c04db1386cbb21a8621188d767d70.png" data-rawwidth="829" data-rawheight="319"&gt;&lt;p&gt;为了作进一步地说明，我们对比了关键词“亚马逊／amazon”与其他电商平台名出现的频率，通过统计，发现在所有的612篇微信推文中，有79篇谈及Wish，占比12.91%；有131篇谈及速卖通，占比21.41%；有176篇谈及Ebay，占比28.76%；而谈及亚马逊的推文则高达463篇，占比75.65%。&lt;/p&gt;&lt;h2&gt;热点解析&lt;/h2&gt;&lt;h4&gt;一、国际电商政策与平台规则是关注焦点&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-69b1fb6a5611e3af1b5026647c505d25.png" data-rawwidth="865" data-rawheight="356"&gt;&lt;p&gt;在所有推文中，部分主要讨论与中国卖家最直接相关的国际电商政策与平台规则，今年8月，亚马逊美国站全球开店正式恢复，相关政策的改变也成为了受到大多数国内跨境电商卖家关注的焦点。 &lt;/p&gt;&lt;h4&gt;二、公众号在推文中的宣传力度加大&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-6938488646f367cc6587d5bfbf780303.png" data-rawwidth="820" data-rawheight="345"&gt;&lt;p&gt;在收集到的推文中，部分内容侧重于与公众号关联的微信群或相关社区推广，随着跨境电商信息平台及相关第三方工具数量的激增，业内竞争压力促使公众号在推文中加大了宣传的力度。 &lt;/p&gt;&lt;h4&gt;三、多角度分析社交媒体对跨境电商的影响&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-0913f85bd3eacc89f60e4dd7ffb4bfa7.png" data-rawwidth="845" data-rawheight="350"&gt;&lt;p&gt;一部分微信推文内容涉及当今网络社交媒体对于跨境电商的影响，谈及了facebook、网络红人以及2016夏季受网民关注较高的奥运会等内容。&lt;/p&gt;&lt;h4&gt;四、“千刀”计划运营日志推广势头强劲&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-914131054170c3803abe01149e307544.png" data-rawwidth="847" data-rawheight="366"&gt;&lt;p&gt;在全部推文中，有一部分均为某跨境电商机构“千刀”活动的运营日志广播或经验交流，相对于其他热点，我们能够在这些推文的词云图中找到“千刀创客”、“Bella女神”这种更具特色的词组，独特的推广交流形式使得这些文章被单独聚为一类，可以看出，在如今的时代，微信公众号在传播信息之外也可能成为“跨境电商明星”诞生的平台。&lt;/p&gt;&lt;h4&gt;五、电商行业大会成为获得行业信息的重要渠道&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-7d9e85730fca8be6f749ba5aa4652776.png" data-rawwidth="888" data-rawheight="360"&gt;&lt;p&gt;还有部分推文主要是各类跨境电商集会或电商行业大会的宣传与通知，随着平台基本规则在国内卖家中的逐渐普及，卖家间的线下交流也成为了获得行业信息的重要渠道，各种电商大会的出现很好的印证了这一点，而微信公众号也成为了其宣传推广的重要渠道。 &lt;/p&gt;&lt;h4&gt;六、“广告引流与Listing优化”话题经久不衰&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-86a2e2089d8739b935b921e048b35b9d.png" data-rawwidth="912" data-rawheight="347"&gt;&lt;p&gt;还有部分推文主要讨论产品的广告引流与Listing关键词优化，这是绝大多数跨境电商卖家打造所谓“爆款”的必经之路，也是整个电商行业范围内经久不衰的话题。 &lt;/p&gt;&lt;h4&gt;七、亚马逊物流体系FBA倍受关注&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a87cf0927123370360b7e6ceb8df77e6.png" data-rawwidth="870" data-rawheight="346"&gt;&lt;p&gt;该类推文主要讨论跨境电商交易过程中涉及到的物流与仓储及售后问题，其中亚马逊物流体系FBA倍受关注。&lt;/p&gt;&lt;h4&gt;八、国际电商发展形势不容忽视  &lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-25c15b45afa87cddc9cd2392ac6a6380.png" data-rawwidth="880" data-rawheight="343"&gt;&lt;p&gt;该类推文主要为一系列国际大型电商平台发展情况的行业数据报道，包括亚马逊的阶段性经营业绩、电商平台的行业竞争数据以及国际间的汇率变动等，这些大环境形势的变化直接关系着个人卖家的有关决策，不容忽视。&lt;/p&gt;&lt;h4&gt;九、电商技巧培训等新型业态悄然发展&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-90be3452f7f66e754df3851bfe2c21a9.png" data-rawwidth="867" data-rawheight="339"&gt;&lt;p&gt;该类推文主要内容为跨境电商在线答疑或培训的推广广告，可以看到，跨境电商蓬勃的发展也促使了电商技巧线上答疑培训这一新型业态的产生。&lt;/p&gt;&lt;h4&gt;十、运营培训班颇受欢迎&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-aeeb42ac05e35bdee11fcdae112cda07.png" data-rawwidth="837" data-rawheight="328"&gt;&lt;p&gt;这其实是一篇亚马逊运营课程报名的推广文，由于重复次数较多，被单独聚类为了一个热点话题。可以看到，除了在线的答疑指导与线下的同业交流，这种贩卖课程体系、专家集中授课的形式也受到了当今跨境电商从业者的欢迎。&lt;/p&gt;&lt;p&gt;根据热点话题发现的结果，本文统计了属于各个热点话题的推文占比，统计结果如下所示：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-80d2b3215930f82e1608160f900b2026.png" data-rawwidth="761" data-rawheight="363"&gt;&lt;p&gt;可以看到，在所有热点话题中，涉及“国际电商政策与平台规则是关注焦点”这一热点的推文总计187篇，占推文总数的31%，为近期跨境电商相关公众号发布最多的文章类型。同时，值得注意的是， “多角度分析社交媒体对跨境电商的影响”相关文章共计63篇，占比达10%以上，超过了“广告引流与Listing优化”和“亚马逊物流体系FBA”等热点，这昭示着社交媒体对于跨境电商作用的日益明显，可能成为未来影响跨境电商行业风向的又一作用力。&lt;/p&gt;&lt;p&gt;经过以上分析，我们大致了解了近期跨境电商微信公众号的发文内容，可以归结为以下几点：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;相比于其他国际电商平台，亚马逊受到了跨境电商公众号最多的关注，这可能是由于我国出口跨境电商在亚马逊上的发展相对成熟；&lt;/li&gt;&lt;li&gt;微信推文内容鱼龙混杂，不乏大量在线或线下付费培训广告，我国跨境电商的发展不但直接作用于进出口贸易额，也带动着一系列新业态的产生；&lt;/li&gt;&lt;li&gt;“国际电商政策与平台规则”成为近期跨境电商微信公众号关注次数最多的焦点，而“多角度分析社交媒体对跨境电商的影响”的推文占比也相对较大，同样应引起广大电商卖家注意。 &lt;/li&gt;&lt;/ol&gt;&lt;p&gt;http://weixin.qq.com/r/WkMCGqvEoPbfre959xZI (二维码自动识别)&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23110482&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Wed, 02 Nov 2016 16:16:22 GMT</pubDate></item><item><title>亚马逊美站 hot new releases 榜单分析</title><link>https://zhuanlan.zhihu.com/p/23368799</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-61490b65a4190772729085f08bc39a59_r.png"&gt;&lt;/p&gt;&lt;p&gt;随着现代科技的进步发展，各式各样的移动设备正逐渐成为人们学习工作生活中的必需品，对于跨境电商而言，手机及其配件的市场规模同样不容小觑。上周数据脉数据发掘中心，通过对亚马逊市场 Cell Phones &amp;amp; Accessories 类目商品榜单进行深入挖掘，还提供了一份专业的数据报告。近些年，境外电商的下一个发展点在数据挖掘领域尤为突出，中国许多优秀的境外电商也在慢慢向数据驱动的商业模式靠拢，这样的数据报告在未来将是境外电商主流的决策参考依据。&lt;/p&gt;&lt;p&gt;报告生成要经过许多专业的数据处理工作：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;数据挖掘：数据收集覆盖了亚马逊 Cell Phones &amp;amp; Accessories 类目的相关榜单。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;（下图为报告的部分内容） &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-36b6f8797942516953904318d51d2d05.png" data-rawwidth="659" data-rawheight="265"&gt;数据分析：每个数据图表后面都会附上相对应的数据分析和解读，主要讨论了数据所显示出的主要信息。 &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-9bc5af55ba06a6bf88ecabe5e00c6c9f.png" data-rawwidth="609" data-rawheight="394"&gt;重要信息提炼：正如下图所示，报告指出上周一款品牌为 Anker 的壁式手机充电器榜单排名增长十分显著。 &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-b94c4f661c962b2fa01cfe2a9ae7218a.png" data-rawwidth="636" data-rawheight="506"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-61490b65a4190772729085f08bc39a59.png" data-rawwidth="630" data-rawheight="422"&gt;以上是对这份报告的简单介绍，希望看到更多更详尽的内容，可以登录数据脉官方网站查看（&lt;a href="https://datartery.com/" data-editable="true" data-title="数据脉 - 亚马逊数据选品运营神器" class=""&gt;数据脉 - 亚马逊数据选品运营神器&lt;/a&gt;），希望所有企业都能享受到数据科学高速发展所带来的成果，用数据脉打造亚马逊上最火的商品！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23368799&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Wed, 02 Nov 2016 13:26:41 GMT</pubDate></item><item><title>如何可视化城市的交通便捷性</title><link>https://zhuanlan.zhihu.com/p/22351423</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/960dd13000dace136234afea98703f0d_r.png"&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.walkscore.com/" data-editable="true" data-title="WalkScore"&gt;WalkScore&lt;/a&gt;(&lt;a href="https://www.walkscore.com/" data-editable="true" data-title="Find Apartments for Rent and Rentals"&gt;Find Apartments for Rent and Rentals&lt;/a&gt;) 是一款用于可视化展示街区交通便捷性的工具，即评估从该街区到附近公园、学校或者餐馆等地方是否便利。借助 Python 中的 &lt;a href="https://udst.github.io/pandana/" data-editable="true" data-title="pandana"&gt;pandana&lt;/a&gt;(&lt;a href="https://udst.github.io/pandana/" data-editable="true" data-title="Pandana — pandana 0.1 documentation" class=""&gt;Pandana — pandana 0.1 documentation&lt;/a&gt;) 库，我们可以很轻松地创建一个类似的可视化工具。Pandana(Fletcher Foti 开发的用于社区分析的 pandas 扩展包) 可以快速地查询某个街区的交通便捷性。本文将介绍如何利用它来可视化展示城市的交通便捷性，本文的相关代码和数据都托管在 &lt;a href="https://github.com/gboeing/urban-data-science/tree/master/20-Network-Analysis-Walkability" data-editable="true" data-title="Github"&gt;Github&lt;/a&gt;(&lt;a href="https://github.com/gboeing/urban-data-science/tree/master/20-Network-Analysis-Walkability" data-editable="true" data-title="urban-data-science/20-Network-Analysis-Walkability at master · gboeing/urban-data-science · GitHub" class=""&gt;urban-data-science/20-Network-Analysis-Walkability at master · gboeing/urban-data-science · GitHub&lt;/a&gt;)上。&lt;/p&gt;&lt;p&gt;首先，我利用 pandana 来绘制旧金山地区伯克利/奥克兰的边界区域。接下来我将从开放街景地图数据库中导入道路关系网和便利设施的位置数据。本文主要关心到达餐馆、酒吧和学校的便捷性，你可以根据自己的研究目的设定相应的便利设施库。最后我将计算街道网络中每个街区节点步行到最近便利设施的距离并进行可视化展示。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/960dd13000dace136234afea98703f0d.png" data-rawwidth="571" data-rawheight="412"&gt;&lt;p&gt;上图中展示了街道网络图中每个街区节点步行到一公里内最近的餐馆、酒吧或者学校的可达性。但是衡量交通便捷性的一个更好的指标是该街区附近是否存在较多的便利设施，因此我们考虑绘制每个街区节点到其第五近的便利设施的距离情况：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/bd1331cb65087b56ef2399f18f909ab3.png" data-rawwidth="578" data-rawheight="410"&gt;上图中很清晰地展示了交通较为便捷的奥克兰中心区和伯克利中心区。此外我们还可以单独评估各个社区到附近不同地点的距离。下图展示了街区节点到最近酒吧的可达性：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/8a7cd4355cd28f0ac0746331baf17355.png" data-rawwidth="565" data-rawheight="415"&gt;&lt;p&gt;下图展示了到最近学校的可达性：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7043b80d500fa79603b39264d43380a0.png" data-rawwidth="586" data-rawheight="419"&gt;不出所料，不同的街区到酒吧和学校的可达性差异较大。而且从上图中我们可以看出，湾区东部有一大块街区附近基本没有学校资源。下一步我们可以进一步分析这些便利设施的布局与当地种族聚集程度、教育水平和收入水平等变量之间的关系。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/ababd5743239a6482b089db6e9472764.png" data-rawwidth="640" data-rawheight="500"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22351423&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Thu, 20 Oct 2016 10:44:31 GMT</pubDate></item></channel></rss>