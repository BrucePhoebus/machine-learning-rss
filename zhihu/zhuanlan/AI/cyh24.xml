<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>AutoVision - 知乎专栏</title><link>https://zhuanlan.zhihu.com/cyh24</link><description>关于计算机视觉、深度学习、无人驾驶~</description><lastBuildDate>Mon, 06 Mar 2017 03:17:07 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>【深度神经网络压缩】Deep Compression （ICLR2016 Best Paper）</title><link>https://zhuanlan.zhihu.com/p/21574328</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/f52297500c30691a525908384867d96e_r.jpg"&gt;&lt;/p&gt;&lt;h2&gt;Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman coding&lt;/h2&gt;&lt;p&gt;这篇论文是Stanford的Song Han的 ICLR2016 的 &lt;b&gt;best paper&lt;/b&gt;，Song Han写了一系列网络压缩的论文，这是其中一篇，更多论文笔记也会在后续博客给出。&lt;/p&gt;&lt;p&gt;首先，给这篇论文的清晰结构点赞，论文题目就已经概括了文章的三个重点，而且每个部分图文并茂，文章看起来一点都不费力，不愧是 ICLR 2016 best paper !&lt;/p&gt;&lt;h2&gt;Abstract&lt;/h2&gt;&lt;p&gt;&lt;b&gt;为什么要压缩网络？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;做过深度学习的应该都知道，NN大法确实效果很赞，在各个领域轻松碾压传统算法，不过真正用到实际项目中却会有很大的问题：&lt;/p&gt;&lt;ol&gt;&lt;li&gt; 计算量非常巨大；&lt;/li&gt;&lt;li&gt; 模型特别吃内存；&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这两个原因，使得很难把NN大法应用到嵌入式系统中去，因为嵌入式系统资源有限，而NN模型动不动就好几百兆。所以，计算量和内存的问题是作者的motivation；&lt;/p&gt;&lt;p&gt;&lt;b&gt;如何压缩？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;论文题目已经一句话概括了：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;Prunes the network&lt;/b&gt;：只保留一些重要的连接；&lt;/li&gt;&lt;li&gt;&lt;b&gt;Quantize the weights&lt;/b&gt;：通过权值量化来共享一些weights；&lt;/li&gt;&lt;li&gt;&lt;b&gt; Huffman coding&lt;/b&gt;：通过霍夫曼编码进一步压缩；&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;效果如何？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Pruning&lt;/b&gt;：把连接数减少到原来的 1/13~1/9；&lt;/p&gt;&lt;p&gt;&lt;b&gt;Quantization&lt;/b&gt;：每一个连接从原来的 32bits 减少到 5bits；&lt;/p&gt;&lt;p&gt;&lt;b&gt;最终效果&lt;/b&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; 把AlextNet压缩了35倍，从 240MB，减小到 6.9MB；&lt;/li&gt;&lt;li&gt; 把VGG-16压缩了49北，从 552MB 减小到 11.3MB；&lt;/li&gt;&lt;li&gt; 计算速度是原来的3~4倍，能源消耗是原来的3~7倍；&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;Network Pruning&lt;/h2&gt;&lt;p&gt;其实 network pruning 技术已经被广泛应用到CNN模型的压缩中了。&lt;/p&gt;&lt;p&gt;早期的一些工作中，LeCun 用它来减少网络复杂度，从而达到避免 over-fitting 的效果；&lt;/p&gt;&lt;p&gt;近期，其实也就是作者的第一篇网络压缩论文中，通过剪枝达到了 state-of-the-art 的结果，而且没有减少模型的准确率；&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/05e75809e036abb1184af9a07c17198e.png" data-rawwidth="924" data-rawheight="393"&gt;&lt;p&gt;从上图的左边的pruning阶段可以看出，其过程是：&lt;/p&gt;&lt;ol&gt;&lt;li&gt; 正常的训练一个网络；&lt;/li&gt;&lt;li&gt; 把一些权值很小的连接进行剪枝：通过一个阈值来剪枝；&lt;/li&gt;&lt;li&gt; retrain 这个剪完枝的稀疏连接的网络；&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;为了进一步压缩，对于weight的index，不再存储绝对位置的index，而是存储跟上一个有效weight的相对位置，这样index的字节数就可以被压缩了。&lt;/p&gt;&lt;p&gt;论文中，对于卷积层用 8bits 来保存这个相对位置的index，在全连接层中用 5bits 来保存；&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/1b0d33c6dc5fc2b65ef5d9685ac550f0.png" data-rawwidth="883" data-rawheight="158"&gt;&lt;p&gt;上图是以用3bits保存相对位置为例子，当相对位置超过8（3bits）的时候，需要在相对位置为8的地方填充一个0，防止溢出；&lt;/p&gt;&lt;h2&gt;Trained Quantization and Weight Sharing&lt;/h2&gt;&lt;p&gt;前面已经通过权值剪枝，去掉了一些不太重要的权值，大大压缩了网络；&lt;/p&gt;&lt;p&gt;为了更进一步压缩，作者又想到一个方法：&lt;i&gt;&lt;b&gt;权值本身的大小能不能压缩？&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;答案当然是可以的，具体怎么做请看下图：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/65cd0cab4d1db7f273c130d03d5b6f23.png" data-rawwidth="899" data-rawheight="371"&gt;假设有一个层，它有4个输入神经元，4个输出神经元，那么它的权值就是4*4的矩阵；&lt;/p&gt;&lt;p&gt;图中左上是weight矩阵，左下是gradient矩阵。&lt;/p&gt;&lt;p&gt;可以看到，图中作者把 &lt;b&gt;weight矩阵 &lt;/b&gt;聚类成了4个cluster（由4种颜色表示）。属于同一类的weight共享同一个权值大小（看中间的白色矩形部分，每种颜色权值对应一个cluster index）；&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;由于同一cluster的weight共享一个权值大小，所以我们只需要存储权值的index&lt;/p&gt;&lt;p&gt;例子中是4个cluster，所以原来每个weight需要32bits，现在只需要2bits，非常简单的压缩了16倍&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;而在 &lt;b&gt;权值更新&lt;/b&gt; 的时候，所有的gradients按照weight矩阵的颜色来分组，同一组的gradient做一个相加的操作，得到是sum乘上learning rate再减去共享的centroids，得到一个fine-tuned centroids，这个过程看上图，画的非常清晰了。&lt;/p&gt;&lt;blockquote&gt;实际中，对于AlexNet，卷积层quantization到8bits（256个共享权值），而全连接层quantization到5bits（32个共享权值），并且这样压缩之后的网络没有降低准确率&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;Weight Sharing&lt;/b&gt;&lt;/p&gt;&lt;p&gt;具体是怎么做的权值共享，或者说是用什么方法对权值聚类的呢？&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;其实就用了非常简单的 K-means，对每一层都做一个weight的聚类，属于同一个 cluster 的就共享同一个权值大小。&lt;/p&gt;&lt;p&gt;注意的一点：跨层的weight不进行共享权值；&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;Initialization of Shared Weights&lt;/b&gt;&lt;/p&gt;&lt;p&gt;做过 K-means 聚类的都知道，初始点的选择对于结果有着非常大的影响，在这里，初始点的选择同样会影响到网络的性能。&lt;/p&gt;&lt;p&gt;作者尝试了很多生产初始点的方法：&lt;b&gt;Forgy(random)&lt;/b&gt;, &lt;b&gt;density-based&lt;/b&gt;, and &lt;b&gt;linear initialization&lt;/b&gt;. &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/eb86a1c6b47de94d2219277a963e5ca5.png" data-rawwidth="772" data-rawheight="448"&gt;&lt;p&gt;画出了AlexNet中conv3层的权重分布，&lt;b&gt;横坐标&lt;/b&gt;是权值大小，&lt;b&gt;纵坐标&lt;/b&gt;表示分布，其中红色曲线表示PDF（概率密度分布），蓝色曲线表示CDF（概率密度函数），&lt;b&gt;圆圈&lt;/b&gt;表示的是centroids：黄色（Forgy）、蓝色（density-based）、红色（linear）。&lt;/p&gt;&lt;blockquote&gt;作者提到：大的权值往往比小的权值起到更重要的作用，不过，大的权值往往数量比较少；&lt;/blockquote&gt;&lt;p&gt;可以从图中看到，Forgy 和 density-based 方法产生的centroids很少落入到大权值的范围中，造成的结果就是忽略了大权值的作用；而Linear initialization产生的centroids非常平均，没有这个问题存在；&lt;/p&gt;&lt;p&gt;后续的实验结果也表明，Linear initialization 的效果最佳。&lt;/p&gt;&lt;h2&gt;Huffman Coding&lt;/h2&gt;&lt;p&gt;Huffman Coding 是一种非常常用的无损编码技术。它按照符号出现的概率来进行变长编码。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/ea3ad29af4833e4b0109266519c33f06.png" data-rawwidth="862" data-rawheight="263"&gt;&lt;p&gt;上图的权重以及权值索引分布来自于AlexNet的最后一个全连接层。由图可以看出，其分布是非均匀的、双峰形状，因此我们可以利用Huffman编码来对其进行处理，最终可以进一步使的网络的存储减少20%~30%。 &lt;/p&gt;&lt;h2&gt;Experiment Results&lt;/h2&gt;&lt;p&gt;简单贴一个最终得到的模型跟BVLC baseline 和其他基于alexnet的压缩网络的性能对比：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/6e4ed911976a9ffc8d7b23617f423117.png" data-rawwidth="826" data-rawheight="226"&gt;&lt;p&gt;作者还做了大量的对比试验，具体请看原paper. &lt;/p&gt;&lt;p&gt;正在读网络压缩的其他paper，后续会有更多论文笔记。&lt;/p&gt;&lt;blockquote&gt;有任何问题，欢迎在下方留言或者发我邮件，期待更多交流机会!&lt;/blockquote&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21574328&amp;pixel&amp;useReferer"/&gt;</description><author>仙道菜</author><pubDate>Tue, 12 Jul 2016 23:56:24 GMT</pubDate></item><item><title>【计算机视觉】《Learning Multi-Domain Convolutional Neural Networks for Visual Tracking》</title><link>https://zhuanlan.zhihu.com/p/21574185</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/3d87e0fe9a1dbb95c946dd6c165eb897_r.jpg"&gt;&lt;/p&gt;&lt;h2&gt;《Learning Multi-Domain Convolutional Neural Networks for Visual Tracking》&lt;/h2&gt;&lt;p&gt;继Object Classification、Detection 之后，Tracking 也"沦陷"啦，又一个被deep learning拿下的领域，这篇论文提出的 MDNet 在wuyi老师发布的OTB50和OTB100数据库上面取得了惊人的成绩。&lt;/p&gt;&lt;h2&gt;Abstract&lt;/h2&gt;&lt;p&gt;这篇论文的方法通过提取具有多domain，更generic的CNN特征进行single object的目标跟踪。&lt;/p&gt;&lt;p&gt;网络结构分为：&lt;b&gt;shared layers&lt;/b&gt; + multiple branches of &lt;b&gt;domain-specific layers&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Train阶段&lt;/b&gt;，利用一些video来共同训练一个shared layers，每个video单独训练一个domain-specific的layer&lt;/p&gt;&lt;p&gt;&lt;b&gt;Test阶段&lt;/b&gt;，利用训练好的shared layers + 新的二分类层，这个二分类层是online的更新。这个二分类层是用来判定一些candidate windows （随机的从上一帧target附近采样出来的） 是否为目标。&lt;/p&gt;&lt;p&gt;这个方法能够达到 state-of-art 的效果。我认为之所以能取得这么好的原因是因为：&lt;/p&gt;&lt;blockquote&gt;&lt;ul&gt;&lt;li&gt; Pretrained CNN 特征具有很好的区分特性（Naiyan Wang 在他的iccv15的 [论文](&lt;a href="http://arxiv.org/abs/1504.06055" class=""&gt;http://arxiv.org/abs/1504.06055&lt;/a&gt;) 中指出，tracking中最重要的就是特征的表达能力，好的特征甚至配一个很一般的分类器都能达到很好的效果）&lt;/li&gt;&lt;li&gt; Online fine-tuning 能够解决target的形变、遮挡、光照变化等等问题，可以使tracking更加robust&lt;/li&gt;&lt;li&gt; Hard minibatch Mining 的使用，使得网络的训练更加成功，特征更加优秀&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;h2&gt;Network Architecture &lt;/h2&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/90662fbc8815cced0ecba58b35e5ef81.png" data-rawwidth="989" data-rawheight="330"&gt;&lt;p&gt;上图是本文提出的网络结构，它包含：&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;Shared Layers&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Input: 107*107 RGB&lt;/li&gt;&lt;li&gt;3个卷积层：conv1、conv2、conv3&lt;/li&gt;&lt;li&gt;2个全连接层：fc4、fc5，输出是512，同时接ReLU和dropout层&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;Domain-speciic Layers&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt; K个全连接层：fc6-1 ~ fc6-K，输出是2，使用softmax cross-entropy loss，用来区分背景跟traget&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;了解AlexNet跟VGG16的读者一定会觉得这个网络是一个很小的网络，这其实是作者刻意为之的，他给出的解释是：&lt;/p&gt;&lt;blockquote&gt;&lt;ol&gt;&lt;li&gt;single object tracking 只需要区分两个类（background or target），这跟ImageNet 1000 类的任务比起来复杂度低很多，所以不需要太大的网络&lt;/li&gt;&lt;li&gt;随着网络越来越深，空间信息越来越稀释，这会使图像的目标定位不是那么精准。&lt;/li&gt;&lt;li&gt;因为tracking的target通常来说比较小，所以input size也需要小，自然的网络就不能太深&lt;/li&gt;&lt;li&gt;小网络显然在速度上会更快&lt;/li&gt;&lt;/ol&gt;&lt;/blockquote&gt;&lt;h2&gt;Learning Algorithm&lt;/h2&gt;&lt;p&gt;这个网络结构的目的是为了让shared layers学到不同domain共同的特性（光照变换、运动模糊、尺度变换等），而让不同分支的 domain-specific layers 学到该domain特有的一些特征。&lt;/p&gt;&lt;p&gt;要想达到这种训练目的，以往的minibatch SGD的训练方式肯定是不行了，作者介绍了他是如何训练网络的。&lt;/p&gt;&lt;blockquote&gt;SGD 的每一次迭代中只涉及一个特定domain。在第k次迭代中，mini-batch中只包含来自第（k mod K）个video的样本，K个domain-specific layers也只激活第（k mod K）个分支。&lt;/blockquote&gt;&lt;p&gt;通过这个学习过程，跟特定domain无关的信息就被学习到并保存在共享层中，这些信息是非常有用的泛化特征表示。&lt;/p&gt;&lt;h2&gt;Online Tracking using MDNet&lt;/h2&gt;&lt;p&gt;前面的train阶段大家应该已经明白流程了，那么test阶段是如何操作的呢？&lt;/p&gt;&lt;p&gt;其实很简单：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;保留前面的所有shared layers&lt;/li&gt;&lt;li&gt;移除domain-specific layers&lt;/li&gt;&lt;li&gt;取而代之的是一个新的初始化的domain-specific layer&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;当来了一个新的test序列，我们就online地对fc4~fc6做一个fine-tuning，具体做法如下&lt;/p&gt;&lt;p&gt;&lt;b&gt;Tracking Control and Network Update&lt;/b&gt;&lt;/p&gt;&lt;p&gt;tracking 中有两个方面的事情比较重要：鲁棒性 和 自适应性。为了同时兼顾这两方面，作者使用了两种策略：long-term 和 short-term 进行模型更新：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;long-term update&lt;/b&gt;: 定期地，使用较长时间内的正样本进行更新；&lt;/li&gt;&lt;li&gt;&lt;b&gt;short-term update&lt;/b&gt;: 当检测到可能有跟踪失败的情况时，使用短期内收集到的正样本进行更新。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;然而不管是哪种策略，都使用short-term内检测到的负样本，因为旧的负样本对于当前帧通常是多余的、不相关的。&lt;/p&gt;&lt;p&gt;为了确定每一帧中目标的状态，在上一帧目标的位置附近sample出 N个候选目标，用网络模型对这N个候选目标进行估计，取得分最高的为最佳目标。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Hard Minibatch Mining&lt;/b&gt;&lt;/p&gt;&lt;p&gt;个人认为这是训练时候非常好的一个tricky，可以很容易推广到其他方法中。&lt;/p&gt;&lt;p&gt;通常在一些 tracking-by-detection 的方法中，获取的negative样本都是冗余无效的，只有一小部分负样本对分类器的训练比较有效。一个非常有效的解决方法就是采用 hard negative mining 的方法，在training 和testing 的过程中，交替地去识别出hard negative.&lt;/p&gt;&lt;p&gt;作者把hard negative mining 的过程融入到minibatch 选择阶段。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;在训练阶段的每一次迭代中，一个mini-batch包含n个正样本和p个困难负样本。&lt;/p&gt;&lt;p&gt;如何选择困难负样本？用模型测试 M（M &amp;gt;&amp;gt; p）个负样本，sort之后取top p个最困难负样本。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;Bounding Box Regression&lt;/b&gt;&lt;/p&gt;&lt;p&gt;了解 object detection 的同学应该知道，resample 出来的bounding box 通常比较 “漂”，一般需要加一个 regression 来矫正位置，tracking 也需要如此。&lt;/p&gt;&lt;p&gt;给定第一帧的时候，会train一个简单的linear regression模型来预测target 的精准位置，在接下来的序列中，用这个模型来调整 (分数&amp;gt;0.5的) bounding box 的位置。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;regression **只在第一帧** 的时候进行训练（使用conv3特征），训练比较耗时；&lt;/p&gt;&lt;p&gt;regression 模型所采用的方法和参数都是跟 Ross 大神的 RCNN 中使用的是一样的；&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;Implementation Details&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Online tracking 的步骤如下图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/00552d6baebb012f3e05fd8bc841da23.png" data-rawwidth="560" data-rawheight="572"&gt;&lt;p&gt;&lt;b&gt;Target candidate generation&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Training data&lt;/b&gt;&lt;/p&gt;&lt;p&gt;为了offline 的 multi-domain 学习，作者在每一帧中采样了50个正样本，200个负样本，采样时根据IoU(&amp;gt;=0.7 或者 &amp;lt;=0.5 )来决定样本正负。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Network Training&lt;/b&gt;&lt;/p&gt;&lt;p&gt;因为有K个分支，所以需要100K次迭代来train模型。&lt;/p&gt;&lt;blockquote&gt;&lt;ul&gt;&lt;li&gt;卷积层的learning rate：0.0001；&lt;/li&gt;&lt;li&gt;全连接层的learning rate：0.001；&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;在测试阶段，第一帧训练的时候，迭代30次来训练fc层：&lt;/p&gt;&lt;blockquote&gt;&lt;ul&gt;&lt;li&gt;fc4-5的learning rate：0.0001&lt;/li&gt;&lt;li&gt;fc6的learning rate：0.001&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;为了online update 时候能够有快速的适应性&lt;/p&gt;&lt;blockquote&gt;作者每次使用10次迭代去更新网络，并且使用的learning rate 是第一帧训练的3倍；&lt;/blockquote&gt;&lt;p&gt;Hard Minibatch Mining 时候&lt;/p&gt;&lt;blockquote&gt;每个mini-batch包含32个正样本，96个负样本（从1024个负样本中选出）&lt;/blockquote&gt;&lt;h2&gt;Experiment&lt;/h2&gt;&lt;p&gt;实验结果是state-of-art，简单贴一下结果图吧：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/d3b22ae4fbfaef2efea7d3bafecd32e9.png" data-rawwidth="960" data-rawheight="311"&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21574185&amp;pixel&amp;useReferer"/&gt;</description><author>仙道菜</author><pubDate>Tue, 12 Jul 2016 23:38:38 GMT</pubDate></item><item><title>【深度学习分布式】Parameter Server 详解</title><link>https://zhuanlan.zhihu.com/p/21569493</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/f485b7a8a3116555784837adb0b0a0af_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://github.com/dmlc/mxnet" data-editable="true" data-title="MXNet"&gt;MXNet&lt;/a&gt; 是李沐和陈天奇等各路英雄豪杰打造的开源深度学习框架（最近不能更火了），其中最吸引我的是它的分布式训练的特性；而提供支持其分布式训练特性的正是当年李少帅和 Alex Smola 等人开发的 &lt;a href="http://parameterserver.org/" data-editable="true" data-title="parameter server"&gt;parameter server&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;本博客以“&lt;i&gt;Scaling Distributed Machine Learning with the Parameter Server&lt;/i&gt;” 为主，从易用性、通信高效性、可扩展性等角度介绍 parameter server .&lt;/p&gt;&lt;h2&gt;Background&lt;/h2&gt;&lt;p&gt;在机器学习和深度学习领域，分布式的优化已经成了一种先决条件。因为单机已经解决不了目前快速增长的数据和参数了。&lt;/p&gt;&lt;p&gt;现实中，训练数据的数量可能达到1TB到1PB之间，而训练过程中的参数可能会达到&lt;equation&gt;10^9&lt;/equation&gt;到&lt;equation&gt;10^{12}&lt;/equation&gt;。而往往这些模型的参数需要被所有的worker节点频繁的访问，这就会带来很多问题和挑战：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; 1. 访问这些巨量的参数，需要大量的网络带宽支持； 2. 很多机器学习算法都是连续型的，只有上一次迭代完成（各个worker都完成）之后，才能进行下一次迭代，这就导致了如果机器之间性能差距大（木桶理论），就会造成性能的极大损失； 3. 在分布式中，容错能力是非常重要的。很多情况下，算法都是部署到云环境中的（这种环境下，机器是不可靠的，并且job也是有可能被抢占的）；&lt;/p&gt;&lt;/blockquote&gt;&lt;h2&gt;Related Work&lt;/h2&gt;&lt;p&gt;对于机器学习分布式优化，有很多大公司在做了，包括：Amazon，Baidu，Facebook，Google，Microsoft 和 Yahoo。也有一些开源的项目，比如：YahooLDA 和 Petuum 和Graphlab。 总结一下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/65e8da480634d833a7e70c18b7a9ed0e.png" data-rawwidth="578" data-rawheight="286"&gt;&lt;p&gt;李少帅的这个ParameterServer &lt;b&gt;属于第三代&lt;/b&gt;的parameter server。&lt;/p&gt;&lt;p&gt;&lt;b&gt;第一代&lt;/b&gt; parameter server：缺少灵活性和性能 —— 仅使用memcached(key, value) 键值对存储作为同步机制。 *YahooLDA* 通过改进这个机制，增加了一个专门的服务器，提供用户能够自定义的更新操作(set, get, update)。&lt;/p&gt;&lt;p&gt;&lt;b&gt;第二代&lt;/b&gt; parameter server：用bounded delay模型来改进YahooLDA，但是却进一步限制了worker线程模型。&lt;/p&gt;&lt;p&gt;&lt;b&gt;第三代&lt;/b&gt; parameter server 能够解决这些局限性。&lt;/p&gt;&lt;blockquote&gt;首先来比较一下parameter server 跟通用的分布式系统之间的差别吧。&lt;/blockquote&gt;&lt;p&gt;通用的分布式系统通常都是：每次迭代都强制同步，通常在几十个节点上，它们的性能可以表现的很好，但是在大规模集群中，这样的每次迭代强制同步的机制会因为木桶效应变得很慢。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Mahout &lt;/b&gt;基于 &lt;b&gt;Hadoop&lt;/b&gt;，&lt;b&gt;MLI &lt;/b&gt;基于 &lt;b&gt;Spark&lt;/b&gt;，它们（Spark与MLI）采用的都是 &lt;b&gt;Iterative MapReduce &lt;/b&gt;的架构。它们能够保持迭代之间的状态，并且执行策略也更加优化了。但是，由于这两种方法都采用同步迭代的通信方式，使得它们很容易因为个别机器的低性能导致全局性能的降低。&lt;/p&gt;&lt;p&gt;直观感受一下，当其中一个节点运行时间过长会发生什么：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/d68b6f8e6d57a7bed9c8fd5e85cca56c.jpg" data-rawwidth="592" data-rawheight="448"&gt;&lt;p&gt;为了解决这个问题，&lt;b&gt;Graphlab &lt;/b&gt;采用图形抽象的方式进行异步调度通信。但是它缺少了以 MapReduce 为基础架构的弹性扩展性，并且它使用粗粒度的snapshots来进行恢复，这两点都会阻碍到可扩展性。parameter server 正是吸取Graphlab异步机制的优势，并且解决了其在可扩展性方面的劣势。&lt;/p&gt;&lt;p&gt;看看异步迭代是如何提高性能的：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/75dc0f59d2630e85a79090c1c7cd7dd5.jpg" data-rawwidth="592" data-rawheight="448"&gt;&lt;h2&gt;Parameter Server 优势&lt;/h2&gt;&lt;p&gt;说完了其他的分布式系统的缺点，该回到本博客的主题了(夸ps)，parameter server 有哪些features？&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;b&gt; 1. Efficient communication：&lt;/b&gt;&lt;/p&gt;&lt;p&gt; 由于是异步的通信，因此，不需要停下来等一些机器执行完一个iteration（除非有必要），这大大减少了延时。为机器学习任务做了一些优化(后续会细讲)，能够大大减少网络流量和开销；&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Flexible consistency models：&lt;/b&gt;&lt;/p&gt;&lt;p&gt; 宽松的一致性要求进一步减少了同步的成本和延时。parameter server 允许算法设计者根据自身的情况来做算法收敛速度和系统性能之间的trade-off。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. Elastic Scalability：&lt;/b&gt;&lt;/p&gt;&lt;p&gt; 使用了一个分布式hash表使得新的server节点可以随时动态的插入到集合中；因此，新增一个节点不需要重新运行系统。&lt;/p&gt;&lt;p&gt;&lt;b&gt; 4. Fault Tolerance and Durability：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们都知道，节点故障是不可避免的，特别是在大规模商用服务器集群中。从非灾难性机器故障中恢复，只需要1秒，而且不需要中断计算。Vector clocks 保证了经历故障之后还是能运行良好；&lt;/p&gt;&lt;p&gt;&lt;b&gt; 5. Ease of Use：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;全局共享的参数可以被表示&lt;/b&gt;成各种形式：vector，matrices 或者相应的sparse类型，这大大方便了机器学习算法的开发。并且提供的线性代数的数据类型都具有高性能的多线程库。&lt;/p&gt;&lt;/blockquote&gt;&lt;h2&gt;Parameter Server 系统架构&lt;/h2&gt;&lt;p&gt;在parameter server中，每个 server 实际上都只负责分到的&lt;b&gt;部分参数&lt;/b&gt;（servers共同维持一个全局的共享参数），而每个 work 也只分到&lt;b&gt;部分数据&lt;/b&gt;和处理任务；&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/2795f017061516b910b2128320dddc41.jpg" data-rawwidth="592" data-rawheight="448"&gt;&lt;p&gt;上图中，每个子节点都只维护自己分配到的参数（图中的黑色），自己部分更新之后，将计算结果（例如：梯度）传回到主节点，进行全局的更新（比如平均操作之类的），主节点再向子节点传送新的参数；&lt;/p&gt;&lt;p&gt;servers 与 workers 之间的通信如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/469fd809b8f83544bf18713eedb500cd.png" data-rawwidth="782" data-rawheight="217"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/bb7bf1c9f3075408328c11d88e9fe430.png" data-rawwidth="622" data-rawheight="333"&gt;&lt;p&gt;&lt;b&gt;server &lt;/b&gt;节点可以跟其他 server 节点通信，每个server负责自己分到的参数，server group 共同维持所有参数的更新。&lt;/p&gt;&lt;p&gt;&lt;b&gt;server manager&lt;/b&gt; node 负责维护一些元数据的一致性，比如各个节点的状态，参数的分配情况等；&lt;/p&gt;&lt;p&gt;&lt;b&gt;worker &lt;/b&gt;节点之间没有通信，只跟自己对应的server进行通信。每个worker group有一个&lt;b&gt;task scheduler&lt;/b&gt;，负责向worker分配任务，并且监控worker的运行情况。当有新的worker加入或者退出，task scheduler 负责重新分配任务。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(key, value)，Range Push and Pull&lt;/b&gt;&lt;/p&gt;&lt;p&gt;parameter server 中，参数都是可以被表示成(key, value)的集合，比如一个最小化损失函数的问题，key就是feature ID，而value就是它的权值。对于稀疏参数，不存在的key，就可以认为是0.&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/ae4b1a4a5ceca63d80be8e7eb06399b6.png" data-rawwidth="794" data-rawheight="117"&gt;&lt;p&gt;把参数表示成k-v， 形式更自然， 易于理，更易于编程解；&lt;/p&gt;&lt;p&gt;workers 跟 servers 之间通过 &lt;b&gt;push &lt;/b&gt;跟 &lt;b&gt;pull &lt;/b&gt;来通信。worker 通过 push 将计算好的梯度发送到server，然后通过 pull 从server更新参数。为了提高计算性能和带宽效率，parameter server 允许用户使用 &lt;b&gt;Range Push&lt;/b&gt; 跟 &lt;b&gt;Range Pull&lt;/b&gt; 操作；&lt;/p&gt;&lt;p&gt;假设 &lt;equation&gt;R&lt;/equation&gt; 是需要push或pull的 key 的range，那么可以进行如下操作：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;w.push(R, dest)

w.pull(R, dest)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;意思应该很明显吧，就是发送和接送特定Range中的w.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Asynchronous Tasks and Dependency&lt;/b&gt;&lt;/p&gt;&lt;p&gt;体会一下Asynchronous Task 跟 Synchronous Task 的区别：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/eacc59e293ab161267ead59a6245a4e7.png" data-rawwidth="817" data-rawheight="304"&gt;&lt;p&gt;如果 iter1 需要在 iter0 computation，push 跟 pull 都完成后才能开始，那么就是Synchronous，反之就是Asynchronous.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Asynchronous Task&lt;/b&gt;：能够提高系统的效率（因为节省了很多等待的过程），但是，它的缺点就是容易降低算法的收敛速率；&lt;/p&gt;&lt;p&gt;所以，系统性能跟算法收敛速率之间是存在一个trade-off的，你需要同时考虑：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;1. 算法对于参数非一致性的敏感度；&lt;/p&gt;&lt;p&gt;2. 训练数据特征之间的关联度；&lt;/p&gt;&lt;p&gt;3. 硬盘的存储容量；&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;考虑到用户使用的时候会有不同的情况，parameter server 为用户提供了多种任务依赖方式：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/1a9c319fc3b6762b5bcc7d635341a6b6.png" data-rawwidth="725" data-rawheight="216"&gt;&lt;p&gt;&lt;b&gt;Sequential&lt;/b&gt;： 这里其实是 synchronous task，任务之间是有顺序的，只有上一个任务完成，才能开始下一个任务；&lt;/p&gt;&lt;p&gt;&lt;b&gt;Eventual&lt;/b&gt;： 跟 sequential 相反，所有任务之间没有顺序，各自独立完成自己的任务， &lt;/p&gt;&lt;p&gt;&lt;b&gt;Bounded Delay&lt;/b&gt;：这是sequential 跟 eventual 之间的trade-off，可以设置一个 $\tau$ 作为最大的延时时间。也就是说，只有 $&amp;gt;\tau$ 之前的任务都被完成了，才能开始一个新的任务；极端的情况：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\tau = 0&lt;/equation&gt;，情况就是 Sequential；&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\tau = \infty&lt;/equation&gt;，情况就是 Eventual；&lt;/p&gt;&lt;p&gt;看一个bounded delay 的 &lt;b&gt;PGD &lt;/b&gt;(proximal gradient descent)算法的系统运行流程：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/9a702af4ec0990c0b9079795e053b662.jpg" data-rawwidth="580" data-rawheight="537"&gt;&lt;p&gt;&lt;b&gt;注意&lt;/b&gt;：&lt;equation&gt;\tau&lt;/equation&gt; 不是越大越好的，具体要取多大，得看具体情况，贴一张李少帅做的实验（Ad click prediction）：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/c0d69feb257e09033381e4ae70e095e9.png" data-rawwidth="622" data-rawheight="264"&gt;&lt;/p&gt;&lt;h2&gt;Implementation&lt;/h2&gt;&lt;p&gt;servers 使用 一致性hash 来存储参数k-v键值对。用链式复制方式来提高系统容错性。使用 range based communication 和 range based vector clocks 来进一步优化系统；&lt;/p&gt;&lt;p&gt;&lt;b&gt; Vector Clock&lt;/b&gt;&lt;/p&gt;&lt;p&gt;parameter server 使用 &lt;b&gt;vector clock&lt;/b&gt; 来记录每个节点中参数的时间戳，能够用来跟踪状态或避免数据的重复发送。但是，假设有n个节点，m个参数，那么vector clock的空间复杂度就是&lt;equation&gt;O(n*m)&lt;/equation&gt;. 显然，当有几千个节点和几十亿的参数时，对于内存和带宽来说都是不可实现的。&lt;/p&gt;&lt;p&gt;好在，parameter server 在push跟pull的时候，都是rang-based，这就带来了一个好处：这个range里面的参数共享的是同一个时间戳，这显然可以大大降低了空间复杂度。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/9de1e18117412db2018e998dc88f8a0b.png" data-rawwidth="467" data-rawheight="203"&gt;&lt;p&gt;每次从一个range里再提取一个range，最多会生成3个新的 vector clocks（一分为三） . 假设&lt;equation&gt;k&lt;/equation&gt;是算法中产生的所有的range，那么空间复杂度就变成了&lt;equation&gt;O(k*m)&lt;/equation&gt;，因为&lt;equation&gt;k&lt;/equation&gt;远小于参数个数，所以空间复杂度大大降低了；&lt;/p&gt;&lt;p&gt;&lt;b&gt;Messages&lt;/b&gt;&lt;/p&gt;&lt;p&gt;一条 message 包括：时间戳，len(range)对k-v.&lt;/p&gt;&lt;equation&gt;[vc(R), (k_1, v_1), . . . , (k_p, v_p)] k_j \in R \; \; and\;\; j \in \{1, . . . p\}&lt;/equation&gt;&lt;p&gt;这是parameter server 中最基本的通信格式，不仅仅是共享的参数才有，task 的message也是这样的格式，只要把这里的(key, value) 改成 (task ID, 参数/返回值)。&lt;/p&gt;&lt;p&gt;由于机器学习问题通常都需要很高的网络带宽，因此信息的压缩是必须的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;key的压缩&lt;/b&gt;：因为训练数据通常在分配之后都不会发生改变，因此worker没有必要每次都发送相同的key，只需要接收方在第一次接收的时候缓存起来就行了。第二次，worker不再需要同时发送key和value，只需要发送value 和 key list的hash就行。这样瞬间减少了一般的通信量。&lt;/p&gt;&lt;p&gt;&lt;b&gt;value的压缩&lt;/b&gt;： 假设参数时稀疏的，那么就会有大量的0存在。因此，为了进一步压缩，我们只需要发送非0值。parameter server使用 &lt;i&gt;&lt;b&gt;Snappy &lt;/b&gt;&lt;/i&gt;快速压缩库来压缩数据、高效去除0值。&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;key &lt;/b&gt;的压缩和 &lt;b&gt;value &lt;/b&gt;的压缩可以同时进行。&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;用户自定义过滤：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;对于机器学习优化问题比如梯度下降来说，并不是每次计算的梯度对于最终优化都是有价值的，用户可以通过自定义的规则过滤一些不必要的传送，再进一步压缩带宽cost：&lt;/p&gt;&lt;p&gt;&lt;b&gt; 1. 发送很小的梯度值是低效的：&lt;/b&gt;&lt;/p&gt;&lt;p&gt; 因此可以自定义设置，只在梯度值较大的时候发送；&lt;/p&gt;&lt;p&gt;&lt;b&gt; 2. 更新接近最优情况的值是低效的：&lt;/b&gt;&lt;/p&gt;&lt;p&gt; 因此，只在非最优的情况下发送，可通过KKT来判断；&lt;/p&gt;&lt;p&gt;&lt;b&gt;Replication and Consistency&lt;/b&gt;&lt;/p&gt;&lt;p&gt;parameter server 在数据一致性上，使用的是传统的&lt;b&gt;一致性哈希&lt;/b&gt;算法，参数key与server node id被插入到一个hash ring中。&lt;/p&gt;&lt;p&gt;具体的一致性hash算法不是本文的重点，这里不过多介绍了，不清楚的同学建议看看其他文献熟悉一下。&lt;/p&gt;&lt;p&gt;只要知道它的作用是在分布式系统中，动态增加和移除节点的同时还能保证系统存储与key分配的性能效率；&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/4da0c5d06e7a9a5594f21f678dd3aa86.png" data-rawwidth="528" data-rawheight="201"&gt;&lt;p&gt;从上图可以看出，每个节点都复制了它逆时钟方向的&lt;equation&gt;k&lt;/equation&gt;个节点中的key。图中，&lt;equation&gt;k=2&lt;/equation&gt;，&lt;equation&gt;S_1&lt;/equation&gt; 赋值了 &lt;equation&gt;S_2&lt;/equation&gt; 和 &lt;equation&gt;S_3&lt;/equation&gt; 内的key。&lt;/p&gt;&lt;p&gt;两种方式保证slave跟master之间的数据一致性：&lt;/p&gt;&lt;p&gt;1.默认的复制方式: &lt;b&gt;Chain replication&lt;/b&gt; (强一致性, 可靠)：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/c6b2275c09f9e332b0a3c6cf22cdc541.png" data-rawwidth="622" data-rawheight="86"&gt;&lt;p&gt;a.&lt;b&gt; 更新&lt;/b&gt;：只能发生在数据头节点,然后更新逐步后移，直到更新到达尾节点，并由尾节点向客户确认更新成功；&lt;/p&gt;&lt;p&gt;b. &lt;b&gt;查询&lt;/b&gt;：为保证强一致性，客户查询只能在尾节点进行；&lt;/p&gt;&lt;p&gt;2.Replication after &lt;b&gt;Aggregation&lt;/b&gt;：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/8c45406fe84fb11b7c494472644fd933.png" data-rawwidth="617" data-rawheight="153"&gt;&lt;p&gt;两个worker 节点分别向server传送x和y。server 首先通过一定方式（如：$f(x+y)$ ）进行aggregate，然后再进行复制操作；&lt;/p&gt;&lt;p&gt;当有&lt;equation&gt;n&lt;/equation&gt;个worker的时候，复制只需要&lt;equation&gt;k/n&lt;/equation&gt;的带宽。通常来说，&lt;equation&gt;k&lt;/equation&gt;（复制次数）是一个很小的常数，而&lt;equation&gt;n&lt;/equation&gt;的值大概是几百到几千；&lt;/p&gt;&lt;p&gt;&lt;b&gt;Server Management&lt;/b&gt;&lt;/p&gt;&lt;p&gt;要想实现系统的容错以及动态的扩展系统规模，必须要求系统能够支持动态添加和移除节点。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;b&gt;当有一个 server节点添加 进来的时候会发生什么呢？&lt;/b&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt; server manager 会对新的节点分配一些range 的key，这会造成其他server节点的key的变化；&lt;/li&gt;&lt;li&gt; 新节点会获取数据做为训练用，另外会复制k份到slave。&lt;/li&gt;&lt;li&gt; server manager 将节点的变化情况广播出去。接收方可能会移除不再属于自己的数据，并且将未完成的任务提交给新来的节点；&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;当有一个 worker节点（$W$）添加 进来的时候会发生什么呢？&lt;/b&gt;跟server差不多，相对更简单一些： &lt;/p&gt;&lt;ol&gt;&lt;li&gt;task scheduler 为$W$分配数据；&lt;/li&gt;&lt;li&gt; 这个 worker 节点通过网络或者文件系统得到分配到的训练数据。接着，$W$会从服务器pull参数；&lt;/li&gt;&lt;li&gt;task scheduler 会广播节点的变化情况，可能会使一些节点释放一部分训练数据；&lt;/li&gt;&lt;/ol&gt;&lt;/blockquote&gt;&lt;h2&gt;参考文献&lt;/h2&gt;&lt;p&gt;【1】Mu Li. &lt;i&gt;Scaling Distributed Machine Learning with the Parameter Server&lt;/i&gt;.&lt;/p&gt;&lt;p&gt;【2】CMU. &lt;a href="http://parameterserver.org/"&gt;http://parameterserver.org/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;【3】Joseph E.Gonzalez. &lt;i&gt;Emerging Systems For Large-scale  Machine Learning&lt;/i&gt;.&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21569493&amp;pixel&amp;useReferer"/&gt;</description><author>仙道菜</author><pubDate>Tue, 12 Jul 2016 16:20:00 GMT</pubDate></item><item><title>【神经网络】激活函数面面观</title><link>https://zhuanlan.zhihu.com/p/21568660</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/97cdded67daec6e769fe2a2873af7553_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;日常 coding 中，我们会很自然的使用一些激活函数，比如：sigmoid、ReLU等等。不过好像忘了问自己一(&lt;equation&gt;n&lt;/equation&gt;)件事：&lt;/p&gt;&lt;blockquote&gt;&lt;ul&gt;&lt;li&gt; 为什么需要激活函数？&lt;/li&gt;&lt;li&gt; 激活函数都有哪些？都长什么样？有哪些优缺点？&lt;/li&gt;&lt;li&gt; 怎么选用激活函数？&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;本文正是基于这些问题展开的，欢迎批评指正！&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/f0de8e94f1f285dc3b48f8e8eadaec73.png" data-rawwidth="957" data-rawheight="373"&gt;&lt;p&gt;(此图并没有什么卵用，纯属为了装x ...)&lt;/p&gt;&lt;h2&gt;Why use activation functions?&lt;/h2&gt;&lt;p&gt;&lt;b&gt;激活函数通常有如下一些性质：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;非线性&lt;/b&gt;： 当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。但是，如果激活函数是恒等激活函数的时候（即 &lt;equation&gt;f(x)=x&lt;/equation&gt;），就不满足这个性质了，而且如果 MLP 使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的。&lt;/li&gt;&lt;li&gt;&lt;b&gt;可微性&lt;/b&gt;： 当优化方法是基于梯度的时候，这个性质是必须的。&lt;/li&gt;&lt;li&gt;&lt;b&gt;单调性&lt;/b&gt;： 当激活函数是单调的时候，单层网络能够保证是凸函数。&lt;/li&gt;&lt;li&gt;&lt;equation&gt;f(x)\approx x&lt;/equation&gt;： 当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要很用心的去设置初始值。&lt;/li&gt;&lt;li&gt;&lt;b&gt;输出值的范围&lt;/b&gt;： 当激活函数输出值是 &lt;b&gt;有限 &lt;/b&gt;的时候，基于梯度的优化方法会更加 &lt;b&gt;稳定&lt;/b&gt;，因为特征的表示受有限权值的影响更显著；当激活函数的输出是 &lt;b&gt;无限&lt;/b&gt; 的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate.&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;这些性质，也正是我们使用激活函数的原因！&lt;/blockquote&gt;&lt;h2&gt;Activation Functions.&lt;/h2&gt;&lt;p&gt;&lt;b&gt;Sigmoid&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/8144d6530db568fa59140907b4377ca9.png" data-rawwidth="770" data-rawheight="308"&gt;&lt;p&gt;Sigmoid 是常用的非线性的激活函数，它的数学形式如下：&lt;equation&gt;f(x)=\frac{1}{1+e^{-x}}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;正如前一节提到的，它能够把输入的连续实值“压缩”到0和1之间。&lt;/p&gt;&lt;p&gt;特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1. &lt;/p&gt;&lt;p&gt;sigmoid 函数曾经被使用的很多，不过近年来，用它的人越来越少了。主要是因为它的一些 &lt;b&gt;缺点&lt;/b&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Sigmoids saturate and kill gradients. &lt;/b&gt;sigmoid 有一个非常致命的缺点，当输入非常大或者非常小的时候，这些神经元的梯度是接近于0的，从图中可以看出梯度的趋势。所以，你需要尤其注意参数的初始值来尽量避免saturation的情况。如果你的初始值很大的话，大部分神经元可能都会处在saturation的状态而把gradient kill掉，这会导致网络变的很难学习。&lt;/li&gt;&lt;li&gt;&lt;b&gt;Sigmoid 的 output 不是0均值.&lt;/b&gt; 这是不可取的，因为这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。产生的一个结果就是：如果数据进入神经元的时候是正的(e.g. x&amp;gt;0 elementwise in &lt;equation&gt;f=w^T x + b&lt;/equation&gt;)，那么 w 计算出的梯度也会始终都是正的。&lt;/li&gt;&lt;li&gt;当然了，如果你是按batch去训练，那么那个batch可能得到不同的信号，所以这个问题还是可以缓解一下的。因此，非0均值这个问题虽然会产生一些不好的影响，不过跟上面提到的 kill gradients 问题相比还是要好很多的。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;tanh&lt;/b&gt;&lt;/p&gt;&lt;p&gt;tanh 是上图中的右图，可以看出，tanh 跟sigmoid还是很像的，实际上，tanh 是sigmoid的变形：&lt;equation&gt;tanh(x)=2sigmoid(2x)-1&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;与 sigmoid 不同的是，tanh 是0均值的。因此，实际应用中，tanh 会比 sigmoid 更好（毕竟去粗取精了嘛）。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/908f2369860d4960adaa7643452139dd.png" data-rawwidth="775" data-rawheight="382"&gt;&lt;p&gt;&lt;b&gt;ReLU&lt;/b&gt;&lt;/p&gt;&lt;p&gt;近年来，ReLU 变的越来越受欢迎。它的数学表达式如下：&lt;/p&gt;&lt;equation&gt;f(x) = max(0, x)&lt;/equation&gt;&lt;p&gt;很显然，从图左可以看出，输入信号 &lt;b&gt;&amp;lt;0&lt;/b&gt; 时，输出都是0; &lt;b&gt;&amp;gt;0 &lt;/b&gt;的情况下，输出等于输入。w  是二维的情况下，使用ReLU之后的效果如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/15281a0bb762fa8ab2c21555b4036b66.png" data-rawwidth="716" data-rawheight="232"&gt;&lt;p&gt;&lt;b&gt;ReLU 的优点：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf" data-editable="true" data-title="Krizhevsky et al"&gt;Krizhevsky et al&lt;/a&gt;.发现使用 ReLU 得到的SGD的收敛速度会比 sigmoid/tanh 快很多(看右图)。有人说这是因为它是linear，而且 non-saturating &lt;/li&gt;&lt;li&gt; 相比于 sigmoid/tanh，ReLU 只需要一个阈值就可以得到激活值，而不用去算一大堆复杂的运算。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;ReLU 的缺点&lt;/b&gt;：当然 ReLU 也有缺点，就是训练的时候很"脆弱"，很容易就"die"了. 什么意思呢？&lt;/p&gt;&lt;blockquote&gt;举个例子：一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了。&lt;/blockquote&gt;&lt;p&gt;如果这个情况发生了，那么这个神经元的梯度就永远都会是0. &lt;/p&gt;&lt;p&gt;实际操作中，如果你的learning rate 很大，那么很有可能你网络中的40%的神经元都"dead"了。 &lt;/p&gt;&lt;p&gt;当然，如果你设置了一个合适的较小的learning rate，这个问题发生的情况其实也不会太频繁。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Leaky-ReLU、P-ReLU、R-ReLU&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Leaky ReLUs&lt;/b&gt;： 就是用来解决这个 &lt;i&gt;&lt;b&gt;"dying ReLU"&lt;/b&gt;&lt;/i&gt; 的问题的。与 ReLU 不同的是：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;f(x)=\alpha x   &lt;/equation&gt;   (x &amp;lt; 0)&lt;/p&gt;&lt;p&gt;&lt;equation&gt;f(x)=x&lt;/equation&gt;     (x &amp;gt;= 0)&lt;/p&gt;&lt;p&gt;这里的 &lt;equation&gt;\alpha &lt;/equation&gt; 是一个很小的常数。这样，即修正了数据分布，又保留了一些负轴的值，使得负轴信息不会全部丢失。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/d30610fa20a28268763fd6355a7e3426.png" data-rawwidth="409" data-rawheight="238"&gt;&lt;p&gt;关于Leaky ReLU 的效果，众说纷纭，没有清晰的定论。有些人做了实验发现 Leaky ReLU 表现的很好；有些实验则证明并不是这样。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/90b8c2e61f9bb5d13310050b81d2b0d0.png" data-rawwidth="668" data-rawheight="237"&gt;&lt;p&gt;&lt;b&gt;Parametric ReLU：&lt;/b&gt; 对于 Leaky ReLU 中的&lt;equation&gt;\alpha&lt;/equation&gt;，通常都是通过先验知识人工赋值的。&lt;/p&gt;&lt;p&gt;然而可以观察到，损失函数对 &lt;equation&gt;\alpha&lt;/equation&gt; 的导数我们是可以求得的，可不可以将它作为一个参数进行训练呢？&lt;/p&gt;&lt;p&gt;Kaiming He的论文《Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification》指出，不仅可以训练，而且效果更好。 &lt;/p&gt;&lt;p&gt;公式非常简单，反向传播至未激活前的神经元的公式就不写了，很容易就能得到。对&lt;equation&gt;\alpha&lt;/equation&gt;的导数如下： &lt;/p&gt;&lt;equation&gt;\frac{\delta y_i}{\delta \alpha}=0, if  y_{i}&amp;gt;0;else =y_{i} &lt;/equation&gt;&lt;p&gt;原文说使用了Parametric ReLU后，最终效果比不用提高了1.03%.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Randomized ReLU：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Randomized Leaky ReLU 是 leaky ReLU 的random 版本 （&lt;equation&gt;\alpha&lt;/equation&gt; 是random的）. &lt;/p&gt;&lt;p&gt;它首次试在 kaggle 的NDSB 比赛中被提出的。 &lt;/p&gt;&lt;blockquote&gt;核心思想就是，在训练过程中，&lt;equation&gt;\alpha&lt;/equation&gt; 是从一个高斯分布 &lt;equation&gt;U(l,u)&lt;/equation&gt; 中 随机出来的，然后再测试过程中进行修正（有点像dropout的用法）。&lt;/blockquote&gt;&lt;p&gt;数学表示如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/cabc07ec964d399a6839ad42c50c39c4.png" data-rawwidth="463" data-rawheight="126"&gt;&lt;p&gt;在测试阶段，把训练过程中所有的 &lt;equation&gt;\alpha_{ij}&lt;/equation&gt; 取个平均值。NDSB 冠军的 &lt;equation&gt;\alpha&lt;/equation&gt; 是从 &lt;equation&gt;U(3,8)&lt;/equation&gt; 中随机出来的。那么，在测试阶段，激活函数就是就是：&lt;equation&gt;y_{ij} = \frac{x_{ij}}{\frac{l+u}{2}}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;看看 cifar-100 中的实验结果：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/112e0a87e3abc3a8310ee54b47d3dabb.png" data-rawwidth="937" data-rawheight="236"&gt;&lt;p&gt;&lt;b&gt;Maxout&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/45c0e0e88dc23e63c0818ed9124cd23d.png" data-rawwidth="809" data-rawheight="307"&gt;&lt;p&gt;Maxout出现在ICML2013上，作者Goodfellow将maxout和dropout结合后，号称在MNIST, CIFAR-10, CIFAR-100, SVHN这4个数据上都取得了start-of-art的识别率。&lt;/p&gt;&lt;p&gt;Maxout 公式如下：&lt;/p&gt;&lt;equation&gt;f_i(x)=max_{j\in [1,k]}z_{ij}&lt;/equation&gt;&lt;p&gt;假设 &lt;equation&gt;w&lt;/equation&gt; 是2维，那么有：&lt;/p&gt;&lt;equation&gt;f(x)=max(w_1^Tx+b_1,w_2^Tx+b_2)&lt;/equation&gt;&lt;p&gt;可以注意到，ReLU 和 Leaky ReLU 都是它的一个变形（比如，&lt;equation&gt;w_1, b_1 = 0&lt;/equation&gt; 的时候，就是 ReLU）. &lt;/p&gt;&lt;p&gt;Maxout的拟合能力是非常强的，它可以拟合任意的的凸函数。作者从数学的角度上也证明了这个结论，即只需2个maxout节点就可以拟合任意的凸函数了（相减），前提是”隐隐含层”节点的个数可以任意多.&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/9e76c409db52a9c88bd4020db6e04faf.png" data-rawwidth="889" data-rawheight="217"&gt;&lt;p&gt;所以，Maxout 具有 ReLU 的&lt;b&gt;优点&lt;/b&gt;（如：计算简单，不会 saturation），同时又没有 ReLU 的一些缺点 （如：容易 go die）。不过呢，还是有一些&lt;b&gt;缺点&lt;/b&gt;的嘛：就是把参数double了。&lt;/p&gt;&lt;blockquote&gt;还有其他一些激活函数，请看下表：&lt;/blockquote&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/a7f872a80b7223ca5bb7690497ab2239.png" data-rawwidth="1005" data-rawheight="783"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/ff0b99ac6ca5e17c1ac58e07e761a856.png" data-rawwidth="1006" data-rawheight="383"&gt;&lt;h2&gt;How to choose a activation function?&lt;/h2&gt;&lt;blockquote&gt;怎么选择激活函数呢？&lt;/blockquote&gt;&lt;p&gt;我觉得这种问题不可能有定论的吧，只能说是个人建议。&lt;/p&gt;&lt;p&gt;如果你使用 ReLU，那么一定要小心设置 learning rate，而且要注意不要让你的网络出现很多 "dead" 神经元，如果这个问题不好解决，那么可以试试 Leaky ReLU、PReLU 或者 Maxout. &lt;/p&gt;&lt;p&gt;&lt;b&gt;友情提醒：&lt;/b&gt;最好不要用 sigmoid，你可以试试 tanh，不过可以预期它的效果会比不上 ReLU 和 Maxout. &lt;/p&gt;&lt;p&gt;还有，通常来说，很少会把各种激活函数串起来在一个网络中使用的。&lt;/p&gt;&lt;h2&gt;Reference&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;[1]. &lt;a href="http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-10.html" class=""&gt;http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-10.html&lt;/a&gt;&lt;/li&gt;&lt;li&gt;[2]. &lt;a href="http://papers.nips.cc/paper/874-how-to-choose-an-activation-function.pdf" class=""&gt;http://papers.nips.cc/paper/874-how-to-choose-an-activation-function.pdf&lt;/a&gt;&lt;/li&gt;&lt;li&gt;[3]. &lt;a href="https://en.wikipedia.org/wiki/Activation_function" class=""&gt;https://en.wikipedia.org/wiki/Activation_function&lt;/a&gt;&lt;/li&gt;&lt;li&gt;[4]. &lt;a href="http://cs231n.github.io/neural-networks-1/" class=""&gt;http://cs231n.github.io/neural-networks-1&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;Please feel free to contract me if you have any questions. &lt;/blockquote&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21568660&amp;pixel&amp;useReferer"/&gt;</description><author>仙道菜</author><pubDate>Tue, 12 Jul 2016 15:43:16 GMT</pubDate></item><item><title>【卷积神经网络-进化史】从LeNet到AlexNet</title><link>https://zhuanlan.zhihu.com/p/21562756</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/08f23ee99b6961f3325776d3b7c12e99_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;本系列博客是对刘昕博士的&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650324619&amp;amp;idx=1&amp;amp;sn=ca1aed9e42d8f020d0971e62148e13be&amp;amp;scene=1&amp;amp;srcid=0503De6zpYN01gagUvn0Ht8D#wechat_redirect" data-editable="true" data-title="《CNN的近期进展与实用技巧》" class=""&gt;《CNN的近期进展与实用技巧》&lt;/a&gt;的一个扩充性资料。&lt;/p&gt;&lt;p&gt;主要讨论CNN的发展，并且引用刘昕博士的思路，对CNN的发展作一个更加详细的介绍，将按下图的CNN发展史进行描述：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/e63b5f227f76850b02c241d75a3c50ba.png" data-rawwidth="944" data-rawheight="381"&gt;&lt;p&gt;上图所示是刘昕博士总结的CNN结构演化的历史，起点是神经认知机模型，此时已经出现了卷积结构，经典的LeNet诞生于1998年。然而之后CNN的锋芒开始被SVM等手工设计的特征盖过。随着ReLU和dropout的提出，以及GPU和大数据带来的历史机遇，CNN在2012年迎来了历史突破 -- &lt;b&gt;AlexNet&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;CNN的演化路径可以总结为以下几个方向：&lt;/p&gt;&lt;blockquote&gt;&lt;ul&gt;&lt;li&gt;从LeNet到AlexNet&lt;/li&gt;&lt;li&gt;进化之路一：网络结构加深&lt;/li&gt;&lt;li&gt;进化之路二：加强卷积功能&lt;/li&gt;&lt;li&gt;进化之路三：从分类到检测&lt;/li&gt;&lt;li&gt;进化之路四：新增功能模块&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;本系列博客将对CNN发展的四条路径中最具代表性的CNN模型结构进行讲解。&lt;h2&gt;一切的开始(&lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf"&gt;LeNet&lt;/a&gt;)&lt;/h2&gt;&lt;p&gt;下图是广为流传LeNet的网络结构，麻雀虽小，但五脏俱全，卷积层、pooling层、全连接层，这些都是现代CNN网络的基本组件。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/6f38ab4ba2122f6a901f5b87ee5878fc.jpg" data-rawwidth="1004" data-rawheight="281"&gt;&lt;ul&gt;&lt;li&gt;输入尺寸：32*32&lt;/li&gt;&lt;li&gt;卷积层：3个&lt;/li&gt;&lt;li&gt;降采样层：2个&lt;/li&gt;&lt;li&gt;全连接层：1个&lt;/li&gt;&lt;li&gt;输出：10个类别（数字0-9的概率）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;因为LeNet可以说是CNN的开端，所以这里简单介绍一下各个组件的用途与意义。&lt;/p&gt;&lt;p&gt;&lt;b&gt; Input (32*32)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;输入图像Size为32*32。这要比mnist数据库中最大的字母(28*28)还大。这样做的目的是希望潜在的明显特征，如笔画断续、角点能够出现在最高层特征监测子感受野的中心。&lt;/p&gt;&lt;p&gt;&lt;b&gt;C1, C3, C5 (卷积层)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;卷积核在二维平面上平移，并且卷积核的每个元素与被卷积图像对应位置相乘，再求和。通过卷积核的不断移动，我们就有了一个新的图像，这个图像完全由卷积核在各个位置时的乘积求和的结果组成。&lt;/p&gt;&lt;p&gt;二维卷积在图像中的效果就是：&lt;/p&gt;&lt;p&gt;对图像的每个像素的邻域（邻域大小就是核的大小）加权求和得到该像素点的输出值。具体做法如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7fce29335f9b43bce1b373daa40cccba.jpg" data-rawwidth="526" data-rawheight="384"&gt;&lt;p&gt;卷积运算一个重要的特点就是: 通过卷积运算，可以&lt;b&gt;使原信号特征增强，并且降低噪音&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;不同的卷积核能够提取到图像中的不同特征，这里有 &lt;a href="https://graphics.stanford.edu/courses/cs178/applets/convolution.html"&gt;在线demo&lt;/a&gt;，下面是不同卷积核得到的不同的feature map，&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/ef68508eb5301888248a1ebfddd22bc8.jpg" data-rawwidth="620" data-rawheight="733"&gt;&lt;p&gt;&lt;b&gt;以C1层进行说明&lt;/b&gt;：C1层是一个卷积层，有6个卷积核（提取6种局部特征），核大小为5*5，能够输出6个特征图Feature Map，大小为28*28。C1有156个可训练参数（每个滤波器5*5=25个unit参数和一个bias参数，一共6个滤波器，共(5*5+1)*6=156个参数），共156 * (28*28)=122,304个连接。&lt;/p&gt;&lt;p&gt;&lt;b&gt;S2, S4 (pooling层)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;S2, S4是下采样层，是为了降低网络训练参数及模型的过拟合程度。池化/采样的方式通常有以下两种：&lt;/p&gt;&lt;p&gt; 1. &lt;b&gt;Max-Pooling&lt;/b&gt;: 选择Pooling窗口中的最大值作为采样值；&lt;/p&gt;&lt;p&gt; 2. &lt;b&gt;Mean-Pooling&lt;/b&gt;: 将Pooling窗口中的所有值相加取平均，以平均值作为采样值；&lt;/p&gt;&lt;p&gt;S2层是6个14*14的feature map，map中的每一个单元于上一层的 2*2 领域相连接，所以，S2层是C1层的1/4。&lt;/p&gt;&lt;p&gt;&lt;b&gt;F6 (全连接层)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;F6是全连接层，类似MLP中的一个layer，共有84个神经元（为什么选这个数字？跟输出层有关），这84个神经元与C5层进行全连接，所以需要训练的参数是：(120+1)*84=10164.&lt;/p&gt;&lt;p&gt;如同经典神经网络，F6层计算输入向量和权重向量之间的点积，再加上一个偏置。然后将其传递给sigmoid函数产生单元i的一个状态。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Output (输出层)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;输出层由欧式径向基函数（Euclidean Radial Basis Function）单元组成，每类一个单元，每个有84个输入。&lt;/p&gt;&lt;p&gt;换句话说，每个输出RBF单元计算输入向量和参数向量之间的欧式距离。输入离参数向量越远，RBF输出的越大。用概率术语来说，RBF输出可以被理解为F6层配置空间的高斯分布的负log-likelihood。给定一个输式，损失函数应能使得F6的配置与RBF参数向量（即模式的期望分类）足够接近。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/8bf713c534382230180194ae1248ba21.png" data-rawwidth="657" data-rawheight="515"&gt;&lt;h2&gt;王者回归(&lt;a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" data-title="AlexNet" class=""&gt;AlexNet&lt;/a&gt;)&lt;/h2&gt;&lt;p&gt;AlexNet 可以说是具有历史意义的一个网络结构，可以说在AlexNet之前，深度学习已经沉寂了很久。历史的转折在2012年到来，AlexNet 在当年的ImageNet图像分类竞赛中，top-5错误率比上一年的冠军下降了十个百分点，而且远远超过当年的第二名。&lt;/p&gt;&lt;p&gt;AlexNet 之所以能够成功，深度学习之所以能够重回历史舞台，原因在于：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;1. 非线性激活函数：ReLU&lt;/p&gt;&lt;p&gt;2. 防止过拟合的方法：Dropout，Data augmentation&lt;/p&gt;&lt;p&gt;3. 大数据训练：百万级ImageNet图像数据&lt;/p&gt;&lt;p&gt;4. 其他：GPU实现，LRN归一化层的使用&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;下面简单介绍一下AlexNet的一些细节：&lt;/p&gt;&lt;p&gt;&lt;b&gt;Data augmentation&lt;/b&gt;&lt;/p&gt;&lt;p&gt;有一种观点认为神经网络是靠数据喂出来的，若增加训练数据，则能够提升算法的准确率，因为这样可以避免过拟合，而避免了过拟合你就可以增大你的网络结构了。当训练数据有限的时候，可以通过一些变换来从已有的训练数据集中生成一些新的数据，来扩大训练数据的size。&lt;/p&gt;&lt;p&gt;其中，最简单、通用的图像数据变形的方式:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;1. 从原始图像（256,256）中，随机的crop出一些图像（224,224）。【平移变换，crop】&lt;/p&gt;&lt;p&gt;2. 水平翻转图像。【反射变换，flip】&lt;/p&gt;&lt;p&gt;3. 给图像增加一些随机的光照。【光照、彩色变换，color jittering】&lt;/p&gt;&lt;/blockquote&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/c46c622ab4f53c6dcee6ad4ea95caa32.png" data-rawwidth="557" data-rawheight="418"&gt;&lt;p&gt;AlexNet 训练的时候，在data augmentation上处理的很好：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;随机crop。训练时候，对于256＊256的图片进行随机crop到224＊224，然后允许水平翻转，那么相当与将样本倍增到((256-224)^2)*2=2048。&lt;/li&gt;&lt;li&gt;测试时候，对左上、右上、左下、右下、中间做了5次crop，然后翻转，共10个crop，之后对结果求平均。作者说，不做随机crop，大网络基本都过拟合(under substantial overfitting)。&lt;/li&gt;&lt;li&gt;对RGB空间做PCA，然后对主成分做一个(0, 0.1)的高斯扰动。结果让错误率又下降了1%。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;ReLU 激活函数&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Sigmoid 是常用的非线性的激活函数，它能够把输入的连续实值“压缩”到0和1之间。特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1. &lt;/p&gt;&lt;p&gt;但是它有一些致命的 &lt;b&gt;缺点&lt;/b&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Sigmoids saturate and kill gradients&lt;/b&gt;. sigmoid 有一个非常致命的缺点，当输入非常大或者非常小的时候，会有饱和现象，这些神经元的梯度是接近于0的。如果你的初始值很大的话，梯度在反向传播的时候因为需要乘上一个sigmoid 的导数，所以会使得梯度越来越小，这会导致网络变的很难学习。&lt;/li&gt;&lt;li&gt;&lt;b&gt;Sigmoid 的 output 不是0均值&lt;/b&gt;. 这是不可取的，因为这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。产生的一个结果就是：如果数据进入神经元的时候是正的(e.g. &lt;equation&gt;x&amp;gt;0&lt;/equation&gt; elementwise in &lt;equation&gt;f=w^{T} x+b&lt;/equation&gt;)，那么 &lt;equation&gt;w&lt;/equation&gt; 计算出的梯度也会始终都是正的。当然了，如果你是按batch去训练，那么那个batch可能得到不同的信号，所以这个问题还是可以缓解一下的。因此，非0均值这个问题虽然会产生一些不好的影响，不过跟上面提到的 kill gradients 问题相比还是要好很多的。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;ReLU 的数学表达式如下：&lt;equation&gt;f(x) = max(0, x)&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;很显然，从图左可以看出，输入信号&lt;equation&gt;&amp;lt;0&lt;/equation&gt;时，输出都是0，&lt;equation&gt;&amp;gt;0&lt;/equation&gt;的情况下，输出等于输入。&lt;equation&gt;w&lt;/equation&gt;是二维的情况下，使用ReLU之后的效果如下：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/15281a0bb762fa8ab2c21555b4036b66.png" data-rawwidth="716" data-rawheight="232"&gt;Alex用ReLU代替了Sigmoid，发现使用 ReLU 得到的SGD的收敛速度会比 sigmoid/tanh 快很多。&lt;/p&gt;&lt;blockquote&gt;主要是因为它是linear，而且 non-saturating（因为ReLU的导数始终是1），相比于 sigmoid/tanh，ReLU 只需要一个阈值就可以得到激活值，而不用去算一大堆复杂的运算。&lt;/blockquote&gt;&lt;p&gt;关于激活函数更多内容，请移步我的另一篇文章：&lt;a href="http://7pn4yt.com1.z0.glb.clouddn.com/blog-relu-perf.png"&gt;激活函数-面面观&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Dropout&lt;/b&gt;&lt;/p&gt;&lt;p&gt;结合预先训练好的许多不同模型，来进行预测是一种非常成功的减少测试误差的方式（Ensemble）。但因为每个模型的训练都需要花了好几天时间，因此这种做法对于大型神经网络来说太过昂贵。&lt;/p&gt;&lt;p&gt;然而，AlexNet 提出了一个非常有效的模型组合版本，它在训练中只需要花费**两倍**于单模型的时间。这种技术叫做 &lt;b&gt;Dropout&lt;/b&gt;，它做的就是以0.5的概率，将每个隐层神经元的输出设置为零。以这种方式“dropped out”的神经元既不参与前向传播，也不参与反向传播。&lt;/p&gt;&lt;p&gt;所以每次输入一个样本，就相当于该神经网络就尝试了一个新的结构，但是所有这些结构之间共享权重。因为神经元不能依赖于其他特定神经元而存在，所以这种技术降低了神经元复杂的互适应关系。&lt;/p&gt;&lt;p&gt;正因如此，网络需要被迫学习更为鲁棒的特征，这些特征在结合其他神经元的一些不同随机子集时有用。在测试时，我们将所有神经元的输出都仅仅只乘以0.5，对于获取指数级dropout网络产生的预测分布的几何平均值，这是一个合理的近似方法。 &lt;/p&gt;&lt;p&gt;&lt;b&gt;多GPU训练&lt;/b&gt;&lt;/p&gt;&lt;p&gt;单个GTX 580 GPU只有3GB内存，这限制了在其上训练的网络的最大规模。因此他们将网络分布在两个GPU上。&lt;/p&gt;&lt;p&gt;目前的GPU特别适合跨GPU并行化，因为它们能够直接从另一个GPU的内存中读出和写入，不需要通过主机内存。&lt;/p&gt;&lt;blockquote&gt;他们采用的并行方案是：在每个GPU中放置一半核（或神经元），还有一个额外的技巧：GPU间的通讯只在某些层进行。&lt;/blockquote&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/91cd64574d63dabf803021e77a8b72fa.jpg" data-rawwidth="821" data-rawheight="254"&gt;&lt;p&gt;例如，第3层的核需要从第2层中所有核映射输入。然而，第4层的核只需要从第3层中位于同一GPU的那些核映射输入。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Local Responce Normalization&lt;/b&gt;&lt;/p&gt;&lt;blockquote&gt;一句话概括：本质上，这个层也是为了防止激活函数的饱和的。&lt;/blockquote&gt;&lt;p&gt;个人理解原理是通过正则化让激活函数的输入靠近“碗”的中间(避免饱和)，从而获得比较大的导数值。&lt;/p&gt;&lt;p&gt;所以从功能上说，跟ReLU是重复的。&lt;/p&gt;&lt;p&gt;不过作者说，从试验结果看，LRN操作可以提高网络的泛化能力，将错误率降低了大约1个百分点。&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;AlexNet 优势在于：网络增大（5个卷积层+3个全连接层+1个softmax层），同时解决过拟合（dropout，data augmentation，LRN），并且利用多GPU加速计算.&lt;/b&gt;&lt;/blockquote&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21562756&amp;pixel&amp;useReferer"/&gt;</description><author>仙道菜</author><pubDate>Tue, 12 Jul 2016 01:33:53 GMT</pubDate></item></channel></rss>