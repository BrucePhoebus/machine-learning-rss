<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>深度学习大讲堂 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/dlclass</link><description>推送深度学习的最新消息，包括最新技术进展，使用以及活动</description><lastBuildDate>Fri, 30 Dec 2016 16:15:28 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>深度学习框架Caffe源码解析</title><link>https://zhuanlan.zhihu.com/p/24343706</link><description>深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;相信社区中很多小伙伴和我一样使用了很长时间的Caffe深度学习框架，也非常希望从代码层次理解Caffe的实现从而实现新功能的定制。本文将从整体架构和底层实现的视角，对Caffe源码进行解析。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.Caffe总体架构&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Caffe框架主要有五个组件，Blob，Solver，Net，Layer，Proto，其结构图如下图1所示。Solver负责深度网络的训练，每个Solver中包含一个训练网络对象和一个测试网络对象。每个网络则由若干个Layer构成。每个Layer的输入和输出Feature map表示为Input Blob和Output Blob。Blob是Caffe实际存储数据的结构，是一个不定维的矩阵，在Caffe中一般用来表示一个拉直的四维矩阵，四个维度分别对应Batch Size（N），Feature Map的通道数（C）,Feature Map高度(H)和宽度(W)。Proto则基于Google的Protobuf开源项目，是一种类似XML的数据交换格式，用户只需要按格式定义对象的数据成员，可以在多种语言中实现对象的序列化与反序列化，在Caffe中用于网络模型的结构定义、存储和读取。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-e3f56d8fd8f00feae16885b44def948a.jpg" data-rawwidth="667" data-rawheight="354"&gt;&lt;p&gt;&lt;b&gt;2.Blob解析&lt;/b&gt;&lt;/p&gt;&lt;p&gt;下面介绍Caffe中的基本数据存储类Blob。Blob使用SyncedMemory类进行数据存储，数据成员 data_指向实际存储数据的内存或显存块，shape_存储了当前blob的维度信息，diff_这个保存了反向传递时候的梯度信息。在Blob中其实不是只有num，channel，height，width这种四维形式，它是一个不定维度的数据结构，将数据展开存储，而维度单独存在一个vector&amp;lt;int&amp;gt; 类型的shape_变量中，这样每个维度都可以任意变化。&lt;/p&gt;&lt;p&gt;来一起看看Blob的关键函数，data_at这个函数可以读取的存储在此类中的数据，diff_at可以用来读取反向传回来的误差。顺便给个提示，尽量使用data_at(const vector&amp;lt;int&amp;gt;&amp;amp; index)来查找数据。Reshape函数可以修改blob的存储大小，count用来返回存储数据的数量。BlobProto类负责了将Blob数据进行打包序列化到Caffe的模型中。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.工厂模式说明&lt;/b&gt;&lt;/p&gt;&lt;p&gt;接下来介绍一种设计模式Factory Pattern，Caffe 中Solver和Layer对象的创建均使用了此模式，首先看工厂模式的UML的类图：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-0fffc1bbdc6f5dc4368cd3faece45d91.jpg" data-rawwidth="539" data-rawheight="320"&gt;如同Factory生成同一功能但是不同型号产品一样，这些产品实现了同样Operation，很多人看了工厂模式的代码，会产生这样的疑问为何不new一个出来呢，这样new一个出来似乎也没什么问题吧。试想如下情况，由于代码重构类的名称改了，或者构造函数参数变化(增加或减少参数)。而你代码中又有N处new了这个类。如果你又没用工厂，就只能一个一个找来改。工厂模式的作用就是让使用者减少对产品本身的了解，降低使用难度。如果用工厂，只需要修改工厂类的创建具体对象方法的实现，而其他代码不会受到影响。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-6d5bfb3c4ea555cdda9b0013c49124b8.jpg" data-rawwidth="132" data-rawheight="183"&gt;&lt;p&gt;举个例子，写代码少不得饿了要加班去吃夜宵，麦当劳的鸡翅和肯德基的鸡翅都是MM爱吃的东西，虽然口味有所不同，但不管你带MM去麦当劳或肯德基，只管向服务员说“来四个鸡翅”就行了。麦当劳和肯德基就是生产鸡翅的Factory。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.Solver解析&lt;/b&gt;&lt;/p&gt;&lt;p&gt;接下来切回正题，我们看看Solver这个优化对象在Caffe中是如何实现的。SolverRegistry这个类就是我们看到的上面的factory类，负责给我们一个优化算法的产品，外部只需要把数据和网络结构定义好，它就可以自己优化了。&lt;/p&gt;&lt;p&gt;Solver&amp;lt;Dtype&amp;gt;* CreateSolver(const SolverParameter&amp;amp; param)这个函数就是工厂模式下的CreateProduct的操作， Caffe中这个SolverRegistry工厂类可以提供给我们6种产品（优化算法）：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-513246b251efeadae7270acb95c7c8bc.jpg" data-rawwidth="632" data-rawheight="146"&gt;&lt;p&gt;这六种产品的功能都是实现网络的参数更新，只是实现方式不一样。那我们来看看他们的使用流程吧。当然这些产品类似上面Product类中的Operation，每一个Solver都会继承Solve和Step函数，而每个Solver中独有的仅仅是ApplyUpdate这个函数里面执行的内容不一样，接口是一致的，这也和我们之前说的工厂生产出来的产品一样功能一样，细节上有差异，比如大多数电饭煲都有煮饭的功能，但是每一种电饭煲煮饭的加热方式可能不同，有底盘加热的还有立体加热的等。接下里我们看看Solver中的关键函数。&lt;/p&gt;&lt;p&gt;Solver中Solve函数的流程图如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-94271e4cd51bf7365ca39fe9ff95d71f.jpg" data-rawwidth="324" data-rawheight="736"&gt;&lt;p&gt;Solver类中Step函数流程图：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-a6e0939b512b981a60e107906b68fe6c.jpg" data-rawwidth="455" data-rawheight="832"&gt;&lt;/p&gt;&lt;p&gt;Solver中关键的就是调用Sovle函数和Step函数的流程，你只需要对照Solver类中两个函数的具体实现，看懂上面两个流程图就可以理解Caffe训练执行的过程了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;5.Net类解析&lt;/b&gt;&lt;/p&gt;&lt;p&gt;分析过Solver之后我们来分析下Net类的一些关键操作。这个是我们使用Proto创建出来的深度网络对象，这个类负责了深度网络的前向和反向传递。以下是Net类的初始化方法NetInit函数调用流程：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-7374ab58320a291a82e023d25142a47c.jpg" data-rawwidth="511" data-rawheight="563"&gt;&lt;p&gt;Net的类中的关键函数简单剖析&lt;/p&gt;&lt;p&gt;&lt;b&gt;1).&lt;/b&gt;ForwardBackward：按顺序调用了Forward和Backward。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2).&lt;/b&gt;ForwardFromTo(int start, int end)：执行从start层到end层的前向传递，采用简单的for循环调用。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3).&lt;/b&gt;BackwardFromTo(int start, int end)：和前面的ForwardFromTo函数类似，调用从start层到end层的反向传递。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4).&lt;/b&gt;ToProto函数完成网络的序列化到文件，循环调用了每个层的ToProto函数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;6.Layer解析&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Layer是Net的基本组成单元，例如一个卷积层或一个Pooling层。本小节将介绍Layer类的实现。&lt;/p&gt;&lt;p&gt;&lt;b&gt;6.1Layer的继承结构&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-fa0709dc53ffb274bd145c54168cb840.jpg" data-rawwidth="353" data-rawheight="395"&gt;&lt;p&gt;&lt;b&gt;6.2 Layer的创建&lt;/b&gt;&lt;/p&gt;&lt;p&gt;与Solver的创建方式很像，Layer的创建使用的也是工厂模式，这里简单说明下几个宏函数：&lt;/p&gt;&lt;p&gt;REGISTER_LAYER_CREATOR负责将创建层的函数放入LayerRegistry。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-048310b542b15ba9e46ea6b2b5ae5372.jpg" data-rawwidth="618" data-rawheight="204"&gt;我们来看看大多数层创建的函数的生成宏REGISTER_LAYER_CLASS，可以看到宏函数比较简单的，将类型作为函数名称的一部分，这样就可以产生出一个创建函数，并将创建函数放入LayerRegistry。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-41705a9e84a1d1feb89994fb0b2b11f5.jpg" data-rawwidth="670" data-rawheight="127"&gt;&lt;p&gt;REGISTER_LAYER_CREATOR(type, Creator_##type##Layer)&lt;/p&gt;&lt;p&gt;这段代码在split_layer.cpp文件中&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-4769a3aa3adbd768c8f8caeac8026143.jpg" data-rawwidth="443" data-rawheight="67"&gt;&lt;p&gt;REGISTER_LAYER_CLASS(Split)。&lt;/p&gt;&lt;p&gt;这样我们将type替换过以后给大家做个范例，参考下面的代码。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-e9a89ab94d52ed2946dd75db49d69957.jpg" data-rawwidth="668" data-rawheight="88"&gt;当然这里的创建函数好像是直接调用，没有涉及到我们之前工厂模式的一些问题。所有的层的类都是这样吗？当然不是，我们仔细观察卷积类。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-f8e7ab462f0f00bb18713ccb39837255.jpg" data-rawwidth="439" data-rawheight="53"&gt;卷积层怎么没有创建函数呢，当然不是，卷积的层的创建函数在LayerFactory.cpp中，截图给大家看下，具体代码如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-745b984d8a6690f170c9d4c11ccd27fb.jpg" data-rawwidth="602" data-rawheight="116"&gt;&lt;p&gt;这样两种类型的Layer的创建函数都有了对应的声明。这里直接说明除了有cudnn实现的层，其他层都是采用第一种方式实现的创建函数，而带有cudnn实现的层都采用的第二种方式实现的创建函数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;6.3 Layer的初始化&lt;/b&gt;&lt;/p&gt;&lt;p&gt;介绍完创建我们看看层里面的几个函数都是什么时候被调用的。&lt;/p&gt;&lt;p&gt;关键函数Setup此函数在之前的流程图中的NetInit时候被调用，代码如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-7e66524e0594ffb366ecbbf7ab3d22de.jpg" data-rawwidth="449" data-rawheight="172"&gt;&lt;p&gt;这样整个Layer初始化的过程中，CheckBlobCounts被最先调用，然后接下来是LayerSetUp，后面才是Reshape，最后才是SetLossWeights。这样Layer初始化的生命周期大家就有了了解。&lt;/p&gt;&lt;p&gt;&lt;b&gt;6.4 Layer的其他函数的介绍&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Layer的Forward函数和Backward函数完成了网络的前向和反向传递，这两个函数在自己实现新的层必须要实现。其中Backward会修改bottom中blob的diff_，这样就完成了误差的方向传导。&lt;/p&gt;&lt;p&gt;&lt;b&gt;7.Protobuf介绍&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Caffe中的Caffe.proto文件负责了整个Caffe网络的构建，又负责了Caffemodel的存储和读取。下面用一个例子介绍Protobuf的工作方式：&lt;/p&gt;&lt;p&gt;利用protobuffer工具存储512维度图像特征：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1).&lt;/b&gt;message 编写：新建txt文件后缀名改为proto,编写自己的message如下，并放入解压的protobuff的文件夹里；&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-332e462127d845ec83698901f6448280.jpg" data-rawwidth="422" data-rawheight="153"&gt;&lt;p&gt;其中，dwFaceFeatSize表示特征点数量；pfFaceFeat表示人脸特征。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2).&lt;/b&gt;打开windows命令窗口(cmd.exe)----&amp;gt;cd空格，把protobuff的文件路径复制粘贴进去------&amp;gt;enter；&lt;/p&gt;&lt;p&gt;&lt;b&gt;3).&lt;/b&gt;输入指令protoc *.proto --cpp_out=.    ---------&amp;gt;enter&lt;/p&gt;&lt;p&gt;&lt;b&gt;4).&lt;/b&gt;可以看到文件夹里面生成“ *.pb.h”和“*.pb.cpp”两个文件，说明成功了&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-c5eb0ba309ceb4dca3cb5a3cea05f2a8.jpg" data-rawwidth="653" data-rawheight="71"&gt;&lt;p&gt;&lt;b&gt;5).&lt;/b&gt;下面可以和自己的代码整合了：&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1) &lt;/b&gt;新建你自己的工程，把“ *.pb.h”和“*.pb.cpp”两个文件添加到自己的工程里，并写上#include" *.pb.h"&lt;/p&gt;&lt;p&gt;&lt;b&gt;(2) &lt;/b&gt;按照配库的教程把库配置下就可以了&lt;/p&gt;&lt;p&gt;VS下Protobuf的配库方法：&lt;/p&gt;&lt;p&gt;解决方案----&amp;gt;右击工程名----&amp;gt;属性&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1)c/c++---&amp;gt;常规---&amp;gt;附加包含目录---&amp;gt; &lt;/b&gt;&lt;/p&gt;&lt;p&gt;($your protobuffer include path)\protobuffer &lt;/p&gt;&lt;p&gt;&lt;b&gt;(2)c/c++---&amp;gt;链接器--&amp;gt;常规---&amp;gt;附加库目录--&amp;gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;($your protobuffer lib path)\protobuffer&lt;/p&gt;&lt;p&gt;&lt;b&gt;(3) c/c++---&amp;gt;链接器--&amp;gt;输入---&amp;gt;附加依赖项--&amp;gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;libprotobufd.lib;(带d的为debug模式)&lt;/p&gt;&lt;p&gt;或libprotobuf.lib;（不带d,为release模式）&lt;/p&gt;&lt;p&gt;使用protobuf进行打包的方法如下代码：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-35b1da04956260b7d4b522fc2f35bdf3.jpg" data-rawwidth="636" data-rawheight="430"&gt;&lt;p&gt;&lt;b&gt;7.1  Caffe的模型序列化&lt;/b&gt;&lt;/p&gt;&lt;p&gt;BlobProto其实就是Blob序列化成Proto的类，Caffe模型文件使用了该类。Net调用每个层的Toproto方法，每个层的Toproto方法调用了Blob类的ToProto方法，这样完整的模型就被都序列化到proto里面了。最后只要将这个proto继承于message类的对象序列化到文件就完成了模型写入文件。Caffe打包模型的时候就只是简单调用了WriteProtoToBinaryFile这个函数，而这个函数里面的内容如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-c82d228cacbdaf06231c6835fca9b692.jpg" data-rawwidth="629" data-rawheight="70"&gt;&lt;p&gt;至此Caffe的序列化模型的方式就完成了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;7.2 Proto.txt的简单说明&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Caffe网络的构建和Solver的参数定义均由此类型文件完成。Net构建过程中调用ReadProtoFromTextFile将所有的网络参数读入。然后调用上面的流程进行整个caffe网络的构建。这个文件决定了怎样使用存在caffe model中的每个blob是用来做什么的，如果没有了这个文件caffe的模型文件将无法使用，因为模型中只存储了各种各样的blob数据，里面只有float值，而怎样切分这些数据是由prototxt文件决定的。&lt;/p&gt;&lt;p&gt;Caffe的架构在框架上采用了反射机制去动态创建层来构建Net，Protobuf本质上定义了graph，反射机制是由宏配合map结构形成的，然后使用工厂模式去实现各种各样层的创建，当然区别于一般定义配置采用xml或者json，该项目的写法采用了proto文件对组件进行组装。&lt;/p&gt;&lt;p&gt;&lt;b&gt;总结&lt;/b&gt;&lt;/p&gt;&lt;p&gt;以上为Caffe代码架构的一个总体介绍，希望能借此帮助社区的小伙伴找到打开定制化Caffe大门的钥匙。本文作者希望借此抛砖引玉，与更多期望了解Caffe和深度学习框架底层实现的同行交流。&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing/answers" data-editable="true" data-title="@果果是枚开心果."&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-19e5440b1ac54a668a2db1437737e69a.jpg" data-rawwidth="110" data-rawheight="123"&gt;&lt;b&gt;薛云峰，&lt;/b&gt;(&lt;a href="https://github.com/HolidayXue" class="" data-editable="true" data-title="HolidayXue (HolidayXue)"&gt;HolidayXue (HolidayXue)&lt;/a&gt;)，主要从事视频图像算法的研究，就职于浙江捷尚视觉科技股份有限公司担任深度学习算法研究员。捷尚致力于视频大数据和视频监控智能化，现诚招业内算法和工程技术人才，招聘主页&lt;a href="http://www.icarevision.cn/job.php" data-editable="true" data-title="浙江捷尚视觉科技股份有限公司--安全服务运营商" class=""&gt;浙江捷尚视觉科技股份有限公司--安全服务运营商&lt;/a&gt;，联系邮箱：hr@icarevision.cn&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s/rWDCYO5k06zT9BcXk21JJg" data-editable="true" data-title="深度学习框架Caffe源码解析"&gt;深度学习框架Caffe源码解析&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24343706&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Tue, 13 Dec 2016 15:52:56 GMT</pubDate></item><item><title>【Technical Review】ECCV16 Grid Loss及其在人脸检测中的应用</title><link>https://zhuanlan.zhihu.com/p/24342365</link><description>深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;什么是人脸检测？简而言之，给定一张图片，判断图中是否有人脸，如果有人脸，进一步给出每一张人脸的位置和大小。这一看似简单的任务，在实际应用中却面临着诸多困难，其中之一就是当人脸被遮挡时，如何才能准确地进行检测。在ECCV 2016上，有一篇文章专门针对检测遮挡人脸的问题进行了探索：Grid Loss: Detecting Occluded Faces，该文章通过设计新的损失函数，综合考虑局部和整体信息对分类的作用，增强了检测器对遮挡的鲁棒性。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-28ec33f69e1e7b1631359ca80e9affae.jpg" data-rawwidth="272" data-rawheight="286"&gt;&lt;p&gt;检测被遮挡的人脸，这一任务的难点在于，遮挡会导致一部分人脸特征缺失，取而代之的是遮挡物的特征，这不仅容易引起分类器误判，还容易造成漏检。解决遮挡人脸检测的问题可以从数据与算法两个方面切入。从数据方面入手的做法较为直接，即在分类器训练阶段，在正样例集中加入一定比例的带遮挡人脸，让分类器从数据中自动去学习带遮挡人脸的变化模式。数据驱动的方式也就意味着对数据的依赖，而遮挡的变化模式复杂多样，如果希望模型能对遮挡有较好的鲁棒性和泛化能力，那将需要非常大量的数据。从算法的角度入手，已有的一些工作在解决遮挡问题时，有些需要在训练数据中标好人脸的五官，这样在训练数据的制备收集阶段要花费更多的精力；有些在人脸检测的预测阶段有额外的计算，这样会因为处理遮挡带来额外的时间开销，而检测本身就是一个对速度极其敏感的任务，这也不是我们希望看到的。&lt;/p&gt;&lt;p&gt;近年来，神经网络在计算机视觉领域得到了广泛应用，也包括人脸检测这个子领域。神经网络的参数优化过程就像是一艘船在茫茫大海上行驶。这一叶扁舟（神经网络的参数），在大海（参数的解空间）上航行，那黑暗中的灯塔（损失函数），放射出耀眼的光辉（梯度），引导着前进的方向（梯度下降）。神经网络具有强大的非线性建模能力，有些时候对于一个问题效果不好，并不是神经网络的表达能力不足，而是损失函数没能引导神经网络的参数落在一个很好的解上。既然如此，可否改进人脸与非人脸分类时使用的损失函数，引导分类网络学习到对遮挡更鲁棒的特征呢？这样，不会在预测阶段带来额外的计算时间，如果损失函数无需额外标注信息，那也不需要额外的数据标注了。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-2db3cb7e6e07d36cdee4bd08babe616d.jpg" data-rawwidth="435" data-rawheight="291"&gt;这篇文章提出的grid loss就是在这个方向上进行了探索研究。一般的损失函数都是直接根据整个图片的信息计算loss，导致学习出的网络会趋向于利用全局信息分类。这篇文章将分类网络最后一个特征图划分为若干个网格（也就是相当于将图片划分为若干个网格），每个小网格看成一个单独的区域，按同样的方式计算一个loss，与整个图片的loss加和作为最终的loss。这样的loss强化了每一个小网络区域单独的判别能力，使得学习出的特征对于遮挡会更加鲁棒。引用论文中的一个图来说明普通loss与grid loss的区别。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-0340424084e5b06abcb9f8626bcdd8ee.jpg" data-rawwidth="647" data-rawheight="219"&gt;grid loss的数学定义如下（这里每一个grid的loss都是一个hinge loss）：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-b23c2d22e83ad7108b2dbcbbe3f08381.jpg" data-rawwidth="650" data-rawheight="74"&gt;其中，N代表grid的个数，wi与bi是最后一个featuremap的第i个grid对应的权值参数与偏置项, w = [w1, w2, …, wN]为最后一个featuremap整体对应的权值参数，b = b1 + b2 + ... + bN 为其对应的偏置项。这样，公式的第一项代表了整个featuremap上的loss，第二项代表了每一个grid的loss。λ是一个平衡系数，权衡全局的loss与局部的loss大小。m为一个常数，为1 / N，因为希望每一个网格区域对分类有相同的贡献。下图是分类网络的最后一个featuremap分块计算loss的示意图。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-e2a62f1b54affaaab3aeac6833785536.jpg" data-rawwidth="549" data-rawheight="230"&gt;&lt;p&gt;在检测阶段，直接将训练得到的w和b换为一个对应的全连接层即可，不需要任何额外的计算量。&lt;/p&gt;&lt;p&gt;grid loss经过作者的实验论证，能够比较明显的提升对于遮挡人脸的检测效果。除此之外，作者还发现grid loss可以使网络学习出更加多样性的特征；同时可以起到正则化的作用，在减少训练数据的时候，使用了grid loss的检测器性能下降会更少一些。这一点可以从模型集成的角度理解，因为现在强化了每一个grid的作用，最终学习出的检测器有一点若干个检测器集成的味道。作者使用的检测器，使用logistic loss时，用fddb数据集图像测试， 100个误检下的召回为0.795，使用了grid loss可以达到0.838，提高了大约4个百分点。&lt;/p&gt;&lt;p&gt;最后总结一下，grid loss这篇文章提出了一种提升被遮挡人脸检测性能的方法，这种方法无需额外的数据标注（如标注人脸中的五官），并且是一种离线训练时的策略，对在线的检测阶段没有影响，不会有额外的时间代价。作者论文中使用的检测框架是一个使用了滑动窗口范式的比较原始的框架，笔者认为，未来尝试将grid loss嵌入到一些先进的检测框架，如Faster RCNN里，是一件值得一试的事情。最后的最后，祝各位读者在生活这片海域里，都能找到自己最想要的那个损失函数，向之前进。&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing/answers" data-editable="true" data-title="@果果是枚开心果."&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-3baac8c343bbcb2e3ec9809bcbd6e1da.jpg" data-rawwidth="120" data-rawheight="118"&gt;&lt;p&gt;&lt;b&gt;时学鹏，&lt;/b&gt;中科院计算所VIPL组15级硕士生。导师为山世光研究员。研究方向为基于深度学习的目标检测，特别是人脸检测。研发了VIPL课题组第五代人脸检测SDK。个人邮箱：xuepeng.shi@vipl.ict.ac.cn。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s/qiwJnXggRwqvHFN74-W2DQ" data-editable="true" data-title="【Technical Review】ECCV16 Grid Loss及其在人脸检测中的应用"&gt;【Technical Review】ECCV16 Grid Loss及其在人脸检测中的应用&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24342365&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Tue, 13 Dec 2016 15:03:27 GMT</pubDate></item><item><title>【ECCV2016论文速读】回归框架下的人脸对齐和三维重建</title><link>https://zhuanlan.zhihu.com/p/23923248</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-1da1577674789735f19ec2ed72326d4b_r.png"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;&lt;b&gt;JointFace Alignment and 3D Face Reconstruction&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-f81e5f04be20315f42f9b15f07d1cf8a.jpg" data-rawwidth="710" data-rawheight="375"&gt;&lt;b&gt;（此处三维重建结果是gif动图，但不知什么原因，我的电脑本地无法保存，所以只好截图上传，请点击链接查看原文中的gif动图：&lt;a href="http://mp.weixin.qq.com/s/udr3573GXQOOF46jLriekg" class=""&gt;http://mp.weixin.qq.com/s/udr3573GXQOOF46jLriekg&lt;/a&gt;）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;三维人脸重建的目标是根据某个人的一张或者多张二维人脸图像重建出其三维人脸模型（此处的三维人脸模型一般仅指形状模型，定义为三维点云）。今天我们只讨论由单张二维图像重建三维人脸的问题。这个问题本身其实是个病态（ill-posed）问题，因为在将人脸从三维空间投影到二维平面上形成我们看到的二维人脸图像的过程中，人脸的绝对尺寸（如鼻子高度）、以及由于自遮挡而不可见的部分等很多信息已经丢失。在不掌握相机和拍摄环境的相关参数的情况下，这个问题其实是没有确定解的。&lt;/p&gt;&lt;p&gt;为了解决这一病态问题，一个直接思路是借助机器视觉中的Shape-from-Shading（SFS）方法。但是该方法依赖于光照条件和光照模型的先验知识，而未考虑人脸结构的特殊性，在任意拍摄的人脸图像上效果一般。后来，Kemelmacher-Shizerman和Basri [1] 引入了平均三维人脸模型作为约束条件对传统的SFS方法进行了改进，取得了不错的效果。然而，重建结果往往都接近平均模型，缺少个性化特征。另一个常用思路是建立三维人脸的统计模型，再将该模型拟合到输入的二维人脸图像上，利用拟合参数实现三维人脸的重建。这类方法基本都是基于Blanz和Vetter提出的三维形变模型（3D Morphable Model，简称3DMM） [2]。由于3DMM采用主成分分析（PCA）方法构建统计模型，而PCA本质上是一种低通滤波，所以这类方法在恢复人脸的细节特征方面效果仍然不理想。此外，上述两类方法在重建过程中对每幅图像都需要求解优化问题，因而实时性较差。&lt;/p&gt;&lt;p&gt;受到近年来回归方法在人脸对齐中的成功应用的启发，我们最早试图建立二维人脸图像上的面部特征点（包括眼角、鼻尖、嘴角等）与人脸三维模型之间的回归关系。这一思路的基本出发点是面部特征点是反映人脸三维结构的最直观依据。我们尝试根据二维特征点的偏差直接预测三维人脸形状的调整量。这就好比我们知道二维特征点是由三维人脸形状投影得到的，如果我们发现二维特征点存在偏差，那么根据这一线索我们就应该能够计算出三维人脸形状应该做怎样的调整。而这个计算过程可以用事先训练好的二维特征点偏差与三维形状调整量之间的回归函数来实现。基于这样的思路，我们成功地设计实现了在给定输入二维人脸图像上的特征点的条件下实时重建其三维模型的新方法。相关结果发布在Arxiv [3]。&lt;/p&gt;&lt;p&gt;沿着上述思路，基于2D人脸特征点和3D人脸形状之间很强的相关性，我们进一步尝试将二维人脸图像特征点检测（即人脸对齐）与三维人脸重建过程耦合起来，在回归的框架下同时实现这两个任务。这就是我们今天要介绍的发表在ECCV2016上的工作 [4] （以下称ECCV2016方法）。扯了这么多（希望不是那么远^_^），下面正式进入正题。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-f5b1a0425d08b0e8229ea9cf9b04dd7e.png" data-rawwidth="1269" data-rawheight="485"&gt;如上图所示，之前研究者大都将2D特征点定位和3D人脸重建两个过程割裂开来解决，而这两个工作本质是一个“鸡生蛋、蛋生鸡”问题。一方面，2D特征点 &lt;em&gt;U &lt;/em&gt;可由中性3D人脸 &lt;em&gt;S&lt;/em&gt; 经过表情（&lt;em&gt;FE &lt;/em&gt;）、姿态变换（ &lt;em&gt;FP&lt;/em&gt;）及投影（&lt;em&gt; FC&lt;/em&gt;）得到，即 &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-6c486d0618f0e5011d4c6abc5d8117a2.jpg" data-rawwidth="158" data-rawheight="32"&gt;&lt;p&gt;另一方面，2D特征点携带有丰富的几何信息，这也是3D重建方法的基础。&lt;/p&gt;&lt;p&gt;现有的2D特征点检测方法大部分是基于2D人脸形状建模的，主要存在以下几个问题：i）很难去刻画3D平面外旋转的人脸特征点；ii）在人脸姿态不是很大的情况下，通过变化人脸轮廓特征点语义位置来解决自遮挡的情况，这样会导致不同姿态下检测的特征点语义信息不一致 [5]（如上图，人脸图像中蓝色点所示）；iii）在更大姿态下，尤其是yaw方向超过60度以后，人脸区域存在近一半自遮挡，遮挡区域的纹理特征信息完全缺失，导致特征点检测失败。&lt;/p&gt;&lt;p&gt;现有的利用2D特征点来恢复3D人脸形状的方法也存在以下几个问题：i）需要第三方2D特征点检测算法或者手动得到2D特征点；ii）不同姿态下检测的特征点语义信息不一致，难以确定3D点云中与其对应的点 [6]；iii）只生成与输入人脸图像同样姿态和表情的3D人脸，而这样的3D人脸，相对于姿态和表情归一化的3D人脸而言，显然并不有利于人脸识别。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-1da1577674789735f19ec2ed72326d4b.png" data-rawwidth="1269" data-rawheight="541"&gt;&lt;p&gt;为了在一个框架内处理2D特征点定位和3D人脸重建，我们利用两组级联的线性回归，一组用来更新2D特征点，另一组用来更新3D人脸形状。在每一次迭代中，先用SDM[7]方法得到特征点更新量，基于方法[3]再用特征点的更新量去估计出3D人脸形状的更新量。新的3D人脸一旦更新就可以粗略地计算出3D-to-2D投影矩阵，同时再利用3D人脸对特征点进行修正，尤其是自遮挡区域的特征点位置及特征点可见性信息。整个过程2D特征点、3D人脸形状、3D-to-2D投影矩阵的更新都是一个由粗到精的估算过程。&lt;/p&gt;&lt;p&gt;我们先给出利用训练好的回归模型检测任意一张二维人脸图像上的特征点，并重建其三维模型的过程。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-1e43478e63428623fe33214e50fd37ee.jpg" data-rawwidth="687" data-rawheight="354"&gt;&lt;p&gt;值得指出的是：Step 5中，从3D人脸投影得到2D特征点对人脸形状和姿态都有很强的约束。而Step 2中，特征点是通过纹理特征指导得到的，其中自遮挡区域由于纹理信息的缺失，回归得到的特征点常常是不准确的。通过此步骤3D投影来修正能够有效地提高特征点检测的准确度。&lt;/p&gt;&lt;p&gt;在训练过程中，为了得到上述回归模型，需要提供成对的标定好特征点的二维人脸图像及其对应的三维人脸数据&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-86498880a9f7b62b9400c934a339f62b.jpg" data-rawwidth="183" data-rawheight="31"&gt;&lt;p&gt;为了更好地处理任意姿态、任意表情的二维人脸图像，训练数据中需要包括尽量多不同姿态和不同表情的人脸，而对应的三维人脸则都是中性表情的、且已经稠密对齐的点云数据。下面我们重点介绍一下用于人脸对齐的2D特征点回归的目标函数和用于三维人脸重建的3D形状回归的目标函数。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-871693051c1627ba1295b0474e5223fe.jpg" data-rawwidth="541" data-rawheight="100"&gt;该目标函数建立当前2D特征点周围的纹理特征与其距离真实位置的偏移量之间的回归关系。我们训练所用2D特征点是从3D形状投影得到的，因而确保了语义上的一致性。同时为了处理大姿态人脸图像，如果某个特征点被判定为不可见点，那这个点的SIFT特征向量置为0。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-41f61ffbe0fc571b4ea8855e20419d0d.jpg" data-rawwidth="518" data-rawheight="97"&gt;&lt;p&gt;3D形状回归建立的是2D特征点修正量与3D形状修正量之间的关系。所有训练3D人脸都进行了稠密对齐，且2D特征点之间也作好了对齐，所以并不需要增加额外的平滑约束，同时也尽量保持了3D人脸的个性化差异。训练数据中的3D形状是姿态-表情归一化（Pose and Expression Normalized，简称PEN）3D人脸，如此重建得到的PEN 3D人脸更适用于人脸识别。&lt;/p&gt;&lt;p&gt;在公开测试集上的实验结果证明了在统一的回归框架下同时解决人脸对齐和三维重建的有效性。ECCV2016论文中还进一步证明了重构出来的姿态与表情归一化的三维人脸在提升人脸识别准确率方面的有效性。最后，我们展示利用ECCV2016方法得到的人脸对齐和三维重建的几个典型结果。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-8bc2ffab81d46defe5fba8be7fab9c38.jpg" data-rawwidth="494" data-rawheight="188"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-456957dc168a202e17e0ece048795c99.jpg" data-rawwidth="505" data-rawheight="372"&gt;&lt;p&gt;&lt;b&gt;参考文献&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[1]&lt;/b&gt; Kemelmacher-Shlizerman, I., Basri, R.: 3D face reconstruction from a single image using a single reference face shape. TPAMI (2011).&lt;/p&gt;&lt;p&gt;&lt;b&gt;[2] &lt;/b&gt;Blanz, V., Vetter, T.: A morphable model for the synthesis of 3D faces. In: SIGGRAPH (1999).&lt;/p&gt;&lt;p&gt;&lt;b&gt;[3]&lt;/b&gt; Liu, F., Zeng, D., Li, J., Zhao, Q.: Cascaded regressor based 3D face reconstruction from a single arbitrary view image. arXiv preprint arXiv:1509.06161 (2015 Version)&lt;/p&gt;&lt;p&gt;&lt;b&gt;[4]&lt;/b&gt; Liu F, Zeng D, Zhao Q, Liu X.: Joint face alignment and 3D face reconstruction. In: ECCV (2016).&lt;/p&gt;&lt;p&gt;&lt;b&gt;[5]&lt;/b&gt; Jourabloo, A., Liu, X.: Pose-invariant 3D face alignment. In: ICCV (2015)&lt;/p&gt;&lt;p&gt;&lt;b&gt;[6]&lt;/b&gt; Qu C, Monari E, Schuchert T. Fast, robust and automatic 3D face model reconstruction from videos. In: AVSS, 113-118 (2014)&lt;/p&gt;&lt;p&gt;&lt;b&gt;[7] &lt;/b&gt;Xiong X, De la Torre F. Supervised descent method and its applications to face alignment. In: CVPR. 532-539 (2013)&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing"&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-6649b87e70274cdc884477599b024f29.jpg" data-rawwidth="121" data-rawheight="122"&gt;&lt;b&gt;刘峰，&lt;/b&gt;四川大学计算机学院生物特征识别实验室博士三年级学生，导师游志胜教授、赵启军博士。研究方向为机器学习与模式识别（三维人脸建模与识别、二维人脸特征点检测等）。个人邮箱：liuf1990@stu.scu.edu.cn。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;/b&gt;&lt;a href="http://mp.weixin.qq.com/s/udr3573GXQOOF46jLriekg" class=""&gt;http://mp.weixin.qq.com/s/udr3573GXQOOF46jLriekg&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23923248&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Thu, 24 Nov 2016 17:57:35 GMT</pubDate></item><item><title>IJCAI16论文速读：Deep Learning论文选读（下）</title><link>https://zhuanlan.zhihu.com/p/23733088</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-1d697eb1755f2db92d1ba82e202d4ad1_r.png"&gt;&lt;/p&gt;&lt;b&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;/b&gt;&lt;p&gt;&lt;b&gt;IJCAI16会议介绍：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;国际人工智能联合会议（ International Joint Conference on Artificial Intelligence，IJCAI ）是聚集人工智能领域研究者和从业者的盛会，也是人工智能领域中最主要的学术会议之一。1969 年到 2015 年，该大会在每个奇数年举办，现已举办了 24 届。随着近几年来人工智能领域的研究和应用的持续升温，从 2016 年开始，IJCAI 大会将变成每年举办一次的年度盛会；今年是该大会第一次在偶数年举办。第 25 届 IJCAI 大会于 7 月 9 日- 15 日在纽约举办。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Guest Editor导读：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本届会议的举办地在繁华喧嚣的纽约时代广场附近，正映衬了人工智能领域几年来的火热氛围。此次大会包括7场特邀演讲、4场获奖演讲、551篇同行评议论文的presentation，41场workshop、37堂tutorial、22个demo等。深度学习成为了IJCAI 2016的关键词之一，以深度学习为主题的论文报告session共计有3个。本期我们从中选择了1篇深度学习领域的相关论文进行了精读，介绍论文的主要思想，并对论文的贡献进行点评。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Semi-Supervised Multimodal Deep Learning for RGB-D Object Recognition&lt;/b&gt;&lt;/p&gt;&lt;p&gt;深度网络在近两年成绩不俗，应用广泛。RGB-D物体识别的研究人员自然也不会无动于衷，他们厉兵秣马，决意大干一番。怎奈何深度模型需要众多标记数据，而贴标签的营生，不是工程浩繁，就是价格昂贵。针对这一情况，血气方刚的微软人创制新算法，以半监督式学习替代全部附上标签的监督式学习。据称，仅需5%的标签，即可取得往常监督学习的成效。他们是变了什么“戏法”，把这么大的标签空缺补得滴水不漏？&lt;/p&gt;&lt;p&gt;简而言之，就是“协同训练”与“色深互补”。&lt;/p&gt;&lt;p&gt;“色深互补”，是典型的3D图像处理模式。三个颜色信息外加深度信息，统合利用。颜色信息包含更多物体类别信息，而深度信息包含更多物体姿态变化。&lt;/p&gt;&lt;p&gt;协同训练，就是有标签的数据，协同没有标签的数据，一起训练。这个方法并非新硎初发，但这里的方法很有新意，作者叫它“Diversity preserving co-training”,颇有求同存异的味道。在后文中详述实现细节。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-bef44e91be845f336cae6bebc8868edd.png" data-rawwidth="701" data-rawheight="311"&gt;网络设计如图所示。我们先看实线连接部分，从有标签的数据库开始。这里面的数据“案底分明”，所以直接用上卷积网络，提取特征。一番勤学苦练，网络就初具规模。提取到的特征，颜色部分提取的特征就去颜色分类器，深度的部分提取的特征送深度分类器，此外两个部分融合起来，送进画在中间的集成分类器。它的意图是为网络的端对端训练。而后连到虚线部分。颜色、深度这两个“判决机构”敲了锤，下面这个黑饼，代表没有标签的数据库，就赶紧来学习“宣判书”，据此把更多数据贴上标签，它们被送到开头有标签的那个库。具体实现是这么个图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-0959c4c1051fe87a3c9194809705be94.png" data-rawwidth="1160" data-rawheight="399"&gt;&lt;p&gt;实线部分的网络，是比较经典的AlexNet，虚线部分是帖标签器。整体是一个AlexNet+Updating LabelPool结构。&lt;/p&gt;&lt;p&gt;值得一提的是，实线部分的FC7层一分叉，走两股。一股走类别分类器，另一股则是隐含属类分类器，作者叫它“多任务学习”。总体的目标是：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-cc43ccbe776dc093dd7efab7febad735.png" data-rawwidth="502" data-rawheight="140"&gt;这里面x是某一具体数据，花体L表示所有带标签的数据构成的库&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-15b3584113f715ba8013e8d9aa170904.png" data-rawwidth="523" data-rawheight="60"&gt;&lt;p&gt;标签中包含颜色信息I，深度信息D，类别信息y。&lt;/p&gt;&lt;p&gt;v表示颜色或是深度模块， z代表属类标签。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-b710860f57699b26eb99e375d9ab4d44.png" data-rawwidth="122" data-rawheight="35"&gt;&lt;p&gt;表示DCNN模型预测的概率。整个损失函数是典型的门闩型损失（hinge loss）。&lt;/p&gt;&lt;p&gt;下面说说虚线部分的事情，也是本文的“大杀器”。我们也许要问，给无标签数据打标签，具体做法是什么。简单来说，核心的技术就是聚类。所有的数据都要参与聚类。聚类的目的是把没有标签的数据去找与其相似的有标签的数据，信心高的就可以帖它们的标签。信心的依据就是“属类”，聚类聚出来的类别。作者说用到方法叫“凸聚类”，这个方法据称可以收敛到全局最小值，自动找到最优的聚类的类别数。目标函数是最大化下面的对数似然型目标函数：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-0f0dc2eda4e9d64951c975c9994a9a90.png" data-rawwidth="562" data-rawheight="121"&gt;这里面q(x)表示某个数据x的“代表度”，需要满足非负性与加和为1的性质，表示判断的信心。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-0aa6380e61112067da507318006f2185.png" data-rawwidth="326" data-rawheight="36"&gt;是欧氏距离，表示两个样本x和x’的差异。它们都“穿”了一身φ（.），表示这二位都是提取的特征，作者使用的是fc7特征。β是个常数，熟悉热力学玻尔兹曼定律的同学会知道，它表征了某种“温度”或者系统活跃程度的东西。我们在机器学习中常用它作弥散核，估计系统能量。Log函数的加和意味着内部的乘积，说明作者认为所有标签独立分布。整体来说，我们最大化目标，就是要合理地把信心q分配到各个聚类的类别里。这与传统聚类是一致的。聚类的结果表示为：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a1ef63a96b40065dfa6d3a8532b85442.png" data-rawwidth="365" data-rawheight="87"&gt;其中C表示类别数。图中的例子里面，颜色信息聚了5类，深度信息聚了3类。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-ebfa07a584d6355957b1be3f8a66030d.png" data-rawwidth="833" data-rawheight="404"&gt;聚类以后就要更新标签库。将没有标签的数据算出相近属类的信心，信心较高的集合表示为：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-af67fd7c8b0d71458616470b3931fa97.png" data-rawwidth="703" data-rawheight="65"&gt;&lt;p&gt;其中&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-107f55b00b15f21d27883c72c47cb51c.png" data-rawwidth="455" data-rawheight="42"&gt;表示无标签数据。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-22d85025caac3421e27a802433eb6082.png" data-rawwidth="432" data-rawheight="38"&gt;表示给数据x标记属类z的概率。f是softmax函数。τ是一个预先选定的阈值，超过这个阈值的x说明和z属类契合度很高，可以标记z属类。v仍是表示模块，颜色或深度。迭代规则就是：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-031b29b1227c84517564b5cd27f9aa60.png" data-rawwidth="448" data-rawheight="61"&gt;&lt;p&gt;无标签数据x通过z的信息，找到最相近的有标签的z迁移它的y。于是“有较高信任度”的x们获得了标签。&lt;/p&gt;&lt;p&gt;聚类以后的结果Z被赋予新的名字：（隐含）属类。于是原先由颜色、深度、标签组成的三元组，变成现在颜色、颜色属类、深度、深度属类、标签构成的五元组。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-fb1ad736aaad338a2b2d3ce13cb0bd73.png" data-rawwidth="476" data-rawheight="49"&gt;&lt;p&gt;有标签的数据，属类都编号整齐了。&lt;/p&gt;&lt;p&gt;最后我们来说预训练的事儿。在许多视觉领域用其他收敛技术取代了这种做法，但是毕竟标签太少，难说初始化的不好会惹出什么乱子；况且，开始的聚类必须具有代表性，万一在开始的时候聚类类别不全，就后患无穷。索性先以重构目标为先锋，全部数据，带不带标签的数据齐出动，打开局面再说。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-6db81c64ab5076dc1a09cc0fed545baa.png" data-rawwidth="816" data-rawheight="127"&gt;实验（当然辉煌地）证明了方法的有效性，在只使用5%训练数据的情况下就取得了与使用完全标注数据的监督学习方法可比的性能。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-c0c3221eec23936fb656012d9daec180.png" data-rawwidth="990" data-rawheight="1220"&gt;&lt;p&gt;在文章的最后，我们总结一下“变戏法”的过程，即来回答未知的标签从哪里产生的。每个类别都聚成很多子类，而后将无标签数据附会为聚类相近的子类。逻辑上，如果夸类别的子类间很近似，就比较容易犯错。但总体而言，仍比只依靠类别信息更准确些。IJCAI的风格多理论性强，小编猜测此文的桥段中，聚类当取鳌头。另外，预训练的AE给网络更好的初始化，是成功进行后续打标签工作的前提。AlexNet+AE预训练的模式仍旧熠熠生辉，可见深度模型的博大精深啊。小编认为未来半监督学习和无监督学习会逐渐地使用深度模型解决各自的问题。是产生标签或是附会标签，抑或是更聪明地缩小图像与标签间的语义鸿沟，将是未来的方向【小编使命脸】。&lt;/p&gt;&lt;p&gt;&lt;b&gt;参与人员：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;胡蓝青&lt;/b&gt;  中科院计算所VIPL研究组博士研究生&lt;/p&gt;&lt;p&gt;&lt;b&gt;尹肖贻&lt;/b&gt;  中科院计算所VIPL研究组博士研究生&lt;/p&gt;&lt;p&gt;&lt;b&gt;刘昊淼&lt;/b&gt;  中科院计算所VIPL研究组博士研究生&lt;/p&gt;&lt;p&gt;&lt;b&gt;刘    昕 &lt;/b&gt; 中科院计算所VIPL研究组博士研究生&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" data-editable="true" data-title="@果果是枚开心果." class=""&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-32826469aaf31cf687d6d4ab45fc4783.png" data-rawwidth="117" data-rawheight="118"&gt;&lt;p&gt;&lt;b&gt;朱鹏飞，&lt;/b&gt;天津大学机器学习与数据挖掘实验室副教授，硕士生导师。分别于2009和2011年在哈尔滨工业大学能源科学与工程学院获得学士和硕士学位，2015年于香港理工大学电子计算学系获得博士学位。目前，在机器学习与计算机视觉国际顶级会议和期刊上发表论文20余篇，包括AAAI、IJCAI、ICCV、ECCV以及IEEE Transactions on Information Forensics and Security等。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325678&amp;amp;idx=1&amp;amp;sn=331963a6674cde509f75a09ae5e331eb&amp;amp;chksm=f235a5a4c5422cb2314d7ed9e172d47ac461b588426bce8edd9d56bde4f1dafd7993c4f02823&amp;amp;scene=0#wechat_redirect" class=""&gt;http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325678&amp;amp;idx=1&amp;amp;sn=331963a6674cde509f75a09ae5e331eb&amp;amp;chksm=f235a5a4c5422cb2314d7ed9e172d47ac461b588426bce8edd9d56bde4f1dafd7993c4f02823&amp;amp;scene=0#wechat_redirect&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23733088&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Thu, 17 Nov 2016 14:29:23 GMT</pubDate></item><item><title>［冠军之道］ECCV16视频性格分析竞赛冠军团队分享</title><link>https://zhuanlan.zhihu.com/p/23510566</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-1a17aae6aea40fcd02321bbe879c429f_r.png"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;英文中有句谚语叫："You never get a second chance to make a first impression."（你永远没有第二个机会去改变你的第一印象。）一个人的第一印象可以用来快速判断其性格特征（Personal traits）及其复杂的社交特质，如友善、和蔼、强硬和控制欲等等。因此，在人工智能大行其道的当下，基于第一印象/表象的性格自动分析也成为计算机视觉和多媒体领域中一类非常重要的研究问题。&lt;/p&gt;&lt;p&gt;前不久，欧洲计算机视觉大会（ECCV 2016）ChaLearn Looking at People Workshop就举办了一场全球范围的（视频）表象性格分析竞赛（Apparent personality analysis）。历时两个多月，我们的参赛队（NJU-LAMDA）在86个参赛者，其中包括有印度“科学皇冠上的瑰宝”之称的Indian Institutes of Technology （IIT）和荷兰名校Radboud University等劲旅中脱引而出，斩获第一。在此与大家分享我们的竞赛模型和比赛细节。&lt;/p&gt;&lt;p&gt;&lt;b&gt;问题重述&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本次ECCV竞赛提供了平均长度为15秒的10000个短视频，其中6000个为训练集，2000个为验证集，剩余2000个作为测试。比赛要求通过对短视频中人物表象（表情、动作及神态等）的分析来精确预测人的五大性格特质，即Big Five Traits，其中包括：经验开放性（Openness to experience）、尽责性（Conscientiousness）、外向性（Extraversion）、亲和性（Agreeableness）和情绪不稳定性（Neuroticism）。视频示例如下所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-1a17aae6aea40fcd02321bbe879c429f.png" data-rawwidth="771" data-rawheight="274"&gt;竞赛数据中五大性格特质的真实标记（Ground truth）通过Amazon Mechanical Turk人工标注获得，每个性格特质对应一个0～1之间的实值。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-d3b3e4836ea7fea91b1f0cf8c6833c8a.png" data-rawwidth="683" data-rawheight="271"&gt;&lt;p&gt;&lt;b&gt;我们的方法&lt;/b&gt;&lt;/p&gt;&lt;p&gt;由于竞赛数据为短视频，我们很自然的把它作为双模态（Bimodal）的数据对象来进行处理，其中一个模态为音频信息（Audio cue），另一个则为视觉信息（Visual cue）。同时，需预测的五大性格特质均为连续值，因此我们将整个问题形式化为一个回归问题（Regression）。我们将提出的这个模型框架称作双模态深度回归（Deep Bimodal Regression，DBR）模型。下面分别从两个模态的处理和最后的模态融合来解析DBR。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-17b800bf60b1cc182b430ff4c45dbbbd.png" data-rawwidth="745" data-rawheight="285"&gt;&lt;p&gt;&lt;b&gt;视觉模态&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在视觉模态中，考虑到对于短视频类数据，时序信息的重要程度并不显著，我们采取了更简单有效的视频处理方式，即直接将视频随机抽取若干帧（Frame），并将其作为视觉模态的原始输入。当然，在DBR中，视觉模态的表示学习部分不能免俗的使用了卷积神经网络（Convolutional Neural Networks，CNN）。同时，我们在现有网络基础上进行了改进，提出了描述子融合网络（Descriptor Aggregation Networks，DAN），从而取得了更好的预测性能。&lt;/p&gt;&lt;p&gt;以VGG-16为例，传统CNN经过若干层卷积（Convolutional）、池化（Pooling）的堆叠，其后一般是两层全链接层（Fully connected layers）作为网络的分类部分，最终输出结果。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-71873c59318e964426a485a8f96a18f5.png" data-rawwidth="715" data-rawheight="232"&gt;受到我们最近工作[2]的启发，在DBR视觉模态的CNN中，我们扔掉了参数冗余的全链接层，取而代之的是将最后一层卷积层学到的深度描述子（Deep descriptor）做融合（Aggregation），之后对其进行L2规范化（L2-normalization），最后基于这样的图像表示做回归（fc+sigmoid作为回归层），构建端到端（End-to-end）的深度学习回归模型。另外，不同融合方式也可视作一种特征层面的集成（Ensemble）。如下图，在DAN中，我们对最后一层卷积得到的深度描述子分别进行最大（Max）和平均（Average）的全局池化（Global pooling）操作，之后对得到的融合结果分别做L2规范化，接下来将两支得到的特征级联（concatenation）后作为最终的图像表示（Image representation）。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-127bca041e07a0ce2593a893ea11c794.png" data-rawwidth="739" data-rawheight="222"&gt;&lt;p&gt;传统CNN中，80%的参数存在于全链接层，而DAN摒弃了全链接，使得DAN相比传统CNN模型拥有更少的参数，同时大幅减少的参数可加速模型的训练速度。另外，全局池化带来了另一个优势即最终的图像表示（512维）相比传统全链接层（4096维）有了更低的维度，有利于模型的可扩展性以处理海量（Large-scale）数据。&lt;/p&gt;&lt;p&gt;此外，为了集成多层信息（Multiple layer ensemble），在DAN基础上我们提出了可端到端训练的DAN+。具体而言，是对ReLU5_2层的深度描述子做上述同样操作，得到对应于ReLU5_2的图像表示，将其与Pool5层的DAN得到的图像表示进行二次级联，最终的向量维度为2048维。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-0e6d8e47359aa115d45a3a0fbff046e5.png" data-rawwidth="666" data-rawheight="413"&gt;&lt;p&gt;除DAN和DAN+外，在视觉模态中，我们还利用了著名的残差网络（Residual Networks）作为模型集成的另一部分。&lt;/p&gt;&lt;p&gt;&lt;b&gt;音频模态&lt;/b&gt;&lt;/p&gt;&lt;p&gt;语音处理中的一种常用的特征为MFCC特征，在竞赛模型中，我们首先从视频中提取原始语音作为输入数据，之后对其抽取MFCC特征。在此需要指出的是，抽取MFCC过程的一个副产品是一种名为logfbank特征，如下图所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-3a27d5d81a8df278fa9f002cd76a63cb.png" data-rawwidth="675" data-rawheight="374"&gt;在抽取logfbank和MFCC特征后，我们同样采取mini-batch形式的训练方式训练线性回归器（Linear regression）。在竞赛中，我们发现logfbank相比MFCC有更优秀的预测效果，如下图所示。其纵轴为回归错误率（越低越好），其横轴为训练轮数，可以发现logfbank在最终的回归错误率上相比MFCC有近0.5%的提升。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-10ad10e7a85e32eb437f3946ce5a55ee.png" data-rawwidth="631" data-rawheight="297"&gt;&lt;p&gt;于是我们选取logfbank特征作为音频模态的特征表示以预测音频模态的回归结果。由于竞赛时间和精力有限，我们在比赛中未使用语音处理领域的深度学习模型。不过，这也是后续可以提高模型性能的一个重要途径。&lt;/p&gt;&lt;p&gt;&lt;b&gt;模态融合（Modality ensemble）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;待两个模态的模型训练完毕，可以得到不同模态不同模型的性格特质预测结果，比赛中我们将其无权重的平均作为该视频最终的性格特质预测结果，如图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-11b42dc2fd1b694909a187596cbeb727.png" data-rawwidth="721" data-rawheight="396"&gt;&lt;p&gt;&lt;b&gt;竞赛结果&lt;/b&gt;&lt;/p&gt;&lt;p&gt;比赛中，我们对一个视频抽取100帧／张图像作为其视觉模态的输入，对应的原始音频作为抽取logfbank特征的语料。训练阶段，针对视觉模态，其100张图像共享对应的性格特质真实标记；预测阶段，其100张图像的平均预测值将作为该视频视觉模态的预测结果。&lt;/p&gt;&lt;p&gt;经下表对比，可以清楚看到，DAN相比VGG-Face，由于没有了冗余的全链接层，其参数只有VGG-Face的约十分之一，而回归预测准确率却优于传统VGG模型，同时特征维度大大减少。此外，相比ResNet，我们提出的模型DAN和DAN+也有不俗表现。此外，在模型预测速度上，DAN和DAN+也快于VGG和ResNet。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-42971d8b8b3c4e7e358a2c313200272c.png" data-rawwidth="729" data-rawheight="425"&gt;模态集成后，我们在五个性格特质预测上取得了四个结果的第一，同时我们也取得了总成绩的冠军。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-1d4616a1f6bc55bfb643c03278a356c7.png" data-rawwidth="748" data-rawheight="199"&gt;&lt;p&gt;&lt;b&gt;模型分析&lt;/b&gt;&lt;/p&gt;&lt;p&gt;最后，我们将模型最后一层卷积／池化的特征做了可视化。可以发现ResNet仅仅将“注意力”聚焦在了视频中的人物上，而我们的DAN和DAN+不仅可以“注意”到人，同时可以将环境和动作信息结合起来进行表象性格预测。另外值得一提的是，其余参赛队均做了人脸检测等预处理操作，从而将人物从视频中“抠”出，但是这样的操作反而降低了整个性格特质预测的性能。俗话说“气由心生”，一个人所处的环境（尤其是卧室、办公室等私人场所）往往可以从侧面反映一个人的性格特性。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-df599f59d6cea93a1fac22e2f525015e.png" data-rawwidth="670" data-rawheight="360"&gt;&lt;p&gt;&lt;b&gt;参考文献&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[1]&lt;/b&gt; Victor Ponce-Lopez, Baiyu Chen, Marc Oliu, Ciprian Cornearu, Albert Clapes, Isabelle Guyon, Xavier Baro, Hugo Jair Escalante and Sergio Escalera. ChaLearn LAP 2016: First Round Challenge on First Impressions - Dataset and Results. European Conference on Computer Vision, 2016.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[2] &lt;/b&gt;Xiu-Shen Wei, Chen-Wei Xie and Jianxin Wu. Mask-CNN: Localizing Parts and Selecting Descriptors for Fine-Grained Image Recognition. arXiv:1605.06878, 2016.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[3] &lt;/b&gt;Chen-Lin Zhang, Hao Zhang, Xiu-Shen Wei and Jianxin Wu. Deep Bimodal Regression for Apparent Personality Analysis. European Conference on Computer Vision, 2016.&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" data-editable="true" data-title="@果果是枚开心果." class=""&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-1a88dceaa7b025012577691eef27c2ef.jpg" data-rawwidth="119" data-rawheight="118"&gt;&lt;b&gt;魏秀参，&lt;/b&gt;为本次竞赛NJU-LAMDA参赛队Team Director。南京大学计算机系机器学习与数据挖掘所（LAMDA）博士生，研究方向为计算机视觉和机器学习。曾在国际顶级期刊和会议发表多篇学术论文，并多次获得国际计算机视觉相关竞赛冠亚军，另撰写的「Must Know Tips/Tricks in Deep Neural Networks」受邀发布于国际知名数据挖掘论坛 KDnuggets 等。 微博ID：Wilson_NJUer&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325653&amp;amp;idx=1&amp;amp;sn=180f7ad377ff7face55814c65502db33&amp;amp;chksm=f235a59fc5422c89861c0565b050ea68ea31c1be6f09bcf14423371e4fee32d325cd72b800c9&amp;amp;scene=0#wechat_redirect" data-editable="true" data-title="［冠军之道］ECCV16视频性格分析竞赛冠军团队分享"&gt;［冠军之道］ECCV16视频性格分析竞赛冠军团队分享&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23510566&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Tue, 08 Nov 2016 15:06:48 GMT</pubDate></item><item><title>深度学习在图像取证中的进展与趋势</title><link>https://zhuanlan.zhihu.com/p/23341157</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-a6d2f174115f956756296d6bda538025_r.png"&gt;&lt;/p&gt;&lt;p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;/p&gt;&lt;p&gt;&lt;b&gt;图像取证&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在当今飞速发展的信息时代，数字图像已经渗透到社会生活的每一个角落，数字图像的广泛使用也促进了数字图像编辑软件的开发与应用，例如：Adobe Photoshop、CorelDRAW、美图秀秀等等。利用这些编辑工具，用户可以随意对图像进行修改，从而达到更好的视觉效果。然而，在方便了用户的同时，也给一些不法分子以可乘之机。在未经授权的情况下，不法分子对图像内容进行非法操作，如违规编辑、合成虚假图像等，从而造成篡改图像在人们社会生活中泛滥成灾。图像取证技术就是在这样的背景下提出，旨在通过盲分析手段认证图像数据的原始性和真实性、鉴别和分析图像所经历的操作处理及估计图像的操作历史[1]。&lt;/p&gt;&lt;p&gt;数字图像的完整周期包含三个部分[2]：图像获取、图像编码、图像编辑，如图1所示。在图像获取过程中，真实场景中的光线通过相机镜头投射到相机传感器（如CCD或者CMOS传感器），产生数字图像信号。在投射到相机传感器之前，通常光首先经过CFA滤波处理，即每个像素点只会包含一种主要的颜色分量（红、绿、蓝）。在相机传感器之后会进行CFA差值(也称去马赛克处理）从而获取每个像素点的红绿蓝三通道分量。然后获取的数字图像信号会经历相机内部的软件处理，比如白平衡、对比度增强、图像锐化、伽马矫正等等。在图像编码部分，经过处理后的数字图像信号为了节省相机内存通常会经过有失真压缩处理，最常见的压缩方式为JPEG压缩。部分压缩后的数字图像为了获得更好的视觉效果会进行后处理操作，任何的图像编辑都可以应用在后处理操作，经常使用的编辑为：几何变换（旋转、缩放等）、模糊、锐化、对比度调整、图像拼接、复制-粘贴。经过编辑后的数字图像重新保存为JPEG格式形成最终的数字图像。&lt;/p&gt;&lt;p&gt;数字图像取证的出发点是通过提取数字图像周期中留下的固有痕迹进行分析和理解数字图像的操作历史。以上介绍的数字图像完整周期的三个部分每一部分都会留下不同的操作痕迹（指纹特性），即获取指纹、编码指纹、编辑指纹。在图像获取指纹研究中，根据镜头特性、传感器特性、CFA模式等引入数字图像中的不同指纹特性对数字图像进行分析。在图像编码指纹研究中，JPEG压缩以及多重JPEG压缩检测是主要关注的问题。在图像编辑指纹特性研究中，基于信号处理和基于物理/几何的技术被提出。利用信号处理技术进行复制粘贴检测、重采样检测、对比度增强检测、线裁剪检测等，利用光线/阴影进行拼接检测以及利用几何关系的一致性检测拼接处理都是取证研究中的热点问题。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-566f80fdb12dae7719721d023c3fab4d.png" data-rawwidth="645" data-rawheight="387"&gt;&lt;p&gt;&lt;b&gt;图像取证深度学习之风&lt;/b&gt;&lt;/p&gt;&lt;p&gt;不同与传统的图像取证算法，深度学习算法将特征提取和特征分类整合到一个网络结构中，实现了一种end-to-end的自动特征学习分类的有效算法。从当前的研究工作来看，深度学习应用于图像取证领域大致可分为三个层次。第一个层次是简单的迁移，即直接将CV领域常用的CNN网络结构引入到图像取证领域。取证领域比较常用的网络结构为AlexNet，选择此网络结构的原因是因为AlexNet网络结构相较于其他网络结构复杂度相对较低并且性能较好，对于解决数据集少的取证问题有更好的尝试性条件。典型案例为Luca Baroffio, Luca Bond等[3]发表的文章“Camera Identification With Deep Convolutional Networks”, 文章提出用深度学习解决取证中的相机源辨别问题。第二个层次是尝试对网络输入的修改，进行此种尝试的初衷是由取证问题和CV问题的本质区别所驱使。取证问题虽可归类于识别、分类、定位问题，但是对于分类问题的类间差别取证分类问题远小于CV分类问题。举个例子：ImageNet中的22000种类别之间的形态差异是较大的，比如猫和狗两个类别之间的差异人眼可辨别；然而对于取证问题，类别之间的形态差异是极其微小的，类间差别以微弱信号的形式存在；比如对于常见的双重JPEG压缩取证，需要解决的问题是区分一副图像是经历过一次JPEG压缩之后的图像，还是经历过两次JPEG压缩之后的图像。在两次压缩使用的压缩因子（压缩因子小于等于90）一致的前提条件下，内容相同的两幅图像的DCT域统计类间差别小于0.4%（数据来源于Detecting Double JPEG Compression With the Same Quantization Matrix[4]）。基于取证问题的此种特性，研究者尝试对网络的输入进行改进，添加预处理层（或信号增强层）放大类间差别，此种尝试取得较好的检测效果。典型案例为Jiansheng Chen, Xiangui Kang等[5]发表的文章“Median Filtering Forensics Based on Convolutional Neural Networks”，根据论文报告的试验结果，预处理层的添加对检测准确率有了7.22个百分点的提升。第三个层次是对网络结构的修改，结合取证的实际问题提出适合于取证问题的网络结构。典型案例为Belhassen Bayar, Matthew C. Stamm[6]发表的文章“A Deep Learning Approach To Universal Image Manipulation Detection Using A New Convolutional Layer”。&lt;/p&gt;&lt;p&gt;下面针对于不同的取证问题介绍深度学习的应用。据我们所知，到目前为止，深度学习应用于取证领域的工作共有5篇，涉及到了取证问题中的相机源取证、中值滤波取证、重获取图像取证以及反反取证。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.1 相机源取证&lt;/b&gt;&lt;/p&gt;&lt;p&gt;相机源取证研究的问题在于如何有效区分图像采集所使用的设备型号或模式，相机源取证可以在一定程度上解决图像版权问题，例如一副具有版权保护的图像未经过作者授权被重新拍摄并发布，可以利用相机源取证技术区分图像是由原始相机拍摄还是其他相机拍摄，从而判断图像版权所属。&lt;/p&gt;&lt;p&gt;不同厂商生产的数码相机之间存在着差异，相同厂商生产的不同型号的数码相机之间也存在着差异，已有的传统方案通过提取不同的相机存在的指纹特性实现对相机源的取证。Luca Baroffio, Luca Bond等人首次提出利用卷积神经网络的方法解决相机源取证问题。文章所使用的网络结构参数如图2所示。该结构使用了三个卷积层和两个全连接层的结构，网络结构与AlexNet结构相似。根据文章报告的实验结果，在相机源取证的benchmark库中测试，对于27种相机模式分类的准确率在94%以上。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-c0bdff0fa450bbb807621507ddea7b50.png" data-rawwidth="361" data-rawheight="431"&gt;&lt;p&gt;&lt;b&gt;2.2 中值滤波图像取证&lt;/b&gt;&lt;/p&gt;&lt;p&gt;中值滤波图像取证问题一直被图像取证领域所关注，取证目的是对图像是否经历过中值滤波操作进行判定。在图像经过篡改之后，为了去除篡改引入图像中的特性，通常会对图像进行中值滤波操作，从而隐藏篡改操作痕迹。图像是否经历过中值滤波操作对于判断图像篡改历史提供了重要线索。传统的图像中值滤波取证算法对于小尺寸图像和做过压缩后处理的图像性能有待提高。&lt;/p&gt;&lt;p&gt;Jiansheng Chen, Xiangui Kang等人首次提出利用深度学习解决中值滤波取证问题，该工作发表在Signal Processing Letters IEEE, 2015。与此同时，这也是深度学习应用在取证领域的第一个工作，为后续深度学习在取证领域的发展起到了重要的借鉴作用。该工作对网络的输入图像做了预处理操作&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-04ab673a9c5aae523c6c2f1f847e4f77.png" data-rawwidth="447" data-rawheight="75"&gt;&lt;em&gt;x&lt;/em&gt;(&lt;em&gt;i &lt;/em&gt;, &lt;em&gt;j&lt;/em&gt;)表示原始图像， medw(.)表示中值滤波操作，其中中值滤波窗口大小为&lt;em&gt;w&lt;/em&gt;,&lt;em&gt;d&lt;/em&gt;(&lt;em&gt;i &lt;/em&gt;,&lt;em&gt; j&lt;/em&gt;)， 代表中值滤波图像与原始图像的差值图像。做这样预处理的动机来源于之前传统方案的设计[7]。通过预处理操作，去除图像内容对检测性能的影响同时也起到了放大图像噪声信号的作用。预处理操作的效果图如图3所示，(a)(b)(c)三幅图像分布代表原始图像，差值图像以及中值滤波与原始图像的差值图像。从图3(c)中可以看出中值滤波后的差值图像中对原始图像内容的反映几乎去除。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-bce7d87ef51655bd68234e60c0bb352d.png" data-rawwidth="649" data-rawheight="322"&gt;&lt;/p&gt;&lt;p&gt;图4中展示了该工作提出的网络结构示意图。滤波层实现的是式(1)的操作，紧接着的网络结构与AlexNet结构类似，5个卷积层以及3个全连接层。根据文章中报告的实验结果，对于压缩的小尺寸图像（64x64、32x32）该方法实现了最好的检测准确率。为了测试滤波层的作用，作者在使用滤波层和不使用两种情况下进行了对比实验，实验结果显示滤波层对于检测准确率有7.22个百分点的提升。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-d083d32a63863eb10f2545efdd6770af.png" data-rawwidth="654" data-rawheight="186"&gt;基于Jiansheng Chen, Xiangui Kang等人的工作，Belhassen Bayar，Matthew C. Stamm[6]提出一种新的卷积结构。作者尝试利用新的卷积结构捕获图像操作过程中引入的图像临近像素之间相关关系的变化，于此同时尽可能压缩图像内容对于图像操作引入的像素相关关系的影响。为了实现这样的卷积结构设计，作者对卷积核的属性进行了限制，使得网络结构可以自动学习预测误差滤波器集合，从而抑制图像内容的影响同时捕获操作特性。限制条件如公式（2）所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-714ff005796be52c0a948e5ce6d9a124.png" data-rawwidth="373" data-rawheight="70"&gt;&lt;em&gt;w&lt;/em&gt;为新的卷积核，&lt;em&gt;w&lt;/em&gt;(0,0)为卷积核中心位置的数值。新的卷积结构只使用在第一层卷积中，从而实现图像预处理卷积核的自动学习。图5、图6分别展示了新的卷积结构和文章提出的网络结构图。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-a52096ae8649630e0400df17074fe007.png" data-rawwidth="591" data-rawheight="404"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-582c6796d529cc413273e6c8613b8e6a.png" data-rawwidth="646" data-rawheight="264"&gt;&lt;p&gt;&lt;b&gt;2.3 重获取图像取证&lt;/b&gt;&lt;/p&gt;&lt;p&gt;重获取图像取证近几年开始受到取证工作者的关注。重获取操作是指原始图像被投影到新的媒介后被再次获取的过程。图7展示了常见的图像重获取操作流程。原始图像首先被投影到新得到媒介：电脑屏幕、手机屏幕、打印纸，然后对投影后的图像进行重新拍摄，形成重获取图像。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-76cb1b176e83e7755890c5bda11e5c77.png" data-rawwidth="648" data-rawheight="270"&gt;&lt;p&gt;重获取图像取证有重要的研究价值，对于篡改操作后的图像通常会在图像中留下指纹性的操作痕迹，消除这些痕迹的最简单的方式就是对篡改后的图像进行重新获取。因其操作的简易性被很多篡改者使用。重获取图像取证通过辨别图像是否经过重获取操作对图像操作历史的鉴别具有重要意义。另一方面，随着人脸识别身份系统的快速发展和广泛使用，一些不法分子试图通过一些手段欺骗身份识别系统，活体检测技术的使用为身份识别系统提供了一层保障，然后face2face系统的推出又为活体检测提出了挑战。重获取图像取证作为一种新的技术手段可以有效地增强身份识别系统的鲁棒性。&lt;/p&gt;&lt;p&gt;基于深度学习的方法，我们[8]提出了一种拉普拉斯卷积神经网络算法检测重获取图像。网络结构如图8所示，对于输入图像我们首先利用信号增强层放大重获取噪声信号，然后利用5个卷积层进行特征提取，最后使用全连接层作为特征分类层。我们提出的算法在不同尺寸的图像库上实现了95%以上的检测性能。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-a6d2f174115f956756296d6bda538025.png" data-rawwidth="648" data-rawheight="410"&gt;&lt;p&gt;&lt;b&gt;2.4 反反取证&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Jingjing Yu, Xiangui Kang等[9]将卷积神经网络应用在多类反反取证问题上。随着取证技术的发展，对抗取证的研究也随之兴起，称为反取证研究。反取证是针对于特定的取证技术提出使其失效的算法。该工作是针对于多种反取证算法进行取证，故而称为反反取证。文章提出的网络结构如下所示，四层卷积结构以及两层全连接结构。文章关注了四类反取证问题：JPEG压缩、中值滤波、重采样、对比度增强，根据文章报告的实验结果，平均检测准确率达到了96.9%。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-fe1da403857178c9fd967cb99f9cfae8.png" data-rawwidth="521" data-rawheight="203"&gt;&lt;p&gt;&lt;b&gt;2.5 隐写分析&lt;/b&gt;&lt;/p&gt;&lt;p&gt;把隐写分析列在这里，是因为隐写分析与取证领域存在极深的渊源。两者虽需要解决的具体问题不同，但是方法论本质上具有一致性，都是探寻微弱“噪声”信号的存在。隐写分析领域和图像取证领域的发展是相辅相成的，两者针对各自领域提出的有效算法通常在彼领域也能得到很好的应用。比如隐写分析领域常用的Rich Models、SPM算法都被应用于取证领域并取得了state-of-the-art的检测性能。&lt;/p&gt;&lt;p&gt;隐写分析是针对隐写问题发展而来的一种技术手段，目的是检测目标中是否包含隐藏信息。待检测目标中嵌入隐藏信息的比特率越低，意味着隐藏信息量越少，检测难度越大。传统的隐写分析都是基于特征提取加特征分类的两段论方案，为了更全面的刻画待测目标中的“微弱”信号，维度不断增加的高维特征被提出，例如Rich Models特征。特征分类方面为了加速高维隐写特征的分类，Fridrich课题组提出了针对隐写分析的特定分类器，集成分类器[10]。深度学习的发展为隐写分析提供了一种新的思路。Qian Y, Dong J等[11]首次将深度学习算法应用于隐写分析领域，并基于隐写分析的领域知识提出高斯激活函数，取得了和传统方案性能相当的检测效果；Guanshuo Xu, Yun-Qing Shi等[12]设计了一种新的网络结构，在网络结构中添加了绝对值层、BN层和全局pooling层，也取得了较好的检测效果。基于以上工作，两者又相继推出了后续工作。Qian Y, Dong J等[13,14]融合迁移学习的方法进一步提高了算法性能；Guanshuo Xu, Yun-Qing Shi等[15]提出了基于集成学习和集成分类的方案。&lt;/p&gt;&lt;p&gt;&lt;b&gt;图像取证深度学习之风何去何从&lt;/b&gt;&lt;/p&gt;&lt;p&gt;如今深度学习的如火如荼让各行各业的同胞摩拳擦掌。就取证领域而言，深度学习的探索之旅还处于小荷才露尖尖角的状态。如施云庆教授在IWDW2016中的谈话所言：“深度学习在取证领域中的进步相较于计算机视觉领域是很小的，如何进一步提升深度学习在取证中的检测性能仍然值得关注”。另外，取证领域的数据集规模相对于计算机视觉领域较小，对于数据驱动型的深度学习算法，更大规模的公开的全面的精确标注的数据集对于图像取证问题无疑是迫切需要的。&lt;/p&gt;&lt;p&gt;这段时间本文作者经过一些探索也取得了一些心得，在此和大家一起探讨。首先就网络的深度而言，浅层的网络结构已然可以得到较好的实验结果。当然网络的加深会对实验结果略有提升，但是并不能和增加层数带来的计算复杂度的提升成比例。其次，预处理操作并不是对于所有取证问题都适用，预处理操作在放大噪声信号的同时也相应的丢失了部分原始信息，对于深度学习数据驱动型算法而言，这些丢失的原始信息对于算法性能的影响比重如何暂时还无定论，所以预处理操作添加与否还需具体情况具体分析。&lt;/p&gt;&lt;p&gt;目前基于深度学习的图像取证研究还有许多问题需要去解决，更多的路需要去探索，本文作者欢迎读者的任何意见或者建议，并期待和大家一起探讨。&lt;/p&gt;&lt;p&gt;&lt;b&gt;参考文献&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[1] &lt;/b&gt;曹刚. 数字图像操作取证技术研究[D]. 北京交通大学, 2013.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[2]&lt;/b&gt; Piva A. An Overview on Image Forensics[J]. Isrn Signal Processing, 2013.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[3] &lt;/b&gt;Baroffio L, Bondi L, Bestagini P, et al. Camera identification with deep convolutional networks[J]. 2016.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[4]&lt;/b&gt; Huang F, Huang J, Shi Y Q. Detecting Double JPEG Compression With the Same Quantization Matrix[J]. IEEE Transactions on Information Forensics &amp;amp; Security, 2010.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[5] &lt;/b&gt;Chen J, Kang X, Liu Y, et al. Median Filtering Forensics Based on Convolutional Neural Networks[J]. Signal Processing Letters IEEE, 2015&lt;/p&gt;&lt;p&gt;&lt;b&gt;[6]&lt;/b&gt; Bayar B, Stamm M C. A Deep Learning Approach to Universal Image Manipulation Detection Using a New Convolutional Layer[C]// ACM Workshop on Information Hiding and Multimedia Security. ACM, 2016.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[7] &lt;/b&gt;Kang X, Stamm M C, Peng A, et al. Robust Median Filtering Forensics Using an Autoregressive Model[J]. IEEE Transactions on Information Forensics &amp;amp; Security, 2013.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[8] &lt;/b&gt;Peng P Y, Rong R N, Yao Z. Recapture Image Forensics Based On Laplacian Convolutional Neural Networks[C]// International Workshop on Digital-forensics and Watermaking, 2016&lt;/p&gt;&lt;p&gt;&lt;b&gt;[9]&lt;/b&gt; Jing J Y, Yi F Z, Jian H Y, et al. A Multi-purpose Image Counter-anti-forensic Method Using Convolutional Neural Networks[C] // International Workshop on Digital-forensics and Watermaking, 2016&lt;/p&gt;&lt;p&gt;&lt;b&gt;[10]&lt;/b&gt; Kodovsky J, Fridrich J, Holub V. Ensemble Classifiers for Steganalysis of Digital Media[J]. IEEE Transactions on Information Forensics &amp;amp; Security, 2012&lt;/p&gt;&lt;p&gt;&lt;b&gt;[11]&lt;/b&gt; Qian Y, Dong J, Wang W, et al. Deep learning for steganalysis via convolutional neural networks[C]//SPIE/IS&amp;amp;T Electronic Imaging. International Society for Optics and Photonics, 2015&lt;/p&gt;&lt;p&gt;&lt;b&gt;[12] &lt;/b&gt;Xu G, Wu H Z, Shi Y Q. Structural Design of Convolutional Neural Networks for Steganalysis[J]. IEEE Signal Processing Letters, 2016&lt;/p&gt;&lt;p&gt;&lt;b&gt;[13] &lt;/b&gt;Qian Y, Dong J, Wang W, et al. Learning Representations for Steganalysis from Regularized CNN Model with Auxiliary Tasks[C]//Proceedings of the 2015 International Conference on Communications, Signal Processing, and Systems. Springer Berlin Heidelberg, 2016&lt;/p&gt;&lt;p&gt;&lt;b&gt;[14]&lt;/b&gt; Qian Y, Dong J, Wang W, et al. Learning and transferring representations for image steganalysis using convolutional neural network[C]//Image Processing (ICIP), 2016 IEEE International Conference on. IEEE, 2016&lt;/p&gt;&lt;p&gt;&lt;b&gt;[15]&lt;/b&gt; Xu G, Wu H Z, Shi Y Q. Ensemble of CNNs for Steganalysis: An Empirical Study[C]//Proceedings of the 4th ACM Workshop on Information Hiding and Multimedia Security. ACM, 2016&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" class="" data-editable="true" data-title="@果果是枚开心果."&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-cce081b46b15055de2aa23c3163b44f8.png" data-rawwidth="115" data-rawheight="121"&gt;杨朋朋，&lt;/b&gt;就读于北京交通大学，信号与信息处理专业博士生二年级，导师倪蓉蓉教授。研究兴趣包括多媒体取证、隐写分析，深度学习。所在团队为教育部创新团队和科技部重点领域创新团队，负责人为赵耀教授。个人邮箱：ppyangforensics@gmail.com 个人主页：&lt;a href="http://www.ppyforensics.com/" data-editable="true" data-title="ppy – homepage"&gt;ppy – homepage&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325617&amp;amp;idx=1&amp;amp;sn=c941e03add12204f43f40a801e554413&amp;amp;chksm=f235a57bc5422c6d10602e1ebf87d231333b1ae869740c90e9092ab654dd92188b4890fffaff&amp;amp;scene=0#wechat_redirect" data-editable="true" data-title="深度学习在图像取证中的进展与趋势"&gt;深度学习在图像取证中的进展与趋势&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23341157&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Tue, 01 Nov 2016 11:15:31 GMT</pubDate></item><item><title>【Technical Review】ECCV16 Center Loss及其在人脸识别中的应用</title><link>https://zhuanlan.zhihu.com/p/23340343</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-a63a4a309c2b7d8915212e7fa885b760_r.png"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;&lt;b&gt;摘要&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在大家吐槽用softmax训练出来的人脸模型性能差，contrastive 和 triplet需要个中谜一样的采样方法之际。ECCV 2016 有篇文章提出了权衡的解决方案。通过添加center loss使得简单的softmax就能够训练出拥有内聚性的特征。該特点在人脸识别上尤为重要，从而使得在很少的数据情况下训练出来的模型也能有不俗的性能。本文尝试用一种比较容易理解的方式来解释这篇文章。&lt;/p&gt;&lt;p&gt;我觉得在开始正式介绍center loss之前，我有必要讲一个故事。请大家回答我，做人脸研究，最期待的事情是什么？来100块Titan……！咦，还不如换成银子呢。笔者认为，对于做人脸的人而言，最幸福的事情就是拿到全世界所有人的若干张人脸数据(画外音：地球人都知道!)。 各位看官先勿开喷，请听故事….假设我拿到了世界所有人的人脸数据，也训练好了一个所谓的映射空间(其实我更喜欢叫嵌入空间)…..此时我们觉得这个空间的画风应该是这样的(密集恐惧症慎入)….&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-09a5af94f9a1adf7654f6ce51ffc333b.png" data-rawwidth="610" data-rawheight="391"&gt;&lt;p&gt;咦？这不是海洋球么→_→，我书读得少你不要骗我？各位看官好眼力，这真的就是海洋球( T___T )。如果我们把一个球就看成一个人的话，球的中心就是类中心，直径就是他若干张照片带来的variance。理想情况我们当然认为每个人所代表的球应该差不多大，人与人之间也是能分开的，就跟海洋球一样。然后海洋球所在的池子呢，就是能容纳所有球的界。&lt;/p&gt;&lt;p&gt;梦做完了，醒醒，我翻了翻硬盘，也就有10W人的数据。现实总是那么骨感…，接下来发生的事情胖子可能会不太爱听,因为我们只有10W人的数据，但是池子还是很大，如果我们用普通的方法训练的话，海洋球就会填不满这个池子。最简单的解决方案就是，我们从50亿个海洋球，变成10W个足球(估计还是不够，没准得用月球( ¯ □ ¯ ))，对，你没听错，通过变成吃货长胖这种方式我们把损失49亿多海洋球弟兄的空间填满了。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-187c3319a609ab98627a8dfebf571366.png" data-rawwidth="621" data-rawheight="495"&gt;看官可能会问，填满了不就填满了么？有什么问题么？请大家注意，虽然那49亿多个兄弟不在你的训练集里面，但是有可能在你的测试集哟。所以当以前的两个离得比较近的海洋球兄弟进入到这个胖子的世界的时候(‘‘)(’’)， 这个新世界会要求新来的弟兄跟他们的国民一样胖，然后悲剧发生了。他们很有可能撞在了一起,以至于他们老妈都很有可能都分不清谁是谁(因为在小数据情况下的可分行并不能推广到全集)。如图三，如果用所有数据训练是可分的话，只用部分数据训练因为变胖问题，会使得两个不同的identity出现碰撞风险。从而会造成错误接受和错误拒绝。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-e713c59543cf38a3ece21dde30419add.png" data-rawwidth="617" data-rawheight="300"&gt;那么请问，什么样的方式才合理呢？鄙人认为，我心目中比较合理的海洋球摆放画风应该是这样的（7颗海洋球用来召唤神龙 (╯-_-)╯）：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-191ea6d0f8c0c02829610214d306308d.png" data-rawwidth="378" data-rawheight="229"&gt;&lt;p&gt;該画风的满足的条件就一个，那10W海洋球兄弟不允许吃胖。这样剩下来的空间就能够给他们每人一个大院子，这样一旦有新的朋友(49亿多个兄弟)来玩耍的时候，不至于撞得他们老妈都分不清楚是谁(至少从概率上极大的降低了)。&lt;/p&gt;&lt;p&gt;不好意思，主持人，故事讲得太爽，忘记我们是来讲一篇paper的了 (╯-_-)╯╧╧  。今天我们要讲的文章名字叫做A Discriminative Feature Learning Approach for Deep Face Recognition. 在放公式驱散一票读者之前，我觉得还是放点图来缓和一下作者和读者之间的紧张关系。论文中提出我们之前训练出来的features只是可分而已(下图左)，如果要得到一个判别性比较高的features应该是右边这种样子的。用我们的胖子理论，不好意思应该叫海洋球理论，大家就可以知道，并且理解为什么右图的特征更合理。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-61a0ceabc735c788bbe70785bcd36e01.png" data-rawwidth="592" data-rawheight="319"&gt;我们用论文中的另一个图来表达，海洋球理论的论据。我们在图6的左上图上画了三条线来模拟feature之间的角度(因为人脸verification基本上就是cosine距离)，我们可以看到，如果每一个类别吃的太胖，可能会导致他的类内距离可能比类间距离大，从而……你懂的(假分很高)。如果我们引入论文中的center loss并且给一个比较高的weight的时候，比如右下图，我基本找不到反例了。同时新的未知类别进来的时候也有更大的概率会和其他的类别分开(包含未知类和已知类)。在这center loss起到的作用就是减肥剂，怎么让一个比较胖的类内数据分布，变瘦，从而使得他有足够的院子容纳新的客人。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-a63a4a309c2b7d8915212e7fa885b760.png" data-rawwidth="660" data-rawheight="480"&gt;以上文字给初学者品尝，接下来的文字将会上公式，请有耐心的初学者和大牛们接着往下看。整个softmax+center loss的定义如下&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-78038563ca566749fc6d0d84852256d3.png" data-rawwidth="619" data-rawheight="152"&gt;&lt;p&gt;我基本不太想解释一遍…….左边是softmax，右边嘛，就是约束当一个样本看到自己的类中心的时候，走上去打个招呼。这样才能瘦。主持人快来…..我编不下去了…..&lt;/p&gt;&lt;p&gt;可能大家会问了，为什么需要右边的约束才能达到这个效果呢？softmax为什么达不到这个效果？或者说softmax有啥缺点？&lt;/p&gt;&lt;p&gt;昂，简单来说就是伟大的softmax管杀不管埋。不对，应该说是只许自己胖，不准别人胖。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-34baa2b41cb5c124b8869739b5030090.png" data-rawwidth="506" data-rawheight="71"&gt;&lt;p&gt;各位看官请看softmax的更新函数，如果出现input的条件概率分数比较低的情况，他就会告诉所有的参数，\theta_j。你们碍着我长胖了，离我远点，然后乘机偷偷的向着input方向长胖一点(不改其吃货本质)。所以每个类都这么流氓的结果，就是大家都变成胖子，每个胖子的鼻子都贴着另一个胖子。&lt;/p&gt;&lt;p&gt;写到这本来想停笔了，但是wanda同学提了个问题。这玩意和contrastive loss和triplet loss之间有什么区别或者是联系没有呀？&lt;/p&gt;&lt;p&gt;上面的公式可以看成softmax + contrastive loss只不过只提供一个类内样本作为pair，而这个类内样本就是整个类的类中心。这在一定程度上解决了contrastive loss坑爹的采样问题，大家都知道contrastive loss需要坑爹的采样很多pair对，一不小心还会搞得positive pair和negative pair严重imbalance。&lt;/p&gt;&lt;p&gt;尽管triplet一定意义上解决了这个imbalance的问题，带来的后果是更复杂的采样策略。导致实际在使用过程中的trick实在太多，什么semi-hard example mining等等。Center loss相当于拆解了contrastive loss的两种情况，positive pair和negative pair。Center loss相当于把每一个进来的样本都自动配了一个positive sample(也就是他们类中心)， negative pair的inter-class separation则是是通过softmax来完成的，就是那么的简单粗暴有效。Wanda童鞋已经重复出很positive的结果并提供了mxnet code &lt;a href="https://github.com/pangyupo/mxnet_center_loss" data-editable="true" data-title="GitHub - pangyupo/mxnet_center_loss: implement center loss operator for mxnet" class=""&gt;GitHub - pangyupo/mxnet_center_loss: implement center loss operator for mxnet&lt;/a&gt;，欢迎试用。&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" class="" data-editable="true" data-title="@果果是枚开心果."&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-7d65c67344405c90976ad204ce1525bf.png" data-rawwidth="119" data-rawheight="123"&gt;&lt;p&gt;&lt;b&gt;祝浩(皮搋子狐狸)，&lt;/b&gt;3M Cogent Beijing R&amp;amp;D 高级算法工程师。本硕分别毕业于哈尔滨工业大学机械专业和北京师范大学计算机专业，并于2012年加入3M。14年拿到NICTA Scholarship 及其 top-up Scholarship，然后老板跑路。熟悉计算机视觉，神经影像和医学图像处理。在各种相关国际会议期刊上发表论文10余篇。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325602&amp;amp;idx=1&amp;amp;sn=9bc6071bf62a4ccdde1df4b4c5ae39f4&amp;amp;chksm=f235a568c5422c7ed7993b9d3b46bcd979d360605511284e4363a4a92f845019795569f8f3e9&amp;amp;scene=4#wechat_redirect" data-editable="true" data-title="【Technical Review】ECCV16 Center Loss及其在人脸识别中的应用" class=""&gt;【Technical Review】ECCV16 Center Loss及其在人脸识别中的应用&lt;/a&gt;\&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23340343&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Tue, 01 Nov 2016 10:25:47 GMT</pubDate></item><item><title>【高手之道】海康威视研究院ImageNet2016竞赛经验分享</title><link>https://zhuanlan.zhihu.com/p/23249000</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-4d69362ea1ffae8644165644861e0a7d_r.jpg"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;b&gt;摘要&lt;/b&gt;&lt;p&gt;海康威视研究院独家授权，分享ImageNet2016竞赛Scene Classification第一名，Object Detection第二名，Object Localization第二名，Scene Parsing第七名背后的技术修炼之道。下面为大家介绍海康威视研究院在本次ImageNet2016竞赛中的相关情况。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-4d69362ea1ffae8644165644861e0a7d.jpg"&gt;团队成员和分工如下：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-7f654278ede90b2148c14eae50088d41.jpg"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-e87993f06ae27890a1b57de718cda0bc.jpg"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-c0aa98650925e3ae481f6f6ac2b80863.jpg"&gt;数据增强对最后的识别性能和泛化能力都有着非常重要的作用。我们使用下面这些数据增强方法。第一，对颜色的数据增强，包括色彩的饱和度、亮度和对比度等方面，主要从Facebook的代码里改过来的。第二，PCA Jittering，最早是由Alex在他2012年赢得ImageNet竞赛的那篇NIPS中提出来的. 我们首先按照RGB三个颜色通道计算了均值和标准差，对网络的输入数据进行规范化，随后我们在整个训练集上计算了协方差矩阵，进行特征分解，得到特征向量和特征值，用来做PCA Jittering。第三，在图像进行裁剪和缩放的时候，我们采用了随机的图像差值方式。第四， Crop Sampling，就是怎么从原始图像中进行缩放裁剪获得网络的输入。比较常用的有2种方法：一是使用Scale Jittering，VGG和ResNet模型的训练都用了这种方法。二是尺度和长宽比增强变换，最早是Google提出来训练他们的Inception网络的。我们对其进行了改进，提出Supervised Data Augmentation方法。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-0e270114ad733a4ea09d978803ddc0a5.jpg"&gt;尺度和长宽比增强变换有个缺点，随机去选Crop Center的时候，选到的区域有时候并不包括真实目标的区域。这意味着，有时候使用了错误的标签去训练模型。如图所示，左下角的图真值标签是风车农场，但实际上裁剪的区域是蓝天白云，其中并没有任何风车和农场的信息。我们在Bolei今年CVPR文章的启发下，提出了有监督的数据增强方法。我们首先按照通常方法训练一个模型，然后用这个模型去生成真值标签的Class Activation Map（或者说Heat Map）, 这个Map指示了目标物体出现在不同位置的概率. 我们依据这个概率，在Map上随机选择一个位置，然后映射回原图，在原图那个位置附近去做Crop。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-cd9bbcf8f62066004ba20b2881c5d598.jpg"&gt;如图所示，对比原始的尺度和长宽比增强变换，我们方法的优点在于，我们根据目标物体出现在不同位置的概率信息，去选择不同的Crop区域，送进模型训练。通过引入这种有监督的信息，我们可以利用正确的信息来更好地训练模型，以提升识别准确率。 (+0.5~0.7)&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-4c54814d715fe5d86a4ca3ec90e9add4.jpg"&gt;场景数据集有800万样本，365个类别，各个类别的样本数非常不平衡，有很多类别的样本数达到了4万，也有很多类别的样本数还不到5000。这么大量的样本和非常不均匀的类别分布，给模型训练带来了难题。在去年冠军团队的Class-Aware Sampling方法的启发下，我们提出了Label Shuffling的类别平衡策略。在Class-Aware Sampling方法中，他们定义了2种列表，一是类别列表，一是每个类别的图像列表，对于365类的分类问题来说，就需要事先定义366个列表，很不方便。我们对此进行了改进，只需要原始的图像列表就可以完成同样的均匀采样任务。以图中的例子来说，步骤如下：首先对原始的图像列表，按照标签顺序进行排序；然后计算每个类别的样本数量，并得到样本最多的那个类别的样本数。根据这个最多的样本数，对每类随机都产生一个随机排列的列表；然后用每个类别的列表中的数对各自类别的样本数求余，得到一个索引值，从该类的图像中提取图像，生成该类的图像随机列表；然后把所有类别的随机列表连在一起，做个Random Shuffling，得到最后的图像列表，用这个列表进行训练。每个列表，到达最后一张图像的时候，然后再重新做一遍这些步骤，得到一个新的列表，接着训练。Label Shuffling方法的优点在于，只需要原始图像列表，所有操作都是在内存中在线完成，非常易于实现。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-b9fb06d90cc5074e4926a91fbbf7e80e.jpg"&gt;我们使用的另外一个方法是Label Smoothing，是今年Google的CVPR论文中提出来的方法。根据我们的混淆矩阵（Confusion Matrix）的分析，发现存在很多跨标签的相似性问题，这可能是由于标签模糊性带来的。所以，我们对混淆矩阵进行排序，得到跟每个标签最相近的4个标签，用它们来定义标签的先验分布，将传统的 one-hot标签，变成一个平滑过的soft标签。通过这种改进，我们发现可以从某种程度上降低过拟合问题。(+0.2~0.3)&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-c88ff71ef491bff70aa1f0dbd5fcd1d0.jpg"&gt;这边还有一些其他的技巧来提升性能，我们将其总结成一个原则：训练和测试要协调。在训练的时候，我们通常都需要做数据增强，在测试的时候，我们通常很少去做数据增强。这其中似乎有些不协调，因为你训练和测试之间有些不一致。通过我们的实验发现，如果你在训练的最后几个epoch，移除数据增强，然后跟传统一样测试，可以提升一点性能。同样，如果训练的时候一直使用尺度和长宽比增强数据增强，在测试的时候也同样做这个变化，随机取32个crop来测试，也可以在最后的模型上提升一点性能。还有一条，就是多尺度的训练，多尺度的测试。另外，值得指出的是，使用训练过程的中间结果，加入做测试，可以一定程度上降低过拟合。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-2ea607bca495fb43e5b476d482f9eca7.jpg"&gt;对于模型结构，没什么特别的改进，我们主要使用了Inception v3和Inception ResNet v2，以及他们加深加宽的版本。还用到了Wide ResNet 。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-38097e389268d90cfdf3ebb0e9e27b76.jpg"&gt;此次竞赛的语义分割任务非常具有挑战性。它一方面需要目标整体层面的信息，同时还需要每个像素的分类准确率。目前有很多语义分割的模型，但哪一种框架是最好的仍然是一个问题。我们设计了一个Mixed Context Network（MCN），它由一系列Mixed Context Blocks（MCB）堆叠而成。如图所示，每个MCB包括两个并行的卷积层：1个1x1卷积和1个3x3 Dilated卷积。Dilated卷积的采样率分别设置成了1,2,4,8,16。除了MCN之外，我们还在最后设计了一个Message Passing Network（MPN），来增强不同标签之间的空间一致性。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-d6f17746ab58e2aa71150fc1221894ee.jpg"&gt;CRF as RNN可以被加到网络的最后，与CNN一起联合训练。但是CRF as RNN比较耗费显存，尤其是在类别数比较大的时候（比如ADE20K有150类）。可以通过降低输入图像的分辨率来节省显存，但是这样做也会带来一些负面影响。为了解决这个问题，我们引入了一个新的比较省显存的模块，叫做MPN。在MPN中，我们首先将Score Map的通道从150降到了32，然后接了一个Permutohedral卷积层，用于做高维的高斯滤波。我们去掉了其中的平滑项，仅仅将1x1卷积层的特征和Permutohedral卷积层的特征连接，然后接一个3x3的卷积。实验证明，这样的结构也能较好地工作。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-93d428d5b2551202c7a487f04bcfadda.jpg"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-95865b813291bec665c1c8130cdf13e9.jpg"&gt;我们的检测和定位，都是基于Faster-RCNN这个框架。图中我们列出了所有用到的技巧。有很多技巧在以前的文献中都可以找到，比如多尺度的训练和测试，难样本挖掘，水平翻转和Box Voting。但我们自己也做了很多新的改进，比如样本均衡，Cascade RPN，预训练的Global Context等。至于网络结构，我们仅仅用了三个ResNet-101模型。一个来自于MSRA，一个来自于Facebook，还有一个是我们自己训练的Identity Mapping版的ResNet-101。我们最好的单模型结果，是源自我们自己训练的Identity Mapping版的ResNet-101。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-be5c4d4759d55c5271acadc3ad2d588b.jpg"&gt;我们设计了一个轻量级的Cascade RPN。2个RPN顺序堆叠在一起。 RPN 1使用滑窗Anchors，然后输出比较精确定位的Proposals。RPN 2使用RPN 1的Proposals作为Anchors。我们发现这个结构可以提升大中尺寸Proposals的定位精度，但不适合小的Proposals。所以在实际中，我们RPN1提取小的Proposals，RPN2提取大中尺寸的Proposals。&lt;b&gt;注:&lt;/b&gt; Proposals尺寸的阈值是64 * 64。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-a5be0c98a1e6829652fc15c81a960e9f.jpg"&gt;另外一个改进就是限制正负Anchor的比例。在传统的RPN中，Batch Size通常是256，理想的正负Anchor比例是1。但是在实际使用中，这个比例往往会很大，通常会大于10。所以我们缩小了Batch Size，控制最大的比例为1.5。最小的Batch Size设置为32。对比实验表明，使用Cascade RPN和限制正负Anchor比例这两个策略，在ImageNet DET的验证集上，AR提升了5.4个点，Recall@0.7提升了9.5个点，而Recall@0.5只提升1个点。这说明Proposals的定位精度得到了显著的改善。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-4932aca2e45c04c8e55ef6de41059980.jpg"&gt;Global Context在去年Kaiming的论文中就已经提到，他们使用这个方法得到了1个点的mAP提升。我们也实现了自己的Global Context方法：除了在在RoI上做RoIPooling 之外，我们还对全图做了RoIPooling来获得全局特征。这个全局特征仅仅被用来分类，不参加bbox回归。我们实验发现，Global Context如果使用随机初始化，其性能提升有限。当我们采用预训练的参数进行精调之后，发现mAP的性能可以提升3.8个点。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-6ca5a27eebe9f4d573a3146a3e9b0a52.jpg"&gt;此外，我们发现，在1000类的LOC上预训练，然后再在DET数据上精调，可以 得到额外0.5个点的mAP提升。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-02d93f5ba2e2fd93a492a58591bee613.jpg"&gt;平衡采样是去年场景分类任务中所用到的一个技巧。我们也将它用来做检测任务。左侧是一个类别的列表，对于每个类别，我们又创建了一个图像列表。训练过程中，我们先从类别列表中选择一个类别，然后从这个类对应的图像列表中采样。和分类任务不同的是，检测任务中一张图像可能包含多个类别的目标。对于这种多标签的图像我们允许它们出现在多个类别的图像列表中。使用平衡采样技术，可以在VOC2007数据集上获得0.7的mAP提升。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-3b9ec5101b7a2248b03ed42135553ad0.jpg"&gt;集成上述所列的各项技术，我们的检测模型取得了SOTA的性能。在ImageNet DET任务中，我们以65.3的mAP获得了第二名。就单个模型而言，我们的模型能以少许优势排名第一。我们使用了相同的检测框架来完成ImageNet LOC任务。在最后的竞赛中，我们以8.7的定位误差排名第二。在PASCAL VOC 2012检测任务中，我们单模型获得了87.9的 mAP，超过了去年Kaiming的模型有4个点之多。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-4420bb556529bc294f99afd3c5109de9.jpg"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-208d59b4cfdcdfd80351d362dd8f5bfc.jpg"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-baa92663a10c48bbb763f36641ca58d8.jpg"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-6f7ff39710d8ee4f38841cceec4d15d2.jpg"&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" data-title="@果果是枚开心果." class="" data-editable="true"&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;海康威视研究院 &lt;/b&gt;是海康威视最重要的核心部门，主要致力于基础技术和前沿技术的探索和创新，在视音频编解码、视频图像处理、视频智能分析、云计算、大数据、云存储、人工智能等方面有深厚的技术积累，为海康威视核心产品和新兴业务拓展提供了有力的支撑，成为公司主营业务和创新业务发展的重要驱动力。研究院在KITTI、MOT、Pascal VOC、ImageNet等世界级人工智能竞赛中均获得过第一的好成绩。欢迎各位老师、学者、专家及其他业内人士，莅临杭州，参观交流。技术探讨、访问交流、求职招聘以及其他相关事宜，欢迎邮件联系谢迪博士：xiedi@hikvision.com。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325586&amp;amp;idx=1&amp;amp;sn=69a7e8482d884dda869290581a50a6d6&amp;amp;chksm=f235a558c5422c4eabe75f6db92fd13a3ca662d648676461914c9bfd1f4e22affe12430afce3&amp;amp;scene=0#wechat_redirect" data-editable="true" data-title="【高手之道】海康威视研究院ImageNet2016竞赛经验分享" class=""&gt;【高手之道】海康威视研究院ImageNet2016竞赛经验分享&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23249000&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Thu, 27 Oct 2016 18:09:30 GMT</pubDate></item><item><title>IJCAI16论文速读：Deep Learning论文选读（上）</title><link>https://zhuanlan.zhihu.com/p/23037608</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-1d697eb1755f2db92d1ba82e202d4ad1_r.png"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;b&gt;摘要&lt;/b&gt;&lt;p&gt;选读两篇IJCAI2016的深度学习论文，IBM东京研究院提出的基于模型性能预测的深度网络超参数快速选择算法和厦门大学纪荣嵘组提出的基于全局误差重构的深度网络模型压缩方法。&lt;/p&gt;&lt;p&gt;&lt;b&gt;IJCAI16会议介绍：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;国际人工智能联合会议（ International Joint Conference on Artificial Intelligence，IJCAI ）是聚集人工智能领域研究者和从业者的盛会，也是人工智能领域中最主要的学术会议之一。1969 年到 2015 年，该大会在每个奇数年举办，现已举办了 24 届。随着近几年来人工智能领域的研究和应用的持续升温，从 2016 年开始，IJCAI 大会将变成每年举办一次的年度盛会；今年是该大会第一次在偶数年举办。第 25 届 IJCAI 大会于 7 月 9 日- 15 日在纽约举办。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Guest Editor导读：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本届会议的举办地在繁华喧嚣的纽约时代广场附近，正映衬了人工智能领域几年来的火热氛围。此次大会包括7场特邀演讲、4场获奖演讲、551篇同行评议论文的presentation，41场workshop、37堂tutorial、22个demo等。深度学习成为了IJCAI 2016的关键词之一，以深度学习为主题的论文报告session共计有3个。本期我们从中选择了两篇篇深度学习领域的相关论文进行选读，组织了相关领域的博士研究生，介绍论文的主要思想，并对论文的贡献进行点评。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Weight Features for Predicting Future Model Performance of Deep Neural Networks&lt;/b&gt;&lt;/p&gt;&lt;p&gt;IBM东京研究院的研究者研究了一个有趣且非常实用的问题，在深度学习的调参中如何快速的丢弃一些超参数的组合来加速调参。传统的方法只利用了网络训练过程中的learning curve（不同训练epoch的模型在测试集上最终性能构成的曲线）, 却没有考虑网络模型参数与最终模型性能的相关性。本文提出利用网络训练阶段的参数作为特征，采用random forest学习回回归函数，直接建模参数和参数变化与最终模型性能的关系，预测网络的最终性能。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-53856b1a0b3b52b2301e9bf79063082b.png"&gt;&lt;b&gt;方法框架：&lt;/b&gt;特征设计和性能预测函数。特征设计部分参考了手工描述子的设计方式，比如对卷积核权重提取了等均值、方法、峰度、散度等特征，性能预测函数部分则直接使用了随机森林算法。通过在已有的learning curve上训练分类器器，可以在其他超参数组合训练的早期（比如前10个epoch）预测其最终模型的性能。&lt;/p&gt;&lt;p&gt;部分实验设计如表1，给出了ImageNet数据集上的超参数搜索空间。需要指出的是，在实践中，卷Act. func会默认为ReLU，大量实验表明ReLU效果更好。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-dbea167900f8c169f93ce9ab03c41a79.png"&gt;下图所示为在三个数数据集的实验，虽然相比learning curve的方式，该方法的性能有显著的提高，但是分类正确率或者top-1错误率估计的均方误差（RMSE）还是达到0.13，这对于锱铢必较的细粒度调参来说暂时还达不到实用程度。但是一定意义上，通过文中方法early stop掉一些明显最终性能预期不好的超参数组合对于加速超参数搜索依旧有其实用价值。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-994a66bd9479d9da4c5a2a87e1c5a55b.png"&gt;总结起来文章提供了一个通过模型参数来预测模型最高性能的方法，提供了超参数选择的一种思路。但是目前的实验结果尚不足以代精细化的调参，比如learning rate 0.01, 0.02….的细粒度调整。&lt;/p&gt;&lt;p&gt;本文方法的未来改进的空间在于由目前手工设计特征+分类器的两段式方法到非端到端的深度学习方法。此外，考虑实际应用中细粒度调参的需要也是本文方法的一个可能改进方向。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Towards Convolutional Neural Networks Compression via Global Error Reconstruction&lt;/b&gt;&lt;/p&gt;&lt;p&gt;大规模深度卷积神经网络，例如VGGNet面临参数量大和计算复杂度高两个问题，现有的网络压缩方法如low-rank分层分解会带来层间的累积误差问题，影响网络全局逼近的精度。厦门大学纪荣嵘组的这篇论文设计了一种两段式网络压缩方法，首先对全连接层进行low–rank分解，接着引入了一个全局重构误差最小化的策略，通过最小化网络的重构误差来对压缩后的网络进行fine-tune，从而有效缓解了分层逼近带来的累积误差问题。&lt;/p&gt;&lt;p&gt;&lt;b&gt;方法框架：&lt;/b&gt;全连接层low-rank分解的方法图示如下，全连接矩阵W被分解为矩阵P和Q。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-cc0ffdcf5461eddcc28c06e5dd1e4700.png"&gt;Low-rank分解的形式化如下：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-cade3cf7af2cd36b773668e5e82463f0.png"&gt;进一步的，基于全局误差最小化的优化目标，对low-rank分解后每一层的参数P1和Q1用误差反向传播算法进行更新。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-5bd08b55775087c43b14d05e41a6c7ff.png"&gt;在VGGNet上的实验结果如下图所示，文章提出的GER算法取得了state-of-the-art的性能。当然高压缩率情况下的GER明显的性能优势，理论上的意义更大一些，因为此时的性能损失已经太大。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-3e203dcb858aef895ce4816fee8b5c9b.jpg"&gt;从实用性的角度，单独压缩全连接层虽然可以显著减少参数，但是理论意义更大，原因有两点：1）实践中已经越来越少使用全连接层，例如ResNet中就没有全连接层（分类器层除外）。2）卷积层占了主要的计算量，虽然显著压缩了参数，但并不能明显改善速度。&lt;/p&gt;&lt;p&gt;本文的潜在优势在于，卷积层的运算实际上可以写成patch展开后的矩阵（Caffe中的Im2Col操作）和kernel matrix的矩阵相乘，也可以采用low-rank的方法逼近，因此本文方法如果扩展到卷积层，则可以直接降低卷积层计算量和参数量，这也是GER在实际问题中的潜力所在。&lt;/p&gt;&lt;p&gt;&lt;b&gt;参与人员：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;胡兰青，&lt;/b&gt;中科院计算所VIPL研究组博士研究生&lt;/p&gt;&lt;p&gt;&lt;b&gt;尹肖贻，&lt;/b&gt;中科院计算所VIPL研究组博士研究生&lt;/p&gt;&lt;p&gt;&lt;b&gt;刘昊淼，&lt;/b&gt;中科院计算所VIPL研究组博士研究生&lt;/p&gt;&lt;p&gt;&lt;b&gt;刘   昕，&lt;/b&gt;中科院计算所VIPL研究组博士研究生&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" data-editable="true" data-title="@果果是枚开心果."&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Guest Editor：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-50f36023f36d78ecb565c4cf89f5a509.jpg" data-rawwidth="118" data-rawheight="117"&gt;朱鹏飞，&lt;/b&gt;天津大学机器学习与数据挖掘实验室副教授，硕士生导师。分别于2009和2011年在哈尔滨工业大学能源科学与工程学院获得学士和硕士学位，2015年于香港理工大学电子计算学系获得博士学位。目前，在机器学习与计算机视觉国际顶级会议和期刊上发表论文20余篇，包括AAAI、IJCAI、ICCV、ECCV以及IEEE Transactions on Information Forensics and Security等。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325557&amp;amp;idx=1&amp;amp;sn=362d476d3b3820ea56e4672369565e4f&amp;amp;chksm=f235a53fc5422c2939f76b7e8f5265333f3159b0ec4275fe733d27e7a03f17395b0460a318d2&amp;amp;scene=0#wechat_redirect" data-editable="true" data-title="IJCAI16论文速读：Deep Learning论文选读（上）"&gt;IJCAI16论文速读：Deep Learning论文选读（上）&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23037608&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Tue, 18 Oct 2016 17:35:58 GMT</pubDate></item><item><title>深度学习在文本简化中的应用进展</title><link>https://zhuanlan.zhihu.com/p/22956057</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-c925d01a44f7371afa5589b02c3a013a_r.jpg"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;b&gt;背景与介绍&lt;/b&gt;&lt;p&gt;近年来，机器翻译任务依靠深度学习技术取得了重大突破。最先进的神经机器翻译模型已经能够在多种语言上超越传统统计机器翻译模型的性能。在传统统计机器翻译模型上积累深厚的谷歌，也终于开始将最新的神经机器翻译系统逐步上线。&lt;/p&gt;&lt;p&gt;目前神经机器翻译的技术基础是端到端的编码器-解码器架构，以源语言句子作为输入，目标语言同义句作为输出。容易想象，只要具备充足的训练数据，类似架构完全有可能推广到其他涉及文本改写的任务上。例如，输入一段文字，希望系统输出一小段核心语义不变、但更为简洁的表达。这样的改写统称为文本简化（text simplification）。近两年深度学习技术应用相对较多的是其中的一个实例，在自然语言生成研究中一般称为语句压缩（sentence compression）或语句简化（sentence simplification），即输入和输出均为语句。语句简化任务要求神经网络结构能够编码输入句中的核心语义信息，才能够提炼出不改变原句主要意义的更简洁表达。&lt;/p&gt;&lt;p&gt;深度学习技术在语句简化上的一个典型应用是新闻标题生成（headline generation）。新闻文章通常具有较为规范的写作形式：文章首句或者首段对新闻内容进行概括介绍，新闻标题则进一步精炼概括出新闻核心事件。目前基于深度学习技术的新闻标题生成研究中，一般以新闻文章的首句作为输入，生成该新闻文章的标题。现有的基于深度学习的新闻标题生成工作通常采用和神经机器翻译类似的编码器-解码器架构，一般不需要手动提取特征或语句改写文法。最常见的仍然是序列到序列（sequence-to-sequence, 简记为seq2seq）模型。典型的序列到序列模型如图1所示。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-62cb9e88d7414159a64a094b1c652b65.jpg" data-rawwidth="666" data-rawheight="198"&gt;&lt;p&gt;在图1中，以文本序列A-B-C-&amp;lt;EOS&amp;gt;输入给一个编码器，编码器用于将输入文本编码成一个语义向量表达（图中对应第四个节点处的隐状态）。输入序列的语义向量表达进而交由解码器用于目标文本序列W-X-Y-Z-&amp;lt;EOS&amp;gt;的生成。这里的&amp;lt;EOS&amp;gt;表示序列结束符号（end of sequence），标志着输入序列或输出序列的结束。解码器接收&amp;lt;EOS&amp;gt;符号后，开始解码的过程，直到生成&amp;lt;EOS&amp;gt;符号标志着解码过程的结束。序列到序列模型是一个按照字符（或单词）序列逐个处理的过程，编码过程中编码器逐个接收输入字符，解码过程中解码器逐个输出生成的字符。在最原始的模型训练过程中，解码器每次接收答案序列中的一个字符（例：W），预测应该输出的下一个字符（例：X）。编码器-解码器架构的经典训练目标，是在给定编码器输入后，使解码器输出的结果能够最大程度地拟合训练集中的答案，在概率模型下即最大化数据似然。在模型预测阶段，答案序列未知，解码器接收&amp;lt;EOS&amp;gt;作为解码开始符，并生成一个输出字符，然后将模型预测出的输出字符作为解码器的下一个输入，重复这个过程直到解码器生成&amp;lt;EOS&amp;gt;符号为止。预测阶段的一般目标是，给定输入句编码后，根据当前模型选择概率最大的解码器输出结果。精确搜索这个最优解一般复杂度极高，所以在实际应用中解码过程通常应用集束搜索（beam search，也可译作柱搜索）近似求解：在每一步保留K个最高得分的输出，最后从K个输出结果中选择得分最高的作为最终的输出。&lt;/p&gt;这样的编码器-解码器模型一般可以处理变长的输入和输出序列，使得它可以被应用于多种文本改写任务上。形式上，给定一个包含M个词的输入文本序列x={x1,x2,…,xM}，在模型中将每个词xt表示成一个向量。词的向量表示会在模型中进行学习，可以用无监督训练得到的一般word embedding向量作为初始化。语句简化的目标是生成输入句x的一个简化y={y1,y2,…,yN}，一般要求y的长度比输入句更短，即N&amp;lt;M。标题生成的目标是寻找y ̂使得给定x的条件下y的条件概率最大化，即：y ̂=arg⁡maxy⁡〖&lt;em&gt;P&lt;/em&gt;(y|x;&lt;em&gt;θ&lt;/em&gt;)〗 ，其中&lt;em&gt;θ&lt;/em&gt;代表需要学习的模型参数。条件概率&lt;em&gt;P&lt;/em&gt;(y|x;&lt;em&gt;θ&lt;/em&gt;)可以由链式法则分解为：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-81b9e8cc835358a137d9cff5113448e0.jpg" data-rawwidth="468" data-rawheight="90"&gt;编码器一般能够处理长度不确定的输入文本序列，将每个词的词向量表示汇总，编码成一个定长的输入文本向量表示。这个编码过程可以采用不同的编码器，如卷积神经网络（CNN），循环神经网络（RNN）等。而解码的过程是根据输入文本序列生成输出文本序列的过程，在大多数模型中，解码器使用的是RNN，常用的RNN节点包括标准的RNN单元以及像LSTM、GRU这样记忆能力更强的带门限单元等。RNN对序列中的每一个单元执行相同的运算过程，从而可以接受任意长的序列作为输入。具体来说，一个标准的RNN以及其按照输入序列展开形式如图2所示。在图2中，xi是第i个输入词语，hi是接收xi之后RNN隐单元的状态。hi+1基于前一个隐状态hi和当前的输入xi+1得到，即hi+1=&lt;em&gt;f&lt;/em&gt;(Uxi+1+whi)。&lt;em&gt;f&lt;/em&gt;是非线性函数，如tanh或者sigmoid。标准的RNN单元在每一步输出yi+1=g(Vhi+1)，g是非线性函数。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-2f4d30016a8f4c3d31cb0e61baf615bb.jpg" data-rawwidth="683" data-rawheight="252"&gt;在序列到序列模型中，如果选用RNN作为编码器，这一部分RNN的输出（yi）一般被忽略；而RNN作为解码器时，每一步输出yi+1对应规模为V的词表上所有词语的概率分布（通常选用softmax函数将V维得分向量标准化得到），产生yi+1的过程依赖于前一步状态hi以及前一步的输出yi。解码过程中，生成单词yi+1的方法是 :&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-8572518c594e5d77b3f69c3e6c2aa242.jpg" data-rawwidth="544" data-rawheight="70"&gt;早期的编码器-解码器模型中，要求编码器结构的最后一个单元能很好地保留输入文本的信息编码。而在实际应用中，这样的定长文本编码并不一定能够捕捉输入句的所有重要信息，尤其是在输入文本较长的情况下。为解决这个问题，有研究工作(Bahdanau et al., 2015)在序列到序列神经机器翻译模型中引入了“注意力”（attention）机制，用于在生成目标文本序列的过程中，为生成每个目标词确定一个有注意力偏差的输入文本编码，使得模型可以学习输出序列到输入序列的一个软对齐（soft alignment）。注意力机制的主要思想是：在每一步生成不同的yi+1时，侧重使用编码器中对应x的不同部分的隐状态信息，即使用编码器中各隐状态ht的加权和作为生成时所需要考虑的“上下文 ”：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-59bd1aaa484e98085d206bdba7ab4b20.jpg" data-rawwidth="219" data-rawheight="50"&gt;通过为生成不同的目标单词学习不同的&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-4fcc6bc925b566047f510f202c4774bc.jpg" data-rawwidth="96" data-rawheight="40"&gt;分布，使得生成不同单词时解码器可以将“注意力”集中在不同的输入词语上。注意力权值&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-4fcc6bc925b566047f510f202c4774bc.jpg" data-rawwidth="96" data-rawheight="40"&gt;可以有多种不同的计算方法，一种常见的实现方法考虑编码器每个隐状态ht和解码器生成词语yi+1时的隐状态hyi+1的相近程度（内积），将权值定义为：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-f0830c3bb70f325b7591b924b1e0e534.jpg" data-rawwidth="331" data-rawheight="86"&gt;&lt;b&gt;初探与进展&lt;/b&gt;&lt;p&gt;基于编码器-解码器架构和注意力机制的序列到序列学习模型最初用于神经机器翻译，但原理上可以直接照搬应用于标题生成(Lopyrev, 2015; Hu et al., 2015)。甚至不采用注意力机制的多层LSTM-RNN编码器-解码器也在一般基于词汇删除的语句压缩任务上取得了一定效果(Filippova et al., 2015)。而神经网络方法在语句简化、标题生成任务上最早的应用中，比较著名的当属Sasha Rush组的相关工作(Rush et al., 2015)。虽然同样是一种编码器-解码器神经网络，但在具体的架构设计上和基于RNN的序列到序列学习有一定差异。&lt;/p&gt;&lt;p&gt;这个工作中对&lt;em&gt;P&lt;/em&gt;(y│x;&lt;em&gt;θ&lt;/em&gt;)应用了C阶马尔科夫假设，即生成一个新的词语yi+1时只依赖于之前&lt;em&gt;C&lt;/em&gt;个已经生成的词语yc=y[i-C+1,…,i]，同时对&lt;em&gt;P&lt;/em&gt;(y│x;&lt;em&gt;θ&lt;/em&gt;)求对数将其分解为求和形式：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-e2094d432cd68c1b0d02aeec789b8be7.jpg" data-rawwidth="380" data-rawheight="79"&gt;局部概率P(yi+1|yc,x;&lt;em&gt;θ&lt;/em&gt;)定义为一个前馈神经网络：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-d8d31ca83c5b1baa80f286ae4a64445f.jpg" data-rawwidth="389" data-rawheight="50"&gt;其中隐状态h由上下文yc的嵌入表示y ̃c通过一层变换得到：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-72d3aee9a95460ca2c2e4d3df1e3bf21.jpg" data-rawwidth="222" data-rawheight="71"&gt;而enc是以x作为输入的编码器模块。文中尝试了三种不同的编码器，分别为词袋模型编码器enc1、卷积编码器enc2和注意力机制的编码器enc3。整个模型架构比较简单，如图3(a)所示。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-3519982a1ed1ad305ed9a13168739252.jpg" data-rawwidth="348" data-rawheight="239"&gt;词袋模型编码器和卷积编码器不考虑注意力机制。词袋模型enc1定义为词向量的简单平均。记每个输入词xi为one-hot表示，左乘词向量编码矩阵F可以得到其对应的word embedding，整个词袋模型编码器可以写作：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-6d8f4a9ff94e7d53642e61d86b902bad.jpg" data-rawwidth="248" data-rawheight="108"&gt;其中p∈[0,1]M是输入词语上的均匀分布，词袋模型编码器要学习的参数只有词向量编码矩阵F。这个编码器忽略输入词语序列的顺序和相邻词之间的关系。&lt;/p&gt;&lt;p&gt;卷积编码器enc2对词袋模型编码器不考虑词语之间关系的特点进行了改进，采用了一种标准的时延神经网络（time-delay neural network, TDNN），使得编码器可以利用词语之间的局部关系。这个编码器包含L层，每层主要由1维的卷积过滤器Q和max-pooling操作构成：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-fffce88554d2e3a6f83b7f835996a74f.jpg" data-rawwidth="624" data-rawheight="212"&gt;而enc3将注意力机制引入到词袋模型编码器中，使得enc3对x进行编码的过程中利用到之前&lt;em&gt;C&lt;/em&gt;个已经生成的词语作为上下文yc。用G表示上下文编码矩阵，模型结构如图3(b)所示，形式上写作：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-7e78c607d56cfa551d72a8e1d9ba3f39.jpg" data-rawwidth="309" data-rawheight="237"&gt;模型训练使用批量随机梯度法最小化训练数据&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-70a742fb8d1b8b29d92474e7c64ca410.jpg" data-rawwidth="155" data-rawheight="48"&gt;的负对数似然：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-62884515d483bf277d1168b9edb2677f.jpg" data-rawwidth="618" data-rawheight="88"&gt;使用动态规划（Viterbi算法）精确求解这个问题的时间复杂度为&lt;em&gt;O&lt;/em&gt;(NVC)，而词汇表大小&lt;em&gt;V&lt;/em&gt;一般较大。前文已经提到，实际应用中一般可以采用集束搜索近似求解，即在生成每一个yi的时候都只保存当前最优的&lt;em&gt;K&lt;/em&gt;个部分解，之后仅从这&lt;em&gt;K&lt;/em&gt;个部分解开始进行下一步生成。这样时间复杂度被降为&lt;em&gt;O&lt;/em&gt;(KNV)。&lt;/p&gt;&lt;p&gt;直觉上，人工语句简化时一般仍会保留一些原句中的词汇。一个好的语句压缩模型最好既能够逐个从词汇表&lt;em&gt;V&lt;/em&gt;生成目标压缩句中的词汇，又能够捕捉从原句中进行词汇抽取的过程。文中(Rush et al., 2015)给出了一个权衡“生成”和“抽取”的初步方案，称为抽取式调节（extractive tuning）。本质上就是经典统计机器翻译中的对数线性模型(Och and Ney, 2002)，通过线性加权将多个特征组合起来定义概率：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-d395719aa8b4849b6f0f1d748aab50db.jpg" data-rawwidth="374" data-rawheight="78"&gt;其中α为5维权向量，对应的5维特征f包括之前模型的概率估计，以及四个和输入句有关的示性函数特征（和输入句存在一元词、二元词、三元词匹配或调序）：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-b4aa0ef5d2424a3710fe7d403afc5dbf.jpg" data-rawwidth="445" data-rawheight="195"&gt;这样的分数定义在形式上仍然根据每一步&lt;em&gt;i&lt;/em&gt;来分解，所以不需要修改使用动态规划或者柱搜索进行解码的过程。而调节权向量α的过程也可以像经典统计机器翻译一样采用最小错误率训练（minimum error rate training, MERT）(Och, 2003)来完成。&lt;p&gt;这个工作完成时间相对较早，并没有使用最适合对序列数据建模的RNN结构。同研究组今年的后续工作(Chopra et al., 2016)中，将解码器由前馈神经网络替换为RNN，并改变了编码器结构：同时为输入词及其所在位置学习embedding，并用卷积计算当前位置上下文表示，作为解码过程中注意力权重计算的依据。最后得到的架构中不再需要前文所述的“抽取式调节”模块，成为更纯粹的端到端系统；在Gigaword数据集上的实验结果也取得了更优的性能。&lt;/p&gt;&lt;p&gt;基于神经网络的语句简化与标题生成后续也在不同方面取得进展。目前生成类任务训练指标主要为训练集数据的似然函数，但生成类任务的常用自动评价准则是ROUGE或BLEU，本质上大约相当于系统生成结果和参考答案之间关于n-gram（连续若干个词）的匹配程度。近期有工作尝试利用最小化风险训练（minimum risk training, MRT）思想(Och, 2003; Smith and Eisner, 2006)改进神经机器翻译，直接对BLEU值进行优化。这一策略在标题生成任务上也同样适用，只需用类似的方式去优化训练集生成结果的ROUGE值(Ayana et al., 2016)。具体而言，用∆(y',y)表示任务相关的实际损失函数，如标题生成任务中将其设为生成结果y'在参考答案y上计算的ROUGE值（这里表达为风险最小化问题，所以还需要取负）。训练目标是最小化期望风险：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-01be58118eb840a020563825d7008750.jpg" data-rawwidth="382" data-rawheight="63"&gt;最小化期望风险的一个好处在于：即使原本损失函数∆(y',y)是定义在离散结构上的离散函数，训练目标关于概率模型的参数也还是连续函数，所以仍然可以求导进行反向传播更新参数。然而，穷举所有可能产生的结果y’开销过大，并不可行。所以只在上面取一个显著抽样&lt;em&gt;S&lt;/em&gt;(x;&lt;em&gt;θ&lt;/em&gt;)来近似整个概率分布，并引入一个较小的超参数ϵ尝试让近似分布更为平滑：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-67e245db82e5cf184bd890d80a8e1eee.jpg" data-rawwidth="422" data-rawheight="59"&gt;实际上，如果固定超参数ϵ为1，这一近似计算最小化期望风险的做法就和强化学习早期工作中的REINFORCE算法(Williams, 1992)不谋而合。近期也有工作从REINFORCE算法出发，对随机初始化概率模型的做法进行改进，提出首先根据正确答案用交叉熵损失学习若干轮、得到较好的初始概率模型，然后利用退火机制逐步将训练过程转向REINFORCE算法(Ranzato et al., 2016)。实验表明，这些对训练目标的改进都可以显著改善自动评价指标所度量的性能。另一方面，原句中可能存在模型词汇表中所没有的词（out of vocabulary, OOV），尤其是很多专有名词，并不在生成词汇的范围&lt;em&gt;V&lt;/em&gt;之中。实现上为了降低解码复杂度，一般都会采用相对较小的词汇表。如果系统不能输出原句中的OOV词、仅能用&amp;lt;UNK&amp;gt;等占位符代替，显然有可能会造成关键信息损失。受指针网（pointer networks，一种输出序列中每个元素分别指向输入序列中元素的编码器-解码器网络）(Vinyals et al., 2015)启发，近期已有多个工作都不约而同地考虑了一种解决思路：在解码的过程中以一部分概率根据当前状态来生成、一部分概率直接从原句中抽取(Gu et al., 2016; Gulcehre et al., 2016; Nallapati et al., 2016)。&lt;/p&gt;&lt;p&gt;另一方面，如何利用其它任务数据作为辅助性监督信息也是一个正在被考虑的方向。例如今年有工作在同一个多层双向RNN网络中进行语句压缩、阅读视线预测（gaze prediction）、组合范畴文法（combinatory category grammar, CCG）超标注（supertagging）的多任务学习，使得语句压缩任务的性能得到改善(Klerke et al., 2016)。这几个任务在直觉上具有一定相关性，有机会起到相互强化的效果。&lt;/p&gt;&lt;p&gt;上面所介绍的架构都属于直接对条件概率&lt;em&gt;P&lt;/em&gt;(y│x;&lt;em&gt;θ&lt;/em&gt;)建模的判别式模型范畴。近期也有利用深层产生式模型来对语句压缩任务建模的工作。常见神经网络结构中，自编码器被广泛应用于表示学习和降维，将类似思想对文本数据建模自然也可能学习到更紧凑的表示。最近就有尝试在变分自编码器（variational auto-encoder, VAE）架构下得到语句压缩模型的工作(Miao and Blunsom, 2016)。关于一般VAE模型的详细信息本文不予赘述，感兴趣的读者可参考相关教程 (Doersch 2016)。原始的VAE可以将输入数据压缩到低维表示，而这个工作类比提出将输入的长句压缩为更紧凑的短句，得到如图4所示的自编码压缩模型。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-0c855811086e8412c0724c27381f2651.jpg" data-rawwidth="652" data-rawheight="326"&gt;用s和c分别记原始输入句和压缩句，整个模型包含两部分：(1) 压缩模型（图4左下部分虚线框，由编码器连接压缩器组成）为以s作输入、c作输出的推断网络qφ (c│s)，以及 (2) 重构模型（图4右上部分虚线框，由压缩器连接解码器组成）为基于压缩表示c重构原始输入句s的生成网络pθ (s│c)。为了让压缩句中仅使用原句中出现过的词，文中选用了指针网(Vinyals et al., 2015)作为压缩模型qφ (c│s)，同时将编码器设计为双向LSTM-RNN，压缩器使用带有注意机制的单向LSTM-RNN。而重构模型pθ (s│c)则直接使用经典的序列到序列结构，即带注意机制的RNN，以压缩器端的c作输入，从解码器端产生原句s。模型训练过程中需要对两组网络参数φ和θ进行更新。与最原始的VAE一样，只需要无标记数据作为输入，使用变分推断来优化数据对数似然的一个下界&lt;em&gt;L&lt;/em&gt;：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-562c3d1c7e5ad87db453a37c8f8a6573.jpg" data-rawwidth="477" data-rawheight="109"&gt;其中需要计算变分分布qφ (c│s)和一个先验语言模型p(c)的KL散度。本文讨论的任务是语句压缩，需要同时保证压缩句尽可能流畅和简洁，所以预训练了一个偏好短句的语言模型作为p(c)。&lt;/p&gt;&lt;p&gt;由于不易对从变分分布q中随机产生的值进行反向传播，原始VAE推断过程使用重参数化（reparameterization）技巧，将产生样本的随机性全部转移到一个辅助的噪声随机变量中，保持和参数直接相关的部分相对固定，从而可以通过对这些非随机部分求导进行反向传播参数更新。但自编码语句压缩模型处理对象为离散结构的文本，重参数化技巧不能直接使用。因此文中使用了前面提到的REINFORCE算法，根据一组随机采样的误差进行反向传播，近似最小化期望损失，并引入偏置项来降低梯度估计的方差。&lt;/p&gt;&lt;p&gt;VAE变分推断进行模型训练的效率十分依赖推断网络q（对应这个工作中的压缩模型部分）的梯度估计质量。为了在训练过程初期就能引导压缩模型产生较好的压缩结果，文中进一步提出另一个模型，称为强制注意力语句压缩（forced-attention sentence compression；图5），强制让注意力的学习和额外的有标记语句压缩数据更吻合。本质上是通过有监督训练来实现前面提到的一种语句简化策略：以一部分概率根据指针网直接从原句中抽词（对应图5中的α）、一部分概率根据当前状态来生成整个词汇表V中可能的词（对应图5中的β）。这样就可以引入语句简化任务的有标记平行语料，进行半监督学习。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-bbde2273886f03ea643d260a28a5a1dc.jpg" data-rawwidth="528" data-rawheight="296"&gt;&lt;b&gt;局限与展望&lt;/b&gt;&lt;/p&gt;&lt;p&gt;需要指出的是，对于任何涉及自然语言生成的任务而言，像ROUGE、BLEU那样基于局部单元匹配的自动指标并不能完全取代基于语义理解的人工评价。目前基于神经网络的相关工作几乎全部缺少人工对语义完整性、流畅度等关键指标的评分（这一点在相关论文的审稿环节理应有人指出；也有可能竞标这类论文的审稿人主要来自对神经网络了解甚于自然语言生成的研究人员）。所以不同方法的实际性能差异究竟有多少，其实尚不明确。&lt;/p&gt;&lt;p&gt;细心的读者可能已经注意到，虽然本文介绍的相关文献标题中有些包含“语句摘要（sentence summarization）”甚至“文本摘要（text summarization）”这样的字眼，但我们在本文的描述中尚未开始使用“摘要”一词。因为目前提出的方法大多仅能够应用于将一两句话作为输入的情形，实际上只是语句级别的压缩或简化。&lt;/p&gt;&lt;p&gt;语句简化的最终目标仍然还是对更大范围内的信息摘要，比如用几句话去概况整篇文档中的主要内容。目前的神经网络方法大多以短文本（如句子、微博）作为输入，鲜有直接对文档结构进行编码的架构，最终解码也只能得到标题长度的信息，尚不足以作为整篇文档的内容总结。对于自动标题生成而言，是否只需要去利用每篇文档最开始一两句话中的信息，也仍有待商榷；这个问题在非新闻语料上可能更为明显。另一方面，对本身已经较短的文本再做进一步简化的实用价值，可能也无法和文档信息摘要相提并论。&lt;/p&gt;&lt;p&gt;关于文档摘要任务，现有的基于神经网络的模型仍以抽取式摘要（即从输入文档中直接抽取若干关键句作为摘要）居多，此时神经网络模型起到的作用也仅限于对文档中每个句子进行估分、排序，这和从文档到摘要进行端到端训练、直接逐词“生成”摘要的理想目标仍有距离。经典序列到序列架构在语句简化、标题生成任务可以取得不错的效果，但在文档摘要任务上还没有出现较为成功的应用。一个可能的原因在于整篇文档篇幅过长，不适合直接套用经典序列架构来编码和解码。&lt;/p&gt;&lt;p&gt;因此，对句子和词进行分级层次化编码(Li et al., 2015)可能是一种可以尝试的路线。今年提出的一种端到端神经摘要模型(Cheng and Lapata, 2016)中，将文档视为语句的序列，用各语句的编码作为编码器RNN中每个单元的输入，而语句的编码由一个CNN通过卷积和池化操作将词汇级信息汇总得到（图6）。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-9f7ffac4b18dc044b454fadd469b861f.jpg" data-rawwidth="623" data-rawheight="492"&gt;这样可以直接实现句子级抽取，比如文中的做法是用一个多层感知机根据当前状态来估计是否抽取该句的概率（pt-1表示前一句应当被抽取的概率）：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-e2b9b82fd646e1b8f7431647bfff3505.jpg" data-rawwidth="401" data-rawheight="102"&gt;为了进一步能够通过原文词汇重组构建和生成“非抽取式”摘要，文中提出一种层次化注意力架构，利用句子级的注意力权值作为输入来计算句子中每一个词的注意力权值。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-7d20b661018a6c4f68e1367997c66de8.jpg" data-rawwidth="653" data-rawheight="293"&gt;这个工作在句子抽取上能取得一定效果，但词汇级生成摘要仍有待提高，不论在自动评价和人工评价结果上都还不够理想。&lt;/p&gt;&lt;p&gt;而另一个侧重于标题生成的工作(Nallapati et al., 2016)中也提出了一种层次化编码思想：使用两级双向RNN分别刻画词和句子的序列结构，解码过程计算每个词的注意力权值时，用所在句子的注意力权值予以加权（reweight）。但很遗憾这样的设计暂时也并没有使得生成多句摘要的任务得到性能上的提升。&lt;/p&gt;&lt;p&gt;总而言之，目前的编码器-解码器架构在短文本简化任务上取得了一定进展。现在应用于文本简化的编码器-解码器架构设计也比较多样，可以为各种不同需求下文本简化的后续研究工作提供多种可能的参考思路。然而，深度学习方法在文档摘要任务上仍存在巨大的提升空间。如果期望使用完全端到端的方式训练文档级摘要模型，可能还需要在编码器和解码器的设计上产生一些新的突破，使得模型可以更好地表示和生成结构性更明显、篇幅更长的内容。&lt;/p&gt;&lt;p&gt;&lt;b&gt;参考文献&lt;/b&gt;&lt;/p&gt;&lt;p&gt;（未附带链接文献大多可在http://aclweb.org/anthology/ 公开下载）&lt;/p&gt;&lt;p&gt;Ayana; Shen, S.; Liu, Z.; and Sun, M. 2016. Neural Headline Generation with Minimum Risk Training. http://arxiv.org/abs/1604.01904 &lt;/p&gt;&lt;p&gt;Bahdanau, D.; Cho, K.; and Bengio, Y. 2015. Neural machine translation by jointly learning to align and translate. In ICLR 2015. http://arxiv.org/abs/1409.0473&lt;/p&gt;&lt;p&gt;Cheng, J.; and Lapata, M. 2016. Neural Summarization by Extracting Sentences and Words. In ACL 2016.&lt;/p&gt;&lt;p&gt;Chopra, S.; Auli, M.; and Rush, A. M. 2016. Abstractive Sentence Summarization with Attentive Recurrent Neural Networks. In NAACL 2016.&lt;/p&gt;&lt;p&gt;Doersch, C. 2016. Tutorial on Variational Autoencoders. http://arxiv.org/abs/1606.05908 Filippova, K.; Alfonseca, E.; Colmenares, C. A.; Kaiser, L.; and Vinyals, O. 2015. Sentence Compression by Deletion with LSTMs. In EMNLP 2015.&lt;/p&gt;&lt;p&gt;Gu, J.; Lu, Z.; Li, H.; and Li, V. O. K. 2016. Incorporating Copying Mechanism in Sequence-to-Sequence Learning. In ACL 2016.&lt;/p&gt;&lt;p&gt;Gulcehre, C.; Ahn, S.; Nallapati, R.; Zhou, B.; and Bengio, Y. 2016. Pointing the Unknown Words. In ACL 2016.&lt;/p&gt;&lt;p&gt;Hu, B.; Chen, Q.; and Zhu, F. 2015. LCSTS: A Large Scale Chinese Short Text Summarization Dataset. In EMNLP 2015.&lt;/p&gt;&lt;p&gt;Klerke, S.; Goldberg, Y.; and Søgaard, A. 2016. Improving Sentence Compression by Learning to Predict Gaze. In NAACL 2016. (Best short paper winner)&lt;/p&gt;&lt;p&gt;Li, J.; Luong, M.; and Jurafsky, D. 2015. A Hierarchical Neural Autoencoder for Paragraphs and Documents. In ACL 2015.&lt;/p&gt;&lt;p&gt;Lopyrev, K. 2015. Generating News Headlines with Recurrent Neural Networks. http://arxiv.org/abs/1512.01712&lt;/p&gt;&lt;p&gt;Miao, Y.; and Blunsom, P. 2016. Language as a Latent Variable: Discrete Generative Models for Sentence Compression. To appear in EMNLP 2016. http://arxiv.org/abs/1609.07317&lt;/p&gt;&lt;p&gt;Nallapati, R.; Zhou, B.; dos Santos, C.; Gulcehre, C.; and Xiang, B. 2016. Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond. In CoNLL 2016.&lt;/p&gt;&lt;p&gt;Och, F. J.; and Ney, H. 2002. Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. In ACL 2002.&lt;/p&gt;&lt;p&gt;Och, F. J. 2003. Minimum error rate training in statistical machine translation. In ACL 2003.Ranzato, M.; Chopra, S.; Auli, M.; and Zaremba, W. 2016. Sequence Level Training with Recurrent Neural Networks. In ICLR 2016. http://arxiv.org/abs/1511.06732&lt;/p&gt;&lt;p&gt;Rush, A. M.; Chopra, S.; and Weston, J. 2015. A neural attention model for abstractive sentence summarization. In EMNLP 2015.&lt;/p&gt;&lt;p&gt;Smith, D. A.; and Eisner, J. 2006. Minimum risk annealing for training log-linear models. In COLING/ACL 2006.&lt;/p&gt;&lt;p&gt;Sutskever, I.; Vinyals, O.; and Le, Q. V. 2014. Sequence to sequence learning with neural networks. In NIPS 2014. http://arxiv.org/abs/1409.3215&lt;/p&gt;&lt;p&gt;Vinyals, O.; Fortunato, M.; and Jaitly, N. 2015. Pointer Networks. In NIPS 2015. http://arxiv.org/abs/1506.03134&lt;/p&gt;&lt;p&gt;Williams, R. J. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229–256, 1992.&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" class="" data-editable="true" data-title="@果果是枚开心果."&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;b&gt; 作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本文作者&lt;b&gt;谭继伟&lt;/b&gt;和&lt;b&gt;姚金戈&lt;/b&gt;均为北京大学计算机科学与技术研究所在读博士生，研究方向主要包括文本信息推荐与自动摘要。联系方式：{tanjiwei, yaojinge} @ pku.edu.cn&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325543&amp;amp;idx=1&amp;amp;sn=db467a2dd2b8c6dae017fc4833650862&amp;amp;chksm=f235a52dc5422c3b33f0b45d1e1a59ad03b49b8768f68e34c0aaae4614448588b3eb8e09fb6b&amp;amp;scene=0#wechat_redirect" class="" data-editable="true" data-title="深度学习在文本简化中的应用进展"&gt;深度学习在文本简化中的应用进展&lt;/a&gt;&lt;/b&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22956057&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Fri, 14 Oct 2016 18:34:47 GMT</pubDate></item></channel></rss>