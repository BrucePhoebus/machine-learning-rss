<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>深度学习大讲堂 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/dlclass</link><description>推送深度学习的最新消息，包括最新技术进展，使用以及活动</description><lastBuildDate>Sat, 11 Mar 2017 21:15:32 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>『深度长文』Tensorflow代码解析（一）</title><link>https://zhuanlan.zhihu.com/p/25646408</link><description>&lt;p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;/p&gt;&lt;p&gt;&lt;b&gt;摘要&lt;/b&gt;&lt;/p&gt;&lt;p&gt;2015年11月9日，Google发布深度学习框架TensorFlow并宣布开源，并迅速得到广泛关注，在图形分类、音频处理、推荐系统和自然语言处理等场景下都被大面积推广。TensorFlow系统更新快速，官方文档教程齐全，上手快速且简单易用，支持Python和C++接口。本文依据对Tensorflow（简称TF）白皮书[1]、TF Github[2]和TF官方教程[3]的理解，从系统和代码实现角度讲解TF的内部实现原理。以Tensorflow r0.8.0为基础，本文由浅入深的阐述Tensor和Flow的概念。先介绍了TensorFlow的核心概念和基本概述，然后剖析了OpKernels模块、Graph模块、Session模块。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. TF系统架构&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.1 TF依赖视图&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TF的依赖视图如图 2 1所示[4]，描述了TF的上下游关系链。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-a581e8d2a1cf438e930429cd9495bad7.png" data-rawwidth="629" data-rawheight="462"&gt;&lt;p&gt;TF托管在github平台，有google groups和contributors共同维护。&lt;/p&gt;&lt;p&gt;TF提供了丰富的深度学习相关的API，支持Python和C/C++接口。&lt;/p&gt;&lt;p&gt;TF提供了可视化分析工具Tensorboard，方便分析和调整模型。&lt;/p&gt;&lt;p&gt;TF支持Linux平台，Windows平台，Mac平台，甚至手机移动设备等各种平台。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.2 TF系统架构&lt;/b&gt;&lt;/p&gt;&lt;p&gt;图 1 2是TF的系统架构，从底向上分为设备管理和通信层、数据操作层、图计算层、API接口层、应用层。其中设备管理和通信层、数据操作层、图计算层是TF的核心层。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-90f292f8917e32182d30aef17cb04e82.png" data-rawwidth="426" data-rawheight="436"&gt;&lt;p&gt;底层设备通信层负责网络通信和设备管理。设备管理可以实现TF设备异构的特性，支持CPU、GPU、Mobile等不同设备。网络通信依赖gRPC通信协议实现不同设备间的数据传输和更新。&lt;/p&gt;&lt;p&gt;第二层是Tensor的OpKernels实现。这些OpKernels以Tensor为处理对象，依赖网络通信和设备内存分配，实现了各种Tensor操作或计算。Opkernels不仅包含MatMul等计算操作，还包含Queue等非计算操作，这些将在第5章Kernels模块详细介绍。&lt;/p&gt;&lt;p&gt;第三层是图计算层（Graph），包含本地计算流图和分布式计算流图的实现。Graph模块包含Graph的创建、编译、优化和执行等部分，Graph中每个节点都是OpKernels类型表示。关于图计算将在第6章Graph模块详细介绍。&lt;/p&gt;&lt;p&gt;第四层是API接口层。Tensor C API是对TF功能模块的接口封装，便于其他语言平台调用。&lt;/p&gt;&lt;p&gt;第四层以上是应用层。不同编程语言在应用层通过API接口层调用TF核心功能实现相关实验和应用。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.3TF代码目录组织&lt;/b&gt;&lt;/p&gt;&lt;p&gt;图 1 3是TF的代码结构视图，下面将简单介绍TF的目录组织结构。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-fa8f49ec0a203aa68f58e7bc6634c183.png" data-rawwidth="403" data-rawheight="584"&gt;&lt;p&gt;Tensorflow/core目录包含了TF核心模块代码。&lt;/p&gt;&lt;p&gt;public: API接口头文件目录，用于外部接口调用的API定义，主要是session.h 和tensor_c_api.h。&lt;/p&gt;&lt;p&gt;client: API接口实现文件目录。&lt;/p&gt;&lt;p&gt;platform: OS系统相关接口文件，如file system, env等。&lt;/p&gt;&lt;p&gt;protobuf: 均为.proto文件，用于数据传输时的结构序列化.&lt;/p&gt;&lt;p&gt;common_runtime: 公共运行库，包含session, executor, threadpool, rendezvous, memory管理, 设备分配算法等。&lt;/p&gt;&lt;p&gt;distributed_runtime: 分布式执行模块，如rpc session, rpc master, rpc worker, graph manager。&lt;/p&gt;&lt;p&gt;framework: 包含基础功能模块，如log, memory, tensor&lt;/p&gt;&lt;p&gt;graph: 计算流图相关操作，如construct, partition, optimize, execute等&lt;/p&gt;&lt;p&gt;kernels: 核心Op，如matmul, conv2d, argmax, batch_norm等&lt;/p&gt;&lt;p&gt;lib: 公共基础库，如gif、gtl(google模板库)、hash、histogram等。&lt;/p&gt;&lt;p&gt;ops: 基本ops运算，ops梯度运算，io相关的ops，控制流和数据流操作&lt;/p&gt;&lt;p&gt;Tensorflow/stream_executor目录是并行计算框架，由google stream executor团队开发。&lt;/p&gt;&lt;p&gt;Tensorflow/contrib目录是contributor开发目录。&lt;/p&gt;&lt;p&gt;Tensroflow/python目录是python API客户端脚本。&lt;/p&gt;&lt;p&gt;Tensorflow/tensorboard目录是可视化分析工具，不仅可以模型可视化，还可以监控模型参数变化。&lt;/p&gt;&lt;p&gt;third_party目录是TF第三方依赖库。&lt;/p&gt;&lt;p&gt;eigen3: eigen矩阵运算库，TF基础ops调用&lt;/p&gt;&lt;p&gt;gpus: 封装了cuda/cudnn编程库&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. TF核心概念&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TF的核心是围绕Graph展开的，简而言之，就是Tensor沿着Graph传递闭包完成Flow的过程。所以在介绍Graph之前需要讲述一下符号编程、计算流图、梯度计算、控制流的概念。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.1 Tensor&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在数学上，Matrix表示二维线性映射，Tensor表示多维线性映射，Tensor是对Matrix的泛化，可以表示1-dim、2-dim、N-dim的高维空间。图 2 1对比了矩阵乘法（Matrix Product）和张量积（Tensor Contract），可以看出Tensor的泛化能力，其中张量积运算在TF的MatMul和Conv2D运算中都有用到，&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-69e6c914bc1854898460d36663ad49c2.png" data-rawwidth="580" data-rawheight="331"&gt;&lt;p&gt;Tensor在高维空间数学运算比Matrix计算复杂，计算量也非常大，加速张量并行运算是TF优先考虑的问题，如add, contract, slice, reshape, reduce, shuffle等运算。&lt;/p&gt;&lt;p&gt;TF中Tensor的维数描述为阶，数值是0阶，向量是1阶，矩阵是2阶，以此类推，可以表示n阶高维数据。&lt;/p&gt;&lt;p&gt;TF中Tensor支持的数据类型有很多，如tf.float16, tf.float32, tf.float64, tf.uint8, tf.int8, tf.int16, tf.int32, tf.int64, tf.string, tf.bool, tf.complex64等，所有Tensor运算都使用泛化的数据类型表示。&lt;/p&gt;&lt;p&gt;TF的Tensor定义和运算主要是调用Eigen矩阵计算库完成的。TF中Tensor的UML定义如图 2 2。其中TensorBuffer指针指向Eigen::Tensor类型。其中，Eigen::Tensor[5][6]不属于Eigen官方维护的程序，由贡献者提供文档和维护，所以Tensor定义在Eigen unsupported模块中。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-85374a1abe35c6fe87f9f84262c97584.png" data-rawwidth="528" data-rawheight="406"&gt;&lt;p&gt;图 2 2中，Tensor主要包含两个变量m_data和m_dimension，m_data保存了Tensor的数据块，T是泛化的数据类型，m_dimensions保存了Tensor的维度信息。&lt;/p&gt;&lt;p&gt;Eigen::Tensor的成员变量很简单，却支持非常多的基本运算，再借助Eigen的加速机制实现快速计算，参考章节3.2。Eigen::Tensor主要包含了&lt;/p&gt;&lt;p&gt;一元运算（Unary），如sqrt、square、exp、abs等。&lt;/p&gt;&lt;p&gt;二元运算（Binary），如add，sub，mul，div等&lt;/p&gt;&lt;p&gt;选择运算（Selection），即if / else条件运算&lt;/p&gt;&lt;p&gt;归纳运算（Reduce），如reduce_sum， reduce_mean等&lt;/p&gt;&lt;p&gt;几何运算（Geometry），如reshape，slice，shuffle，chip，reverse，pad，concatenate，extract_patches，extract_image_patches等&lt;/p&gt;&lt;p&gt;张量积（Contract）和卷积运算（Convolve）是重点运算，后续会详细讲解。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.2 符号编程&lt;/b&gt;&lt;/p&gt;&lt;p&gt;编程模式通常分为命令式编程（imperative style programs）和符号式编程（symbolic style programs）。&lt;/p&gt;&lt;p&gt;命令式编程容易理解和调试，命令语句基本没有优化，按原有逻辑执行。符号式编程涉及较多的嵌入和优化，不容易理解和调试，但运行速度有同比提升。&lt;/p&gt;&lt;p&gt;这两种编程模式在实际中都有应用，Torch是典型的命令式风格，caffe、theano、mxnet和Tensorflow都使用了符号式编程。其中caffe、mxnet采用了两种编程模式混合的方法，而Tensorflow是完全采用了符号式编程，Theano和Tensorflow的编程模式更相近。&lt;/p&gt;&lt;p&gt;命令式编程是常见的编程模式，编程语言如python/C++都采用命令式编程。命令式编程明确输入变量，并根据程序逻辑逐步运算，这种模式非常在调试程序时进行单步跟踪，分析中间变量。举例来说，设A=10, B=10，计算逻辑：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-3c9d9a87b293ccfd1b4c7d3f719877cd.png" data-rawwidth="102" data-rawheight="109"&gt;&lt;p&gt;第一步计算得出C=100，第二步计算得出D=101，输出结果D=101。&lt;/p&gt;&lt;p&gt;符号式编程将计算过程抽象为计算图，计算流图可以方便的描述计算过程，所有输入节点、运算节点、输出节点均符号化处理。计算图通过建立输入节点到输出节点的传递闭包，从输入节点出发，沿着传递闭包完成数值计算和数据流动，直到达到输出节点。这个过程经过计算图优化，以数据（计算）流方式完成，节省内存空间使用，计算速度快，但不适合程序调试，通常不用于编程语言中。举上面的例子，先根据计算逻辑编写符号式程序并生成计算图&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-04963c9e12e0c10a664fb70c422bda06.png" data-rawwidth="275" data-rawheight="259"&gt;其中A和B是输入符号变量，C和D是运算符号变量，compile函数生成计算图F，如图 2 3所示。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-080a192a0d0a8fef5bdd6d6b78411d59.png" data-rawwidth="479" data-rawheight="328"&gt;最后得到A=10, B=10时变量D的值，这里D可以复用C的内存空间，省去了中间变量的空间存储。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-2ec035b10393bdbf876eb7e14307b90e.png" data-rawwidth="272" data-rawheight="62"&gt;&lt;p&gt;图 2 4是TF中的计算流图，C=F(Relu(Add(MatMul(W, x), b)))，其中每个节点都是符号化表示的。通过session创建graph，在调用session.run执行计算。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-37f2f7d0e112c7637fa74dbc4ce99d39.png" data-rawwidth="363" data-rawheight="575"&gt;&lt;p&gt;和目前的符号语言比起来，TF最大的特点是强化了数据流图，引入了mutation的概念。这一点是TF和包括Theano在内的符号编程框架最大的不同。所谓mutation，就是可以在计算的过程更改一个变量的值，而这个变量在计算的过程中会被带入到下一轮迭代里面去。&lt;/p&gt;&lt;p&gt;Mutation是机器学习优化算法几乎必须要引入的东西（虽然也可以通过immutable replacement来代替，但是会有效率的问题）。 Theano的做法是引入了update statement来处理mutation。TF选择了纯符号计算的路线，并且直接把更新引入了数据流图中去。从目前的白皮书看还会支持条件和循环。这样就几乎让TF本身成为一门独立的语言。不过这一点会导致最后的API设计和使用需要特别小心，把mutation 引入到数据流图中会带来一些新的问题，比如如何处理写与写之间的依赖。[7]&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.3 梯度计算&lt;/b&gt;&lt;/p&gt;&lt;p&gt;梯度计算主要应用在误差反向传播和数据更新，是深度学习平台要解决的核心问题。梯度计算涉及每个计算节点，每个自定义的前向计算图都包含一个隐式的反向计算图。从数据流向上看，正向计算图是数据从输入节点到输出节点的流向过程，反向计算图是数据从输出节点到输入节点的流向过程。&lt;/p&gt;&lt;p&gt;图 2 5是2.2节中图 2 3对应的反向计算图。图中，由于C=A*B，则dA=B*dC, dB=A*dC。在反向计算图中，输入节点dD，输出节点dA和dB，计算表达式为dA=B*dC=B*dD, dB=A*dC=A*dD。每一个正向计算节点对应一个隐式梯度计算节点。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-00f3681ce908c326bca72be62f2dcb01.png" data-rawwidth="653" data-rawheight="150"&gt;&lt;p&gt;反向计算限制了符号编程中内存空间复用的优势，因为在正向计算中的计算数据在反向计算中也可能要用到。从这一点上讲，粗粒度的计算节点比细粒度的计算节点更有优势，而TF大部分为细粒度操作，虽然灵活性很强，但细粒度操作涉及到更多的优化方案，在工程实现上开销较大，不及粗粒度简单直接。在神经网络模型中，TF将逐步侧重粗粒度运算。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.4 控制流&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TF的计算图如同数据流一样，数据流向表示计算过程，如图 2 6。数据流图可以很好的表达计算过程，为了扩展TF的表达能力，TF中引入控制流。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-547cfe1350de20e7e6bbf799ee925866.png" data-rawwidth="303" data-rawheight="566"&gt;在编程语言中，if…else…是最常见的逻辑控制，在TF的数据流中也可以通过这种方式控制数据流向。接口函数如下，pred为判别表达式，fn1和fn2为运算表达式。当pred为true是，执行fn1操作；当pred为false时，执行fn2操作。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-f7e4c2d90d36dfaa5cef091bb53845a1.png" data-rawwidth="350" data-rawheight="40"&gt;TF还可以协调多个数据流，在存在依赖节点的场景下非常有用，例如节点B要读取模型参数θ更新后的值，而节点A负责更新参数θ，则节点B必须等节点A完成后才能执行，否则读取的参数θ为更新前的数值，这时需要一个运算控制器。接口函数如下，tf.control_dependencies函数可以控制多个数据流执行完成后才能执行接下来的操作，通常与tf.group函数结合使用。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-3d1542e535b13563ecb04bd6706d0706.png" data-rawwidth="492" data-rawheight="38"&gt;&lt;p&gt;TF支持的控制算子有Switch、Merge、Enter、Leave和NextIteration等。&lt;/p&gt;&lt;p&gt;TF不仅支持逻辑控制，还支持循环控制。TF使用和MIT Token-Tagged machine相似的表示系统，将循环的每次迭代标记为一个tag，迭代的执行状态标记为一个frame，但迭代所需的数据准备好的时候，就可以开始计算，从而多个迭代可以同时执行。&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing/answers" class="" data-editable="true" data-title="@果果是枚开心果."&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-ce0088075a75093502d427ca9ca2a91f.png" data-rawwidth="348" data-rawheight="351"&gt;姚健，&lt;/b&gt;毕业于中科院计算所网络数据实验室，毕业后就职于360天眼实验室，主要从事深度学习和增强学习相关研究工作。目前就职于腾讯MIG事业部，从事神经机器翻译工作。联系方式： yao_62995@163.com&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s/wC2EKp14lShUf5tAIBg5ow" data-editable="true" data-title="『深度长文』Tensorflow代码解析（一）" class=""&gt;『深度长文』Tensorflow代码解析（一）&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/25646408&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Wed, 08 Mar 2017 19:07:05 GMT</pubDate></item><item><title>【青年学者专栏】解读GAN及其 2016 年度进展</title><link>https://zhuanlan.zhihu.com/p/25000523</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-f029a6ed55518682b39fdd3f941926fc_r.png"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;&lt;b&gt;摘要&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本文主要包括两方面内容：首先对GAN的基础概念与理论基础进行介绍，并分析了其模型构建原理与优势等；其次对其在2016年度的主要进展进行梳理和概括，主要包括从模型理论框架到实际应用问题中的相关扩展与改进及其训练技巧等工作。（以下仅为个人观点，不当之处欢迎大家批评指正。）&lt;/p&gt;&lt;p&gt;&lt;b&gt;导读：&lt;/b&gt;在刚刚过去的2016，人工智能仍然是科技领域最火爆的话题，如何依靠人工智能改造传统行业，提高效率、降低成本成为学界业界的共同课题。一些过去被认为只有人类能创造的图像、文字及语音等，经由GAN模型对样本的学习，可以由AI代劳，在传统的图样设计行业可以代替水平参差不齐、成本越来越高的设计师，在媒体行业可以将编辑从一些繁琐重复的资料转文字工作中解放出来，在创作背景音乐时可以比人类作曲家快千百倍。&lt;/p&gt;&lt;p&gt;（本导读由线性资本提供）&lt;/p&gt;&lt;p&gt;GAN，全称为Generative Adversarial Nets，直译为生成式对抗网络。它一方面将产生式模型拉回到了一直由判别式模型称霸的AI竞技场，引起了学者甚至大众对产生式模型的研究兴趣，同时也将对抗训练从常规的游戏竞技领域引到了更一般领域，引起了从学术界到工业界的普遍关注。笔者对几大会议等进行了不完全统计，其中：&lt;/p&gt;&lt;p&gt;ICLR-2017提交论文：45篇产生式模型相关，37篇与对抗训练相关；&lt;/p&gt;&lt;p&gt;NIPS-2016：在会议大纲中GAN被提及超过120次；同时，会议专门针对“Adversarial Training”组织了一个workshop，收录了32篇文章，绝大多数与GAN直接相关；此外，正会还收录了17篇产生式模型相关文章，11篇对抗训练相关文章；&lt;/p&gt;&lt;p&gt;Arxiv：在Computer Science分类下约有500篇与对抗网络相关文章，其中绝大多数为2016年的工作；&lt;/p&gt;&lt;p&gt;相信接下来偏重实际应用的CVPR-2017与ICCV-2017也会有一大波视觉上应用和改进GAN的文章来袭......&lt;/p&gt;&lt;p&gt;除了学术圈外，谷歌、Facebook、Twitter、苹果、OPENAI（GAN作者所在公司）等众多工业界AI相关的公司也都在今年发表了数量不一的基于GAN的相关研究成果。&lt;/p&gt;&lt;p&gt;那么问题来了，GAN在2014年就被提出，为什么不是在2014年或2015年火爆？为什么是2016呢？个人觉得GAN的对抗训练理论和它利用这一点来构建产生式模型的概念一直都很吸引人，但它刚提出之时对模型的实际训练中所遇到的许多问题都还没有得到很好的解决（当然，现在也仍然有许多问题），但经过2014和2015大半年的酝酿，到15年下半年和16年初就逐渐开始出现许多GAN的训练技巧总结分享以及对模型本身进行改进的文章，这些都极大地带动了后续的发展，特别是在数据生成等相关问题中的应用，造就了2016年对于产生式模型以及对抗训练的研究热潮。&lt;/p&gt;&lt;p&gt;&lt;b&gt;解读GAN&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;一. 基本的GAN模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.1 基本框架：&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-be4eef07d36420139808b9eaf9388bba.png" data-rawwidth="1578" data-rawheight="416"&gt;&lt;p&gt;原始GAN模型的基本框架如上图所示，其主要目的是要由判别器D辅助生成器G产生出与真实数据分布一致的伪数据。模型的输入为随机噪声信号z；该噪声信号经由生成器G映射到某个新的数据空间，得到生成的数据G(z)；接下来，由判别器D根据真实数据x与生成数据G(z)的输入来分别输出一个概率值或者说一个标量值，表示D对于输入是真实数据还是生成数据的置信度，以此判断G的产生数据的性能好坏；当最终D不能区分真实数据x和生成数据G(z)时，就认为生成器G达到了最优。&lt;/p&gt;&lt;p&gt;D为了能够区分开两者，其目标是使D(x)与D(G(z))尽量往相反的方向跑，增加两者的差异，比如使D(x)尽量大而同时使D(G(z))尽量小；而G的目标是使自己产生的数据在D上的表现D(G(z))尽量与真实数据的表现D(x)一致，让D不能区分生成数据与真实数据。因此，这两个模块的优化过程是一个相互竞争相互对抗的过程，两者的性能在迭代过程中不断提高，直到最终D(G(z))与真实数据的表现D(x)一致，此时G和D都不能再进一步优化。&lt;/p&gt;&lt;p&gt;我们若对这个过程直接建模，应当是如下的minimax问题（我将其称为&lt;b&gt;朴素minimax问题&lt;/b&gt;）：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-81103eb892830c06718a5db8977bffe1.png" data-rawwidth="186" data-rawheight="34"&gt;但是，原文[1]中的目标函数却为如下形式：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-bbcc4519d7b321a8b134b9a4c94d6b83.png" data-rawwidth="793" data-rawheight="59"&gt;&lt;p&gt;可以看到，GAN的目标函数与朴素minimax问题相比，主要多了三个变化：&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1) &lt;/b&gt;加入了求对数操作：求对数的操作在理论上并不会影响最终收敛到的最优值，但对数算子的变换可以缓解数据分布偏差问题，比如减少数据分布的单边效应的影响，减少数据分布形式上的波动等，同时在实际的程序实现中，对数算子也可以避免许多数值问题，因此成为统计学中的常用做法，比如在许多问题中我们并不直接求最大似然，而是去求最大对数似然也是类似的原因；&lt;/p&gt;&lt;p&gt;&lt;b&gt;(2) &lt;/b&gt;加入求期望的操作：这一点是由于我们的最终目的是希望生成数据的分布Pg(G(z))能够与真实数据的分布Pdata(x)一致，换句话说，相当于在数据量无限的条件下，通过拟合G(z)得到的分布与通过拟合x得到的分布尽量一致，这一点不同于要求各个G(z)本身和各个真实数据x本身相同，这样才能保证G产生出的数据既与真实数据有一定相似性，同时又不同于真实数据；&lt;/p&gt;&lt;p&gt;&lt;b&gt;(3)&lt;/b&gt; -D(G(z))变成了1-D(G(z))：直观上来说，加了对数操作以后，要求logf的f必须是正数，所以不能直接用-D(G(z))，这一点并不影响最终的最优解。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.2 理论保证：GAN除了提供了一种对抗训练的框架外，另一个重要贡献是其收敛性的理论证明。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;作者通过将GAN的优化过程进行分解，从数学推导上严格证明了：在假设G和D都有足够的capacity的条件下，如果在迭代过程中的每一步，D都可以达到当下在给定G时的最优值，并在这之后再更新G，那么最终Pg就一定会收敛于Pdata。也正是基于上述的理论，原始文章中是每次迭代中优先保证D在给定当前G下达到最优，然后再去更新G到最优，如此循环迭代完成训练。这一证明为GAN的后续发展奠定了坚实基础，使其没有像许多其它深度模型一样只是被应用而没有广而深的改进。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.3 改进方向：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;原文只针对框架本身进行了理论证明和实验验证，表明了GAN的理论基础及其有效性，而对于其中的许多细节并没深究（相当于开采了一个大坑等人来填），比如文章中的输入信号只是随机噪声，比如原文中的G和D都只是以最简单的MLP来建模；另外，作者在文章结尾还列出了该模型可以改进的5个参考方向，进一步为后来逐渐广泛的研究做了铺垫。作者给出的参考方向主要包括：&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1) &lt;/b&gt;将GAN改进为条件产生式模型：这一点最早在GAN公开后的半年就得到了部分解决，即conditional GAN（ARXIV-2014）的工作，该模型实现了给定条件的数据生成，但现在在各个领域特别是图像和视频相关的生成工作中，也依然有许多对于给定条件生成数据的任务的相关改进与研究；&lt;/p&gt;&lt;p&gt;&lt;b&gt;(2)&lt;/b&gt; 改进输入z：不直接用随机噪声信号，而是可以用其它网络根据真实数据x学习一个z，然后再输入G，相当于是对数据x做了一个编码；这一点目前基本上在多数基于GAN的应用中都被采纳；&lt;/p&gt;&lt;p&gt;&lt;b&gt;(3) &lt;/b&gt;对条件分布建模，由已有数据预测未出现的数据：往这个方向改进的相关工作相对出现较晚，直到2016年才逐步开始有相关工作出现；&lt;/p&gt;&lt;p&gt;&lt;b&gt;(4) &lt;/b&gt;半监督学习：在2015年年底出现了将GAN用于半监督问题的工作；另外，现有的许多GAN工作也都表明通过加入少量类别标签，引入有标签数据的类别损失度量，不仅功能上实现了半监督学习，同时也有助于GAN的稳定训练；&lt;/p&gt;&lt;p&gt;&lt;b&gt;(5)&lt;/b&gt; 提升GAN的训练效率：目前比GAN的训练效率更加要紧的训练稳定性问题还没有得到很好的解决，因此相对来说，目前这一点的研究并不广泛，而且相比较其它的产生式模型而言，GAN的速度也不算是一个非常“拖后腿”的点。&lt;/p&gt;&lt;p&gt;除了作者给出的以上几个参考方向外，目前GAN在计算机视觉中的超分辨率图像生成、视频帧的生成、艺术风格迁移等问题中都得到了广泛关注。&lt;/p&gt;&lt;p&gt;&lt;b&gt;二. GAN相关工作对比：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们这里主要对比两类工作，一类是像GAN一样将产生器与判别器联合学习的工作，另一类是深度学习领域中当前应用比较多的深度产生式模型：&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.1 对比1：产生器与判别器联合学习的相关工作：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在传统的机器学习领域，很早就引入了将判别式模型与产生式模型进行联合学习的想法，比如Tony Jebara早在2001年的毕业论文[2]中就以最大熵的形式将判别式模型与产生式模型结合起来联合学习；2007年UC的Zhuowen Tu也提出将基于boosting分类器的判别器与基于采样的产生式模型相结合[3]，来产生出服从真实分布的样本；2012年清华的Jun Zhu老师发表在JMLR上的工作[4]则是将最大间隔机制与贝叶斯模型相结合进行产生式模型的学习。与这些模型相比，GAN更加迎合了当下大数据的需求和深度学习的热潮，并且更重要的是它给出了一个大的框架而不是一个具体的模型；&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.2 对比2：其它同类深度产生式模型工作：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1) &lt;/b&gt;GAN与VAE：VAE中模型性能的好坏直接依赖于其假设的近似分布q的好坏，对于q的选择需要一定的经验信息；并且受变分方法本身的限制，其最终模拟出的概率分布一定会存在有偏置，而GAN本身则不存在这个问题，且其原则上可以渐进的逼近任意概率分布，可以认为是一种非参数的产生式建模方法；&lt;/p&gt;&lt;p&gt;&lt;b&gt;2) &lt;/b&gt;GAN与pixel RNN/CNN：pixel RNN中是将图像的生成问题转化为像素序列的预测和生成问题，因此需要对每个像素逐个操作，而GAN是直接对整幅图像进行衡量、评价和生成，因此相对来说考虑了整体信息且速度相对较快。&lt;/p&gt;&lt;p&gt;这里主要是从GAN的优点的角度来进行对比的，但GAN模型也存在有不如其它模型的地方，比如目前的最大缺陷是其训练过程中的稳定性和收敛性难以保证，在2016年有许多相关工作都在尝试解决其训练稳定性问题。&lt;/p&gt;&lt;p&gt;&lt;b&gt;GAN在2016年的部分进展&lt;/b&gt;&lt;/p&gt;&lt;p&gt;GAN在2016年得到了几乎全方位的研究，包括其训练技巧与模型架构改进，理论扩展与实际应用问题等多个角度，都有大量成果出现，难以全部一一列出。下面仅选取其中较为突出的或者被广泛关注和研究的部分工作进行介绍。&lt;/p&gt;&lt;p&gt;&lt;b&gt;一. 理论框架层面扩展与改进：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;对GAN模型的理论框架层面的改进工作主要可以归纳为两类：一类是从第三方的角度，而不是从GAN模型本身，来看待GAN并进行改进和扩展的方法；第二类是从GAN模型框架的稳定性、实用性等角度出发对模型本身进行改进的工作。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.1 下面首先介绍以第三方角度看待GAN的两个典型工作：f-GAN与EBGAN，一个从距离度量的角度出发，一个从能量模型的角度出发，分别对GAN进行阐释和改进，非常有助于我们对GAN做出新的理解：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1) f-GAN (NIPS-2016)：&lt;/b&gt;该文 [5]提出了一种f-divergence，并证明了GAN只是在f-divergence取某种特定度量时的特殊情况。这个f-divergence包括了常见的多种概率分布的距离度量，比如KL散度、Pearson散度等。具体来说，作者将GAN的优化问题求解中的步骤进行分解，将真实数据分布的估计问题转化为某种divergence的最小化问题，而这个divergence就正是作者定义的f-divergence的一种特例。最后，作者利用GAN模型框架结合不同度量条件，即不同divergence进行图像生成。其中，在选择KL散度的度量方式时，对比结果如下图所示，可以看出两者的效果其实相差不大。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-fd43a829bd8639140186445caf8ea2e4.png" data-rawwidth="798" data-rawheight="432"&gt;&lt;p&gt;&lt;b&gt;(2) EBGAN (ICLR-2017，submitted) : &lt;/b&gt;EBGAN [6]是Yann LeCun课题组提交到ICLR2017的一个工作，从能量模型的角度对GAN进行了扩展。EBGAN将判别器看做是一个能量函数，这个能量函数在真实数据域附近的区域中能量值会比较小，而在其他区域（即非真实数据域区域）都拥有较高能量值。因此，EBGAN中给予GAN一种能量模型的解释，即生成器是以产生能量最小的样本为目的，而判别器则以对这些产生的样本赋予较高的能量为目的。&lt;/p&gt;&lt;p&gt;从能量模型的角度来看待判别器和GAN的好处是，我们可以用更多更宽泛的结构和损失函数来训练GAN结构，比如文中就用自编码器(AE) 的结构来作为判别器实现整体的GAN框架，如下图所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-9cd403fd63a02b020f6b5c7cfdd9f9cb.png" data-rawwidth="1205" data-rawheight="356"&gt;在训练过程中，EBGAN比GAN展示出了更稳定的性能，也产生出了更加清晰的图像，如下图所示。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-880b6ef05a0dbfafb6819bdbaec5d85e.png" data-rawwidth="1202" data-rawheight="333"&gt;&lt;p&gt;&lt;b&gt;1.2 接下来，从GAN网络本身出发而对GAN进行改进并且取得了良好效果的一个重要工作，就是大名鼎鼎的Info GAN模型。它以成功习得数据的disentangled的表示和拥有稳定的训练效果而受到广泛关注:&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Info GAN (NIPS-2016)&lt;/b&gt; [7]是OPENAI对GAN的一个重要扩展，被认为是OPENAI去年的五大突破之一。原始的GAN模型通过对抗学习最终可以得到一个能够与真实数据分布一致的模型分布,此时虽然模型相当于已经学到了数据的有效语义特征，但输入信号z中的具体维度与数据的语义特征之间的对应关系并不清楚，比如z中的哪些维度对应于光照变化或哪些维度对应于pose变化是不明确的。而infoGAN不仅能对这些对应关系建模，同时可以通过控制相应维度的变量来达到相应的变化，比如光照的变化或pose的变化。其思想是把输入噪声z分成两个部分：不可压缩的噪声信号z和可解释的有隐含意义的c。比如对于mnist手写数字来说，c可以对应于笔画粗细、图像光照、字体倾斜度等，用C1,C2,…,CL表示，我们称之为latent code，而z则可以认为是剩下的不知道怎么描述的或者说不能明确描述的信息。此时生成器的输出就从原来的G(z)变成了G(z,c)；在学习过程中，为了避免学到一些trivial的latent code而忽略了重要的code，文章对原始的GAN目标函数加了一个约束，即约束latent code c和生成器的输出G(z,c)之间的互信息I(c;G(z,c))越高越好，以此希望能学到比较重要的有意义的codes c，从而此时的目标函数即为：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-53415f2999d5686d0521aa50f51db538.png" data-rawwidth="263" data-rawheight="30"&gt;&lt;p&gt;在具体的优化过程中，作者采用变分推断的思想，引入变分分布来逼近真实分布，并与最优互信息下界的步骤轮流迭代进行，实现最终的求解。&lt;/p&gt;&lt;p&gt;在实验中，作者通过只改变latent code c中的某一个维度，来观察生成数据的变化。其实验确实证明：latent code确实学到了一些维度，如对应于图像的角度或光照的因素，也即说明InfoGAN确实学习到了数据中的disentangled的可解释部分的表示。其效果参考下图。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-0013dd0afa46ebe3b6ef4f80d2dd15d9.png" data-rawwidth="1361" data-rawheight="480"&gt;&lt;p&gt;&lt;b&gt;1.3 另外一种从GAN模型本身出发进行改进的工作是将GAN与其它模型结合，综合利用GAN模型与其它模型的优点来完成数据生成任务。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;比如ARXIV-2016上将GAN与RNN进行结合，ICML-2016上的[8]提出的将GAN与VAE的思想相结合的工作等，这里以GAN+VAE (ICML-2016) [8]为例进行介绍：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;虽然在第一个部分我们强调了GAN与VAE的不同，但这两者并不是完全冲突的，工作[8]从另一个角度将两者结合到了一起。主要是作者利用GAN中的判别器学习到的特征来构建VAE的重构误差，而不采用常规的VAE中的像素级的误差，其结构如下图所示。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-f029a6ed55518682b39fdd3f941926fc.png" data-rawwidth="776" data-rawheight="336"&gt;&lt;p&gt;在上面的框架中，我们可以把前两个部分 (encoder + decoder/generator) 看做一个产生式模型的整体，从而和最后一个部分 (discriminator) 构成了扩展的GAN模型；也可以将最后两个部分 (decoder + discriminator) 看做是一个整体，其中discriminator的存在看做是用于替换原来的element-wise的重构误差的计算，相当于对decoder的一个辅助，从而此时整个模型架构可以看做是一个自编码器。因此整体上来说，该模型综合了VAE与GAN两种模型的优点，属于将GAN与其他方法结合的一个代表性工作。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.4 最后一种改进，是从GAN本源出发，对GAN进行半监督形式的扩展，这类工作目前的做法都大同小异，通过引入类别损失来进行GAN的学习。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;这里以CatGAN (ICLR-2016) [9] 为例进行说明：&lt;/b&gt;通常无监督的分类方法会被转化为一个聚类问题，在判别式的聚类方法中通常是以某种距离作为度量准则，从而将数据划分为多个类别，而本文则是采用数据的熵来作为衡量标准构建来CatGAN (ICLR-2016) 。具体来说，对于真实的数据，模型希望判别器不仅能具有较大的确信度将其划分为真实样本，同时还有较大的确信度将数据划分到某一个现有的类别中去；而对于生成数据却不是十分确定要将其划分到哪一个现有的类别，也就是这个不确信度比较大，从而生成器的目标即为产生出那些“将其划分到某一类别中去”的确信度较高的样本，尝试骗过判别器。接下来，为了衡量这个确信程度，作者用熵来表示，熵值越大，即为越不确定；而熵值越小，则表示越确定。然后，将该确信度目标与原始GAN的真伪鉴别的优化目标结合，即得到了CatGAN的最终优化目标。&lt;/p&gt;&lt;p&gt;对于半监督的情况，即当部分数据有标签时，那么对有标签数据计算交叉熵损失，而对其他数据计算上面的基于熵的损失，然后在原来的目标函数的基础上进行叠加即得，当用该半监督方法进行目标识别与分类时，其效果虽然相对较优，但相对当下state-of-the-art的方法并没有比较明显的提升。但其基于熵损失的无监督训练方法却表现较好，其实验效果如下图所示，可以看到，对于如下的典型环形数据，CatGAN可以较好地找到两者的分类面，实现无监督聚类的功能。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-ea0b41c2e6d65753e586c5c8bac26eb2.png" data-rawwidth="1675" data-rawheight="318"&gt;&lt;p&gt;&lt;b&gt;二. 模型改进（偏应用层面）：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.1 提到GAN在应用层面的改进，就不得不说perceptual similarity，该度量改变了以往的按照图像的像素级差异来衡量损失的情况，使模型更加鲁棒。在当下的多数图像生成以及视频数据处理等模型中都有将perceptual similarity加入考虑。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1) Perceptual Similarity Metrics (NIPS-2016)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Perceptual Similarity Metrics [10]的主要贡献在于提出了一种新的度量，有助于使GAN产生清晰图像。其方法是将通常在原始图像空间的损失度量替换为在特征空间的损失度量。具体来说，在训练GAN时，除了原始GAN中的对抗训练损失，额外加入了两个损失项，共计三个损失项，分别为：&lt;/p&gt;&lt;p&gt;— 特征空间损失Lfeat：文章构建了一个比较器网络C，然后比较真实样本和产生的样本分别作为输入时，网络的特征图(feature map)的差异性，即&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-e91d3dbf468d301896b06507f715be20.png" data-rawwidth="197" data-rawheight="30"&gt;&lt;p&gt;这里的一个问题是网络中间层的特征图的相似性，只能代表高层的相似性，会使产生出的相对低层的像素级数据出现畸形，因此需要加入图像的一些先验信息进行约束。而这个先验信息就通过对抗损失来体现，从而有了下面的对抗损失；&lt;/p&gt;&lt;p&gt;— 对抗损失：这里的对抗损失，即与生成器一起训练一个判别器，其中判别模块的目的是为了区分开产生数据与真实数据，而生成器的目的则是为了尽量的迷惑判别器，其数学形式与原始GAN损失相似，即&lt;/p&gt;&lt;p&gt;判别器D以最小化如下损失为目标：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-295892e6ffc6fce442f2002a22eda066.png" data-rawwidth="297" data-rawheight="30"&gt;&lt;p&gt;生成器G以最小化如下损失为目标：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-e5fddc734468e71c72f2823286cbec76.png" data-rawwidth="187" data-rawheight="40"&gt;— 图像空间损失：用生成数据与真实数据的L2损失来表示，对像素层面的相似性进行约束，即为&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-62c7aee39d00660541691b9a2ca28e16.png" data-rawwidth="157" data-rawheight="22"&gt;&lt;p&gt;最终的目标函数为三个loss项的加权和。&lt;/p&gt;&lt;p&gt;其实验结果非常值得关注，因其清晰的表明了各个loss的作用，如下图所示。可以看出，如果没有对抗损失Ladv，产生的结果非常差；如果没有特征空间的损失项Lfeat，会使产生的图像只有大概的轮廓信息，但会丢失许多细节信息；如果没有图像空间损失Limg，最终产生的结果跟有Limg差不多，但在训练的时候没有这一项的话会使网络更容易不稳定；而同时利用三项loss的结果则可以相对稳定的产生出较为清晰的图像。目前该度量在许多基于GAN的模型中都得到了应用。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-76f06a9214b8ba64b1be042e57032e82.png" data-rawwidth="629" data-rawheight="456"&gt;&lt;p&gt;&lt;b&gt;(2) 超分图像生成 (ECCV-2016; ARXIV-2016)：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在与上述工作的几乎同时期(相差仅一个月)，Li Fei-Fei团队也提出了类似的perceptual loss ([11], ECCV-2016)，通过网络中间层的特征图的差异来作为代价函数，利用GAN的框架，进行&lt;b&gt;风格迁移和超分图像的生成任务；&lt;/b&gt;&lt;/p&gt;&lt;p&gt;时隔约半年后，2016年9月Twitter的SRGAN[12]基于上述损失，提出一种新的损失函数与GAN本身的loss结合，实现了从低分辨率图像到超分辨率图像的生成。SRGAN与上述NIPS-2016工作的主要不同是：(1) 将图像空间的损失替换成了一个对生成图像整体方差的约束项，以保证图像的平滑性；(2) 采用了某种规则化的特征图差异损失，而不是直接累加求和：SRGAN将生成数据和真实数据分别输入VGG-19网络，根据得到的feature map的差异来定义损失项，其形式与NIPS-2016的主要不同在于加入了规则化的处理 (normalization)，从而变成：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-180d9f7aeea793b8de22a82bca6bac74.png" data-rawwidth="352" data-rawheight="35"&gt;其中Wij, Hij为feature map的宽和高，ϕ(i,j)表示在VGG-19的网络中第i个max pooling层前的第j个卷积层。最后，结合这三个损失项：对抗损失、图像平滑项、特征图差异，送入GAN框架，可以生成相对其它方法明显效果好的超分辨率图像，其对比如下图所示：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-0ed7ddcce0a452c7ad35d33ab1b91ad0.png" data-rawwidth="1505" data-rawheight="584"&gt;&lt;p&gt;&lt;b&gt;2.2 常规的从噪声数据生成图像和给定属性产生图像的任务可以看做是从噪声到图（输入为噪声，输出为图像）和从图到图（输入为图像，输出为图像）的问题，而ICML-2016上的工作[13]另辟蹊径，实现Image Captioning的反任务，即从文本描述生成图像。该文也是第一个提出用GAN的框架来实现从文本生成图像的工作，对于推动GAN以及产生式模型在实际中的进一步应用具有一定意义：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;该文实现的任务是产生满足文本描述的图像，相当于是以文本描述为条件来产生图像，因此可以在某种程度上看做是对原始的conditional GAN模型(ARXIV-2014) 的一种扩展和应用。其模型架构如下图所示，将文本进行编码后的特征与随机噪声信息串接输入产生器产生图像；而编码后的文本特征也同时作为监督信号输入判别器以构建目标函数。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-9337ae58a6b327c02151d617a69a55b5.png" data-rawwidth="1459" data-rawheight="372"&gt;其效果也非常可观，如下图所示，可以看出，大部分时候都能产生出与文本意义相对应的图像。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-58da14b66e5ae965319e9cf547722c53.png" data-rawwidth="1457" data-rawheight="300"&gt;&lt;p&gt;&lt;b&gt;2.3 在处理静态图像的生成任务的同时，GAN也逐渐被扩展到了视频处理领域，NIPS-2016上的[14]即为一个代表性工作，该工作可以同时生成和预测下一视频帧：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;为了产生出具有时域变化的视频帧，该模型在生成器部分将动态前景部分和静态背景部分分开建模和生成，构建two-stream的样本生成器，然后将产生的前景和背景进行组合得到产生出的video;对于判别器，主要完成两个任务：区分出产生数据与真实数据，同时要识别出视频帧间进行的行为，从而指导生成器去产生数据。其结构如下图所示。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-c2a969a6d548aa7a5acd64dfcdd6f91f.png" data-rawwidth="1118" data-rawheight="504"&gt;&lt;p&gt;&lt;b&gt;三. 训练技巧：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;由于GAN的模型不稳定性问题比较突出，因而在2016年出现的关于GAN训练技巧的成果有许多，目前被广泛应用的主要包括：DCGAN (ICLR-2016) 和Improved GAN (NIPS-2016 workshop)，特别是DCGAN，几乎在各大GAN模型中都能看到它的身影。&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-39fa4ba7c4d8b4de69b6d56a1e0d52e0.png" data-rawwidth="821" data-rawheight="335"&gt;&lt;p&gt;&lt;b&gt;3.1 DCGAN (ICLR-2016)：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;DCGAN 的模型结构如上图所示[15]，其输入为100维的噪声向量，经过一系列的strided conv操作，形成64x64的图像，即为G(z)，而判别器结构与之类似，只是是由一系列的卷积操作构成 (而非strided conv)，最后由average pooling形成判别器的标量输出。在本文中，最主要的是提出了以下三条有助于稳定训练GAN的方法：&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1)&lt;/b&gt; 去掉max pooling操作：用strided conv代替原来的pooling操作，使网络自动学习合适的采样核函数；&lt;/p&gt;&lt;p&gt;&lt;b&gt;(2) &lt;/b&gt;去掉全连接层：用global average pooling代替全连接层；虽然该操作可能会导致收敛速度变慢，但有助于整体训练的稳定性；&lt;/p&gt;&lt;p&gt;&lt;b&gt;(3) &lt;/b&gt;加入BN层：之前的LAPGAN (NIPS-2015) 指出如果像常规模型一样对所有层都施加BN，则会引起GAN的模型崩溃，而DCGAN通过对generator的输出层和discriminator的输入层不用BN，而其他层都用BN，则缓解了模型崩溃问题，并且有效避免了模型的振荡和不稳定问题。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(4) &lt;/b&gt;激活函数的选择：在generator中除了输出层用tanh外，其余都用RELU函数；而在discriminator中采用leaky ReLU函数。&lt;/p&gt;&lt;p&gt;目前前三点已几乎成为当下诸多GAN模型实现的标配；而许多基于GAN模型的实验设计也都是基于DCGAN的结构或总结的上述原则而进行的，包括前面OpenAI的Info-GAN、NIPS-2016的视频生成模型、Twitter的超分辨率的图像生成模型等等。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.2 Improved GAN (NIPS-2016 workshop)：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;该工作[16]主要给出了5条有助于GAN稳定训练的经验：&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1)&lt;/b&gt; 特征匹配：让生成器产生的样本与真实样本在判别器中间层的响应一致，即使判别器从真实数据和生成数据中提取的特征一致，而不是在判别器网络的最后一层才做判断，有助于提高模型的稳定性；其实验也表明在一些常规方法训练GAN不稳定的情况中，若用特征匹配则可以有效避免这种不稳定；&lt;/p&gt;&lt;p&gt;&lt;b&gt;(2)&lt;/b&gt; Minibatch Discrimination：在判别器中，不再每次对每一个生成数据与真实数据的差异性进行比较，而是一次比较一批生成数据与真实数据的差异性。这种做法提高了模型的鲁棒性，可以缓解生成器输出全部相似或相同的问题；&lt;/p&gt;&lt;p&gt;&lt;b&gt;(3)&lt;/b&gt; Historical Averaging：受fictitious play的游戏算法启发，作者提出在生成器和判别器的目标函数中各加一个对参数的约束项&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-52fc29d4816a4e9a6b1870793d1a4762.png" data-rawwidth="113" data-rawheight="40"&gt;&lt;p&gt;其中θ[i]表示在时刻i的模型参数，该操作可以在一些情况下帮助模型达到模型的平衡点；&lt;/p&gt;&lt;p&gt;&lt;b&gt;(4) &lt;/b&gt;单边标签平滑 (One-sided Label Smoothing)：当向GAN中引入标签数据时，最好是将常规的0、1取值的二值标签替换为如0.1,0.9之类的平滑标签，可以增加网络的抗干扰能力；但这里之所以说单边平滑，是因为假设生成数据为0.1而非0的话会使判别器的最优判别函数的形状发生变化，会使生成器偏向于产生相似的输出，因此对于取值0的标签保持不变，不用0.1一类的小数据替换，即为单边标签平滑；&lt;/p&gt;&lt;p&gt;&lt;b&gt;(5)&lt;/b&gt; Virtual Batch Normalization：VBN相当于是BN的进阶版，BN是一次对一批数据进行归一化，这样的一个副作用是当“批”的大小不同时，BN操作之后的归一化常量会引起训练过程的波动，甚至超过输入信号z的影响（因z是随机噪声）；而VBN通过引入一个参考集合，每次将当下的数据x加入参考集合构建一个新的虚拟的batch，然后在这个虚拟的batch上进行归一化，如此可以缓解原始BN操作所引起的波动问题。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.3 Github资源：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;针对GAN训练过程的不稳定问题， Soumith等人 在github上专门总结了一个文档，其地址为&lt;a href="https://github.com/soumith/ganhacks" class="" data-editable="true" data-title="soumith/ganhacks"&gt;soumith/ganhacks&lt;/a&gt;，总结了17种可以尝试的方法，可以为多数训练过程提供参考。 &lt;/p&gt;&lt;p&gt;&lt;b&gt;总结&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本文主要列出了2016年中部分较有代表性或应用较为广泛的工作，而实际上，在过去一年里对于GAN的扩展和改进工作还有许多，包括GAN与其他方法如强化学习的融合、GAN在半监督学习领域的扩展等都值得关注。&lt;/p&gt;&lt;p&gt;GAN提供的不仅仅是单一的某个模型，而是一种框架，从这个角度来说，GAN可以与许多其它方法进行融合，在GAN的框架下进行融合；但目前GAN的训练稳定性问题仍未能很好地解决，甚至Ian J. Goodfellow自己也认为相对于GAN的稳定性问题而言，GAN新架构的开发反而显得关系不大；同时GAN也面临着一个稍许尴尬的问题，即缺乏客观评估，其产生样本的质量好坏仍然依赖人眼去判断。另外，从应用的角度来说，目前多数方法都是在GAN原始框架的基础上稍作修改，比如修改损失函数，或者在条件GAN或LAPGAN的基础上改进，但目前并没有一个具有突破性压倒性的图像生成模型，可能这也和GAN缺乏客观的评估指标有关；综合这些问题，GAN还有很长的路可以走......&lt;/p&gt;&lt;p&gt;&lt;b&gt;参考文献&lt;/b&gt;&lt;/p&gt;&lt;p&gt;[1] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, Yoshua Bengio. Generative Adversarial Nets. NIPS 2014: 2672-2680&lt;/p&gt;&lt;p&gt;[2] T. Jebara. Discriminative, generative and imitative learning . PhD Thesis, Media Laboratory, MIT, December 2001.&lt;/p&gt;&lt;p&gt;[3] Zhuowen Tu, Learning Generative Models via Discriminative Approaches. CVPR 2007&lt;/p&gt;&lt;p&gt;[4] Jun Zhu, Amr Ahmed, Eric P. Xing. MedLDA: maximum margin supervised topic models. Journal of Machine Learning Research 13: 2237-2278 (2012)&lt;/p&gt;&lt;p&gt;[5] Sebastian Nowozin, Botond Cseke, Ryota Tomioka. f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization. NIPS (2016)&lt;/p&gt;&lt;p&gt;[6] Junbo Zhao, Michael Mathieu, Yann LeCun，Energy-based Generative Adversarial Network，ICLR-2017，open review.&lt;/p&gt;&lt;p&gt;[7] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, ,Ilya Sutskever, Pieter Abbeel, InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets&lt;/p&gt;&lt;p&gt;[8] Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, Ole Winther. Autoencoding beyond pixels using a learned similarity metric, ICML-2016.&lt;/p&gt;&lt;p&gt;[9] Jost Tobias Springenberg , unsupervised and semi-supervised learning with categorical generative adversarial networks, ICLR-2016.&lt;/p&gt;&lt;p&gt;[10] Alexey Dosovitskiy, Thomas Brox. Generating Images with Perceptual Similarity Metrics based on Deep Networks. NIPS 2016.&lt;/p&gt;&lt;p&gt;[11] Justin Johnson, Alexandre Alahi, Li Fei-Fei. Perceptual Losses for Real-Time Style Transfer and Super-Resolution. ECCV-2016.&lt;/p&gt;&lt;p&gt;[12] Christian Ledig, Lucas Theis, Ferenc Husz´ ar, Jose Caballero, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi. Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.ARXIV-2016&lt;/p&gt;&lt;p&gt;[13] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, Honglak Lee. Generative Adversarial Text to Image Synthesis, ICML-2016.&lt;/p&gt;&lt;p&gt;[14] Carl Vondrick, Hamed Pirsiavash, Antonio Torralba. Generating Videos with Scene Dynamics, NIPS-2016;&lt;/p&gt;&lt;p&gt;[15] Alec Radford, Luke Metz, Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks, ICLR-2016;&lt;/p&gt;&lt;p&gt;[16] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen. Improved Techniques for Training GANs, nips-2016, workshop.&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing/answers" data-title="@果果是枚开心果." class="" data-editable="true"&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-639e9479f6094230c79df15271825ca3.png" data-rawwidth="119" data-rawheight="116"&gt;&lt;b&gt;杨双，&lt;/b&gt;Shuang Yang Received her Ph.D. degree from the Institute of Automation (IA), University of Chinese Academy of Sciences (CAS) in 2016. She is currently an assistant professor with the Key Laboratory of Intelligent Information Processing, Institute of Computing Technology (ICT), Chinese Academy of Sciences (CAS). Her research interests cover deep learning, generative methods, Bayesian modeling and inference, sequence modeling, etc.&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s/5JbL2nelyBD_9rFKUypE5g" data-editable="true" data-title="【青年学者专栏】解读GAN及其 2016 年度进展"&gt;【青年学者专栏】解读GAN及其 2016 年度进展&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/25000523&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Mon, 23 Jan 2017 15:31:55 GMT</pubDate></item><item><title>ILSVRC2016目标检测任务回顾（下）--视频目标检测（VID）</title><link>https://zhuanlan.zhihu.com/p/24937107</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-8f0fa2534ea94706a408b7846f9e68fc_r.jpg"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;图像目标检测任务在过去三年的时间取得了巨大的进展，检测性能得到明显提升。但在视频监控、车辆辅助驾驶等领域，基于视频的目标检测有着更为广泛的需求。由于视频中存在运动模糊，遮挡，形态变化多样性，光照变化多样性等问题，仅利用图像目标检测技术检测视频中的目标并不能得到很好的检测结果。如何利用视频中目标时序信息和上下文等信息成为提升视频目标检测性能的关键。&lt;/p&gt;&lt;p&gt;ILSVRC2015新增加了视频目标检测任务（Object detection from video, VID），这为研究者提供了良好的数据支持。ILSVRC2015的VID评价指标与图像目标检测评价指标相同——计算检测窗口的mAP。然而对于视频目标检测来说，一个好的检测器不仅要保证在每帧图像上检测准确，还要保证检测结果具有一致性/连续性（即对于一个特定目标，优秀的检测器应持续检测此目标并且不会将其与其他目标混淆）。ILSVRC2016针对这个问题在VID任务上新增加了一个子任务（详见第四部分——视频目标检测时序一致性介绍）。&lt;/p&gt;&lt;p&gt;在ILSVRC2016上，在不使用外部数据的VID两个子任务上，前三名由国内队伍包揽（见表1、表2）。本文主要结合NUIST，CUVideo，MCG-ICT-CAS以及ITLab-Inha四个队伍公布的相关资料对ILSVRC2016中的视频目标检测方法进行了总结。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-17628572368fde4edf3034fb760528cf.jpg" data-rawwidth="530" data-rawheight="439"&gt;&lt;p&gt;通过对参赛队伍的相关报告[2-5]进行学习了解，视频目标检测算法目前主要使用了如下的框架:&lt;/p&gt;&lt;p&gt;将视频帧视为独立的图像，利用图像目标检测算法获取检测结果；&lt;/p&gt;&lt;p&gt;利用视频的时序信息和上下文信息对检测结果进行修正；&lt;/p&gt;&lt;p&gt;基于高质量检测窗口的跟踪轨迹对检测结果进一步进行修正。&lt;/p&gt;&lt;p&gt;本文分为四部分，前三个部分介绍如何提升视频目标检测的精度，最后介绍如何保证视频目标检测的一致性。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.单帧图像目标检测&lt;/b&gt;&lt;/p&gt;&lt;p&gt;此阶段通常将视频拆分成相互独立的视频帧来处理，通过选取优秀的图像目标检测框架以及各种提高图像检测精度的技巧来获取较为鲁棒的单帧检测结果。《ILSVRC2016目标检测任务回顾（上）--图像目标检测》已对此进行详细总结，这里不再重复。&lt;/p&gt;&lt;p&gt;结合自己实验及各参赛队伍的相关文档，我们认为&lt;b&gt;训练数据的选取&lt;/b&gt;以及&lt;b&gt;网络结构的选择&lt;/b&gt;对提升目标检测性能有至关重要的作用。&lt;/p&gt;&lt;p&gt;&lt;b&gt;训练数据选取&lt;/b&gt;&lt;/p&gt;&lt;p&gt;首先对ILSVRC2016 VID训练数据进行分析: VID数据库包含30个类别，训练集共有3862个视频片段，总帧数超过112万。单从数字上看，这么大的数据量训练30个类别的检测器似乎已经足够。然而，同一个视频片段背景单一，相邻多帧的图像差异较小。所以要训练现有目标检测模型，VID训练集&lt;b&gt;存在大量数据冗余&lt;/b&gt;，并且&lt;b&gt;数据多样性较差&lt;/b&gt;，有必要对其进行扩充。在比赛任务中，可以从ILSVRC DET和ILSVRC LOC数据中抽取包含VID类别的图片进行扩充。CUVideo、NUIST和MCG-ICT-CAS使用ILSVRC VID+DET作为训练集，ITLab-Inha使了ILSVRC VID+DET、COCO DET等作为训练集。需要注意的是在构建新的训练集的时候要注意平衡样本并去除冗余（CUVideo和MCG-ICT-CAS抽取部分VID训练集训练模型，ITLab-Inha在每个类别选择一定数量图像参与训练，NUIST使用在DET上训练的模型对VID数据进行筛选）。对于同样的网络，使用扩充后的数据集可以提高10%左右的检测精度。&lt;/p&gt;&lt;p&gt;&lt;b&gt;网络结构选取&lt;/b&gt;&lt;/p&gt;&lt;p&gt;不同的网络结构对于检测性能也有很大影响。我们在VID验证集上进行实验：同样的训练数据，基于ResNet101[6]的Faster R-CNN[7]模型的检测精度比基于VGG16[8]的Faster R-CNN模型的检测精度高12%左右。这也是MSRA在2015年ILSVRC和COCO比赛上的制胜关键。今年比赛前几名的队伍基本上也是使用ResNet/Inception的基础网络，CUVideo使用269层的GBD-Net[9]。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.改进分类损失&lt;/b&gt;&lt;/p&gt;&lt;p&gt;目标在某些视频帧上会存在运动模糊，分辨率较低，遮挡等问题，即便是目前最好的图像目标检算法也不能很好地检测目标。幸运的是，视频中的时序信息和上下文信息能够帮助我们处理这类问题。比较有代表性的方法有T-CNN[10]中的运动指导传播（Motion-guided Propagation, MGP）和多上下文抑制（Multi-context suppression, MCS）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;MGP&lt;/b&gt;&lt;/p&gt;&lt;p&gt;单帧检测结果存在很多漏检目标，而相邻帧图像检测结果中可能包含这些漏检目标。所以我们可以借助光流信息将当前帧的检测结果前向后向传播，经过MGP处理可以提高目标的召回率。如图1所示将T时刻的检测窗口分别向前向后传播，可以很好地填补T-1和T+1时刻的漏检目标。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-8f0fa2534ea94706a408b7846f9e68fc.jpg" data-rawwidth="632" data-rawheight="386"&gt;&lt;p&gt;&lt;b&gt;MCS&lt;/b&gt;&lt;/p&gt;&lt;p&gt;使用图像检测算法将视频帧当做独立的图像来处理并没有充分利用整个视频的上下文信息。虽然说视频中可能出现任意类别的目标，但对于单个视频片段，只会出现比较少的几个类别，而且这几个类别之间有共现关系（出现船只的视频段中可能会有鲸鱼，但基本不可能出现斑马）。所以，可以借助整个视频段上的检测结果进行统计分析：对所有检测窗口按得分排序，选出得分较高的类别，剩余那些得分较低的类别很可能是误检，需对其得分进行压制（如图2）。经过MCS处理后的检测结果中正确的类别靠前，错误的类别靠后，从而提升目标检测的精度。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-4699fda9b18125df3181b619d9e331f7.jpg" data-rawwidth="650" data-rawheight="310"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-4699fda9b18125df3181b619d9e331f7.jpg" data-rawwidth="650" data-rawheight="310"&gt;&lt;p&gt;&lt;b&gt;3.利用跟踪信息修正&lt;/b&gt;&lt;/p&gt;&lt;p&gt;上文提到的MGP可以填补某些视频帧上漏检的目标，但对于多帧连续漏检的目标不是很有效，而目标跟踪可以很好地解决这个问题。CUVideo, NUIST, MCG-ICT-CAS以及ITLab-Inha四支参赛队伍都使用了跟踪算法进一步提高视频目标检测的召回率。使用跟踪算法获取目标序列基本流程如下：&lt;/p&gt;&lt;p&gt;使用图像目标检测算法获取较好的检测结果；&lt;/p&gt;&lt;p&gt;从中选取检测得分最高的目标作为跟踪的起始锚点；&lt;/p&gt;&lt;p&gt;基于选取的锚点向前向后在整个视频片段上进行跟踪，生成跟踪轨迹；&lt;/p&gt;&lt;p&gt;从剩余目标中选择得分最高的进行跟踪，需要注意的是如果此窗口在之前的跟踪轨迹中出现过，那么直接跳过，选择下一个目标进行跟踪；&lt;/p&gt;&lt;p&gt;算法迭代执行，可以使用得分阈值作为终止条件。&lt;/p&gt;&lt;p&gt;得到的跟踪轨迹既可以用来提高目标召回率，也可以作为长序列上下文信息对结果进行修正。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.网络选择与训练技巧&lt;/b&gt;&lt;/p&gt;&lt;p&gt;对于视频目标检测，除了要保证每帧图像的检测精度，还应该保证长时间稳定地跟踪每个目标。为此，ILSVRC2016新增一个VID子任务，此任务计算每个目标跟踪轨迹(tracklet)/管道(tubelet)的mAP来评测检测算法的时序一致性或者说跟踪连续性的性能。&lt;/p&gt;&lt;p&gt;评价指标：图像目标检测mAP评测对象是每个检测窗口是否精准，而视频时序一致性评测对象是目标跟踪轨迹是否精准；图像目标检测中如果检测窗口跟Ground Truth类别相同，窗口IoU大于0.5就认定为正例。而评价时序一致性时，如果检测得到的跟踪轨迹和Ground Truth（目标真实跟踪轨迹）是同一个目标（trackId相同），并且其中检测出的窗口与Ground Truth窗口的IoU大于0.5的数量超过一个比例，那么认为得到的跟踪轨迹是正例；跟踪轨迹的得分是序列上所有窗口得分的平均值。分析可知，如果一个目标的轨迹被分成多段或者一个目标的跟踪轨迹中混入其他的目标都会降低一致性。&lt;/p&gt;&lt;p&gt;那么如何保证视频检测中目标的时序一致性呢？本文认为可以从以下三个方面入手：（1）保证图像检测阶段每帧图像检测的结果尽量精准；（2）对高质量检测窗口进行跟踪并保证跟踪的质量（尽量降低跟踪中出现的漂移现象）；（3）前面两步获取到的跟踪结果会存在重叠或者临接的情况，需针对性地进行后处理。&lt;/p&gt;&lt;p&gt;ITLab-Inha团队提出了基于变换点检测的多目标跟踪算法[11]，该算法首先检测出目标，然后对其进行跟踪，并在跟踪过程中对跟踪轨迹点进行分析处理，可以较好地缓解跟踪时的漂移现象，并能在轨迹异常时及时终止跟踪。&lt;/p&gt;&lt;p&gt;针对视频目标检测的一致性问题，作者所在的MCG-ICT-CAS提出了基于检测和跟踪的目标管道生成方法。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-68eefe0171db426625b8ae97bf9200c0.jpg" data-rawwidth="652" data-rawheight="464"&gt;&lt;p&gt;图3-a表示使用跟踪算法获取到的目标管道（红色包围框），绿色包围框代表目标的Ground Truth。可以看到随着时间推移，跟踪窗口逐渐偏移目标，最后甚至可能丢失目标。MCG-ICT-CAS提出了基于检测的目标管道生成方法，如图3-b所示，基于检测的管道窗口（红色包围框）定位较为准确，但由于目标的运动模糊使检测器出现漏检。从上面分析可知：跟踪算法生成的目标管道召回率较高，但定位不准；而基于检测窗口生成的目标管道目标定位较为精准，但召回率相对前者较低。由于两者存在互补性，所以MCG-ICT-CAS进一步提出了管道融合算法，对检测管道和跟踪管道进行融合，融合重复出现的窗口并且拼接间断的管道。&lt;/p&gt;&lt;p&gt;如图4所示，相对于单独的检测或者跟踪生成的目标管道，融合后目标管道对应的检测窗口的召回率随着IoU阈值的增加一直保持较高的值，说明了融合后的窗口既能保持较高的窗口召回率，也有较为精准的定位。融合后的目标管道mAP在VID测试集上提升了12.1%。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-4301743ca9b80528edc894c87114ef55.jpg" data-rawwidth="652" data-rawheight="424"&gt;&lt;p&gt;&lt;b&gt;总结&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本文主要结合ILSVRC2016 VID竞赛任务对视频目标检测算法进行介绍。相对于图像目标检测，当前的视频目标检测算法流程比较繁琐且视频自身包含的信息没有被充分挖掘。如何精简视频目标检测流程使其具有实时性，如何进一步挖掘视频包含的丰富信息使其具有更高的检测精度，以及如何保证视频目标检测的一致性或许是视频目标检测接下来要着重解决的问题。&lt;/p&gt;&lt;p&gt;[1]ILSVRC2016相关报告：&lt;/p&gt;&lt;p&gt;&lt;a href="http://image-net.org/challenges/ilsvrc+coco2016" data-editable="true" data-title="ILSVRC and COCO workshop 2016"&gt;ILSVRC and COCO workshop 2016&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[2]CUVideo slide下载链接：&lt;/p&gt;&lt;p&gt;&lt;a href="http://image-net.org/challenges/talks/2016/GBD-Net.pdf" data-editable="true" data-title="image-net.org 的页面"&gt;http://image-net.org/challenges/talks/2016/GBD-Net.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[3]NUIST slide下载链接&lt;/p&gt;&lt;p&gt;&lt;a href="http://image-net.org/challenges/talks/2016/Imagenet%202016%20VID.pptx" data-editable="true" data-title="image-net.org 的页面"&gt;http://image-net.org/challenges/talks/2016/Imagenet%202016%20VID.pptx&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[4]MCG-ICT-CAS slide下载链接&lt;/p&gt;&lt;p&gt;&lt;a href="http://image-net.org/challenges/talks/2016/MCG-ICT-CAS-ILSVRC2016-Talk-final.pdf" data-editable="true" data-title="image-net.org 的页面"&gt;http://image-net.org/challenges/talks/2016/MCG-ICT-CAS-ILSVRC2016-Talk-final.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[5]ITLab-Inha slide 下载链接&lt;/p&gt;&lt;p&gt;&lt;a href="http://image-net.org/challenges/talks/2016/ILSVRC2016_ITLab_for_pdf.pdf" data-editable="true" data-title="image-net.org 的页面"&gt;http://image-net.org/challenges/talks/2016/ILSVRC2016_ITLab_for_pdf.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[6]He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[J]. arXiv preprint arXiv:1512.03385, 2015.&lt;/p&gt;&lt;p&gt;[7]Ren S, He K, Girshick R, et al. Faster R-CNN: Towards real-time object detection with region proposal networks[C]//Advances in neural information processing systems. 2015: 91-99.&lt;/p&gt;&lt;p&gt;[8]Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.&lt;/p&gt;&lt;p&gt;[9]Zeng X, Ouyang W, Yang B, et al. Gated bi-directional cnn for object detection[C]//European Conference on Computer Vision. Springer International Publishing, 2016: 354-369.&lt;/p&gt;&lt;p&gt;[10]Kang K, Li H, Yan J, et al. T-cnn: Tubelets with convolutional neural networks for object detection from videos[J]. arXiv preprint arXiv:1604.02532, 2016.&lt;/p&gt;&lt;p&gt;[11]Lee B, Erdenee E, Jin S, et al. Multi-class Multi-object Tracking Using Changing Point Detection[C]//European Conference on Computer Vision. Springer International Publishing, 2016: 68-83.&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing/answers" class="" data-editable="true" data-title="@果果是枚开心果."&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-97571b85f2ee2d0a7907d9b8befb65be.jpg" data-rawwidth="120" data-rawheight="117"&gt;&lt;b&gt;王斌，&lt;/b&gt;中科院计算所前瞻研究实验室跨媒体计算组博士生，导师张勇东研究员。2016年在唐胜副研究员的带领下，作为计算所MCG-ICT-CAS团队核心主力队员（王斌、肖俊斌），参加了ImageNet大规模视觉识别挑战赛(ILSVRC)的视频目标检测（VID）任务并获得第三名。目标检测相关工作受邀在ECCV 2016 ImageNet和COCO竞赛联合工作组会议（ImageNet and COCO Visual Recognition Challenges Joint Workshop）上做大会报告。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s/mQ78KNuaHUTox3ql6rU-Nw" data-editable="true" data-title="ILSVRC2016目标检测任务回顾（下）--视频目标检测（VID）" class=""&gt;ILSVRC2016目标检测任务回顾（下）--视频目标检测（VID）&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24937107&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Wed, 18 Jan 2017 14:50:21 GMT</pubDate></item><item><title>ILSVRC2016目标检测任务回顾（上）--图像目标检测（DET）</title><link>https://zhuanlan.zhihu.com/p/24934382</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-a198f7d2b2263a65bcef73d58d9406a4_r.jpg"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-a198f7d2b2263a65bcef73d58d9406a4.jpg" data-rawwidth="638" data-rawheight="509"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-2f9473f84f19d29095e3be07c460e40d.jpg" data-rawwidth="640" data-rawheight="631"&gt;&lt;p&gt;计算机视觉领域权威评测——ImageNet大规模图像识别挑战赛（Large Scale Visual Recognition Challenge）自2010年开始举办以来，一直备受关注。2016年，在该比赛的图像目标检测任务中，国内队伍大放异彩，包揽该任务前五名（如图1所示）。我们将根据前五名参赛队伍提交的摘要与公开发表的论文或技术文档，简析比赛中用到的图像目标检测方法。&lt;/p&gt;&lt;p&gt;总体上说，参赛队伍大多采用ResNet/Inception网络+Faster R-CNN框架，注重网络的预训练，改进RPN，并利用Context信息，测试时结合普遍被使用的多尺度测试、水平翻转、窗口投票等方法，最终融合多个模型得到结果。&lt;/p&gt;&lt;p&gt;下面我们将细数参赛方法中的诸多亮点。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.利用Context信息&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;GBD-Net&lt;/b&gt;&lt;/p&gt;&lt;p&gt;GBD-Net（Gated Bi-Directional CNN [1]）是CUImage团队的成果，也是今年DET任务中的一大亮点。该方法利用双向门控的CNN网络在不同尺度的上下文窗口中选择性地传递信息，以此对context建模。&lt;/p&gt;&lt;p&gt;GBD-Net的研究动机源于对context信息在候选窗口分类过程中起到的作用的仔细分析。首先，Context信息在对窗口分类时能起到关键的作用，如图2(a)(b)所示，图中的红框必须结合Context信息才能准确判断其类别（包括判断为背景）。所以很多时候，我们可以利用context信息作出如图1(c)所示的判断。但是如图1(d)所示，并不是所有的context信息都能给我们正确的指导，所以context信息需要选择性的利用。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-b16815442ea36ea3bbc955c29715b47d.jpg" data-rawwidth="645" data-rawheight="409"&gt;基于这一点，CUImage提出了GBD-Net。如图3所示，GBD-Net采集Context信息的方式与[2][3]一脉相承，直接在目标窗口基础上放大窗口以获得更多的context信息，或缩小窗口以保留更多的目标细节，以此得到多个support region，双向连接的网络让不同尺度和分辨率的信息在每个support region之间相互传递，从而综合学习到最优的特征。然而如研究动机中所说，并非所有的上下文信息都能给决策带来“正能量”，所以在双向互通的连接上都加了一个“门”，以此控制context信息的相互传播。GBD-Net在ImageNet DET数据集上，在ResNet-269为基础网络，带来了2.2%的mAP提升。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-65f46c883d49e04fb022a7f3cb9014bb.jpg" data-rawwidth="653" data-rawheight="354"&gt;&lt;p&gt;&lt;b&gt;Dilation as context&lt;/b&gt;&lt;/p&gt;&lt;p&gt;360+MCG-ICG-CAS_DET团队[12]将[4]中提出的用膨胀卷积获取context信息的方法迁移至目标检测任务，将[4]中的8个膨胀卷积层削减到3层，在ROI pooling前就组织好每个像素点对应的context信息，如图4，省去了对每个ROI反复提取context特征的操作。该方法在VOC07数据集上，以Res50为基础网络，能获得1.5%的提升。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-39d5825e8cb13709e91280d9f1074cf9.jpg" data-rawwidth="669" data-rawheight="237"&gt;&lt;p&gt;&lt;b&gt;Global context&lt;/b&gt;&lt;/p&gt;&lt;p&gt;2015年[5]中提到利用ROI pooling对全图进行pooling获取context信息的方法，Hikvision团队在此基础上进一步细化，提出了图5(a)所示的global context方法，在ILSVRC DET验证集上获得了3.8%的mAP性能提升。该方法此前的文章[13]中有详细描述，此处不再赘述。&lt;/p&gt;&lt;p&gt;除了基于ROI pooling的global context方法，CUImage沿用[6]中提到的global context方法，为每个ROI加入全局的分类结果信息，如图5(b)示。该方法在GBD-net局部context的基础上又加入了全局的context信息，进一步将mAP提高了1.3%。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-cdaae8c89fcafbac49a52fff6cb1740e.jpg" data-rawwidth="636" data-rawheight="355"&gt;&lt;p&gt;&lt;b&gt;2.改进分类损失&lt;/b&gt;&lt;/p&gt;&lt;p&gt;360+MCG-ICG-CAS_DET团队提出了两种改进的softmax损失[12]：将背景类分成若干隐式子类别（Implicit sub-categories for background）、必要时加入sink类别（Sink class when necessary）。&lt;/p&gt;&lt;p&gt;Faster R-CNN中将所有与Ground Truth的IOU大于0.5的窗口当做正样本，IOU介于0.1~0.4之间的当做背景类别样本，所以虽然正常目标类别的样本之间有较大的相似性，但背景类别的样本之间差异却非常大，在这种情况下，仍然同等对待目标类别和背景类别对背景类别来说是不公平的。所以背景隐式子类别方法将背景类别分为若干个子类别，想让更多的参数去描述多变的背景，在softmax之前重新将所有子类别聚合为一个背景类，以避免显式定义各个子类别的问题（如图6(a)所示）。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-34d023fe9d291123ed4e532d3825f9d7.jpg" data-rawwidth="637" data-rawheight="402"&gt;&lt;p&gt;另外，由于训练数据本身的一些冲突性（比如同样的图像内容在不同场景下会同时成为正样本和负样本，或不同类别的样本之间非常相似），对于某些窗口，ground truth类别的得分始终不是很高，而其他一些错误类别的得分会超过ground truth类别。所以sink方法加入一个sink类，在ground truth得分不在Top-K时，同时优化sink类别和ground truth类别，否则正常优化ground truth类别。以此将那些错误类别上的得分引流到sink类别上，使得在对窗口分类时，即使ground truth类别得分不是特别高，仍然可以高于其他类别，如图6(b)所示。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.改进RPN&lt;/b&gt;&lt;/p&gt;&lt;p&gt;CUImage和Hikvision都提出改进RPN，并且两者的改进策略都源于CRAFT[7]（如图7所示），在RPN之后再连接一个二分类的Fast R-CNN，进一步减少窗口数量并提高定位精度。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-250def80cd423ba412d42a3507279542.jpg" data-rawwidth="660" data-rawheight="269"&gt;&lt;p&gt;CUImage进一步将CRAFT升级为CRAFT-v3，训练过程加入随机crop，测试中采取多尺度策略，并且平衡正负样本比例，用2个模型进行融合，将ILSVRC DET val2上的recall@300 proposal提升到95.3%[14]。&lt;/p&gt;&lt;p&gt;Hikvision则是直接按照box refinement的思想，直接在RPN网络基础上进行一次级联，如图8所示。同时他们注意到，Faster R-CNN在理想情况下希望PRN的正负样本比是1：1，而实际运行中，正样本数量往往较少，使得正负样本比差异较大，所以将正负样本比强制限制在不超过1：1.5后，recall提升3%。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-0579c90d5882b734afc6472586fee46d.jpg" data-rawwidth="652" data-rawheight="461"&gt;&lt;p&gt;&lt;b&gt;4.网络选择与训练技巧&lt;/b&gt;&lt;/p&gt;&lt;p&gt;自ILSVRC2015后，ResNet[4]和后续的Inception v4[8]，Identity mapping[9]由于其强大的分类性能，被广泛使用到目标检测、场景分割等应用中。不同的网络通常能收敛到不同的极值点，这种网络差异性是模型融合获得较大提升的关键。CUImage、Hikvision、Trimps Soushen、360+MCG-ICT-CAS_DET、NUIST都用不同的基础网络训练了多个模型用于融合。&lt;/p&gt;&lt;p&gt;在训练目标检测模型之前，具有针对性的模型预训练通常可以使得最后训练的目标检测模型能收敛到更优的位置。Hikvision提到在初始化global context的分支时使用预训练的模型效果远远好于随机初始化。另外，他们用ILSVRC LOC的数据首先在1000个类别上预训练一个精细分类的目标检测模型，再迁移到DET数据上训练200类的模型。CUImage同样提到模型预训练的重要性。他们在1000类Image-centric方式训练分类网络后，又采取基于ROI-Pooling的Object-centric方式训练分类网络，预训练网络使最终目标检测模型的mAP提升约1%。&lt;/p&gt;&lt;p&gt;此外，Hikvision提出在训练过程中强制平衡正负样本比会产生积极的影响。OHEM[10]、多尺度训练等技巧都是简单有效的提高mAP的方式。&lt;/p&gt;&lt;p&gt;&lt;b&gt;5.测试技巧&lt;/b&gt;&lt;/p&gt;&lt;p&gt;测试过程中可采用的技巧很多，会对最终目标检测结果起到锦上添花的作用。多尺度测试、水平翻转、窗口微调与多窗口投票、多模型融合、NMS阈值调整、多模型融合等方法被广泛使用，并经过普遍验证证明了其有效性。&lt;/p&gt;&lt;p&gt;Trimps Soushen、360+MCG-ICT-CAS_DET采用了Feature Maxout[11]的方法融入多尺度测试，尽量让每个窗口都缩放到接近224x224的尺度上进行测试，充分利用预训练网络的性能。窗口微调与多窗口投票（box refinement and box voting[2]）方法首先利用Fast R-CNN系列框架中对窗口进行回归的这个过程，反复迭代，然后用所有窗口投票，决定最终的目标类别与位置。在往年比赛中很少提到目标检测如何进行模型融合，ILSVRC2016中，CUImage[14]、Hikvision[15]、Trimps Soushen[16]、360+MCG-ICT-CAS_DET[12]都采用了几乎一致的融合策略，即先用一个或多个模型的RPN网络产生固定的ROI，再把这些ROI经过不同模型得到的分类和回归结果相加，得到最终的融合结果。经过多种融合方法的实验，分数相加的方式能获得较好的融合性能。&lt;/p&gt;&lt;p&gt;&lt;b&gt;总结&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本文对2016年ILSVRC DET任务中用到的方法进行了概括性的归纳和介绍。目标检测系统步骤甚多，过程繁琐，其中的每一个细节都非常重要。研究过程中，在把握整体结构的同时，如何处理好重要的细节会成为一种方法是否有效的关键。&lt;/p&gt;&lt;p&gt;&lt;b&gt;参考文献&lt;/b&gt;&lt;/p&gt;&lt;p&gt;[1] Zeng, Xingyu, et al. "Gated bi-directional cnn for object detection." European Conference on Computer Vision. Springer International Publishing, 2016.&lt;/p&gt;&lt;p&gt;[2] Gidaris, Spyros, and Nikos Komodakis. "Object detection via a multi-region and semantic segmentation-aware cnn model." Proceedings of the IEEE International Conference on Computer Vision. 2015.&lt;/p&gt;&lt;p&gt;[3] Zagoruyko, Sergey, et al. "A MultiPath Network for Object Detection." arXiv preprint arXiv:1604.02135 (2016).&lt;/p&gt;&lt;p&gt;[4] Yu, Fisher, and Vladlen Koltun. "Multi-scale context aggregation by dilated convolutions." arXiv preprint arXiv:1511.07122 (2015).&lt;/p&gt;&lt;p&gt;[5] He, Kaiming, et al. "Deep residual learning for image recognition." arXiv preprint arXiv:1512.03385 (2015).&lt;/p&gt;&lt;p&gt;[6] Ouyang, Wanli, et al. "Deepid-net: Deformable deep convolutional neural networks for object detection." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.&lt;/p&gt;&lt;p&gt;[7] Yang, Bin, et al. "Craft objects from images." arXiv preprint arXiv:1604.03239 (2016).&lt;/p&gt;&lt;p&gt;[8] Szegedy, Christian, Sergey Ioffe, and Vincent Vanhoucke. "Inception-v4, inception-resnet and the impact of residual connections on learning." arXiv preprint arXiv:1602.07261 (2016).&lt;/p&gt;&lt;p&gt;[9] He, Kaiming, et al. "Identity mappings in deep residual networks." arXiv preprint arXiv:1603.05027 (2016).&lt;/p&gt;&lt;p&gt;[10] Shrivastava, Abhinav, Abhinav Gupta, and Ross Girshick. "Training region-based object detectors with online hard example mining." arXiv preprint arXiv:1604.03540 (2016).&lt;/p&gt;&lt;p&gt;[11] Ren, Shaoqing, et al. "Object detection networks on convolutional feature maps." arXiv preprint arXiv:1504.06066 (2015).&lt;/p&gt;&lt;p&gt;[12] Sheng Tang, Yu Li, Bin Wang, Junbin Xiao, Rui Zhang etal. "MCG-ICT-CAS Object Detection at ILSVRC 2016" (Slides), the Second ImageNet and COCO Visual Recognition Challenges Joint Workshop in conjunction with ECCV 2016, &lt;a href="http://image-net.org/challenges/talks/2016/MCG-ICT-CAS-ILSVRC2016-Talk-final.pdf" data-editable="true" data-title="image-net.org 的页面"&gt;http://image-net.org/challenges/talks/2016/MCG-ICT-CAS-ILSVRC2016-Talk-final.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[13] 钟巧勇“技术揭秘：海康威视PASCAL VOC2012目标检测权威评测夺冠之道”，深度学习大讲堂往期文章&lt;a href="http://chuansong.me/n/839745651477" data-editable="true" data-title="技术揭秘：海康威视PASCAL VOC2012目标检测权威评测夺冠之道"&gt;技术揭秘：海康威视PASCAL VOC2012目标检测权威评测夺冠之道&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[14] Wanli Ouyang, Junjie Yan, Xingyu Zeng etal. “Crafting GBD-Net”(Slides), the Second ImageNet and COCO Visual Recognition Challenges Joint Workshop in conjunction with ECCV 2016, &lt;a href="http://image-net.org/challenges/talks/2016/GBD-Net.pdf" data-editable="true" data-title="image-net.org 的页面"&gt;http://image-net.org/challenges/talks/2016/GBD-Net.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[15] Qiaoyong Zhong, Chao Li, Yingying Zhang etal. “Towards Good Practices for Recognition &amp;amp; Detection” (Slides), the Second ImageNet and COCO Visual Recognition Challenges Joint Workshop in conjunction with ECCV 2016, &lt;a href="http://image-net.org/challenges/talks/2016/Hikvision_at_ImageNet_2016.pdf" data-editable="true" data-title="image-net.org 的页面"&gt;http://image-net.org/challenges/talks/2016/Hikvision_at_ImageNet_2016.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[16] Jie SHAO, Xiaoteng ZHANG, Zhengyan DING etal. “Good Practices for Deep Feature Fusion” (Slides), the Second ImageNet and COCO Visual Recognition Challenges Joint Workshop in conjunction with ECCV 2016, &lt;a href="http://image-net.org/challenges/talks/2016/Trimps-Soushen@ILSVRC2016.pdf" data-editable="true" data-title="image-net.org 的页面"&gt;http://image-net.org/challenges/talks/2016/Trimps-Soushen@ILSVRC2016.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[17]ILSVRC2016比赛结果&lt;a href="http://image-net.org/challenges/LSVRC/2016/results" data-editable="true" data-title="ILSVRC2016"&gt;ILSVRC2016&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[18] Dai, Jifeng, et al. "R-FCN: Object Detection via Region-based Fully Convolutional Networks." (2016).&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing/answers" class="" data-editable="true" data-title="@果果是枚开心果."&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-a8d08a9298eeb4c5feaba54dd1d4c92c.jpg" data-rawwidth="114" data-rawheight="115"&gt;李瑜，&lt;/b&gt;中科院计算所前瞻研究实验室跨媒体组硕博生，硕士导师唐胜副研究员，博士导师李锦涛研究员。2016年，作为360+MCG-ICT-CAS_DET团队核心主力参加了ImageNet大规模视觉识别挑战赛(ILSVRC)的 DET任务并获得第四名。目标检测相关工作受邀在ECCV 2016 ImageNet和COCO视觉识别挑战赛联合工作组会议（ImageNet and COCO Visual Recognition Challenges Joint Workshop）上做大会报告。个人邮箱：liyu@ict.ac.cn。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s/t3U_gUfe5KekrH-jDSDk_w" data-editable="true" data-title="ILSVRC2016目标检测任务回顾（上）--图像目标检测（DET）"&gt;ILSVRC2016目标检测任务回顾（上）--图像目标检测（DET）&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24934382&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Wed, 18 Jan 2017 14:37:04 GMT</pubDate></item><item><title>[深度学习大讲堂]从NNVM看2016年深度学习框架发展趋势</title><link>https://zhuanlan.zhihu.com/p/24710026</link><description>深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;&lt;b&gt;虚拟框架杀入&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-db8b87ad733c06bc04ab05174f845aa9.png" data-rawwidth="635" data-rawheight="336"&gt;&lt;p&gt;从发现问题到解决问题&lt;/p&gt;&lt;p&gt;半年前的这时候，暑假，我在SIAT MMLAB实习。&lt;/p&gt;&lt;p&gt;看着同事一会儿跑Torch，一会儿跑MXNet，一会儿跑Theano。&lt;/p&gt;&lt;p&gt;SIAT的服务器一般是不给sudo权限的，我看着同事挣扎在编译这一坨框架的海洋中，开始思考：&lt;/p&gt;&lt;p&gt;是否可以写一个框架：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-47332d834e1d10a78ef0d2e520560e77.png" data-rawwidth="315" data-rawheight="91"&gt;&lt;p&gt;这样，利用工厂模式只编译执行部件的做法，只需编译唯一的后端即可，框架的不同仅仅在于前端脚本的不同。&lt;/p&gt;&lt;p&gt;Caffe2Keras的做法似乎是这样，但Keras本身是基于Theano的编译后端，而我们的更希望Theano都不用编译。&lt;/p&gt;&lt;p&gt;当我9月份拍出一个能跑cifar10的大概原型的时候：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-769467a72f769b604ea22b5af432a982.png" data-rawwidth="633" data-rawheight="408"&gt;&lt;p&gt;我为这种怪异的写法取名叫CGVM(Computational Graph Virtual Machine)然后过了几天，在微博上看到了陈天奇在MXNet的进一步工作NNVM的发布 (o(╯□╰)o)......&lt;/p&gt;&lt;p&gt;NNVM使用2000行模拟出了TensorFlow，我大概用了500行模拟出了Caffe1。&lt;/p&gt;&lt;p&gt;VM(Virtual Machine)的想法其实是一个很正常的想法，这几年我们搞了很多新框架，名字一个比一个炫，但是本质都差不多，框架的使用者实际上是苦不堪言的：&lt;/p&gt;&lt;p&gt;○ 这篇paper使用了A框架，我要花1天配置A框架。&lt;/p&gt;&lt;p&gt;○ 这篇paper使用了B框架，我要花1天配置B框架。&lt;/p&gt;&lt;p&gt;.......&lt;/p&gt;&lt;p&gt;正如LLVM不是一种编译器，NNVM也不是一种框架，看起来更像是框架的屠杀者。&lt;/p&gt;&lt;p&gt;NNVM的可行性恰恰证明了现行的各大框架底层的重复性，而上层的多样性只是一个幌子。&lt;/p&gt;&lt;p&gt;我们真的需要为仅仅是函数封装不同的框架买单吗？这是值得思考的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;计算图走向成熟&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-e67a596dad4b72894775bfac762fab16.png" data-rawwidth="399" data-rawheight="288"&gt;&lt;p&gt;&lt;b&gt;计算图的两种形式&lt;/b&gt;&lt;/p&gt;&lt;p&gt;计算图最早的出处应该是追溯到Bengio在09年的《Learning Deep Architectures for AI》，Bengio使用了有向图结构来描述神经网络的计算:&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-8fce7ea0f6639c42d812d739af689fca.png" data-rawwidth="205" data-rawheight="346"&gt;&lt;p&gt;如图，符号集合{*，+，sin} 构成图的结点，整张图可看成三部分：输入结点、输出结点、从输入到输出的计算函数。&lt;/p&gt;&lt;p&gt;随后在Bengio组的Theano框架执行中，Graph就被隐式应用于Op的连接。&lt;/p&gt;&lt;p&gt;不过这时候，Op还是执行时-动态编译的。&lt;/p&gt;&lt;p&gt;Caffe1中计算图其实就是Net，因为Net可以被Graph模拟出来(CGVM和Caffe2Keras都实现了)。&lt;/p&gt;&lt;p&gt;贾扬清在Caffe1中显式化了计算图的表示，用户可以通过编辑net.prototxt来设计计算图。&lt;/p&gt;&lt;p&gt;Caffe1在Jonathan Long和Evan Shelhamer接手后，他们开发了PyCaffe。&lt;/p&gt;&lt;p&gt;PyCaffe通过Python天然的工厂(__getattr__)，实现了net.prototxt的隐式生成。&lt;/p&gt;&lt;p&gt;之后的Caffe2，也就直接取消了net.prototxt的编辑，同样利用Python的(__getattr__)获取符号类型定义。&lt;/p&gt;&lt;p&gt;Caffe1带来一种新的计算图组织Op的描述方式，不同于Theano直接翻译Op为C执行代码，然后动态编译，软件工程中的高级设计模式——工厂模式被广泛使用。&lt;/p&gt;&lt;p&gt;计算图被划分为三个阶段，定义阶段、构造阶段、执行阶段：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1、&lt;/b&gt;定义阶段：定义Layer/Op的name、type、bottom(input)，top(output)及预设参数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2、&lt;/b&gt;构造阶段：通过工厂模式，由字符串化的定义脚本构造类对象。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3、&lt;/b&gt;执行阶段：根据传入的bottom(input)，得到额外参数(如shape)，此时计算图才能开始执行。阶段划分带来的主要问题是限制了编译代码的完整性和优化程度。&lt;/p&gt;&lt;p&gt;在Theano中，C代码生成是最后一步，编译前你可以组合数个细粒度符号，依靠编译器做一次硬件执行上的优化。&lt;/p&gt;&lt;p&gt;而工厂模式编译符号时只考虑了单元，编译器没有上下文可供参考优化，故最终只能顺序执行多个预先编译的符号单元。&lt;/p&gt;&lt;p&gt;当符号粒度过细时，一个Layer的实现就会变成连续执行多个子过程，导致“TensorFlowSlow”。&lt;/p&gt;&lt;p&gt;&lt;b&gt;计算图作为中间表示(IR)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;PyCaffe和Caffe2将定义阶段移到Python中，而将构造和执行阶段保留在C++中做法，是计算图作为IR的思想启蒙。&lt;/p&gt;&lt;p&gt;Python与C++最大的不同在于：一个是脚本代码，用于前端。一个是本地代码，用于后端。&lt;/p&gt;&lt;p&gt;脚本代码创建/修改模型方便(无需因模型变动而重新编译)、执行慢，本地代码则正好相反。&lt;/p&gt;&lt;p&gt;两者取长补短，所以深度学习框架在2016年，迎来了前后端开发的黄金时代。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-53c105dc87bcea007e96b1fe61a13a2e.png" data-rawwidth="643" data-rawheight="153"&gt;&lt;p&gt;如上图，无论是9月份先提出的NNVM，还是最近Intel曝光的Nervana，都分离了前后端。&lt;/p&gt;&lt;p&gt;后端的独立，不仅减少了编译工作，最大的优势在于降低了传统框架做跨设备计算的代码耦合度。&lt;/p&gt;&lt;p&gt;在paper每周都有一大堆的现在，如果后端的每一次变动都要大量修改前端，那么框架的维护开销是非常大的。&lt;/p&gt;&lt;p&gt;在前端定义用于描述输入-输出关系的计算图有着良好的交互性，我们可以通过函数和重载脚本语言的操作符，定义出媲美MATLAB的运算语言，这些语言以显式的Tensor作为数据结构，Operator作为计算符和函数，Theano和MXNet都是这样隐蔽处理由表达式向计算图过渡的。&lt;/p&gt;&lt;p&gt;而Caffe2则比较直接，你需要先创建一个Graph，然后显示地调用Graph.AddOperator(xxx) TensorFlow同样可以显式化处理Graph。&lt;/p&gt;&lt;p&gt;与用户交互得到的计算图描述字串是唯一的，但是与用户交互的方式却是不唯一的。&lt;/p&gt;&lt;p&gt;所以IR之上，分为两派：&lt;/p&gt;&lt;p&gt;第一派要搞自己的API，函数封装非常有个性，宣示这是自己的专利、独门语言。&lt;/p&gt;&lt;p&gt;第二派不搞自己的API，反而去模拟现有的API，表示我很低调。&lt;/p&gt;&lt;p&gt;显然，用户更喜欢用自己熟悉框架的写法去描述模型，不喜欢天天背着个函数速查手册。&lt;/p&gt;&lt;p&gt;&lt;b&gt;计算图优化&lt;/b&gt;&lt;/p&gt;&lt;p&gt;用于中间表示得到的计算图描述最好不要直接构造，因为存在冗余的求解目标，且可共享变量尚未提取。&lt;/p&gt;&lt;p&gt;当限制计算图描述为有向无环图(DAG)时，一些基本的图论算法便可应用于计算图描述的化简与变换。&lt;/p&gt;&lt;p&gt;陈天奇在今年的MSR Talk：Programming Models and Systems Design for Deep Learning中，总结了计算图优化的三个点：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-169a1f8b895a3386cc0f56dafedfe7e7.png" data-rawwidth="642" data-rawheight="422"&gt;&lt;p&gt;&lt;b&gt;①依赖性剪枝&lt;/b&gt;&lt;/p&gt;&lt;p&gt;分为前向传播剪枝，例：已知A+B=X，A+B=Y，求X？&lt;/p&gt;&lt;p&gt;反向传播剪枝,  例：A+B=X，A+B=Y，求X、Y，dX/dA？&lt;/p&gt;&lt;p&gt;根据用户的求解需求，可以剪掉没有求解的图分支。&lt;/p&gt;&lt;p&gt;&lt;b&gt;②符号融合&lt;/b&gt;&lt;/p&gt;&lt;p&gt;符号融合的自动实现是困难的，因为Kernel基本不再实时编译了，所以更多体现在符号粗细粒度的设计上。&lt;/p&gt;&lt;p&gt;粗粒度的符号融合了数个细粒度的符号，一次编译出连续多个执行步骤的高效率代码。&lt;/p&gt;&lt;p&gt;粗粒度和细粒度并无好坏区分，一个速度快，一个更灵活。&lt;/p&gt;&lt;p&gt;从贪心角度，VM框架通常会提供粗细粒度两种实现給用户，因而需要更多人力维护编译后端。&lt;/p&gt;&lt;p&gt;&lt;b&gt;③内存共享&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Caffe1对于激活函数大多使用的inplace处理——即bottom和top是同一个Blob。&lt;/p&gt;&lt;p&gt;inplace使用新的输出y立即覆盖的输入x，需要以下两个条件：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1、&lt;/b&gt;bottom和top数量都为1，即：计算图中构成一条直线路径，&lt;/p&gt;&lt;p&gt;&lt;b&gt;2、&lt;/b&gt;d(y)/d(x)与x是无关的，所以x被y覆盖不影响求导结果。&lt;/p&gt;&lt;p&gt;常见的激活函数都符号以上两个条件，因而可以减少内存的开销。&lt;/p&gt;&lt;p&gt;但是Caffe1在多网络内存共享优化上极其糟糕的，以至于Caffe1并不适合用来跑GAN，以及更复杂的网络。&lt;/p&gt;&lt;p&gt;一个简单例子是交叉验证上的优化：训练网络和验证网络的大部分Layer都是可以共享的，但是由于Caffe1错误地将Blob独立的放在每个Net里，使得跨Net间很难共享数据。&lt;/p&gt;&lt;p&gt;除此之外，Caffe1还错误地将临时变量Blob独立放在每个Layer里，导致列卷积重复占用几个G内存。&lt;/p&gt;&lt;p&gt;让Net和Layer都能共享内存，只需要将Tensor/Blob置于最顶层，采用MVC来写框架即可。&lt;/p&gt;&lt;p&gt;Caffe2引入了Workspace来管理Tensor，并将工作空间的指针传给每一个Op、每一个Graph的构造函数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;新的风暴已经出现&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-dcad4b344a0914e91e9f2867031795c8.png" data-rawwidth="643" data-rawheight="307"&gt;&lt;p&gt;&lt;b&gt;VM的侧重点&lt;/b&gt;&lt;/p&gt;&lt;p&gt;CGVM和NNVM的侧重点是不太一样的，CGVM更强调前端上的扩展化，后端上的唯一化。&lt;/p&gt;&lt;p&gt;所以CGVM不会去支持Torch编译后端，也不会去支持Caffe编译后端。&lt;/p&gt;&lt;p&gt;在NNVM的知乎讨论帖中，有一种观点认为VM是轻视Operator的实现。&lt;/p&gt;&lt;p&gt;但实际上，我们手里的一堆框架，在Operator、Kernel、Math级别的不少实现是没有多少区别的。&lt;/p&gt;&lt;p&gt;但恰恰折磨用户的正是这些没有多少区别的编译后端：各种依赖库、装Linux、编译各种错。&lt;/p&gt;&lt;p&gt;所以我个人更倾向整个DL社区能够提供一份完善的跨平台、跨设备解决方案，而不是多而杂的备选方案。&lt;/p&gt;&lt;p&gt;从这点来看，CGVM似乎是一个更彻底的框架杀手，但在ICML'15上， Jürgen Schmidhuber指出：&lt;/p&gt;&lt;p&gt;真正运行AI 的代码是非常简短的，甚至高中生都能玩转它。不用有任何担心会有行业垄断AI及其研究。&lt;/p&gt;&lt;p&gt;简短的AI代码，未必就是简单的框架提供的，有可能是自己熟悉的框架，这种需求体现在前端而不是后端。&lt;/p&gt;&lt;p&gt;VM指出了一条多框架混合思路：功能A，框架X写简单。功能B，框架Y写简单。&lt;/p&gt;&lt;p&gt;功能A和功能B又要end-to-end，那么显然混起来用不就行了。&lt;/p&gt;&lt;p&gt;只有使用频率不高的框架才会消亡，VM将框架混合使用后，熟悉的味道更浓了，那么便构不成”框架屠杀者“。&lt;/p&gt;&lt;p&gt;强大的AI代码，未必就是VM提供的，有可能是庞大的后端提供的。&lt;/p&gt;&lt;p&gt;随着paper的快速迭代，后端的扩展仍然是最繁重的编程任务。&lt;/p&gt;&lt;p&gt;VM和后端侧重点各有不同，难分好坏。但分离两者的做法确实是成功的一步。&lt;/p&gt;&lt;p&gt;&lt;b&gt;VM的形式&lt;/b&gt;&lt;/p&gt;&lt;p&gt;VM及计算图描述方式是连接前后端的桥梁。&lt;/p&gt;&lt;p&gt;即便后端是唯一的，根据支持前端的不同，各家写的VM也很难统一。&lt;/p&gt;&lt;p&gt;实际上这就把框架之间的斗争引向了VM之间的斗争。&lt;/p&gt;&lt;p&gt;两人见面谈笑风生，与其问对方用什么框架，不如问对方用什么VM。&lt;/p&gt;&lt;p&gt;&lt;b&gt;VM的主要工作&lt;/b&gt;&lt;/p&gt;&lt;p&gt;合成计算图描述的过程是乏味的，在Caffe1中，我们恐怕已经受够了人工编辑prototxt。&lt;/p&gt;&lt;p&gt;API交互方面，即便是MXNet提供给用户的API也是复杂臃肿的，或许仍然需要一个handbook。&lt;/p&gt;&lt;p&gt;TensorFlow中的TensorBoard借鉴了WebOS，VM上搞一个交互性更强的操作系统也是可行的。&lt;/p&gt;&lt;p&gt;除此之外，我可能比较熟悉一些经典框架，那么不妨让VM去实现那些耳熟能详的函数吧！&lt;/p&gt;&lt;p&gt;&lt;b&gt;1、&lt;/b&gt;模拟Theano.function&lt;/p&gt;&lt;p&gt;Theano的function是一个非常贴近数学表达计算图掩饰工具。function内部转化表达式为计算图定义，同时返回一个lambda函数引向计算图的执行。总之这是一个百看不腻的API。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2、&lt;/b&gt;模拟Theano.grad&lt;/p&gt;&lt;p&gt;结合计算图优化，我们现在可以指定任意一对求导二元组(cost, wrt)。因而，放开手，让自动求导在你的模型中飞舞吧。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3、&lt;/b&gt;模拟Theano.scan&lt;/p&gt;&lt;p&gt;Theano.scan是一个用来搭建RNN的神器。尽管最近Caffe1更新了RNN，但是只支持固定循环步数的RNN。而Theano.scan则可以根据Tensor的shape，为RNN建动态的计算图，这适合在NLP任务中处理不定长句子。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4、&lt;/b&gt;模拟PyCaffe&lt;/p&gt;&lt;p&gt;PyCaffe近来在RCNN、FCN、DeepDream中得到广泛应用，成为搞CV小伙伴们的最爱。PyCaffe大部分是由C++数据结构通过Boost.Python导出的，不幸的是，Boost.Thread导出之后与Python的GIL冲突，导致PyCaffe里无法执行C++线程。尝试模拟移除Boost.Python后的PyCaffe，在Python里把Solver、Net、Layer給写出来吧。&lt;/p&gt;&lt;p&gt;&lt;b&gt;5、&lt;/b&gt;模拟你熟悉的任意框架&lt;/p&gt;&lt;p&gt;.......等等，怎么感觉在写模拟器.....当然写模拟器基本就是在重复造轮子，这个在NNVM的知乎讨论帖中已经指明了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;VM的重要性&lt;/b&gt;&lt;/p&gt;&lt;p&gt;VM是深度学习框架去中心化、解耦化发展迈出的重要一步。&lt;/p&gt;&lt;p&gt;同时暴露了目前框架圈混乱的本质：计算图之下，众生平等。计算图之上，群魔乱舞。&lt;/p&gt;&lt;p&gt;在今年我们可以看多许多框架PK对比的文章，然而大多只是从用户观点出发的简单评测。&lt;/p&gt;&lt;p&gt;对比之下，NNVM关注度不高、反对者还不少这种情况，确实让人感到意外。&lt;/p&gt;&lt;p&gt;&lt;b&gt;回顾与展望&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-e6923d26ce910541a40a4b52a747bd37.png" data-rawwidth="302" data-rawheight="317"&gt;&lt;p&gt;&lt;b&gt;回顾2016：框架圈减肥大作战的开始&lt;/b&gt;&lt;/p&gt;&lt;p&gt;高调宣布开源XXX框架，再封装一些API，实际上已经多余了。&lt;/p&gt;&lt;p&gt;VM的出现，将上层接口的编写引向模拟经典的框架，从而达到减肥的目的。&lt;/p&gt;&lt;p&gt;框架维护者应当将大部分精力主要放在Kernel的编写上，而不是考虑搞一些大新闻。&lt;/p&gt;&lt;p&gt;&lt;b&gt;展望2017：DL社区能否联合开源出跨平台、跨设备的后端解决方案&lt;/b&gt;&lt;/p&gt;&lt;p&gt;后端上，随着ARM、神经芯片的引入，我们迫切需要紧跟着硬件来完成繁重的编程。&lt;/p&gt;&lt;p&gt;后端是一个敏感词，因为硬件可以拿来卖钱，所以更倾向于闭源。&lt;/p&gt;&lt;p&gt;除此之外，即便出现了开源的后端，在山寨和混战之前是否能普及也是一个问题。&lt;/p&gt;&lt;p&gt;&lt;b&gt;展望2017：来写框架吧&lt;/b&gt;&lt;/p&gt;&lt;p&gt;VM的出现，带来另一个值得思考的问题：现在是不是人人应该学写框架了？&lt;/p&gt;&lt;p&gt;传统框架编写的困难在代码耦合度高，学习成本昂贵。&lt;/p&gt;&lt;p&gt;VM流框架分离了前后端之后，前端编写难度很低，后端的则相对固定。&lt;/p&gt;&lt;p&gt;这样一来，框架的编程层次更加分明，Keras地位似乎要危险了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;展望2017：更快迭代的框架，更多变的风格，更难的垄断地位&lt;/b&gt;&lt;/p&gt;&lt;p&gt;相比于paper的迭代，框架的迭代似乎更快了一点。&lt;/p&gt;&lt;p&gt;余凯老师前段时间发出了TensorFlow垄断的担忧，但我们可以很乐观地看到：越来越多的用户，在深入框架的底层。&lt;/p&gt;&lt;p&gt;TensorFlow并不是最好的框架，MXNet也不是，最好的框架是自己用的舒服的框架，最好是一行行自己敲出来的。&lt;/p&gt;&lt;p&gt;如果你已经积累的数个框架的使用经验，是时候把它们无缝衔接在一起了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing/answers" data-editable="true" data-title="@果果是枚开心果." class=""&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-23a1eb185e88eae3c3c4a952f36b5214.png" data-rawwidth="118" data-rawheight="116"&gt;&lt;p&gt;&lt;b&gt;潘汀，&lt;/b&gt;合肥工业大学计算机专业大三本科生，中科院深圳先进院集成所MMLAB访问学生。原ACM-ICPC算法竞赛选手，2015年获CCPC铜牌。2015年初开始研究机器学习，研究兴趣集中于对深度学习理论、应用(CV&amp;amp;NLP)及系统架构设计的综合探索。关于深度学习在面部情感分析方面应用的论文被《自动化学报》录用。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s/N-aMqpWNClCcioNq9q5brA" data-editable="true" data-title="[深度学习大讲堂]从NNVM看2016年深度学习框架发展趋势"&gt;[深度学习大讲堂]从NNVM看2016年深度学习框架发展趋势&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24710026&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Wed, 04 Jan 2017 15:06:07 GMT</pubDate></item><item><title>Caffe代码夜话1</title><link>https://zhuanlan.zhihu.com/p/24709689</link><description>深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;&lt;b&gt;1. 从零开始的Label&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在使用SoftmaxLoss层作为损失函数层的单标签分类问题中，label要求从零开始，例如1000类的ImageNet分类任务，label的范围是0~999。这个限制来自于Caffe的一个实现机制，label会直接作为数组的下标使用，具体代码SoftmaxLoss.cpp中133行和139行的实现代码。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-afe67690613c1082f1630213e7d8c867.png" data-rawwidth="594" data-rawheight="436"&gt;&lt;p&gt;132行第一层for循环中的outer_num等于batch size，对于人脸识别和图像分类等单标签分类任务而言，inner_num等于1。如果label从1开始，会导致bottom_diff数组访问越界。&lt;/p&gt;&lt;p&gt;留两个思考题：&lt;/p&gt;&lt;p&gt;&lt;b&gt;思考题1：&lt;/b&gt;为什么Caffe中引入了这个inner_num，inner_num等于什么，提示一下从FCN全卷积网络的方向去思考。&lt;/p&gt;&lt;p&gt;&lt;b&gt;思考题2：&lt;/b&gt;在标签正确的前提下，如果倒数第一个全连接层num_output &amp;gt; 实际的类别数，Caffe的训练是否会报错？&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. BN中的use_global_status&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-27cd7588eb37c23aa63d3aae45ffd2fe.png" data-rawwidth="298" data-rawheight="213"&gt;&lt;p&gt;但是如果直接拿这个Proto用于训练（基于随机初始化），则会导致模型不收敛，原因在于在Caffe的batch_norm_layer.cpp实现中，use_global_stats==true时会强制使用模型中存储的BatchNorm层均值与方差参数，而非基于当前batch内计算均值和方差。&lt;/p&gt;&lt;p&gt;首先看use_global_stats变量是如何计算的：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-7cf1989ed2deaea7527d901da645b391.png" data-rawwidth="432" data-rawheight="100"&gt;再看这个变量的作用：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-1deb2dc80da57d8fef12137c7e37e3da.png" data-rawwidth="622" data-rawheight="390"&gt;以下代码在use_global_stats为false的时候通过moving average策略计算模型中最终存储的均值和方差：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-0e431babec4c70df98155db5bc83bb2a.png" data-rawwidth="614" data-rawheight="506"&gt;&lt;p&gt;因此，对于随机初始化训练BatchNorm层，只需要在Proto文件中移除use_global_stats参数即可，Caffe会根据当前的Phase(TRAIN或者TEST)自动去设置use_global_stats的值。&lt;/p&gt;&lt;p&gt;再留一个思考题。&lt;/p&gt;&lt;p&gt;&lt;b&gt;思考题3：&lt;/b&gt;BatchNorm层是否支持in place运算，为什么？&lt;/p&gt;&lt;p&gt;本期Caffe夜话就到这里。在后续几期的代码夜话中，我们将陆续介绍网络参数的初始化、Fine-tune的参数拷贝、Net类的组装、Solver类的参数更新机制、添加新的Caffe Layer等内容，敬请期待。&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing/answers"&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;深度学习大讲堂内容组。一个神秘的组织，还没有介绍。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s/IH0ZDkGwv4nBIr23r-dN_g" data-editable="true" data-title="Caffe代码夜话1"&gt;Caffe代码夜话1&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24709689&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Wed, 04 Jan 2017 14:43:20 GMT</pubDate></item><item><title>深度学习框架Caffe源码解析</title><link>https://zhuanlan.zhihu.com/p/24343706</link><description>深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;相信社区中很多小伙伴和我一样使用了很长时间的Caffe深度学习框架，也非常希望从代码层次理解Caffe的实现从而实现新功能的定制。本文将从整体架构和底层实现的视角，对Caffe源码进行解析。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.Caffe总体架构&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Caffe框架主要有五个组件，Blob，Solver，Net，Layer，Proto，其结构图如下图1所示。Solver负责深度网络的训练，每个Solver中包含一个训练网络对象和一个测试网络对象。每个网络则由若干个Layer构成。每个Layer的输入和输出Feature map表示为Input Blob和Output Blob。Blob是Caffe实际存储数据的结构，是一个不定维的矩阵，在Caffe中一般用来表示一个拉直的四维矩阵，四个维度分别对应Batch Size（N），Feature Map的通道数（C）,Feature Map高度(H)和宽度(W)。Proto则基于Google的Protobuf开源项目，是一种类似XML的数据交换格式，用户只需要按格式定义对象的数据成员，可以在多种语言中实现对象的序列化与反序列化，在Caffe中用于网络模型的结构定义、存储和读取。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-e3f56d8fd8f00feae16885b44def948a.jpg" data-rawwidth="667" data-rawheight="354"&gt;&lt;p&gt;&lt;b&gt;2.Blob解析&lt;/b&gt;&lt;/p&gt;&lt;p&gt;下面介绍Caffe中的基本数据存储类Blob。Blob使用SyncedMemory类进行数据存储，数据成员 data_指向实际存储数据的内存或显存块，shape_存储了当前blob的维度信息，diff_这个保存了反向传递时候的梯度信息。在Blob中其实不是只有num，channel，height，width这种四维形式，它是一个不定维度的数据结构，将数据展开存储，而维度单独存在一个vector&amp;lt;int&amp;gt; 类型的shape_变量中，这样每个维度都可以任意变化。&lt;/p&gt;&lt;p&gt;来一起看看Blob的关键函数，data_at这个函数可以读取的存储在此类中的数据，diff_at可以用来读取反向传回来的误差。顺便给个提示，尽量使用data_at(const vector&amp;lt;int&amp;gt;&amp;amp; index)来查找数据。Reshape函数可以修改blob的存储大小，count用来返回存储数据的数量。BlobProto类负责了将Blob数据进行打包序列化到Caffe的模型中。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.工厂模式说明&lt;/b&gt;&lt;/p&gt;&lt;p&gt;接下来介绍一种设计模式Factory Pattern，Caffe 中Solver和Layer对象的创建均使用了此模式，首先看工厂模式的UML的类图：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-0fffc1bbdc6f5dc4368cd3faece45d91.jpg" data-rawwidth="539" data-rawheight="320"&gt;如同Factory生成同一功能但是不同型号产品一样，这些产品实现了同样Operation，很多人看了工厂模式的代码，会产生这样的疑问为何不new一个出来呢，这样new一个出来似乎也没什么问题吧。试想如下情况，由于代码重构类的名称改了，或者构造函数参数变化(增加或减少参数)。而你代码中又有N处new了这个类。如果你又没用工厂，就只能一个一个找来改。工厂模式的作用就是让使用者减少对产品本身的了解，降低使用难度。如果用工厂，只需要修改工厂类的创建具体对象方法的实现，而其他代码不会受到影响。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-6d5bfb3c4ea555cdda9b0013c49124b8.jpg" data-rawwidth="132" data-rawheight="183"&gt;&lt;p&gt;举个例子，写代码少不得饿了要加班去吃夜宵，麦当劳的鸡翅和肯德基的鸡翅都是MM爱吃的东西，虽然口味有所不同，但不管你带MM去麦当劳或肯德基，只管向服务员说“来四个鸡翅”就行了。麦当劳和肯德基就是生产鸡翅的Factory。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.Solver解析&lt;/b&gt;&lt;/p&gt;&lt;p&gt;接下来切回正题，我们看看Solver这个优化对象在Caffe中是如何实现的。SolverRegistry这个类就是我们看到的上面的factory类，负责给我们一个优化算法的产品，外部只需要把数据和网络结构定义好，它就可以自己优化了。&lt;/p&gt;&lt;p&gt;Solver&amp;lt;Dtype&amp;gt;* CreateSolver(const SolverParameter&amp;amp; param)这个函数就是工厂模式下的CreateProduct的操作， Caffe中这个SolverRegistry工厂类可以提供给我们6种产品（优化算法）：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-513246b251efeadae7270acb95c7c8bc.jpg" data-rawwidth="632" data-rawheight="146"&gt;&lt;p&gt;这六种产品的功能都是实现网络的参数更新，只是实现方式不一样。那我们来看看他们的使用流程吧。当然这些产品类似上面Product类中的Operation，每一个Solver都会继承Solve和Step函数，而每个Solver中独有的仅仅是ApplyUpdate这个函数里面执行的内容不一样，接口是一致的，这也和我们之前说的工厂生产出来的产品一样功能一样，细节上有差异，比如大多数电饭煲都有煮饭的功能，但是每一种电饭煲煮饭的加热方式可能不同，有底盘加热的还有立体加热的等。接下里我们看看Solver中的关键函数。&lt;/p&gt;&lt;p&gt;Solver中Solve函数的流程图如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-94271e4cd51bf7365ca39fe9ff95d71f.jpg" data-rawwidth="324" data-rawheight="736"&gt;&lt;p&gt;Solver类中Step函数流程图：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-a6e0939b512b981a60e107906b68fe6c.jpg" data-rawwidth="455" data-rawheight="832"&gt;&lt;/p&gt;&lt;p&gt;Solver中关键的就是调用Sovle函数和Step函数的流程，你只需要对照Solver类中两个函数的具体实现，看懂上面两个流程图就可以理解Caffe训练执行的过程了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;5.Net类解析&lt;/b&gt;&lt;/p&gt;&lt;p&gt;分析过Solver之后我们来分析下Net类的一些关键操作。这个是我们使用Proto创建出来的深度网络对象，这个类负责了深度网络的前向和反向传递。以下是Net类的初始化方法NetInit函数调用流程：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-7374ab58320a291a82e023d25142a47c.jpg" data-rawwidth="511" data-rawheight="563"&gt;&lt;p&gt;Net的类中的关键函数简单剖析&lt;/p&gt;&lt;p&gt;&lt;b&gt;1).&lt;/b&gt;ForwardBackward：按顺序调用了Forward和Backward。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2).&lt;/b&gt;ForwardFromTo(int start, int end)：执行从start层到end层的前向传递，采用简单的for循环调用。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3).&lt;/b&gt;BackwardFromTo(int start, int end)：和前面的ForwardFromTo函数类似，调用从start层到end层的反向传递。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4).&lt;/b&gt;ToProto函数完成网络的序列化到文件，循环调用了每个层的ToProto函数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;6.Layer解析&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Layer是Net的基本组成单元，例如一个卷积层或一个Pooling层。本小节将介绍Layer类的实现。&lt;/p&gt;&lt;p&gt;&lt;b&gt;6.1Layer的继承结构&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-fa0709dc53ffb274bd145c54168cb840.jpg" data-rawwidth="353" data-rawheight="395"&gt;&lt;p&gt;&lt;b&gt;6.2 Layer的创建&lt;/b&gt;&lt;/p&gt;&lt;p&gt;与Solver的创建方式很像，Layer的创建使用的也是工厂模式，这里简单说明下几个宏函数：&lt;/p&gt;&lt;p&gt;REGISTER_LAYER_CREATOR负责将创建层的函数放入LayerRegistry。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-048310b542b15ba9e46ea6b2b5ae5372.jpg" data-rawwidth="618" data-rawheight="204"&gt;我们来看看大多数层创建的函数的生成宏REGISTER_LAYER_CLASS，可以看到宏函数比较简单的，将类型作为函数名称的一部分，这样就可以产生出一个创建函数，并将创建函数放入LayerRegistry。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-41705a9e84a1d1feb89994fb0b2b11f5.jpg" data-rawwidth="670" data-rawheight="127"&gt;&lt;p&gt;REGISTER_LAYER_CREATOR(type, Creator_##type##Layer)&lt;/p&gt;&lt;p&gt;这段代码在split_layer.cpp文件中&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-4769a3aa3adbd768c8f8caeac8026143.jpg" data-rawwidth="443" data-rawheight="67"&gt;&lt;p&gt;REGISTER_LAYER_CLASS(Split)。&lt;/p&gt;&lt;p&gt;这样我们将type替换过以后给大家做个范例，参考下面的代码。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-e9a89ab94d52ed2946dd75db49d69957.jpg" data-rawwidth="668" data-rawheight="88"&gt;当然这里的创建函数好像是直接调用，没有涉及到我们之前工厂模式的一些问题。所有的层的类都是这样吗？当然不是，我们仔细观察卷积类。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-f8e7ab462f0f00bb18713ccb39837255.jpg" data-rawwidth="439" data-rawheight="53"&gt;卷积层怎么没有创建函数呢，当然不是，卷积的层的创建函数在LayerFactory.cpp中，截图给大家看下，具体代码如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-745b984d8a6690f170c9d4c11ccd27fb.jpg" data-rawwidth="602" data-rawheight="116"&gt;&lt;p&gt;这样两种类型的Layer的创建函数都有了对应的声明。这里直接说明除了有cudnn实现的层，其他层都是采用第一种方式实现的创建函数，而带有cudnn实现的层都采用的第二种方式实现的创建函数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;6.3 Layer的初始化&lt;/b&gt;&lt;/p&gt;&lt;p&gt;介绍完创建我们看看层里面的几个函数都是什么时候被调用的。&lt;/p&gt;&lt;p&gt;关键函数Setup此函数在之前的流程图中的NetInit时候被调用，代码如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-7e66524e0594ffb366ecbbf7ab3d22de.jpg" data-rawwidth="449" data-rawheight="172"&gt;&lt;p&gt;这样整个Layer初始化的过程中，CheckBlobCounts被最先调用，然后接下来是LayerSetUp，后面才是Reshape，最后才是SetLossWeights。这样Layer初始化的生命周期大家就有了了解。&lt;/p&gt;&lt;p&gt;&lt;b&gt;6.4 Layer的其他函数的介绍&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Layer的Forward函数和Backward函数完成了网络的前向和反向传递，这两个函数在自己实现新的层必须要实现。其中Backward会修改bottom中blob的diff_，这样就完成了误差的方向传导。&lt;/p&gt;&lt;p&gt;&lt;b&gt;7.Protobuf介绍&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Caffe中的Caffe.proto文件负责了整个Caffe网络的构建，又负责了Caffemodel的存储和读取。下面用一个例子介绍Protobuf的工作方式：&lt;/p&gt;&lt;p&gt;利用protobuffer工具存储512维度图像特征：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1).&lt;/b&gt;message 编写：新建txt文件后缀名改为proto,编写自己的message如下，并放入解压的protobuff的文件夹里；&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-332e462127d845ec83698901f6448280.jpg" data-rawwidth="422" data-rawheight="153"&gt;&lt;p&gt;其中，dwFaceFeatSize表示特征点数量；pfFaceFeat表示人脸特征。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2).&lt;/b&gt;打开windows命令窗口(cmd.exe)----&amp;gt;cd空格，把protobuff的文件路径复制粘贴进去------&amp;gt;enter；&lt;/p&gt;&lt;p&gt;&lt;b&gt;3).&lt;/b&gt;输入指令protoc *.proto --cpp_out=.    ---------&amp;gt;enter&lt;/p&gt;&lt;p&gt;&lt;b&gt;4).&lt;/b&gt;可以看到文件夹里面生成“ *.pb.h”和“*.pb.cpp”两个文件，说明成功了&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-c5eb0ba309ceb4dca3cb5a3cea05f2a8.jpg" data-rawwidth="653" data-rawheight="71"&gt;&lt;p&gt;&lt;b&gt;5).&lt;/b&gt;下面可以和自己的代码整合了：&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1) &lt;/b&gt;新建你自己的工程，把“ *.pb.h”和“*.pb.cpp”两个文件添加到自己的工程里，并写上#include" *.pb.h"&lt;/p&gt;&lt;p&gt;&lt;b&gt;(2) &lt;/b&gt;按照配库的教程把库配置下就可以了&lt;/p&gt;&lt;p&gt;VS下Protobuf的配库方法：&lt;/p&gt;&lt;p&gt;解决方案----&amp;gt;右击工程名----&amp;gt;属性&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1)c/c++---&amp;gt;常规---&amp;gt;附加包含目录---&amp;gt; &lt;/b&gt;&lt;/p&gt;&lt;p&gt;($your protobuffer include path)\protobuffer &lt;/p&gt;&lt;p&gt;&lt;b&gt;(2)c/c++---&amp;gt;链接器--&amp;gt;常规---&amp;gt;附加库目录--&amp;gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;($your protobuffer lib path)\protobuffer&lt;/p&gt;&lt;p&gt;&lt;b&gt;(3) c/c++---&amp;gt;链接器--&amp;gt;输入---&amp;gt;附加依赖项--&amp;gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;libprotobufd.lib;(带d的为debug模式)&lt;/p&gt;&lt;p&gt;或libprotobuf.lib;（不带d,为release模式）&lt;/p&gt;&lt;p&gt;使用protobuf进行打包的方法如下代码：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-35b1da04956260b7d4b522fc2f35bdf3.jpg" data-rawwidth="636" data-rawheight="430"&gt;&lt;p&gt;&lt;b&gt;7.1  Caffe的模型序列化&lt;/b&gt;&lt;/p&gt;&lt;p&gt;BlobProto其实就是Blob序列化成Proto的类，Caffe模型文件使用了该类。Net调用每个层的Toproto方法，每个层的Toproto方法调用了Blob类的ToProto方法，这样完整的模型就被都序列化到proto里面了。最后只要将这个proto继承于message类的对象序列化到文件就完成了模型写入文件。Caffe打包模型的时候就只是简单调用了WriteProtoToBinaryFile这个函数，而这个函数里面的内容如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-c82d228cacbdaf06231c6835fca9b692.jpg" data-rawwidth="629" data-rawheight="70"&gt;&lt;p&gt;至此Caffe的序列化模型的方式就完成了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;7.2 Proto.txt的简单说明&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Caffe网络的构建和Solver的参数定义均由此类型文件完成。Net构建过程中调用ReadProtoFromTextFile将所有的网络参数读入。然后调用上面的流程进行整个caffe网络的构建。这个文件决定了怎样使用存在caffe model中的每个blob是用来做什么的，如果没有了这个文件caffe的模型文件将无法使用，因为模型中只存储了各种各样的blob数据，里面只有float值，而怎样切分这些数据是由prototxt文件决定的。&lt;/p&gt;&lt;p&gt;Caffe的架构在框架上采用了反射机制去动态创建层来构建Net，Protobuf本质上定义了graph，反射机制是由宏配合map结构形成的，然后使用工厂模式去实现各种各样层的创建，当然区别于一般定义配置采用xml或者json，该项目的写法采用了proto文件对组件进行组装。&lt;/p&gt;&lt;p&gt;&lt;b&gt;总结&lt;/b&gt;&lt;/p&gt;&lt;p&gt;以上为Caffe代码架构的一个总体介绍，希望能借此帮助社区的小伙伴找到打开定制化Caffe大门的钥匙。本文作者希望借此抛砖引玉，与更多期望了解Caffe和深度学习框架底层实现的同行交流。&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing/answers" data-editable="true" data-title="@果果是枚开心果."&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-19e5440b1ac54a668a2db1437737e69a.jpg" data-rawwidth="110" data-rawheight="123"&gt;&lt;b&gt;薛云峰，&lt;/b&gt;(&lt;a href="https://github.com/HolidayXue" class="" data-editable="true" data-title="HolidayXue (HolidayXue)"&gt;HolidayXue (HolidayXue)&lt;/a&gt;)，主要从事视频图像算法的研究，就职于浙江捷尚视觉科技股份有限公司担任深度学习算法研究员。捷尚致力于视频大数据和视频监控智能化，现诚招业内算法和工程技术人才，招聘主页&lt;a href="http://www.icarevision.cn/job.php" data-editable="true" data-title="浙江捷尚视觉科技股份有限公司--安全服务运营商" class=""&gt;浙江捷尚视觉科技股份有限公司--安全服务运营商&lt;/a&gt;，联系邮箱：hr@icarevision.cn&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s/rWDCYO5k06zT9BcXk21JJg" data-editable="true" data-title="深度学习框架Caffe源码解析"&gt;深度学习框架Caffe源码解析&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24343706&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Tue, 13 Dec 2016 15:52:56 GMT</pubDate></item><item><title>【Technical Review】ECCV16 Grid Loss及其在人脸检测中的应用</title><link>https://zhuanlan.zhihu.com/p/24342365</link><description>深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;什么是人脸检测？简而言之，给定一张图片，判断图中是否有人脸，如果有人脸，进一步给出每一张人脸的位置和大小。这一看似简单的任务，在实际应用中却面临着诸多困难，其中之一就是当人脸被遮挡时，如何才能准确地进行检测。在ECCV 2016上，有一篇文章专门针对检测遮挡人脸的问题进行了探索：Grid Loss: Detecting Occluded Faces，该文章通过设计新的损失函数，综合考虑局部和整体信息对分类的作用，增强了检测器对遮挡的鲁棒性。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-28ec33f69e1e7b1631359ca80e9affae.jpg" data-rawwidth="272" data-rawheight="286"&gt;&lt;p&gt;检测被遮挡的人脸，这一任务的难点在于，遮挡会导致一部分人脸特征缺失，取而代之的是遮挡物的特征，这不仅容易引起分类器误判，还容易造成漏检。解决遮挡人脸检测的问题可以从数据与算法两个方面切入。从数据方面入手的做法较为直接，即在分类器训练阶段，在正样例集中加入一定比例的带遮挡人脸，让分类器从数据中自动去学习带遮挡人脸的变化模式。数据驱动的方式也就意味着对数据的依赖，而遮挡的变化模式复杂多样，如果希望模型能对遮挡有较好的鲁棒性和泛化能力，那将需要非常大量的数据。从算法的角度入手，已有的一些工作在解决遮挡问题时，有些需要在训练数据中标好人脸的五官，这样在训练数据的制备收集阶段要花费更多的精力；有些在人脸检测的预测阶段有额外的计算，这样会因为处理遮挡带来额外的时间开销，而检测本身就是一个对速度极其敏感的任务，这也不是我们希望看到的。&lt;/p&gt;&lt;p&gt;近年来，神经网络在计算机视觉领域得到了广泛应用，也包括人脸检测这个子领域。神经网络的参数优化过程就像是一艘船在茫茫大海上行驶。这一叶扁舟（神经网络的参数），在大海（参数的解空间）上航行，那黑暗中的灯塔（损失函数），放射出耀眼的光辉（梯度），引导着前进的方向（梯度下降）。神经网络具有强大的非线性建模能力，有些时候对于一个问题效果不好，并不是神经网络的表达能力不足，而是损失函数没能引导神经网络的参数落在一个很好的解上。既然如此，可否改进人脸与非人脸分类时使用的损失函数，引导分类网络学习到对遮挡更鲁棒的特征呢？这样，不会在预测阶段带来额外的计算时间，如果损失函数无需额外标注信息，那也不需要额外的数据标注了。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-2db3cb7e6e07d36cdee4bd08babe616d.jpg" data-rawwidth="435" data-rawheight="291"&gt;这篇文章提出的grid loss就是在这个方向上进行了探索研究。一般的损失函数都是直接根据整个图片的信息计算loss，导致学习出的网络会趋向于利用全局信息分类。这篇文章将分类网络最后一个特征图划分为若干个网格（也就是相当于将图片划分为若干个网格），每个小网格看成一个单独的区域，按同样的方式计算一个loss，与整个图片的loss加和作为最终的loss。这样的loss强化了每一个小网络区域单独的判别能力，使得学习出的特征对于遮挡会更加鲁棒。引用论文中的一个图来说明普通loss与grid loss的区别。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-0340424084e5b06abcb9f8626bcdd8ee.jpg" data-rawwidth="647" data-rawheight="219"&gt;grid loss的数学定义如下（这里每一个grid的loss都是一个hinge loss）：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-b23c2d22e83ad7108b2dbcbbe3f08381.jpg" data-rawwidth="650" data-rawheight="74"&gt;其中，N代表grid的个数，wi与bi是最后一个featuremap的第i个grid对应的权值参数与偏置项, w = [w1, w2, …, wN]为最后一个featuremap整体对应的权值参数，b = b1 + b2 + ... + bN 为其对应的偏置项。这样，公式的第一项代表了整个featuremap上的loss，第二项代表了每一个grid的loss。λ是一个平衡系数，权衡全局的loss与局部的loss大小。m为一个常数，为1 / N，因为希望每一个网格区域对分类有相同的贡献。下图是分类网络的最后一个featuremap分块计算loss的示意图。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-e2a62f1b54affaaab3aeac6833785536.jpg" data-rawwidth="549" data-rawheight="230"&gt;&lt;p&gt;在检测阶段，直接将训练得到的w和b换为一个对应的全连接层即可，不需要任何额外的计算量。&lt;/p&gt;&lt;p&gt;grid loss经过作者的实验论证，能够比较明显的提升对于遮挡人脸的检测效果。除此之外，作者还发现grid loss可以使网络学习出更加多样性的特征；同时可以起到正则化的作用，在减少训练数据的时候，使用了grid loss的检测器性能下降会更少一些。这一点可以从模型集成的角度理解，因为现在强化了每一个grid的作用，最终学习出的检测器有一点若干个检测器集成的味道。作者使用的检测器，使用logistic loss时，用fddb数据集图像测试， 100个误检下的召回为0.795，使用了grid loss可以达到0.838，提高了大约4个百分点。&lt;/p&gt;&lt;p&gt;最后总结一下，grid loss这篇文章提出了一种提升被遮挡人脸检测性能的方法，这种方法无需额外的数据标注（如标注人脸中的五官），并且是一种离线训练时的策略，对在线的检测阶段没有影响，不会有额外的时间代价。作者论文中使用的检测框架是一个使用了滑动窗口范式的比较原始的框架，笔者认为，未来尝试将grid loss嵌入到一些先进的检测框架，如Faster RCNN里，是一件值得一试的事情。最后的最后，祝各位读者在生活这片海域里，都能找到自己最想要的那个损失函数，向之前进。&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing/answers" data-editable="true" data-title="@果果是枚开心果."&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-3baac8c343bbcb2e3ec9809bcbd6e1da.jpg" data-rawwidth="120" data-rawheight="118"&gt;&lt;p&gt;&lt;b&gt;时学鹏，&lt;/b&gt;中科院计算所VIPL组15级硕士生。导师为山世光研究员。研究方向为基于深度学习的目标检测，特别是人脸检测。研发了VIPL课题组第五代人脸检测SDK。个人邮箱：xuepeng.shi@vipl.ict.ac.cn。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s/qiwJnXggRwqvHFN74-W2DQ" data-editable="true" data-title="【Technical Review】ECCV16 Grid Loss及其在人脸检测中的应用"&gt;【Technical Review】ECCV16 Grid Loss及其在人脸检测中的应用&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24342365&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Tue, 13 Dec 2016 15:03:27 GMT</pubDate></item><item><title>【ECCV2016论文速读】回归框架下的人脸对齐和三维重建</title><link>https://zhuanlan.zhihu.com/p/23923248</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-1da1577674789735f19ec2ed72326d4b_r.png"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;&lt;b&gt;JointFace Alignment and 3D Face Reconstruction&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-f81e5f04be20315f42f9b15f07d1cf8a.jpg" data-rawwidth="710" data-rawheight="375"&gt;&lt;b&gt;（此处三维重建结果是gif动图，但不知什么原因，我的电脑本地无法保存，所以只好截图上传，请点击链接查看原文中的gif动图：&lt;a href="http://mp.weixin.qq.com/s/udr3573GXQOOF46jLriekg" class=""&gt;http://mp.weixin.qq.com/s/udr3573GXQOOF46jLriekg&lt;/a&gt;）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;三维人脸重建的目标是根据某个人的一张或者多张二维人脸图像重建出其三维人脸模型（此处的三维人脸模型一般仅指形状模型，定义为三维点云）。今天我们只讨论由单张二维图像重建三维人脸的问题。这个问题本身其实是个病态（ill-posed）问题，因为在将人脸从三维空间投影到二维平面上形成我们看到的二维人脸图像的过程中，人脸的绝对尺寸（如鼻子高度）、以及由于自遮挡而不可见的部分等很多信息已经丢失。在不掌握相机和拍摄环境的相关参数的情况下，这个问题其实是没有确定解的。&lt;/p&gt;&lt;p&gt;为了解决这一病态问题，一个直接思路是借助机器视觉中的Shape-from-Shading（SFS）方法。但是该方法依赖于光照条件和光照模型的先验知识，而未考虑人脸结构的特殊性，在任意拍摄的人脸图像上效果一般。后来，Kemelmacher-Shizerman和Basri [1] 引入了平均三维人脸模型作为约束条件对传统的SFS方法进行了改进，取得了不错的效果。然而，重建结果往往都接近平均模型，缺少个性化特征。另一个常用思路是建立三维人脸的统计模型，再将该模型拟合到输入的二维人脸图像上，利用拟合参数实现三维人脸的重建。这类方法基本都是基于Blanz和Vetter提出的三维形变模型（3D Morphable Model，简称3DMM） [2]。由于3DMM采用主成分分析（PCA）方法构建统计模型，而PCA本质上是一种低通滤波，所以这类方法在恢复人脸的细节特征方面效果仍然不理想。此外，上述两类方法在重建过程中对每幅图像都需要求解优化问题，因而实时性较差。&lt;/p&gt;&lt;p&gt;受到近年来回归方法在人脸对齐中的成功应用的启发，我们最早试图建立二维人脸图像上的面部特征点（包括眼角、鼻尖、嘴角等）与人脸三维模型之间的回归关系。这一思路的基本出发点是面部特征点是反映人脸三维结构的最直观依据。我们尝试根据二维特征点的偏差直接预测三维人脸形状的调整量。这就好比我们知道二维特征点是由三维人脸形状投影得到的，如果我们发现二维特征点存在偏差，那么根据这一线索我们就应该能够计算出三维人脸形状应该做怎样的调整。而这个计算过程可以用事先训练好的二维特征点偏差与三维形状调整量之间的回归函数来实现。基于这样的思路，我们成功地设计实现了在给定输入二维人脸图像上的特征点的条件下实时重建其三维模型的新方法。相关结果发布在Arxiv [3]。&lt;/p&gt;&lt;p&gt;沿着上述思路，基于2D人脸特征点和3D人脸形状之间很强的相关性，我们进一步尝试将二维人脸图像特征点检测（即人脸对齐）与三维人脸重建过程耦合起来，在回归的框架下同时实现这两个任务。这就是我们今天要介绍的发表在ECCV2016上的工作 [4] （以下称ECCV2016方法）。扯了这么多（希望不是那么远^_^），下面正式进入正题。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-f5b1a0425d08b0e8229ea9cf9b04dd7e.png" data-rawwidth="1269" data-rawheight="485"&gt;如上图所示，之前研究者大都将2D特征点定位和3D人脸重建两个过程割裂开来解决，而这两个工作本质是一个“鸡生蛋、蛋生鸡”问题。一方面，2D特征点 &lt;em&gt;U &lt;/em&gt;可由中性3D人脸 &lt;em&gt;S&lt;/em&gt; 经过表情（&lt;em&gt;FE &lt;/em&gt;）、姿态变换（ &lt;em&gt;FP&lt;/em&gt;）及投影（&lt;em&gt; FC&lt;/em&gt;）得到，即 &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-6c486d0618f0e5011d4c6abc5d8117a2.jpg" data-rawwidth="158" data-rawheight="32"&gt;&lt;p&gt;另一方面，2D特征点携带有丰富的几何信息，这也是3D重建方法的基础。&lt;/p&gt;&lt;p&gt;现有的2D特征点检测方法大部分是基于2D人脸形状建模的，主要存在以下几个问题：i）很难去刻画3D平面外旋转的人脸特征点；ii）在人脸姿态不是很大的情况下，通过变化人脸轮廓特征点语义位置来解决自遮挡的情况，这样会导致不同姿态下检测的特征点语义信息不一致 [5]（如上图，人脸图像中蓝色点所示）；iii）在更大姿态下，尤其是yaw方向超过60度以后，人脸区域存在近一半自遮挡，遮挡区域的纹理特征信息完全缺失，导致特征点检测失败。&lt;/p&gt;&lt;p&gt;现有的利用2D特征点来恢复3D人脸形状的方法也存在以下几个问题：i）需要第三方2D特征点检测算法或者手动得到2D特征点；ii）不同姿态下检测的特征点语义信息不一致，难以确定3D点云中与其对应的点 [6]；iii）只生成与输入人脸图像同样姿态和表情的3D人脸，而这样的3D人脸，相对于姿态和表情归一化的3D人脸而言，显然并不有利于人脸识别。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-1da1577674789735f19ec2ed72326d4b.png" data-rawwidth="1269" data-rawheight="541"&gt;&lt;p&gt;为了在一个框架内处理2D特征点定位和3D人脸重建，我们利用两组级联的线性回归，一组用来更新2D特征点，另一组用来更新3D人脸形状。在每一次迭代中，先用SDM[7]方法得到特征点更新量，基于方法[3]再用特征点的更新量去估计出3D人脸形状的更新量。新的3D人脸一旦更新就可以粗略地计算出3D-to-2D投影矩阵，同时再利用3D人脸对特征点进行修正，尤其是自遮挡区域的特征点位置及特征点可见性信息。整个过程2D特征点、3D人脸形状、3D-to-2D投影矩阵的更新都是一个由粗到精的估算过程。&lt;/p&gt;&lt;p&gt;我们先给出利用训练好的回归模型检测任意一张二维人脸图像上的特征点，并重建其三维模型的过程。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-1e43478e63428623fe33214e50fd37ee.jpg" data-rawwidth="687" data-rawheight="354"&gt;&lt;p&gt;值得指出的是：Step 5中，从3D人脸投影得到2D特征点对人脸形状和姿态都有很强的约束。而Step 2中，特征点是通过纹理特征指导得到的，其中自遮挡区域由于纹理信息的缺失，回归得到的特征点常常是不准确的。通过此步骤3D投影来修正能够有效地提高特征点检测的准确度。&lt;/p&gt;&lt;p&gt;在训练过程中，为了得到上述回归模型，需要提供成对的标定好特征点的二维人脸图像及其对应的三维人脸数据&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-86498880a9f7b62b9400c934a339f62b.jpg" data-rawwidth="183" data-rawheight="31"&gt;&lt;p&gt;为了更好地处理任意姿态、任意表情的二维人脸图像，训练数据中需要包括尽量多不同姿态和不同表情的人脸，而对应的三维人脸则都是中性表情的、且已经稠密对齐的点云数据。下面我们重点介绍一下用于人脸对齐的2D特征点回归的目标函数和用于三维人脸重建的3D形状回归的目标函数。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-871693051c1627ba1295b0474e5223fe.jpg" data-rawwidth="541" data-rawheight="100"&gt;该目标函数建立当前2D特征点周围的纹理特征与其距离真实位置的偏移量之间的回归关系。我们训练所用2D特征点是从3D形状投影得到的，因而确保了语义上的一致性。同时为了处理大姿态人脸图像，如果某个特征点被判定为不可见点，那这个点的SIFT特征向量置为0。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-41f61ffbe0fc571b4ea8855e20419d0d.jpg" data-rawwidth="518" data-rawheight="97"&gt;&lt;p&gt;3D形状回归建立的是2D特征点修正量与3D形状修正量之间的关系。所有训练3D人脸都进行了稠密对齐，且2D特征点之间也作好了对齐，所以并不需要增加额外的平滑约束，同时也尽量保持了3D人脸的个性化差异。训练数据中的3D形状是姿态-表情归一化（Pose and Expression Normalized，简称PEN）3D人脸，如此重建得到的PEN 3D人脸更适用于人脸识别。&lt;/p&gt;&lt;p&gt;在公开测试集上的实验结果证明了在统一的回归框架下同时解决人脸对齐和三维重建的有效性。ECCV2016论文中还进一步证明了重构出来的姿态与表情归一化的三维人脸在提升人脸识别准确率方面的有效性。最后，我们展示利用ECCV2016方法得到的人脸对齐和三维重建的几个典型结果。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-8bc2ffab81d46defe5fba8be7fab9c38.jpg" data-rawwidth="494" data-rawheight="188"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-456957dc168a202e17e0ece048795c99.jpg" data-rawwidth="505" data-rawheight="372"&gt;&lt;p&gt;&lt;b&gt;参考文献&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[1]&lt;/b&gt; Kemelmacher-Shlizerman, I., Basri, R.: 3D face reconstruction from a single image using a single reference face shape. TPAMI (2011).&lt;/p&gt;&lt;p&gt;&lt;b&gt;[2] &lt;/b&gt;Blanz, V., Vetter, T.: A morphable model for the synthesis of 3D faces. In: SIGGRAPH (1999).&lt;/p&gt;&lt;p&gt;&lt;b&gt;[3]&lt;/b&gt; Liu, F., Zeng, D., Li, J., Zhao, Q.: Cascaded regressor based 3D face reconstruction from a single arbitrary view image. arXiv preprint arXiv:1509.06161 (2015 Version)&lt;/p&gt;&lt;p&gt;&lt;b&gt;[4]&lt;/b&gt; Liu F, Zeng D, Zhao Q, Liu X.: Joint face alignment and 3D face reconstruction. In: ECCV (2016).&lt;/p&gt;&lt;p&gt;&lt;b&gt;[5]&lt;/b&gt; Jourabloo, A., Liu, X.: Pose-invariant 3D face alignment. In: ICCV (2015)&lt;/p&gt;&lt;p&gt;&lt;b&gt;[6]&lt;/b&gt; Qu C, Monari E, Schuchert T. Fast, robust and automatic 3D face model reconstruction from videos. In: AVSS, 113-118 (2014)&lt;/p&gt;&lt;p&gt;&lt;b&gt;[7] &lt;/b&gt;Xiong X, De la Torre F. Supervised descent method and its applications to face alignment. In: CVPR. 532-539 (2013)&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing"&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-6649b87e70274cdc884477599b024f29.jpg" data-rawwidth="121" data-rawheight="122"&gt;&lt;b&gt;刘峰，&lt;/b&gt;四川大学计算机学院生物特征识别实验室博士三年级学生，导师游志胜教授、赵启军博士。研究方向为机器学习与模式识别（三维人脸建模与识别、二维人脸特征点检测等）。个人邮箱：liuf1990@stu.scu.edu.cn。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;/b&gt;&lt;a href="http://mp.weixin.qq.com/s/udr3573GXQOOF46jLriekg" class=""&gt;http://mp.weixin.qq.com/s/udr3573GXQOOF46jLriekg&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23923248&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Thu, 24 Nov 2016 17:57:35 GMT</pubDate></item><item><title>IJCAI16论文速读：Deep Learning论文选读（下）</title><link>https://zhuanlan.zhihu.com/p/23733088</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-1d697eb1755f2db92d1ba82e202d4ad1_r.png"&gt;&lt;/p&gt;&lt;b&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;/b&gt;&lt;p&gt;&lt;b&gt;IJCAI16会议介绍：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;国际人工智能联合会议（ International Joint Conference on Artificial Intelligence，IJCAI ）是聚集人工智能领域研究者和从业者的盛会，也是人工智能领域中最主要的学术会议之一。1969 年到 2015 年，该大会在每个奇数年举办，现已举办了 24 届。随着近几年来人工智能领域的研究和应用的持续升温，从 2016 年开始，IJCAI 大会将变成每年举办一次的年度盛会；今年是该大会第一次在偶数年举办。第 25 届 IJCAI 大会于 7 月 9 日- 15 日在纽约举办。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Guest Editor导读：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本届会议的举办地在繁华喧嚣的纽约时代广场附近，正映衬了人工智能领域几年来的火热氛围。此次大会包括7场特邀演讲、4场获奖演讲、551篇同行评议论文的presentation，41场workshop、37堂tutorial、22个demo等。深度学习成为了IJCAI 2016的关键词之一，以深度学习为主题的论文报告session共计有3个。本期我们从中选择了1篇深度学习领域的相关论文进行了精读，介绍论文的主要思想，并对论文的贡献进行点评。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Semi-Supervised Multimodal Deep Learning for RGB-D Object Recognition&lt;/b&gt;&lt;/p&gt;&lt;p&gt;深度网络在近两年成绩不俗，应用广泛。RGB-D物体识别的研究人员自然也不会无动于衷，他们厉兵秣马，决意大干一番。怎奈何深度模型需要众多标记数据，而贴标签的营生，不是工程浩繁，就是价格昂贵。针对这一情况，血气方刚的微软人创制新算法，以半监督式学习替代全部附上标签的监督式学习。据称，仅需5%的标签，即可取得往常监督学习的成效。他们是变了什么“戏法”，把这么大的标签空缺补得滴水不漏？&lt;/p&gt;&lt;p&gt;简而言之，就是“协同训练”与“色深互补”。&lt;/p&gt;&lt;p&gt;“色深互补”，是典型的3D图像处理模式。三个颜色信息外加深度信息，统合利用。颜色信息包含更多物体类别信息，而深度信息包含更多物体姿态变化。&lt;/p&gt;&lt;p&gt;协同训练，就是有标签的数据，协同没有标签的数据，一起训练。这个方法并非新硎初发，但这里的方法很有新意，作者叫它“Diversity preserving co-training”,颇有求同存异的味道。在后文中详述实现细节。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-bef44e91be845f336cae6bebc8868edd.png" data-rawwidth="701" data-rawheight="311"&gt;网络设计如图所示。我们先看实线连接部分，从有标签的数据库开始。这里面的数据“案底分明”，所以直接用上卷积网络，提取特征。一番勤学苦练，网络就初具规模。提取到的特征，颜色部分提取的特征就去颜色分类器，深度的部分提取的特征送深度分类器，此外两个部分融合起来，送进画在中间的集成分类器。它的意图是为网络的端对端训练。而后连到虚线部分。颜色、深度这两个“判决机构”敲了锤，下面这个黑饼，代表没有标签的数据库，就赶紧来学习“宣判书”，据此把更多数据贴上标签，它们被送到开头有标签的那个库。具体实现是这么个图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-0959c4c1051fe87a3c9194809705be94.png" data-rawwidth="1160" data-rawheight="399"&gt;&lt;p&gt;实线部分的网络，是比较经典的AlexNet，虚线部分是帖标签器。整体是一个AlexNet+Updating LabelPool结构。&lt;/p&gt;&lt;p&gt;值得一提的是，实线部分的FC7层一分叉，走两股。一股走类别分类器，另一股则是隐含属类分类器，作者叫它“多任务学习”。总体的目标是：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-cc43ccbe776dc093dd7efab7febad735.png" data-rawwidth="502" data-rawheight="140"&gt;这里面x是某一具体数据，花体L表示所有带标签的数据构成的库&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-15b3584113f715ba8013e8d9aa170904.png" data-rawwidth="523" data-rawheight="60"&gt;&lt;p&gt;标签中包含颜色信息I，深度信息D，类别信息y。&lt;/p&gt;&lt;p&gt;v表示颜色或是深度模块， z代表属类标签。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-b710860f57699b26eb99e375d9ab4d44.png" data-rawwidth="122" data-rawheight="35"&gt;&lt;p&gt;表示DCNN模型预测的概率。整个损失函数是典型的门闩型损失（hinge loss）。&lt;/p&gt;&lt;p&gt;下面说说虚线部分的事情，也是本文的“大杀器”。我们也许要问，给无标签数据打标签，具体做法是什么。简单来说，核心的技术就是聚类。所有的数据都要参与聚类。聚类的目的是把没有标签的数据去找与其相似的有标签的数据，信心高的就可以帖它们的标签。信心的依据就是“属类”，聚类聚出来的类别。作者说用到方法叫“凸聚类”，这个方法据称可以收敛到全局最小值，自动找到最优的聚类的类别数。目标函数是最大化下面的对数似然型目标函数：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-0f0dc2eda4e9d64951c975c9994a9a90.png" data-rawwidth="562" data-rawheight="121"&gt;这里面q(x)表示某个数据x的“代表度”，需要满足非负性与加和为1的性质，表示判断的信心。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-0aa6380e61112067da507318006f2185.png" data-rawwidth="326" data-rawheight="36"&gt;是欧氏距离，表示两个样本x和x’的差异。它们都“穿”了一身φ（.），表示这二位都是提取的特征，作者使用的是fc7特征。β是个常数，熟悉热力学玻尔兹曼定律的同学会知道，它表征了某种“温度”或者系统活跃程度的东西。我们在机器学习中常用它作弥散核，估计系统能量。Log函数的加和意味着内部的乘积，说明作者认为所有标签独立分布。整体来说，我们最大化目标，就是要合理地把信心q分配到各个聚类的类别里。这与传统聚类是一致的。聚类的结果表示为：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a1ef63a96b40065dfa6d3a8532b85442.png" data-rawwidth="365" data-rawheight="87"&gt;其中C表示类别数。图中的例子里面，颜色信息聚了5类，深度信息聚了3类。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-ebfa07a584d6355957b1be3f8a66030d.png" data-rawwidth="833" data-rawheight="404"&gt;聚类以后就要更新标签库。将没有标签的数据算出相近属类的信心，信心较高的集合表示为：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-af67fd7c8b0d71458616470b3931fa97.png" data-rawwidth="703" data-rawheight="65"&gt;&lt;p&gt;其中&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-107f55b00b15f21d27883c72c47cb51c.png" data-rawwidth="455" data-rawheight="42"&gt;表示无标签数据。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-22d85025caac3421e27a802433eb6082.png" data-rawwidth="432" data-rawheight="38"&gt;表示给数据x标记属类z的概率。f是softmax函数。τ是一个预先选定的阈值，超过这个阈值的x说明和z属类契合度很高，可以标记z属类。v仍是表示模块，颜色或深度。迭代规则就是：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-031b29b1227c84517564b5cd27f9aa60.png" data-rawwidth="448" data-rawheight="61"&gt;&lt;p&gt;无标签数据x通过z的信息，找到最相近的有标签的z迁移它的y。于是“有较高信任度”的x们获得了标签。&lt;/p&gt;&lt;p&gt;聚类以后的结果Z被赋予新的名字：（隐含）属类。于是原先由颜色、深度、标签组成的三元组，变成现在颜色、颜色属类、深度、深度属类、标签构成的五元组。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-fb1ad736aaad338a2b2d3ce13cb0bd73.png" data-rawwidth="476" data-rawheight="49"&gt;&lt;p&gt;有标签的数据，属类都编号整齐了。&lt;/p&gt;&lt;p&gt;最后我们来说预训练的事儿。在许多视觉领域用其他收敛技术取代了这种做法，但是毕竟标签太少，难说初始化的不好会惹出什么乱子；况且，开始的聚类必须具有代表性，万一在开始的时候聚类类别不全，就后患无穷。索性先以重构目标为先锋，全部数据，带不带标签的数据齐出动，打开局面再说。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-6db81c64ab5076dc1a09cc0fed545baa.png" data-rawwidth="816" data-rawheight="127"&gt;实验（当然辉煌地）证明了方法的有效性，在只使用5%训练数据的情况下就取得了与使用完全标注数据的监督学习方法可比的性能。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-c0c3221eec23936fb656012d9daec180.png" data-rawwidth="990" data-rawheight="1220"&gt;&lt;p&gt;在文章的最后，我们总结一下“变戏法”的过程，即来回答未知的标签从哪里产生的。每个类别都聚成很多子类，而后将无标签数据附会为聚类相近的子类。逻辑上，如果夸类别的子类间很近似，就比较容易犯错。但总体而言，仍比只依靠类别信息更准确些。IJCAI的风格多理论性强，小编猜测此文的桥段中，聚类当取鳌头。另外，预训练的AE给网络更好的初始化，是成功进行后续打标签工作的前提。AlexNet+AE预训练的模式仍旧熠熠生辉，可见深度模型的博大精深啊。小编认为未来半监督学习和无监督学习会逐渐地使用深度模型解决各自的问题。是产生标签或是附会标签，抑或是更聪明地缩小图像与标签间的语义鸿沟，将是未来的方向【小编使命脸】。&lt;/p&gt;&lt;p&gt;&lt;b&gt;参与人员：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;胡蓝青&lt;/b&gt;  中科院计算所VIPL研究组博士研究生&lt;/p&gt;&lt;p&gt;&lt;b&gt;尹肖贻&lt;/b&gt;  中科院计算所VIPL研究组博士研究生&lt;/p&gt;&lt;p&gt;&lt;b&gt;刘昊淼&lt;/b&gt;  中科院计算所VIPL研究组博士研究生&lt;/p&gt;&lt;p&gt;&lt;b&gt;刘    昕 &lt;/b&gt; 中科院计算所VIPL研究组博士研究生&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" data-editable="true" data-title="@果果是枚开心果." class=""&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-32826469aaf31cf687d6d4ab45fc4783.png" data-rawwidth="117" data-rawheight="118"&gt;&lt;p&gt;&lt;b&gt;朱鹏飞，&lt;/b&gt;天津大学机器学习与数据挖掘实验室副教授，硕士生导师。分别于2009和2011年在哈尔滨工业大学能源科学与工程学院获得学士和硕士学位，2015年于香港理工大学电子计算学系获得博士学位。目前，在机器学习与计算机视觉国际顶级会议和期刊上发表论文20余篇，包括AAAI、IJCAI、ICCV、ECCV以及IEEE Transactions on Information Forensics and Security等。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325678&amp;amp;idx=1&amp;amp;sn=331963a6674cde509f75a09ae5e331eb&amp;amp;chksm=f235a5a4c5422cb2314d7ed9e172d47ac461b588426bce8edd9d56bde4f1dafd7993c4f02823&amp;amp;scene=0#wechat_redirect" class=""&gt;http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325678&amp;amp;idx=1&amp;amp;sn=331963a6674cde509f75a09ae5e331eb&amp;amp;chksm=f235a5a4c5422cb2314d7ed9e172d47ac461b588426bce8edd9d56bde4f1dafd7993c4f02823&amp;amp;scene=0#wechat_redirect&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23733088&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Thu, 17 Nov 2016 14:29:23 GMT</pubDate></item></channel></rss>