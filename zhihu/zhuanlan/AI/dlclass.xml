<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>深度学习大讲堂 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/dlclass</link><description>推送深度学习的最新消息，包括最新技术进展，使用以及活动</description><lastBuildDate>Sun, 15 Jan 2017 08:15:30 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>[深度学习大讲堂]从NNVM看2016年深度学习框架发展趋势</title><link>https://zhuanlan.zhihu.com/p/24710026</link><description>深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;&lt;b&gt;虚拟框架杀入&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-db8b87ad733c06bc04ab05174f845aa9.png" data-rawwidth="635" data-rawheight="336"&gt;&lt;p&gt;从发现问题到解决问题&lt;/p&gt;&lt;p&gt;半年前的这时候，暑假，我在SIAT MMLAB实习。&lt;/p&gt;&lt;p&gt;看着同事一会儿跑Torch，一会儿跑MXNet，一会儿跑Theano。&lt;/p&gt;&lt;p&gt;SIAT的服务器一般是不给sudo权限的，我看着同事挣扎在编译这一坨框架的海洋中，开始思考：&lt;/p&gt;&lt;p&gt;是否可以写一个框架：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-47332d834e1d10a78ef0d2e520560e77.png" data-rawwidth="315" data-rawheight="91"&gt;&lt;p&gt;这样，利用工厂模式只编译执行部件的做法，只需编译唯一的后端即可，框架的不同仅仅在于前端脚本的不同。&lt;/p&gt;&lt;p&gt;Caffe2Keras的做法似乎是这样，但Keras本身是基于Theano的编译后端，而我们的更希望Theano都不用编译。&lt;/p&gt;&lt;p&gt;当我9月份拍出一个能跑cifar10的大概原型的时候：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-769467a72f769b604ea22b5af432a982.png" data-rawwidth="633" data-rawheight="408"&gt;&lt;p&gt;我为这种怪异的写法取名叫CGVM(Computational Graph Virtual Machine)然后过了几天，在微博上看到了陈天奇在MXNet的进一步工作NNVM的发布 (o(╯□╰)o)......&lt;/p&gt;&lt;p&gt;NNVM使用2000行模拟出了TensorFlow，我大概用了500行模拟出了Caffe1。&lt;/p&gt;&lt;p&gt;VM(Virtual Machine)的想法其实是一个很正常的想法，这几年我们搞了很多新框架，名字一个比一个炫，但是本质都差不多，框架的使用者实际上是苦不堪言的：&lt;/p&gt;&lt;p&gt;○ 这篇paper使用了A框架，我要花1天配置A框架。&lt;/p&gt;&lt;p&gt;○ 这篇paper使用了B框架，我要花1天配置B框架。&lt;/p&gt;&lt;p&gt;.......&lt;/p&gt;&lt;p&gt;正如LLVM不是一种编译器，NNVM也不是一种框架，看起来更像是框架的屠杀者。&lt;/p&gt;&lt;p&gt;NNVM的可行性恰恰证明了现行的各大框架底层的重复性，而上层的多样性只是一个幌子。&lt;/p&gt;&lt;p&gt;我们真的需要为仅仅是函数封装不同的框架买单吗？这是值得思考的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;计算图走向成熟&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-e67a596dad4b72894775bfac762fab16.png" data-rawwidth="399" data-rawheight="288"&gt;&lt;p&gt;&lt;b&gt;计算图的两种形式&lt;/b&gt;&lt;/p&gt;&lt;p&gt;计算图最早的出处应该是追溯到Bengio在09年的《Learning Deep Architectures for AI》，Bengio使用了有向图结构来描述神经网络的计算:&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-8fce7ea0f6639c42d812d739af689fca.png" data-rawwidth="205" data-rawheight="346"&gt;&lt;p&gt;如图，符号集合{*，+，sin} 构成图的结点，整张图可看成三部分：输入结点、输出结点、从输入到输出的计算函数。&lt;/p&gt;&lt;p&gt;随后在Bengio组的Theano框架执行中，Graph就被隐式应用于Op的连接。&lt;/p&gt;&lt;p&gt;不过这时候，Op还是执行时-动态编译的。&lt;/p&gt;&lt;p&gt;Caffe1中计算图其实就是Net，因为Net可以被Graph模拟出来(CGVM和Caffe2Keras都实现了)。&lt;/p&gt;&lt;p&gt;贾扬清在Caffe1中显式化了计算图的表示，用户可以通过编辑net.prototxt来设计计算图。&lt;/p&gt;&lt;p&gt;Caffe1在Jonathan Long和Evan Shelhamer接手后，他们开发了PyCaffe。&lt;/p&gt;&lt;p&gt;PyCaffe通过Python天然的工厂(__getattr__)，实现了net.prototxt的隐式生成。&lt;/p&gt;&lt;p&gt;之后的Caffe2，也就直接取消了net.prototxt的编辑，同样利用Python的(__getattr__)获取符号类型定义。&lt;/p&gt;&lt;p&gt;Caffe1带来一种新的计算图组织Op的描述方式，不同于Theano直接翻译Op为C执行代码，然后动态编译，软件工程中的高级设计模式——工厂模式被广泛使用。&lt;/p&gt;&lt;p&gt;计算图被划分为三个阶段，定义阶段、构造阶段、执行阶段：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1、&lt;/b&gt;定义阶段：定义Layer/Op的name、type、bottom(input)，top(output)及预设参数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2、&lt;/b&gt;构造阶段：通过工厂模式，由字符串化的定义脚本构造类对象。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3、&lt;/b&gt;执行阶段：根据传入的bottom(input)，得到额外参数(如shape)，此时计算图才能开始执行。阶段划分带来的主要问题是限制了编译代码的完整性和优化程度。&lt;/p&gt;&lt;p&gt;在Theano中，C代码生成是最后一步，编译前你可以组合数个细粒度符号，依靠编译器做一次硬件执行上的优化。&lt;/p&gt;&lt;p&gt;而工厂模式编译符号时只考虑了单元，编译器没有上下文可供参考优化，故最终只能顺序执行多个预先编译的符号单元。&lt;/p&gt;&lt;p&gt;当符号粒度过细时，一个Layer的实现就会变成连续执行多个子过程，导致“TensorFlowSlow”。&lt;/p&gt;&lt;p&gt;&lt;b&gt;计算图作为中间表示(IR)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;PyCaffe和Caffe2将定义阶段移到Python中，而将构造和执行阶段保留在C++中做法，是计算图作为IR的思想启蒙。&lt;/p&gt;&lt;p&gt;Python与C++最大的不同在于：一个是脚本代码，用于前端。一个是本地代码，用于后端。&lt;/p&gt;&lt;p&gt;脚本代码创建/修改模型方便(无需因模型变动而重新编译)、执行慢，本地代码则正好相反。&lt;/p&gt;&lt;p&gt;两者取长补短，所以深度学习框架在2016年，迎来了前后端开发的黄金时代。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-53c105dc87bcea007e96b1fe61a13a2e.png" data-rawwidth="643" data-rawheight="153"&gt;&lt;p&gt;如上图，无论是9月份先提出的NNVM，还是最近Intel曝光的Nervana，都分离了前后端。&lt;/p&gt;&lt;p&gt;后端的独立，不仅减少了编译工作，最大的优势在于降低了传统框架做跨设备计算的代码耦合度。&lt;/p&gt;&lt;p&gt;在paper每周都有一大堆的现在，如果后端的每一次变动都要大量修改前端，那么框架的维护开销是非常大的。&lt;/p&gt;&lt;p&gt;在前端定义用于描述输入-输出关系的计算图有着良好的交互性，我们可以通过函数和重载脚本语言的操作符，定义出媲美MATLAB的运算语言，这些语言以显式的Tensor作为数据结构，Operator作为计算符和函数，Theano和MXNet都是这样隐蔽处理由表达式向计算图过渡的。&lt;/p&gt;&lt;p&gt;而Caffe2则比较直接，你需要先创建一个Graph，然后显示地调用Graph.AddOperator(xxx) TensorFlow同样可以显式化处理Graph。&lt;/p&gt;&lt;p&gt;与用户交互得到的计算图描述字串是唯一的，但是与用户交互的方式却是不唯一的。&lt;/p&gt;&lt;p&gt;所以IR之上，分为两派：&lt;/p&gt;&lt;p&gt;第一派要搞自己的API，函数封装非常有个性，宣示这是自己的专利、独门语言。&lt;/p&gt;&lt;p&gt;第二派不搞自己的API，反而去模拟现有的API，表示我很低调。&lt;/p&gt;&lt;p&gt;显然，用户更喜欢用自己熟悉框架的写法去描述模型，不喜欢天天背着个函数速查手册。&lt;/p&gt;&lt;p&gt;&lt;b&gt;计算图优化&lt;/b&gt;&lt;/p&gt;&lt;p&gt;用于中间表示得到的计算图描述最好不要直接构造，因为存在冗余的求解目标，且可共享变量尚未提取。&lt;/p&gt;&lt;p&gt;当限制计算图描述为有向无环图(DAG)时，一些基本的图论算法便可应用于计算图描述的化简与变换。&lt;/p&gt;&lt;p&gt;陈天奇在今年的MSR Talk：Programming Models and Systems Design for Deep Learning中，总结了计算图优化的三个点：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-169a1f8b895a3386cc0f56dafedfe7e7.png" data-rawwidth="642" data-rawheight="422"&gt;&lt;p&gt;&lt;b&gt;①依赖性剪枝&lt;/b&gt;&lt;/p&gt;&lt;p&gt;分为前向传播剪枝，例：已知A+B=X，A+B=Y，求X？&lt;/p&gt;&lt;p&gt;反向传播剪枝,  例：A+B=X，A+B=Y，求X、Y，dX/dA？&lt;/p&gt;&lt;p&gt;根据用户的求解需求，可以剪掉没有求解的图分支。&lt;/p&gt;&lt;p&gt;&lt;b&gt;②符号融合&lt;/b&gt;&lt;/p&gt;&lt;p&gt;符号融合的自动实现是困难的，因为Kernel基本不再实时编译了，所以更多体现在符号粗细粒度的设计上。&lt;/p&gt;&lt;p&gt;粗粒度的符号融合了数个细粒度的符号，一次编译出连续多个执行步骤的高效率代码。&lt;/p&gt;&lt;p&gt;粗粒度和细粒度并无好坏区分，一个速度快，一个更灵活。&lt;/p&gt;&lt;p&gt;从贪心角度，VM框架通常会提供粗细粒度两种实现給用户，因而需要更多人力维护编译后端。&lt;/p&gt;&lt;p&gt;&lt;b&gt;③内存共享&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Caffe1对于激活函数大多使用的inplace处理——即bottom和top是同一个Blob。&lt;/p&gt;&lt;p&gt;inplace使用新的输出y立即覆盖的输入x，需要以下两个条件：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1、&lt;/b&gt;bottom和top数量都为1，即：计算图中构成一条直线路径，&lt;/p&gt;&lt;p&gt;&lt;b&gt;2、&lt;/b&gt;d(y)/d(x)与x是无关的，所以x被y覆盖不影响求导结果。&lt;/p&gt;&lt;p&gt;常见的激活函数都符号以上两个条件，因而可以减少内存的开销。&lt;/p&gt;&lt;p&gt;但是Caffe1在多网络内存共享优化上极其糟糕的，以至于Caffe1并不适合用来跑GAN，以及更复杂的网络。&lt;/p&gt;&lt;p&gt;一个简单例子是交叉验证上的优化：训练网络和验证网络的大部分Layer都是可以共享的，但是由于Caffe1错误地将Blob独立的放在每个Net里，使得跨Net间很难共享数据。&lt;/p&gt;&lt;p&gt;除此之外，Caffe1还错误地将临时变量Blob独立放在每个Layer里，导致列卷积重复占用几个G内存。&lt;/p&gt;&lt;p&gt;让Net和Layer都能共享内存，只需要将Tensor/Blob置于最顶层，采用MVC来写框架即可。&lt;/p&gt;&lt;p&gt;Caffe2引入了Workspace来管理Tensor，并将工作空间的指针传给每一个Op、每一个Graph的构造函数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;新的风暴已经出现&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-dcad4b344a0914e91e9f2867031795c8.png" data-rawwidth="643" data-rawheight="307"&gt;&lt;p&gt;&lt;b&gt;VM的侧重点&lt;/b&gt;&lt;/p&gt;&lt;p&gt;CGVM和NNVM的侧重点是不太一样的，CGVM更强调前端上的扩展化，后端上的唯一化。&lt;/p&gt;&lt;p&gt;所以CGVM不会去支持Torch编译后端，也不会去支持Caffe编译后端。&lt;/p&gt;&lt;p&gt;在NNVM的知乎讨论帖中，有一种观点认为VM是轻视Operator的实现。&lt;/p&gt;&lt;p&gt;但实际上，我们手里的一堆框架，在Operator、Kernel、Math级别的不少实现是没有多少区别的。&lt;/p&gt;&lt;p&gt;但恰恰折磨用户的正是这些没有多少区别的编译后端：各种依赖库、装Linux、编译各种错。&lt;/p&gt;&lt;p&gt;所以我个人更倾向整个DL社区能够提供一份完善的跨平台、跨设备解决方案，而不是多而杂的备选方案。&lt;/p&gt;&lt;p&gt;从这点来看，CGVM似乎是一个更彻底的框架杀手，但在ICML'15上， Jürgen Schmidhuber指出：&lt;/p&gt;&lt;p&gt;真正运行AI 的代码是非常简短的，甚至高中生都能玩转它。不用有任何担心会有行业垄断AI及其研究。&lt;/p&gt;&lt;p&gt;简短的AI代码，未必就是简单的框架提供的，有可能是自己熟悉的框架，这种需求体现在前端而不是后端。&lt;/p&gt;&lt;p&gt;VM指出了一条多框架混合思路：功能A，框架X写简单。功能B，框架Y写简单。&lt;/p&gt;&lt;p&gt;功能A和功能B又要end-to-end，那么显然混起来用不就行了。&lt;/p&gt;&lt;p&gt;只有使用频率不高的框架才会消亡，VM将框架混合使用后，熟悉的味道更浓了，那么便构不成”框架屠杀者“。&lt;/p&gt;&lt;p&gt;强大的AI代码，未必就是VM提供的，有可能是庞大的后端提供的。&lt;/p&gt;&lt;p&gt;随着paper的快速迭代，后端的扩展仍然是最繁重的编程任务。&lt;/p&gt;&lt;p&gt;VM和后端侧重点各有不同，难分好坏。但分离两者的做法确实是成功的一步。&lt;/p&gt;&lt;p&gt;&lt;b&gt;VM的形式&lt;/b&gt;&lt;/p&gt;&lt;p&gt;VM及计算图描述方式是连接前后端的桥梁。&lt;/p&gt;&lt;p&gt;即便后端是唯一的，根据支持前端的不同，各家写的VM也很难统一。&lt;/p&gt;&lt;p&gt;实际上这就把框架之间的斗争引向了VM之间的斗争。&lt;/p&gt;&lt;p&gt;两人见面谈笑风生，与其问对方用什么框架，不如问对方用什么VM。&lt;/p&gt;&lt;p&gt;&lt;b&gt;VM的主要工作&lt;/b&gt;&lt;/p&gt;&lt;p&gt;合成计算图描述的过程是乏味的，在Caffe1中，我们恐怕已经受够了人工编辑prototxt。&lt;/p&gt;&lt;p&gt;API交互方面，即便是MXNet提供给用户的API也是复杂臃肿的，或许仍然需要一个handbook。&lt;/p&gt;&lt;p&gt;TensorFlow中的TensorBoard借鉴了WebOS，VM上搞一个交互性更强的操作系统也是可行的。&lt;/p&gt;&lt;p&gt;除此之外，我可能比较熟悉一些经典框架，那么不妨让VM去实现那些耳熟能详的函数吧！&lt;/p&gt;&lt;p&gt;&lt;b&gt;1、&lt;/b&gt;模拟Theano.function&lt;/p&gt;&lt;p&gt;Theano的function是一个非常贴近数学表达计算图掩饰工具。function内部转化表达式为计算图定义，同时返回一个lambda函数引向计算图的执行。总之这是一个百看不腻的API。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2、&lt;/b&gt;模拟Theano.grad&lt;/p&gt;&lt;p&gt;结合计算图优化，我们现在可以指定任意一对求导二元组(cost, wrt)。因而，放开手，让自动求导在你的模型中飞舞吧。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3、&lt;/b&gt;模拟Theano.scan&lt;/p&gt;&lt;p&gt;Theano.scan是一个用来搭建RNN的神器。尽管最近Caffe1更新了RNN，但是只支持固定循环步数的RNN。而Theano.scan则可以根据Tensor的shape，为RNN建动态的计算图，这适合在NLP任务中处理不定长句子。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4、&lt;/b&gt;模拟PyCaffe&lt;/p&gt;&lt;p&gt;PyCaffe近来在RCNN、FCN、DeepDream中得到广泛应用，成为搞CV小伙伴们的最爱。PyCaffe大部分是由C++数据结构通过Boost.Python导出的，不幸的是，Boost.Thread导出之后与Python的GIL冲突，导致PyCaffe里无法执行C++线程。尝试模拟移除Boost.Python后的PyCaffe，在Python里把Solver、Net、Layer給写出来吧。&lt;/p&gt;&lt;p&gt;&lt;b&gt;5、&lt;/b&gt;模拟你熟悉的任意框架&lt;/p&gt;&lt;p&gt;.......等等，怎么感觉在写模拟器.....当然写模拟器基本就是在重复造轮子，这个在NNVM的知乎讨论帖中已经指明了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;VM的重要性&lt;/b&gt;&lt;/p&gt;&lt;p&gt;VM是深度学习框架去中心化、解耦化发展迈出的重要一步。&lt;/p&gt;&lt;p&gt;同时暴露了目前框架圈混乱的本质：计算图之下，众生平等。计算图之上，群魔乱舞。&lt;/p&gt;&lt;p&gt;在今年我们可以看多许多框架PK对比的文章，然而大多只是从用户观点出发的简单评测。&lt;/p&gt;&lt;p&gt;对比之下，NNVM关注度不高、反对者还不少这种情况，确实让人感到意外。&lt;/p&gt;&lt;p&gt;&lt;b&gt;回顾与展望&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-e6923d26ce910541a40a4b52a747bd37.png" data-rawwidth="302" data-rawheight="317"&gt;&lt;p&gt;&lt;b&gt;回顾2016：框架圈减肥大作战的开始&lt;/b&gt;&lt;/p&gt;&lt;p&gt;高调宣布开源XXX框架，再封装一些API，实际上已经多余了。&lt;/p&gt;&lt;p&gt;VM的出现，将上层接口的编写引向模拟经典的框架，从而达到减肥的目的。&lt;/p&gt;&lt;p&gt;框架维护者应当将大部分精力主要放在Kernel的编写上，而不是考虑搞一些大新闻。&lt;/p&gt;&lt;p&gt;&lt;b&gt;展望2017：DL社区能否联合开源出跨平台、跨设备的后端解决方案&lt;/b&gt;&lt;/p&gt;&lt;p&gt;后端上，随着ARM、神经芯片的引入，我们迫切需要紧跟着硬件来完成繁重的编程。&lt;/p&gt;&lt;p&gt;后端是一个敏感词，因为硬件可以拿来卖钱，所以更倾向于闭源。&lt;/p&gt;&lt;p&gt;除此之外，即便出现了开源的后端，在山寨和混战之前是否能普及也是一个问题。&lt;/p&gt;&lt;p&gt;&lt;b&gt;展望2017：来写框架吧&lt;/b&gt;&lt;/p&gt;&lt;p&gt;VM的出现，带来另一个值得思考的问题：现在是不是人人应该学写框架了？&lt;/p&gt;&lt;p&gt;传统框架编写的困难在代码耦合度高，学习成本昂贵。&lt;/p&gt;&lt;p&gt;VM流框架分离了前后端之后，前端编写难度很低，后端的则相对固定。&lt;/p&gt;&lt;p&gt;这样一来，框架的编程层次更加分明，Keras地位似乎要危险了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;展望2017：更快迭代的框架，更多变的风格，更难的垄断地位&lt;/b&gt;&lt;/p&gt;&lt;p&gt;相比于paper的迭代，框架的迭代似乎更快了一点。&lt;/p&gt;&lt;p&gt;余凯老师前段时间发出了TensorFlow垄断的担忧，但我们可以很乐观地看到：越来越多的用户，在深入框架的底层。&lt;/p&gt;&lt;p&gt;TensorFlow并不是最好的框架，MXNet也不是，最好的框架是自己用的舒服的框架，最好是一行行自己敲出来的。&lt;/p&gt;&lt;p&gt;如果你已经积累的数个框架的使用经验，是时候把它们无缝衔接在一起了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing/answers" data-editable="true" data-title="@果果是枚开心果." class=""&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-23a1eb185e88eae3c3c4a952f36b5214.png" data-rawwidth="118" data-rawheight="116"&gt;&lt;p&gt;&lt;b&gt;潘汀，&lt;/b&gt;合肥工业大学计算机专业大三本科生，中科院深圳先进院集成所MMLAB访问学生。原ACM-ICPC算法竞赛选手，2015年获CCPC铜牌。2015年初开始研究机器学习，研究兴趣集中于对深度学习理论、应用(CV&amp;amp;NLP)及系统架构设计的综合探索。关于深度学习在面部情感分析方面应用的论文被《自动化学报》录用。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s/N-aMqpWNClCcioNq9q5brA" data-editable="true" data-title="[深度学习大讲堂]从NNVM看2016年深度学习框架发展趋势"&gt;[深度学习大讲堂]从NNVM看2016年深度学习框架发展趋势&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24710026&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Wed, 04 Jan 2017 15:06:07 GMT</pubDate></item><item><title>Caffe代码夜话1</title><link>https://zhuanlan.zhihu.com/p/24709689</link><description>深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;&lt;b&gt;1. 从零开始的Label&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在使用SoftmaxLoss层作为损失函数层的单标签分类问题中，label要求从零开始，例如1000类的ImageNet分类任务，label的范围是0~999。这个限制来自于Caffe的一个实现机制，label会直接作为数组的下标使用，具体代码SoftmaxLoss.cpp中133行和139行的实现代码。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-afe67690613c1082f1630213e7d8c867.png" data-rawwidth="594" data-rawheight="436"&gt;&lt;p&gt;132行第一层for循环中的outer_num等于batch size，对于人脸识别和图像分类等单标签分类任务而言，inner_num等于1。如果label从1开始，会导致bottom_diff数组访问越界。&lt;/p&gt;&lt;p&gt;留两个思考题：&lt;/p&gt;&lt;p&gt;&lt;b&gt;思考题1：&lt;/b&gt;为什么Caffe中引入了这个inner_num，inner_num等于什么，提示一下从FCN全卷积网络的方向去思考。&lt;/p&gt;&lt;p&gt;&lt;b&gt;思考题2：&lt;/b&gt;在标签正确的前提下，如果倒数第一个全连接层num_output &amp;gt; 实际的类别数，Caffe的训练是否会报错？&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. BN中的use_global_status&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-27cd7588eb37c23aa63d3aae45ffd2fe.png" data-rawwidth="298" data-rawheight="213"&gt;&lt;p&gt;但是如果直接拿这个Proto用于训练（基于随机初始化），则会导致模型不收敛，原因在于在Caffe的batch_norm_layer.cpp实现中，use_global_stats==true时会强制使用模型中存储的BatchNorm层均值与方差参数，而非基于当前batch内计算均值和方差。&lt;/p&gt;&lt;p&gt;首先看use_global_stats变量是如何计算的：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-7cf1989ed2deaea7527d901da645b391.png" data-rawwidth="432" data-rawheight="100"&gt;再看这个变量的作用：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-1deb2dc80da57d8fef12137c7e37e3da.png" data-rawwidth="622" data-rawheight="390"&gt;以下代码在use_global_stats为false的时候通过moving average策略计算模型中最终存储的均值和方差：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-0e431babec4c70df98155db5bc83bb2a.png" data-rawwidth="614" data-rawheight="506"&gt;&lt;p&gt;因此，对于随机初始化训练BatchNorm层，只需要在Proto文件中移除use_global_stats参数即可，Caffe会根据当前的Phase(TRAIN或者TEST)自动去设置use_global_stats的值。&lt;/p&gt;&lt;p&gt;再留一个思考题。&lt;/p&gt;&lt;p&gt;&lt;b&gt;思考题3：&lt;/b&gt;BatchNorm层是否支持in place运算，为什么？&lt;/p&gt;&lt;p&gt;本期Caffe夜话就到这里。在后续几期的代码夜话中，我们将陆续介绍网络参数的初始化、Fine-tune的参数拷贝、Net类的组装、Solver类的参数更新机制、添加新的Caffe Layer等内容，敬请期待。&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing/answers"&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;深度学习大讲堂内容组。一个神秘的组织，还没有介绍。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s/IH0ZDkGwv4nBIr23r-dN_g" data-editable="true" data-title="Caffe代码夜话1"&gt;Caffe代码夜话1&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24709689&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Wed, 04 Jan 2017 14:43:20 GMT</pubDate></item><item><title>深度学习框架Caffe源码解析</title><link>https://zhuanlan.zhihu.com/p/24343706</link><description>深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;相信社区中很多小伙伴和我一样使用了很长时间的Caffe深度学习框架，也非常希望从代码层次理解Caffe的实现从而实现新功能的定制。本文将从整体架构和底层实现的视角，对Caffe源码进行解析。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.Caffe总体架构&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Caffe框架主要有五个组件，Blob，Solver，Net，Layer，Proto，其结构图如下图1所示。Solver负责深度网络的训练，每个Solver中包含一个训练网络对象和一个测试网络对象。每个网络则由若干个Layer构成。每个Layer的输入和输出Feature map表示为Input Blob和Output Blob。Blob是Caffe实际存储数据的结构，是一个不定维的矩阵，在Caffe中一般用来表示一个拉直的四维矩阵，四个维度分别对应Batch Size（N），Feature Map的通道数（C）,Feature Map高度(H)和宽度(W)。Proto则基于Google的Protobuf开源项目，是一种类似XML的数据交换格式，用户只需要按格式定义对象的数据成员，可以在多种语言中实现对象的序列化与反序列化，在Caffe中用于网络模型的结构定义、存储和读取。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-e3f56d8fd8f00feae16885b44def948a.jpg" data-rawwidth="667" data-rawheight="354"&gt;&lt;p&gt;&lt;b&gt;2.Blob解析&lt;/b&gt;&lt;/p&gt;&lt;p&gt;下面介绍Caffe中的基本数据存储类Blob。Blob使用SyncedMemory类进行数据存储，数据成员 data_指向实际存储数据的内存或显存块，shape_存储了当前blob的维度信息，diff_这个保存了反向传递时候的梯度信息。在Blob中其实不是只有num，channel，height，width这种四维形式，它是一个不定维度的数据结构，将数据展开存储，而维度单独存在一个vector&amp;lt;int&amp;gt; 类型的shape_变量中，这样每个维度都可以任意变化。&lt;/p&gt;&lt;p&gt;来一起看看Blob的关键函数，data_at这个函数可以读取的存储在此类中的数据，diff_at可以用来读取反向传回来的误差。顺便给个提示，尽量使用data_at(const vector&amp;lt;int&amp;gt;&amp;amp; index)来查找数据。Reshape函数可以修改blob的存储大小，count用来返回存储数据的数量。BlobProto类负责了将Blob数据进行打包序列化到Caffe的模型中。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.工厂模式说明&lt;/b&gt;&lt;/p&gt;&lt;p&gt;接下来介绍一种设计模式Factory Pattern，Caffe 中Solver和Layer对象的创建均使用了此模式，首先看工厂模式的UML的类图：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-0fffc1bbdc6f5dc4368cd3faece45d91.jpg" data-rawwidth="539" data-rawheight="320"&gt;如同Factory生成同一功能但是不同型号产品一样，这些产品实现了同样Operation，很多人看了工厂模式的代码，会产生这样的疑问为何不new一个出来呢，这样new一个出来似乎也没什么问题吧。试想如下情况，由于代码重构类的名称改了，或者构造函数参数变化(增加或减少参数)。而你代码中又有N处new了这个类。如果你又没用工厂，就只能一个一个找来改。工厂模式的作用就是让使用者减少对产品本身的了解，降低使用难度。如果用工厂，只需要修改工厂类的创建具体对象方法的实现，而其他代码不会受到影响。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-6d5bfb3c4ea555cdda9b0013c49124b8.jpg" data-rawwidth="132" data-rawheight="183"&gt;&lt;p&gt;举个例子，写代码少不得饿了要加班去吃夜宵，麦当劳的鸡翅和肯德基的鸡翅都是MM爱吃的东西，虽然口味有所不同，但不管你带MM去麦当劳或肯德基，只管向服务员说“来四个鸡翅”就行了。麦当劳和肯德基就是生产鸡翅的Factory。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.Solver解析&lt;/b&gt;&lt;/p&gt;&lt;p&gt;接下来切回正题，我们看看Solver这个优化对象在Caffe中是如何实现的。SolverRegistry这个类就是我们看到的上面的factory类，负责给我们一个优化算法的产品，外部只需要把数据和网络结构定义好，它就可以自己优化了。&lt;/p&gt;&lt;p&gt;Solver&amp;lt;Dtype&amp;gt;* CreateSolver(const SolverParameter&amp;amp; param)这个函数就是工厂模式下的CreateProduct的操作， Caffe中这个SolverRegistry工厂类可以提供给我们6种产品（优化算法）：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-513246b251efeadae7270acb95c7c8bc.jpg" data-rawwidth="632" data-rawheight="146"&gt;&lt;p&gt;这六种产品的功能都是实现网络的参数更新，只是实现方式不一样。那我们来看看他们的使用流程吧。当然这些产品类似上面Product类中的Operation，每一个Solver都会继承Solve和Step函数，而每个Solver中独有的仅仅是ApplyUpdate这个函数里面执行的内容不一样，接口是一致的，这也和我们之前说的工厂生产出来的产品一样功能一样，细节上有差异，比如大多数电饭煲都有煮饭的功能，但是每一种电饭煲煮饭的加热方式可能不同，有底盘加热的还有立体加热的等。接下里我们看看Solver中的关键函数。&lt;/p&gt;&lt;p&gt;Solver中Solve函数的流程图如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-94271e4cd51bf7365ca39fe9ff95d71f.jpg" data-rawwidth="324" data-rawheight="736"&gt;&lt;p&gt;Solver类中Step函数流程图：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-a6e0939b512b981a60e107906b68fe6c.jpg" data-rawwidth="455" data-rawheight="832"&gt;&lt;/p&gt;&lt;p&gt;Solver中关键的就是调用Sovle函数和Step函数的流程，你只需要对照Solver类中两个函数的具体实现，看懂上面两个流程图就可以理解Caffe训练执行的过程了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;5.Net类解析&lt;/b&gt;&lt;/p&gt;&lt;p&gt;分析过Solver之后我们来分析下Net类的一些关键操作。这个是我们使用Proto创建出来的深度网络对象，这个类负责了深度网络的前向和反向传递。以下是Net类的初始化方法NetInit函数调用流程：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-7374ab58320a291a82e023d25142a47c.jpg" data-rawwidth="511" data-rawheight="563"&gt;&lt;p&gt;Net的类中的关键函数简单剖析&lt;/p&gt;&lt;p&gt;&lt;b&gt;1).&lt;/b&gt;ForwardBackward：按顺序调用了Forward和Backward。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2).&lt;/b&gt;ForwardFromTo(int start, int end)：执行从start层到end层的前向传递，采用简单的for循环调用。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3).&lt;/b&gt;BackwardFromTo(int start, int end)：和前面的ForwardFromTo函数类似，调用从start层到end层的反向传递。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4).&lt;/b&gt;ToProto函数完成网络的序列化到文件，循环调用了每个层的ToProto函数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;6.Layer解析&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Layer是Net的基本组成单元，例如一个卷积层或一个Pooling层。本小节将介绍Layer类的实现。&lt;/p&gt;&lt;p&gt;&lt;b&gt;6.1Layer的继承结构&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-fa0709dc53ffb274bd145c54168cb840.jpg" data-rawwidth="353" data-rawheight="395"&gt;&lt;p&gt;&lt;b&gt;6.2 Layer的创建&lt;/b&gt;&lt;/p&gt;&lt;p&gt;与Solver的创建方式很像，Layer的创建使用的也是工厂模式，这里简单说明下几个宏函数：&lt;/p&gt;&lt;p&gt;REGISTER_LAYER_CREATOR负责将创建层的函数放入LayerRegistry。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-048310b542b15ba9e46ea6b2b5ae5372.jpg" data-rawwidth="618" data-rawheight="204"&gt;我们来看看大多数层创建的函数的生成宏REGISTER_LAYER_CLASS，可以看到宏函数比较简单的，将类型作为函数名称的一部分，这样就可以产生出一个创建函数，并将创建函数放入LayerRegistry。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-41705a9e84a1d1feb89994fb0b2b11f5.jpg" data-rawwidth="670" data-rawheight="127"&gt;&lt;p&gt;REGISTER_LAYER_CREATOR(type, Creator_##type##Layer)&lt;/p&gt;&lt;p&gt;这段代码在split_layer.cpp文件中&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-4769a3aa3adbd768c8f8caeac8026143.jpg" data-rawwidth="443" data-rawheight="67"&gt;&lt;p&gt;REGISTER_LAYER_CLASS(Split)。&lt;/p&gt;&lt;p&gt;这样我们将type替换过以后给大家做个范例，参考下面的代码。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-e9a89ab94d52ed2946dd75db49d69957.jpg" data-rawwidth="668" data-rawheight="88"&gt;当然这里的创建函数好像是直接调用，没有涉及到我们之前工厂模式的一些问题。所有的层的类都是这样吗？当然不是，我们仔细观察卷积类。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-f8e7ab462f0f00bb18713ccb39837255.jpg" data-rawwidth="439" data-rawheight="53"&gt;卷积层怎么没有创建函数呢，当然不是，卷积的层的创建函数在LayerFactory.cpp中，截图给大家看下，具体代码如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-745b984d8a6690f170c9d4c11ccd27fb.jpg" data-rawwidth="602" data-rawheight="116"&gt;&lt;p&gt;这样两种类型的Layer的创建函数都有了对应的声明。这里直接说明除了有cudnn实现的层，其他层都是采用第一种方式实现的创建函数，而带有cudnn实现的层都采用的第二种方式实现的创建函数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;6.3 Layer的初始化&lt;/b&gt;&lt;/p&gt;&lt;p&gt;介绍完创建我们看看层里面的几个函数都是什么时候被调用的。&lt;/p&gt;&lt;p&gt;关键函数Setup此函数在之前的流程图中的NetInit时候被调用，代码如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-7e66524e0594ffb366ecbbf7ab3d22de.jpg" data-rawwidth="449" data-rawheight="172"&gt;&lt;p&gt;这样整个Layer初始化的过程中，CheckBlobCounts被最先调用，然后接下来是LayerSetUp，后面才是Reshape，最后才是SetLossWeights。这样Layer初始化的生命周期大家就有了了解。&lt;/p&gt;&lt;p&gt;&lt;b&gt;6.4 Layer的其他函数的介绍&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Layer的Forward函数和Backward函数完成了网络的前向和反向传递，这两个函数在自己实现新的层必须要实现。其中Backward会修改bottom中blob的diff_，这样就完成了误差的方向传导。&lt;/p&gt;&lt;p&gt;&lt;b&gt;7.Protobuf介绍&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Caffe中的Caffe.proto文件负责了整个Caffe网络的构建，又负责了Caffemodel的存储和读取。下面用一个例子介绍Protobuf的工作方式：&lt;/p&gt;&lt;p&gt;利用protobuffer工具存储512维度图像特征：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1).&lt;/b&gt;message 编写：新建txt文件后缀名改为proto,编写自己的message如下，并放入解压的protobuff的文件夹里；&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-332e462127d845ec83698901f6448280.jpg" data-rawwidth="422" data-rawheight="153"&gt;&lt;p&gt;其中，dwFaceFeatSize表示特征点数量；pfFaceFeat表示人脸特征。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2).&lt;/b&gt;打开windows命令窗口(cmd.exe)----&amp;gt;cd空格，把protobuff的文件路径复制粘贴进去------&amp;gt;enter；&lt;/p&gt;&lt;p&gt;&lt;b&gt;3).&lt;/b&gt;输入指令protoc *.proto --cpp_out=.    ---------&amp;gt;enter&lt;/p&gt;&lt;p&gt;&lt;b&gt;4).&lt;/b&gt;可以看到文件夹里面生成“ *.pb.h”和“*.pb.cpp”两个文件，说明成功了&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-c5eb0ba309ceb4dca3cb5a3cea05f2a8.jpg" data-rawwidth="653" data-rawheight="71"&gt;&lt;p&gt;&lt;b&gt;5).&lt;/b&gt;下面可以和自己的代码整合了：&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1) &lt;/b&gt;新建你自己的工程，把“ *.pb.h”和“*.pb.cpp”两个文件添加到自己的工程里，并写上#include" *.pb.h"&lt;/p&gt;&lt;p&gt;&lt;b&gt;(2) &lt;/b&gt;按照配库的教程把库配置下就可以了&lt;/p&gt;&lt;p&gt;VS下Protobuf的配库方法：&lt;/p&gt;&lt;p&gt;解决方案----&amp;gt;右击工程名----&amp;gt;属性&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1)c/c++---&amp;gt;常规---&amp;gt;附加包含目录---&amp;gt; &lt;/b&gt;&lt;/p&gt;&lt;p&gt;($your protobuffer include path)\protobuffer &lt;/p&gt;&lt;p&gt;&lt;b&gt;(2)c/c++---&amp;gt;链接器--&amp;gt;常规---&amp;gt;附加库目录--&amp;gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;($your protobuffer lib path)\protobuffer&lt;/p&gt;&lt;p&gt;&lt;b&gt;(3) c/c++---&amp;gt;链接器--&amp;gt;输入---&amp;gt;附加依赖项--&amp;gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;libprotobufd.lib;(带d的为debug模式)&lt;/p&gt;&lt;p&gt;或libprotobuf.lib;（不带d,为release模式）&lt;/p&gt;&lt;p&gt;使用protobuf进行打包的方法如下代码：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-35b1da04956260b7d4b522fc2f35bdf3.jpg" data-rawwidth="636" data-rawheight="430"&gt;&lt;p&gt;&lt;b&gt;7.1  Caffe的模型序列化&lt;/b&gt;&lt;/p&gt;&lt;p&gt;BlobProto其实就是Blob序列化成Proto的类，Caffe模型文件使用了该类。Net调用每个层的Toproto方法，每个层的Toproto方法调用了Blob类的ToProto方法，这样完整的模型就被都序列化到proto里面了。最后只要将这个proto继承于message类的对象序列化到文件就完成了模型写入文件。Caffe打包模型的时候就只是简单调用了WriteProtoToBinaryFile这个函数，而这个函数里面的内容如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-c82d228cacbdaf06231c6835fca9b692.jpg" data-rawwidth="629" data-rawheight="70"&gt;&lt;p&gt;至此Caffe的序列化模型的方式就完成了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;7.2 Proto.txt的简单说明&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Caffe网络的构建和Solver的参数定义均由此类型文件完成。Net构建过程中调用ReadProtoFromTextFile将所有的网络参数读入。然后调用上面的流程进行整个caffe网络的构建。这个文件决定了怎样使用存在caffe model中的每个blob是用来做什么的，如果没有了这个文件caffe的模型文件将无法使用，因为模型中只存储了各种各样的blob数据，里面只有float值，而怎样切分这些数据是由prototxt文件决定的。&lt;/p&gt;&lt;p&gt;Caffe的架构在框架上采用了反射机制去动态创建层来构建Net，Protobuf本质上定义了graph，反射机制是由宏配合map结构形成的，然后使用工厂模式去实现各种各样层的创建，当然区别于一般定义配置采用xml或者json，该项目的写法采用了proto文件对组件进行组装。&lt;/p&gt;&lt;p&gt;&lt;b&gt;总结&lt;/b&gt;&lt;/p&gt;&lt;p&gt;以上为Caffe代码架构的一个总体介绍，希望能借此帮助社区的小伙伴找到打开定制化Caffe大门的钥匙。本文作者希望借此抛砖引玉，与更多期望了解Caffe和深度学习框架底层实现的同行交流。&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing/answers" data-editable="true" data-title="@果果是枚开心果."&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-19e5440b1ac54a668a2db1437737e69a.jpg" data-rawwidth="110" data-rawheight="123"&gt;&lt;b&gt;薛云峰，&lt;/b&gt;(&lt;a href="https://github.com/HolidayXue" class="" data-editable="true" data-title="HolidayXue (HolidayXue)"&gt;HolidayXue (HolidayXue)&lt;/a&gt;)，主要从事视频图像算法的研究，就职于浙江捷尚视觉科技股份有限公司担任深度学习算法研究员。捷尚致力于视频大数据和视频监控智能化，现诚招业内算法和工程技术人才，招聘主页&lt;a href="http://www.icarevision.cn/job.php" data-editable="true" data-title="浙江捷尚视觉科技股份有限公司--安全服务运营商" class=""&gt;浙江捷尚视觉科技股份有限公司--安全服务运营商&lt;/a&gt;，联系邮箱：hr@icarevision.cn&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s/rWDCYO5k06zT9BcXk21JJg" data-editable="true" data-title="深度学习框架Caffe源码解析"&gt;深度学习框架Caffe源码解析&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24343706&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Tue, 13 Dec 2016 15:52:56 GMT</pubDate></item><item><title>【Technical Review】ECCV16 Grid Loss及其在人脸检测中的应用</title><link>https://zhuanlan.zhihu.com/p/24342365</link><description>深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;什么是人脸检测？简而言之，给定一张图片，判断图中是否有人脸，如果有人脸，进一步给出每一张人脸的位置和大小。这一看似简单的任务，在实际应用中却面临着诸多困难，其中之一就是当人脸被遮挡时，如何才能准确地进行检测。在ECCV 2016上，有一篇文章专门针对检测遮挡人脸的问题进行了探索：Grid Loss: Detecting Occluded Faces，该文章通过设计新的损失函数，综合考虑局部和整体信息对分类的作用，增强了检测器对遮挡的鲁棒性。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-28ec33f69e1e7b1631359ca80e9affae.jpg" data-rawwidth="272" data-rawheight="286"&gt;&lt;p&gt;检测被遮挡的人脸，这一任务的难点在于，遮挡会导致一部分人脸特征缺失，取而代之的是遮挡物的特征，这不仅容易引起分类器误判，还容易造成漏检。解决遮挡人脸检测的问题可以从数据与算法两个方面切入。从数据方面入手的做法较为直接，即在分类器训练阶段，在正样例集中加入一定比例的带遮挡人脸，让分类器从数据中自动去学习带遮挡人脸的变化模式。数据驱动的方式也就意味着对数据的依赖，而遮挡的变化模式复杂多样，如果希望模型能对遮挡有较好的鲁棒性和泛化能力，那将需要非常大量的数据。从算法的角度入手，已有的一些工作在解决遮挡问题时，有些需要在训练数据中标好人脸的五官，这样在训练数据的制备收集阶段要花费更多的精力；有些在人脸检测的预测阶段有额外的计算，这样会因为处理遮挡带来额外的时间开销，而检测本身就是一个对速度极其敏感的任务，这也不是我们希望看到的。&lt;/p&gt;&lt;p&gt;近年来，神经网络在计算机视觉领域得到了广泛应用，也包括人脸检测这个子领域。神经网络的参数优化过程就像是一艘船在茫茫大海上行驶。这一叶扁舟（神经网络的参数），在大海（参数的解空间）上航行，那黑暗中的灯塔（损失函数），放射出耀眼的光辉（梯度），引导着前进的方向（梯度下降）。神经网络具有强大的非线性建模能力，有些时候对于一个问题效果不好，并不是神经网络的表达能力不足，而是损失函数没能引导神经网络的参数落在一个很好的解上。既然如此，可否改进人脸与非人脸分类时使用的损失函数，引导分类网络学习到对遮挡更鲁棒的特征呢？这样，不会在预测阶段带来额外的计算时间，如果损失函数无需额外标注信息，那也不需要额外的数据标注了。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-2db3cb7e6e07d36cdee4bd08babe616d.jpg" data-rawwidth="435" data-rawheight="291"&gt;这篇文章提出的grid loss就是在这个方向上进行了探索研究。一般的损失函数都是直接根据整个图片的信息计算loss，导致学习出的网络会趋向于利用全局信息分类。这篇文章将分类网络最后一个特征图划分为若干个网格（也就是相当于将图片划分为若干个网格），每个小网格看成一个单独的区域，按同样的方式计算一个loss，与整个图片的loss加和作为最终的loss。这样的loss强化了每一个小网络区域单独的判别能力，使得学习出的特征对于遮挡会更加鲁棒。引用论文中的一个图来说明普通loss与grid loss的区别。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-0340424084e5b06abcb9f8626bcdd8ee.jpg" data-rawwidth="647" data-rawheight="219"&gt;grid loss的数学定义如下（这里每一个grid的loss都是一个hinge loss）：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-b23c2d22e83ad7108b2dbcbbe3f08381.jpg" data-rawwidth="650" data-rawheight="74"&gt;其中，N代表grid的个数，wi与bi是最后一个featuremap的第i个grid对应的权值参数与偏置项, w = [w1, w2, …, wN]为最后一个featuremap整体对应的权值参数，b = b1 + b2 + ... + bN 为其对应的偏置项。这样，公式的第一项代表了整个featuremap上的loss，第二项代表了每一个grid的loss。λ是一个平衡系数，权衡全局的loss与局部的loss大小。m为一个常数，为1 / N，因为希望每一个网格区域对分类有相同的贡献。下图是分类网络的最后一个featuremap分块计算loss的示意图。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-e2a62f1b54affaaab3aeac6833785536.jpg" data-rawwidth="549" data-rawheight="230"&gt;&lt;p&gt;在检测阶段，直接将训练得到的w和b换为一个对应的全连接层即可，不需要任何额外的计算量。&lt;/p&gt;&lt;p&gt;grid loss经过作者的实验论证，能够比较明显的提升对于遮挡人脸的检测效果。除此之外，作者还发现grid loss可以使网络学习出更加多样性的特征；同时可以起到正则化的作用，在减少训练数据的时候，使用了grid loss的检测器性能下降会更少一些。这一点可以从模型集成的角度理解，因为现在强化了每一个grid的作用，最终学习出的检测器有一点若干个检测器集成的味道。作者使用的检测器，使用logistic loss时，用fddb数据集图像测试， 100个误检下的召回为0.795，使用了grid loss可以达到0.838，提高了大约4个百分点。&lt;/p&gt;&lt;p&gt;最后总结一下，grid loss这篇文章提出了一种提升被遮挡人脸检测性能的方法，这种方法无需额外的数据标注（如标注人脸中的五官），并且是一种离线训练时的策略，对在线的检测阶段没有影响，不会有额外的时间代价。作者论文中使用的检测框架是一个使用了滑动窗口范式的比较原始的框架，笔者认为，未来尝试将grid loss嵌入到一些先进的检测框架，如Faster RCNN里，是一件值得一试的事情。最后的最后，祝各位读者在生活这片海域里，都能找到自己最想要的那个损失函数，向之前进。&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing/answers" data-editable="true" data-title="@果果是枚开心果."&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-3baac8c343bbcb2e3ec9809bcbd6e1da.jpg" data-rawwidth="120" data-rawheight="118"&gt;&lt;p&gt;&lt;b&gt;时学鹏，&lt;/b&gt;中科院计算所VIPL组15级硕士生。导师为山世光研究员。研究方向为基于深度学习的目标检测，特别是人脸检测。研发了VIPL课题组第五代人脸检测SDK。个人邮箱：xuepeng.shi@vipl.ict.ac.cn。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s/qiwJnXggRwqvHFN74-W2DQ" data-editable="true" data-title="【Technical Review】ECCV16 Grid Loss及其在人脸检测中的应用"&gt;【Technical Review】ECCV16 Grid Loss及其在人脸检测中的应用&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/24342365&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Tue, 13 Dec 2016 15:03:27 GMT</pubDate></item><item><title>【ECCV2016论文速读】回归框架下的人脸对齐和三维重建</title><link>https://zhuanlan.zhihu.com/p/23923248</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-1da1577674789735f19ec2ed72326d4b_r.png"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;&lt;b&gt;JointFace Alignment and 3D Face Reconstruction&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-f81e5f04be20315f42f9b15f07d1cf8a.jpg" data-rawwidth="710" data-rawheight="375"&gt;&lt;b&gt;（此处三维重建结果是gif动图，但不知什么原因，我的电脑本地无法保存，所以只好截图上传，请点击链接查看原文中的gif动图：&lt;a href="http://mp.weixin.qq.com/s/udr3573GXQOOF46jLriekg" class=""&gt;http://mp.weixin.qq.com/s/udr3573GXQOOF46jLriekg&lt;/a&gt;）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;三维人脸重建的目标是根据某个人的一张或者多张二维人脸图像重建出其三维人脸模型（此处的三维人脸模型一般仅指形状模型，定义为三维点云）。今天我们只讨论由单张二维图像重建三维人脸的问题。这个问题本身其实是个病态（ill-posed）问题，因为在将人脸从三维空间投影到二维平面上形成我们看到的二维人脸图像的过程中，人脸的绝对尺寸（如鼻子高度）、以及由于自遮挡而不可见的部分等很多信息已经丢失。在不掌握相机和拍摄环境的相关参数的情况下，这个问题其实是没有确定解的。&lt;/p&gt;&lt;p&gt;为了解决这一病态问题，一个直接思路是借助机器视觉中的Shape-from-Shading（SFS）方法。但是该方法依赖于光照条件和光照模型的先验知识，而未考虑人脸结构的特殊性，在任意拍摄的人脸图像上效果一般。后来，Kemelmacher-Shizerman和Basri [1] 引入了平均三维人脸模型作为约束条件对传统的SFS方法进行了改进，取得了不错的效果。然而，重建结果往往都接近平均模型，缺少个性化特征。另一个常用思路是建立三维人脸的统计模型，再将该模型拟合到输入的二维人脸图像上，利用拟合参数实现三维人脸的重建。这类方法基本都是基于Blanz和Vetter提出的三维形变模型（3D Morphable Model，简称3DMM） [2]。由于3DMM采用主成分分析（PCA）方法构建统计模型，而PCA本质上是一种低通滤波，所以这类方法在恢复人脸的细节特征方面效果仍然不理想。此外，上述两类方法在重建过程中对每幅图像都需要求解优化问题，因而实时性较差。&lt;/p&gt;&lt;p&gt;受到近年来回归方法在人脸对齐中的成功应用的启发，我们最早试图建立二维人脸图像上的面部特征点（包括眼角、鼻尖、嘴角等）与人脸三维模型之间的回归关系。这一思路的基本出发点是面部特征点是反映人脸三维结构的最直观依据。我们尝试根据二维特征点的偏差直接预测三维人脸形状的调整量。这就好比我们知道二维特征点是由三维人脸形状投影得到的，如果我们发现二维特征点存在偏差，那么根据这一线索我们就应该能够计算出三维人脸形状应该做怎样的调整。而这个计算过程可以用事先训练好的二维特征点偏差与三维形状调整量之间的回归函数来实现。基于这样的思路，我们成功地设计实现了在给定输入二维人脸图像上的特征点的条件下实时重建其三维模型的新方法。相关结果发布在Arxiv [3]。&lt;/p&gt;&lt;p&gt;沿着上述思路，基于2D人脸特征点和3D人脸形状之间很强的相关性，我们进一步尝试将二维人脸图像特征点检测（即人脸对齐）与三维人脸重建过程耦合起来，在回归的框架下同时实现这两个任务。这就是我们今天要介绍的发表在ECCV2016上的工作 [4] （以下称ECCV2016方法）。扯了这么多（希望不是那么远^_^），下面正式进入正题。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-f5b1a0425d08b0e8229ea9cf9b04dd7e.png" data-rawwidth="1269" data-rawheight="485"&gt;如上图所示，之前研究者大都将2D特征点定位和3D人脸重建两个过程割裂开来解决，而这两个工作本质是一个“鸡生蛋、蛋生鸡”问题。一方面，2D特征点 &lt;em&gt;U &lt;/em&gt;可由中性3D人脸 &lt;em&gt;S&lt;/em&gt; 经过表情（&lt;em&gt;FE &lt;/em&gt;）、姿态变换（ &lt;em&gt;FP&lt;/em&gt;）及投影（&lt;em&gt; FC&lt;/em&gt;）得到，即 &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-6c486d0618f0e5011d4c6abc5d8117a2.jpg" data-rawwidth="158" data-rawheight="32"&gt;&lt;p&gt;另一方面，2D特征点携带有丰富的几何信息，这也是3D重建方法的基础。&lt;/p&gt;&lt;p&gt;现有的2D特征点检测方法大部分是基于2D人脸形状建模的，主要存在以下几个问题：i）很难去刻画3D平面外旋转的人脸特征点；ii）在人脸姿态不是很大的情况下，通过变化人脸轮廓特征点语义位置来解决自遮挡的情况，这样会导致不同姿态下检测的特征点语义信息不一致 [5]（如上图，人脸图像中蓝色点所示）；iii）在更大姿态下，尤其是yaw方向超过60度以后，人脸区域存在近一半自遮挡，遮挡区域的纹理特征信息完全缺失，导致特征点检测失败。&lt;/p&gt;&lt;p&gt;现有的利用2D特征点来恢复3D人脸形状的方法也存在以下几个问题：i）需要第三方2D特征点检测算法或者手动得到2D特征点；ii）不同姿态下检测的特征点语义信息不一致，难以确定3D点云中与其对应的点 [6]；iii）只生成与输入人脸图像同样姿态和表情的3D人脸，而这样的3D人脸，相对于姿态和表情归一化的3D人脸而言，显然并不有利于人脸识别。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-1da1577674789735f19ec2ed72326d4b.png" data-rawwidth="1269" data-rawheight="541"&gt;&lt;p&gt;为了在一个框架内处理2D特征点定位和3D人脸重建，我们利用两组级联的线性回归，一组用来更新2D特征点，另一组用来更新3D人脸形状。在每一次迭代中，先用SDM[7]方法得到特征点更新量，基于方法[3]再用特征点的更新量去估计出3D人脸形状的更新量。新的3D人脸一旦更新就可以粗略地计算出3D-to-2D投影矩阵，同时再利用3D人脸对特征点进行修正，尤其是自遮挡区域的特征点位置及特征点可见性信息。整个过程2D特征点、3D人脸形状、3D-to-2D投影矩阵的更新都是一个由粗到精的估算过程。&lt;/p&gt;&lt;p&gt;我们先给出利用训练好的回归模型检测任意一张二维人脸图像上的特征点，并重建其三维模型的过程。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-1e43478e63428623fe33214e50fd37ee.jpg" data-rawwidth="687" data-rawheight="354"&gt;&lt;p&gt;值得指出的是：Step 5中，从3D人脸投影得到2D特征点对人脸形状和姿态都有很强的约束。而Step 2中，特征点是通过纹理特征指导得到的，其中自遮挡区域由于纹理信息的缺失，回归得到的特征点常常是不准确的。通过此步骤3D投影来修正能够有效地提高特征点检测的准确度。&lt;/p&gt;&lt;p&gt;在训练过程中，为了得到上述回归模型，需要提供成对的标定好特征点的二维人脸图像及其对应的三维人脸数据&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-86498880a9f7b62b9400c934a339f62b.jpg" data-rawwidth="183" data-rawheight="31"&gt;&lt;p&gt;为了更好地处理任意姿态、任意表情的二维人脸图像，训练数据中需要包括尽量多不同姿态和不同表情的人脸，而对应的三维人脸则都是中性表情的、且已经稠密对齐的点云数据。下面我们重点介绍一下用于人脸对齐的2D特征点回归的目标函数和用于三维人脸重建的3D形状回归的目标函数。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-871693051c1627ba1295b0474e5223fe.jpg" data-rawwidth="541" data-rawheight="100"&gt;该目标函数建立当前2D特征点周围的纹理特征与其距离真实位置的偏移量之间的回归关系。我们训练所用2D特征点是从3D形状投影得到的，因而确保了语义上的一致性。同时为了处理大姿态人脸图像，如果某个特征点被判定为不可见点，那这个点的SIFT特征向量置为0。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-41f61ffbe0fc571b4ea8855e20419d0d.jpg" data-rawwidth="518" data-rawheight="97"&gt;&lt;p&gt;3D形状回归建立的是2D特征点修正量与3D形状修正量之间的关系。所有训练3D人脸都进行了稠密对齐，且2D特征点之间也作好了对齐，所以并不需要增加额外的平滑约束，同时也尽量保持了3D人脸的个性化差异。训练数据中的3D形状是姿态-表情归一化（Pose and Expression Normalized，简称PEN）3D人脸，如此重建得到的PEN 3D人脸更适用于人脸识别。&lt;/p&gt;&lt;p&gt;在公开测试集上的实验结果证明了在统一的回归框架下同时解决人脸对齐和三维重建的有效性。ECCV2016论文中还进一步证明了重构出来的姿态与表情归一化的三维人脸在提升人脸识别准确率方面的有效性。最后，我们展示利用ECCV2016方法得到的人脸对齐和三维重建的几个典型结果。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-8bc2ffab81d46defe5fba8be7fab9c38.jpg" data-rawwidth="494" data-rawheight="188"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-456957dc168a202e17e0ece048795c99.jpg" data-rawwidth="505" data-rawheight="372"&gt;&lt;p&gt;&lt;b&gt;参考文献&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[1]&lt;/b&gt; Kemelmacher-Shlizerman, I., Basri, R.: 3D face reconstruction from a single image using a single reference face shape. TPAMI (2011).&lt;/p&gt;&lt;p&gt;&lt;b&gt;[2] &lt;/b&gt;Blanz, V., Vetter, T.: A morphable model for the synthesis of 3D faces. In: SIGGRAPH (1999).&lt;/p&gt;&lt;p&gt;&lt;b&gt;[3]&lt;/b&gt; Liu, F., Zeng, D., Li, J., Zhao, Q.: Cascaded regressor based 3D face reconstruction from a single arbitrary view image. arXiv preprint arXiv:1509.06161 (2015 Version)&lt;/p&gt;&lt;p&gt;&lt;b&gt;[4]&lt;/b&gt; Liu F, Zeng D, Zhao Q, Liu X.: Joint face alignment and 3D face reconstruction. In: ECCV (2016).&lt;/p&gt;&lt;p&gt;&lt;b&gt;[5]&lt;/b&gt; Jourabloo, A., Liu, X.: Pose-invariant 3D face alignment. In: ICCV (2015)&lt;/p&gt;&lt;p&gt;&lt;b&gt;[6]&lt;/b&gt; Qu C, Monari E, Schuchert T. Fast, robust and automatic 3D face model reconstruction from videos. In: AVSS, 113-118 (2014)&lt;/p&gt;&lt;p&gt;&lt;b&gt;[7] &lt;/b&gt;Xiong X, De la Torre F. Supervised descent method and its applications to face alignment. In: CVPR. 532-539 (2013)&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing"&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-6649b87e70274cdc884477599b024f29.jpg" data-rawwidth="121" data-rawheight="122"&gt;&lt;b&gt;刘峰，&lt;/b&gt;四川大学计算机学院生物特征识别实验室博士三年级学生，导师游志胜教授、赵启军博士。研究方向为机器学习与模式识别（三维人脸建模与识别、二维人脸特征点检测等）。个人邮箱：liuf1990@stu.scu.edu.cn。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;/b&gt;&lt;a href="http://mp.weixin.qq.com/s/udr3573GXQOOF46jLriekg" class=""&gt;http://mp.weixin.qq.com/s/udr3573GXQOOF46jLriekg&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23923248&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Thu, 24 Nov 2016 17:57:35 GMT</pubDate></item><item><title>IJCAI16论文速读：Deep Learning论文选读（下）</title><link>https://zhuanlan.zhihu.com/p/23733088</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-1d697eb1755f2db92d1ba82e202d4ad1_r.png"&gt;&lt;/p&gt;&lt;b&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;/b&gt;&lt;p&gt;&lt;b&gt;IJCAI16会议介绍：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;国际人工智能联合会议（ International Joint Conference on Artificial Intelligence，IJCAI ）是聚集人工智能领域研究者和从业者的盛会，也是人工智能领域中最主要的学术会议之一。1969 年到 2015 年，该大会在每个奇数年举办，现已举办了 24 届。随着近几年来人工智能领域的研究和应用的持续升温，从 2016 年开始，IJCAI 大会将变成每年举办一次的年度盛会；今年是该大会第一次在偶数年举办。第 25 届 IJCAI 大会于 7 月 9 日- 15 日在纽约举办。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Guest Editor导读：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本届会议的举办地在繁华喧嚣的纽约时代广场附近，正映衬了人工智能领域几年来的火热氛围。此次大会包括7场特邀演讲、4场获奖演讲、551篇同行评议论文的presentation，41场workshop、37堂tutorial、22个demo等。深度学习成为了IJCAI 2016的关键词之一，以深度学习为主题的论文报告session共计有3个。本期我们从中选择了1篇深度学习领域的相关论文进行了精读，介绍论文的主要思想，并对论文的贡献进行点评。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Semi-Supervised Multimodal Deep Learning for RGB-D Object Recognition&lt;/b&gt;&lt;/p&gt;&lt;p&gt;深度网络在近两年成绩不俗，应用广泛。RGB-D物体识别的研究人员自然也不会无动于衷，他们厉兵秣马，决意大干一番。怎奈何深度模型需要众多标记数据，而贴标签的营生，不是工程浩繁，就是价格昂贵。针对这一情况，血气方刚的微软人创制新算法，以半监督式学习替代全部附上标签的监督式学习。据称，仅需5%的标签，即可取得往常监督学习的成效。他们是变了什么“戏法”，把这么大的标签空缺补得滴水不漏？&lt;/p&gt;&lt;p&gt;简而言之，就是“协同训练”与“色深互补”。&lt;/p&gt;&lt;p&gt;“色深互补”，是典型的3D图像处理模式。三个颜色信息外加深度信息，统合利用。颜色信息包含更多物体类别信息，而深度信息包含更多物体姿态变化。&lt;/p&gt;&lt;p&gt;协同训练，就是有标签的数据，协同没有标签的数据，一起训练。这个方法并非新硎初发，但这里的方法很有新意，作者叫它“Diversity preserving co-training”,颇有求同存异的味道。在后文中详述实现细节。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-bef44e91be845f336cae6bebc8868edd.png" data-rawwidth="701" data-rawheight="311"&gt;网络设计如图所示。我们先看实线连接部分，从有标签的数据库开始。这里面的数据“案底分明”，所以直接用上卷积网络，提取特征。一番勤学苦练，网络就初具规模。提取到的特征，颜色部分提取的特征就去颜色分类器，深度的部分提取的特征送深度分类器，此外两个部分融合起来，送进画在中间的集成分类器。它的意图是为网络的端对端训练。而后连到虚线部分。颜色、深度这两个“判决机构”敲了锤，下面这个黑饼，代表没有标签的数据库，就赶紧来学习“宣判书”，据此把更多数据贴上标签，它们被送到开头有标签的那个库。具体实现是这么个图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-0959c4c1051fe87a3c9194809705be94.png" data-rawwidth="1160" data-rawheight="399"&gt;&lt;p&gt;实线部分的网络，是比较经典的AlexNet，虚线部分是帖标签器。整体是一个AlexNet+Updating LabelPool结构。&lt;/p&gt;&lt;p&gt;值得一提的是，实线部分的FC7层一分叉，走两股。一股走类别分类器，另一股则是隐含属类分类器，作者叫它“多任务学习”。总体的目标是：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-cc43ccbe776dc093dd7efab7febad735.png" data-rawwidth="502" data-rawheight="140"&gt;这里面x是某一具体数据，花体L表示所有带标签的数据构成的库&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-15b3584113f715ba8013e8d9aa170904.png" data-rawwidth="523" data-rawheight="60"&gt;&lt;p&gt;标签中包含颜色信息I，深度信息D，类别信息y。&lt;/p&gt;&lt;p&gt;v表示颜色或是深度模块， z代表属类标签。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-b710860f57699b26eb99e375d9ab4d44.png" data-rawwidth="122" data-rawheight="35"&gt;&lt;p&gt;表示DCNN模型预测的概率。整个损失函数是典型的门闩型损失（hinge loss）。&lt;/p&gt;&lt;p&gt;下面说说虚线部分的事情，也是本文的“大杀器”。我们也许要问，给无标签数据打标签，具体做法是什么。简单来说，核心的技术就是聚类。所有的数据都要参与聚类。聚类的目的是把没有标签的数据去找与其相似的有标签的数据，信心高的就可以帖它们的标签。信心的依据就是“属类”，聚类聚出来的类别。作者说用到方法叫“凸聚类”，这个方法据称可以收敛到全局最小值，自动找到最优的聚类的类别数。目标函数是最大化下面的对数似然型目标函数：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-0f0dc2eda4e9d64951c975c9994a9a90.png" data-rawwidth="562" data-rawheight="121"&gt;这里面q(x)表示某个数据x的“代表度”，需要满足非负性与加和为1的性质，表示判断的信心。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-0aa6380e61112067da507318006f2185.png" data-rawwidth="326" data-rawheight="36"&gt;是欧氏距离，表示两个样本x和x’的差异。它们都“穿”了一身φ（.），表示这二位都是提取的特征，作者使用的是fc7特征。β是个常数，熟悉热力学玻尔兹曼定律的同学会知道，它表征了某种“温度”或者系统活跃程度的东西。我们在机器学习中常用它作弥散核，估计系统能量。Log函数的加和意味着内部的乘积，说明作者认为所有标签独立分布。整体来说，我们最大化目标，就是要合理地把信心q分配到各个聚类的类别里。这与传统聚类是一致的。聚类的结果表示为：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-a1ef63a96b40065dfa6d3a8532b85442.png" data-rawwidth="365" data-rawheight="87"&gt;其中C表示类别数。图中的例子里面，颜色信息聚了5类，深度信息聚了3类。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-ebfa07a584d6355957b1be3f8a66030d.png" data-rawwidth="833" data-rawheight="404"&gt;聚类以后就要更新标签库。将没有标签的数据算出相近属类的信心，信心较高的集合表示为：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-af67fd7c8b0d71458616470b3931fa97.png" data-rawwidth="703" data-rawheight="65"&gt;&lt;p&gt;其中&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-107f55b00b15f21d27883c72c47cb51c.png" data-rawwidth="455" data-rawheight="42"&gt;表示无标签数据。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-22d85025caac3421e27a802433eb6082.png" data-rawwidth="432" data-rawheight="38"&gt;表示给数据x标记属类z的概率。f是softmax函数。τ是一个预先选定的阈值，超过这个阈值的x说明和z属类契合度很高，可以标记z属类。v仍是表示模块，颜色或深度。迭代规则就是：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-031b29b1227c84517564b5cd27f9aa60.png" data-rawwidth="448" data-rawheight="61"&gt;&lt;p&gt;无标签数据x通过z的信息，找到最相近的有标签的z迁移它的y。于是“有较高信任度”的x们获得了标签。&lt;/p&gt;&lt;p&gt;聚类以后的结果Z被赋予新的名字：（隐含）属类。于是原先由颜色、深度、标签组成的三元组，变成现在颜色、颜色属类、深度、深度属类、标签构成的五元组。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-fb1ad736aaad338a2b2d3ce13cb0bd73.png" data-rawwidth="476" data-rawheight="49"&gt;&lt;p&gt;有标签的数据，属类都编号整齐了。&lt;/p&gt;&lt;p&gt;最后我们来说预训练的事儿。在许多视觉领域用其他收敛技术取代了这种做法，但是毕竟标签太少，难说初始化的不好会惹出什么乱子；况且，开始的聚类必须具有代表性，万一在开始的时候聚类类别不全，就后患无穷。索性先以重构目标为先锋，全部数据，带不带标签的数据齐出动，打开局面再说。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-6db81c64ab5076dc1a09cc0fed545baa.png" data-rawwidth="816" data-rawheight="127"&gt;实验（当然辉煌地）证明了方法的有效性，在只使用5%训练数据的情况下就取得了与使用完全标注数据的监督学习方法可比的性能。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-c0c3221eec23936fb656012d9daec180.png" data-rawwidth="990" data-rawheight="1220"&gt;&lt;p&gt;在文章的最后，我们总结一下“变戏法”的过程，即来回答未知的标签从哪里产生的。每个类别都聚成很多子类，而后将无标签数据附会为聚类相近的子类。逻辑上，如果夸类别的子类间很近似，就比较容易犯错。但总体而言，仍比只依靠类别信息更准确些。IJCAI的风格多理论性强，小编猜测此文的桥段中，聚类当取鳌头。另外，预训练的AE给网络更好的初始化，是成功进行后续打标签工作的前提。AlexNet+AE预训练的模式仍旧熠熠生辉，可见深度模型的博大精深啊。小编认为未来半监督学习和无监督学习会逐渐地使用深度模型解决各自的问题。是产生标签或是附会标签，抑或是更聪明地缩小图像与标签间的语义鸿沟，将是未来的方向【小编使命脸】。&lt;/p&gt;&lt;p&gt;&lt;b&gt;参与人员：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;胡蓝青&lt;/b&gt;  中科院计算所VIPL研究组博士研究生&lt;/p&gt;&lt;p&gt;&lt;b&gt;尹肖贻&lt;/b&gt;  中科院计算所VIPL研究组博士研究生&lt;/p&gt;&lt;p&gt;&lt;b&gt;刘昊淼&lt;/b&gt;  中科院计算所VIPL研究组博士研究生&lt;/p&gt;&lt;p&gt;&lt;b&gt;刘    昕 &lt;/b&gt; 中科院计算所VIPL研究组博士研究生&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" data-editable="true" data-title="@果果是枚开心果." class=""&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-32826469aaf31cf687d6d4ab45fc4783.png" data-rawwidth="117" data-rawheight="118"&gt;&lt;p&gt;&lt;b&gt;朱鹏飞，&lt;/b&gt;天津大学机器学习与数据挖掘实验室副教授，硕士生导师。分别于2009和2011年在哈尔滨工业大学能源科学与工程学院获得学士和硕士学位，2015年于香港理工大学电子计算学系获得博士学位。目前，在机器学习与计算机视觉国际顶级会议和期刊上发表论文20余篇，包括AAAI、IJCAI、ICCV、ECCV以及IEEE Transactions on Information Forensics and Security等。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325678&amp;amp;idx=1&amp;amp;sn=331963a6674cde509f75a09ae5e331eb&amp;amp;chksm=f235a5a4c5422cb2314d7ed9e172d47ac461b588426bce8edd9d56bde4f1dafd7993c4f02823&amp;amp;scene=0#wechat_redirect" class=""&gt;http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325678&amp;amp;idx=1&amp;amp;sn=331963a6674cde509f75a09ae5e331eb&amp;amp;chksm=f235a5a4c5422cb2314d7ed9e172d47ac461b588426bce8edd9d56bde4f1dafd7993c4f02823&amp;amp;scene=0#wechat_redirect&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23733088&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Thu, 17 Nov 2016 14:29:23 GMT</pubDate></item><item><title>［冠军之道］ECCV16视频性格分析竞赛冠军团队分享</title><link>https://zhuanlan.zhihu.com/p/23510566</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-1a17aae6aea40fcd02321bbe879c429f_r.png"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;英文中有句谚语叫："You never get a second chance to make a first impression."（你永远没有第二个机会去改变你的第一印象。）一个人的第一印象可以用来快速判断其性格特征（Personal traits）及其复杂的社交特质，如友善、和蔼、强硬和控制欲等等。因此，在人工智能大行其道的当下，基于第一印象/表象的性格自动分析也成为计算机视觉和多媒体领域中一类非常重要的研究问题。&lt;/p&gt;&lt;p&gt;前不久，欧洲计算机视觉大会（ECCV 2016）ChaLearn Looking at People Workshop就举办了一场全球范围的（视频）表象性格分析竞赛（Apparent personality analysis）。历时两个多月，我们的参赛队（NJU-LAMDA）在86个参赛者，其中包括有印度“科学皇冠上的瑰宝”之称的Indian Institutes of Technology （IIT）和荷兰名校Radboud University等劲旅中脱引而出，斩获第一。在此与大家分享我们的竞赛模型和比赛细节。&lt;/p&gt;&lt;p&gt;&lt;b&gt;问题重述&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本次ECCV竞赛提供了平均长度为15秒的10000个短视频，其中6000个为训练集，2000个为验证集，剩余2000个作为测试。比赛要求通过对短视频中人物表象（表情、动作及神态等）的分析来精确预测人的五大性格特质，即Big Five Traits，其中包括：经验开放性（Openness to experience）、尽责性（Conscientiousness）、外向性（Extraversion）、亲和性（Agreeableness）和情绪不稳定性（Neuroticism）。视频示例如下所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-1a17aae6aea40fcd02321bbe879c429f.png" data-rawwidth="771" data-rawheight="274"&gt;竞赛数据中五大性格特质的真实标记（Ground truth）通过Amazon Mechanical Turk人工标注获得，每个性格特质对应一个0～1之间的实值。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-d3b3e4836ea7fea91b1f0cf8c6833c8a.png" data-rawwidth="683" data-rawheight="271"&gt;&lt;p&gt;&lt;b&gt;我们的方法&lt;/b&gt;&lt;/p&gt;&lt;p&gt;由于竞赛数据为短视频，我们很自然的把它作为双模态（Bimodal）的数据对象来进行处理，其中一个模态为音频信息（Audio cue），另一个则为视觉信息（Visual cue）。同时，需预测的五大性格特质均为连续值，因此我们将整个问题形式化为一个回归问题（Regression）。我们将提出的这个模型框架称作双模态深度回归（Deep Bimodal Regression，DBR）模型。下面分别从两个模态的处理和最后的模态融合来解析DBR。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-17b800bf60b1cc182b430ff4c45dbbbd.png" data-rawwidth="745" data-rawheight="285"&gt;&lt;p&gt;&lt;b&gt;视觉模态&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在视觉模态中，考虑到对于短视频类数据，时序信息的重要程度并不显著，我们采取了更简单有效的视频处理方式，即直接将视频随机抽取若干帧（Frame），并将其作为视觉模态的原始输入。当然，在DBR中，视觉模态的表示学习部分不能免俗的使用了卷积神经网络（Convolutional Neural Networks，CNN）。同时，我们在现有网络基础上进行了改进，提出了描述子融合网络（Descriptor Aggregation Networks，DAN），从而取得了更好的预测性能。&lt;/p&gt;&lt;p&gt;以VGG-16为例，传统CNN经过若干层卷积（Convolutional）、池化（Pooling）的堆叠，其后一般是两层全链接层（Fully connected layers）作为网络的分类部分，最终输出结果。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-71873c59318e964426a485a8f96a18f5.png" data-rawwidth="715" data-rawheight="232"&gt;受到我们最近工作[2]的启发，在DBR视觉模态的CNN中，我们扔掉了参数冗余的全链接层，取而代之的是将最后一层卷积层学到的深度描述子（Deep descriptor）做融合（Aggregation），之后对其进行L2规范化（L2-normalization），最后基于这样的图像表示做回归（fc+sigmoid作为回归层），构建端到端（End-to-end）的深度学习回归模型。另外，不同融合方式也可视作一种特征层面的集成（Ensemble）。如下图，在DAN中，我们对最后一层卷积得到的深度描述子分别进行最大（Max）和平均（Average）的全局池化（Global pooling）操作，之后对得到的融合结果分别做L2规范化，接下来将两支得到的特征级联（concatenation）后作为最终的图像表示（Image representation）。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-127bca041e07a0ce2593a893ea11c794.png" data-rawwidth="739" data-rawheight="222"&gt;&lt;p&gt;传统CNN中，80%的参数存在于全链接层，而DAN摒弃了全链接，使得DAN相比传统CNN模型拥有更少的参数，同时大幅减少的参数可加速模型的训练速度。另外，全局池化带来了另一个优势即最终的图像表示（512维）相比传统全链接层（4096维）有了更低的维度，有利于模型的可扩展性以处理海量（Large-scale）数据。&lt;/p&gt;&lt;p&gt;此外，为了集成多层信息（Multiple layer ensemble），在DAN基础上我们提出了可端到端训练的DAN+。具体而言，是对ReLU5_2层的深度描述子做上述同样操作，得到对应于ReLU5_2的图像表示，将其与Pool5层的DAN得到的图像表示进行二次级联，最终的向量维度为2048维。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-0e6d8e47359aa115d45a3a0fbff046e5.png" data-rawwidth="666" data-rawheight="413"&gt;&lt;p&gt;除DAN和DAN+外，在视觉模态中，我们还利用了著名的残差网络（Residual Networks）作为模型集成的另一部分。&lt;/p&gt;&lt;p&gt;&lt;b&gt;音频模态&lt;/b&gt;&lt;/p&gt;&lt;p&gt;语音处理中的一种常用的特征为MFCC特征，在竞赛模型中，我们首先从视频中提取原始语音作为输入数据，之后对其抽取MFCC特征。在此需要指出的是，抽取MFCC过程的一个副产品是一种名为logfbank特征，如下图所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-3a27d5d81a8df278fa9f002cd76a63cb.png" data-rawwidth="675" data-rawheight="374"&gt;在抽取logfbank和MFCC特征后，我们同样采取mini-batch形式的训练方式训练线性回归器（Linear regression）。在竞赛中，我们发现logfbank相比MFCC有更优秀的预测效果，如下图所示。其纵轴为回归错误率（越低越好），其横轴为训练轮数，可以发现logfbank在最终的回归错误率上相比MFCC有近0.5%的提升。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-10ad10e7a85e32eb437f3946ce5a55ee.png" data-rawwidth="631" data-rawheight="297"&gt;&lt;p&gt;于是我们选取logfbank特征作为音频模态的特征表示以预测音频模态的回归结果。由于竞赛时间和精力有限，我们在比赛中未使用语音处理领域的深度学习模型。不过，这也是后续可以提高模型性能的一个重要途径。&lt;/p&gt;&lt;p&gt;&lt;b&gt;模态融合（Modality ensemble）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;待两个模态的模型训练完毕，可以得到不同模态不同模型的性格特质预测结果，比赛中我们将其无权重的平均作为该视频最终的性格特质预测结果，如图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-11b42dc2fd1b694909a187596cbeb727.png" data-rawwidth="721" data-rawheight="396"&gt;&lt;p&gt;&lt;b&gt;竞赛结果&lt;/b&gt;&lt;/p&gt;&lt;p&gt;比赛中，我们对一个视频抽取100帧／张图像作为其视觉模态的输入，对应的原始音频作为抽取logfbank特征的语料。训练阶段，针对视觉模态，其100张图像共享对应的性格特质真实标记；预测阶段，其100张图像的平均预测值将作为该视频视觉模态的预测结果。&lt;/p&gt;&lt;p&gt;经下表对比，可以清楚看到，DAN相比VGG-Face，由于没有了冗余的全链接层，其参数只有VGG-Face的约十分之一，而回归预测准确率却优于传统VGG模型，同时特征维度大大减少。此外，相比ResNet，我们提出的模型DAN和DAN+也有不俗表现。此外，在模型预测速度上，DAN和DAN+也快于VGG和ResNet。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-42971d8b8b3c4e7e358a2c313200272c.png" data-rawwidth="729" data-rawheight="425"&gt;模态集成后，我们在五个性格特质预测上取得了四个结果的第一，同时我们也取得了总成绩的冠军。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-1d4616a1f6bc55bfb643c03278a356c7.png" data-rawwidth="748" data-rawheight="199"&gt;&lt;p&gt;&lt;b&gt;模型分析&lt;/b&gt;&lt;/p&gt;&lt;p&gt;最后，我们将模型最后一层卷积／池化的特征做了可视化。可以发现ResNet仅仅将“注意力”聚焦在了视频中的人物上，而我们的DAN和DAN+不仅可以“注意”到人，同时可以将环境和动作信息结合起来进行表象性格预测。另外值得一提的是，其余参赛队均做了人脸检测等预处理操作，从而将人物从视频中“抠”出，但是这样的操作反而降低了整个性格特质预测的性能。俗话说“气由心生”，一个人所处的环境（尤其是卧室、办公室等私人场所）往往可以从侧面反映一个人的性格特性。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-df599f59d6cea93a1fac22e2f525015e.png" data-rawwidth="670" data-rawheight="360"&gt;&lt;p&gt;&lt;b&gt;参考文献&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[1]&lt;/b&gt; Victor Ponce-Lopez, Baiyu Chen, Marc Oliu, Ciprian Cornearu, Albert Clapes, Isabelle Guyon, Xavier Baro, Hugo Jair Escalante and Sergio Escalera. ChaLearn LAP 2016: First Round Challenge on First Impressions - Dataset and Results. European Conference on Computer Vision, 2016.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[2] &lt;/b&gt;Xiu-Shen Wei, Chen-Wei Xie and Jianxin Wu. Mask-CNN: Localizing Parts and Selecting Descriptors for Fine-Grained Image Recognition. arXiv:1605.06878, 2016.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[3] &lt;/b&gt;Chen-Lin Zhang, Hao Zhang, Xiu-Shen Wei and Jianxin Wu. Deep Bimodal Regression for Apparent Personality Analysis. European Conference on Computer Vision, 2016.&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" data-editable="true" data-title="@果果是枚开心果." class=""&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-1a88dceaa7b025012577691eef27c2ef.jpg" data-rawwidth="119" data-rawheight="118"&gt;&lt;b&gt;魏秀参，&lt;/b&gt;为本次竞赛NJU-LAMDA参赛队Team Director。南京大学计算机系机器学习与数据挖掘所（LAMDA）博士生，研究方向为计算机视觉和机器学习。曾在国际顶级期刊和会议发表多篇学术论文，并多次获得国际计算机视觉相关竞赛冠亚军，另撰写的「Must Know Tips/Tricks in Deep Neural Networks」受邀发布于国际知名数据挖掘论坛 KDnuggets 等。 微博ID：Wilson_NJUer&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325653&amp;amp;idx=1&amp;amp;sn=180f7ad377ff7face55814c65502db33&amp;amp;chksm=f235a59fc5422c89861c0565b050ea68ea31c1be6f09bcf14423371e4fee32d325cd72b800c9&amp;amp;scene=0#wechat_redirect" data-editable="true" data-title="［冠军之道］ECCV16视频性格分析竞赛冠军团队分享"&gt;［冠军之道］ECCV16视频性格分析竞赛冠军团队分享&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23510566&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Tue, 08 Nov 2016 15:06:48 GMT</pubDate></item><item><title>深度学习在图像取证中的进展与趋势</title><link>https://zhuanlan.zhihu.com/p/23341157</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-a6d2f174115f956756296d6bda538025_r.png"&gt;&lt;/p&gt;&lt;p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;/p&gt;&lt;p&gt;&lt;b&gt;图像取证&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在当今飞速发展的信息时代，数字图像已经渗透到社会生活的每一个角落，数字图像的广泛使用也促进了数字图像编辑软件的开发与应用，例如：Adobe Photoshop、CorelDRAW、美图秀秀等等。利用这些编辑工具，用户可以随意对图像进行修改，从而达到更好的视觉效果。然而，在方便了用户的同时，也给一些不法分子以可乘之机。在未经授权的情况下，不法分子对图像内容进行非法操作，如违规编辑、合成虚假图像等，从而造成篡改图像在人们社会生活中泛滥成灾。图像取证技术就是在这样的背景下提出，旨在通过盲分析手段认证图像数据的原始性和真实性、鉴别和分析图像所经历的操作处理及估计图像的操作历史[1]。&lt;/p&gt;&lt;p&gt;数字图像的完整周期包含三个部分[2]：图像获取、图像编码、图像编辑，如图1所示。在图像获取过程中，真实场景中的光线通过相机镜头投射到相机传感器（如CCD或者CMOS传感器），产生数字图像信号。在投射到相机传感器之前，通常光首先经过CFA滤波处理，即每个像素点只会包含一种主要的颜色分量（红、绿、蓝）。在相机传感器之后会进行CFA差值(也称去马赛克处理）从而获取每个像素点的红绿蓝三通道分量。然后获取的数字图像信号会经历相机内部的软件处理，比如白平衡、对比度增强、图像锐化、伽马矫正等等。在图像编码部分，经过处理后的数字图像信号为了节省相机内存通常会经过有失真压缩处理，最常见的压缩方式为JPEG压缩。部分压缩后的数字图像为了获得更好的视觉效果会进行后处理操作，任何的图像编辑都可以应用在后处理操作，经常使用的编辑为：几何变换（旋转、缩放等）、模糊、锐化、对比度调整、图像拼接、复制-粘贴。经过编辑后的数字图像重新保存为JPEG格式形成最终的数字图像。&lt;/p&gt;&lt;p&gt;数字图像取证的出发点是通过提取数字图像周期中留下的固有痕迹进行分析和理解数字图像的操作历史。以上介绍的数字图像完整周期的三个部分每一部分都会留下不同的操作痕迹（指纹特性），即获取指纹、编码指纹、编辑指纹。在图像获取指纹研究中，根据镜头特性、传感器特性、CFA模式等引入数字图像中的不同指纹特性对数字图像进行分析。在图像编码指纹研究中，JPEG压缩以及多重JPEG压缩检测是主要关注的问题。在图像编辑指纹特性研究中，基于信号处理和基于物理/几何的技术被提出。利用信号处理技术进行复制粘贴检测、重采样检测、对比度增强检测、线裁剪检测等，利用光线/阴影进行拼接检测以及利用几何关系的一致性检测拼接处理都是取证研究中的热点问题。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-566f80fdb12dae7719721d023c3fab4d.png" data-rawwidth="645" data-rawheight="387"&gt;&lt;p&gt;&lt;b&gt;图像取证深度学习之风&lt;/b&gt;&lt;/p&gt;&lt;p&gt;不同与传统的图像取证算法，深度学习算法将特征提取和特征分类整合到一个网络结构中，实现了一种end-to-end的自动特征学习分类的有效算法。从当前的研究工作来看，深度学习应用于图像取证领域大致可分为三个层次。第一个层次是简单的迁移，即直接将CV领域常用的CNN网络结构引入到图像取证领域。取证领域比较常用的网络结构为AlexNet，选择此网络结构的原因是因为AlexNet网络结构相较于其他网络结构复杂度相对较低并且性能较好，对于解决数据集少的取证问题有更好的尝试性条件。典型案例为Luca Baroffio, Luca Bond等[3]发表的文章“Camera Identification With Deep Convolutional Networks”, 文章提出用深度学习解决取证中的相机源辨别问题。第二个层次是尝试对网络输入的修改，进行此种尝试的初衷是由取证问题和CV问题的本质区别所驱使。取证问题虽可归类于识别、分类、定位问题，但是对于分类问题的类间差别取证分类问题远小于CV分类问题。举个例子：ImageNet中的22000种类别之间的形态差异是较大的，比如猫和狗两个类别之间的差异人眼可辨别；然而对于取证问题，类别之间的形态差异是极其微小的，类间差别以微弱信号的形式存在；比如对于常见的双重JPEG压缩取证，需要解决的问题是区分一副图像是经历过一次JPEG压缩之后的图像，还是经历过两次JPEG压缩之后的图像。在两次压缩使用的压缩因子（压缩因子小于等于90）一致的前提条件下，内容相同的两幅图像的DCT域统计类间差别小于0.4%（数据来源于Detecting Double JPEG Compression With the Same Quantization Matrix[4]）。基于取证问题的此种特性，研究者尝试对网络的输入进行改进，添加预处理层（或信号增强层）放大类间差别，此种尝试取得较好的检测效果。典型案例为Jiansheng Chen, Xiangui Kang等[5]发表的文章“Median Filtering Forensics Based on Convolutional Neural Networks”，根据论文报告的试验结果，预处理层的添加对检测准确率有了7.22个百分点的提升。第三个层次是对网络结构的修改，结合取证的实际问题提出适合于取证问题的网络结构。典型案例为Belhassen Bayar, Matthew C. Stamm[6]发表的文章“A Deep Learning Approach To Universal Image Manipulation Detection Using A New Convolutional Layer”。&lt;/p&gt;&lt;p&gt;下面针对于不同的取证问题介绍深度学习的应用。据我们所知，到目前为止，深度学习应用于取证领域的工作共有5篇，涉及到了取证问题中的相机源取证、中值滤波取证、重获取图像取证以及反反取证。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.1 相机源取证&lt;/b&gt;&lt;/p&gt;&lt;p&gt;相机源取证研究的问题在于如何有效区分图像采集所使用的设备型号或模式，相机源取证可以在一定程度上解决图像版权问题，例如一副具有版权保护的图像未经过作者授权被重新拍摄并发布，可以利用相机源取证技术区分图像是由原始相机拍摄还是其他相机拍摄，从而判断图像版权所属。&lt;/p&gt;&lt;p&gt;不同厂商生产的数码相机之间存在着差异，相同厂商生产的不同型号的数码相机之间也存在着差异，已有的传统方案通过提取不同的相机存在的指纹特性实现对相机源的取证。Luca Baroffio, Luca Bond等人首次提出利用卷积神经网络的方法解决相机源取证问题。文章所使用的网络结构参数如图2所示。该结构使用了三个卷积层和两个全连接层的结构，网络结构与AlexNet结构相似。根据文章报告的实验结果，在相机源取证的benchmark库中测试，对于27种相机模式分类的准确率在94%以上。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-c0bdff0fa450bbb807621507ddea7b50.png" data-rawwidth="361" data-rawheight="431"&gt;&lt;p&gt;&lt;b&gt;2.2 中值滤波图像取证&lt;/b&gt;&lt;/p&gt;&lt;p&gt;中值滤波图像取证问题一直被图像取证领域所关注，取证目的是对图像是否经历过中值滤波操作进行判定。在图像经过篡改之后，为了去除篡改引入图像中的特性，通常会对图像进行中值滤波操作，从而隐藏篡改操作痕迹。图像是否经历过中值滤波操作对于判断图像篡改历史提供了重要线索。传统的图像中值滤波取证算法对于小尺寸图像和做过压缩后处理的图像性能有待提高。&lt;/p&gt;&lt;p&gt;Jiansheng Chen, Xiangui Kang等人首次提出利用深度学习解决中值滤波取证问题，该工作发表在Signal Processing Letters IEEE, 2015。与此同时，这也是深度学习应用在取证领域的第一个工作，为后续深度学习在取证领域的发展起到了重要的借鉴作用。该工作对网络的输入图像做了预处理操作&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-04ab673a9c5aae523c6c2f1f847e4f77.png" data-rawwidth="447" data-rawheight="75"&gt;&lt;em&gt;x&lt;/em&gt;(&lt;em&gt;i &lt;/em&gt;, &lt;em&gt;j&lt;/em&gt;)表示原始图像， medw(.)表示中值滤波操作，其中中值滤波窗口大小为&lt;em&gt;w&lt;/em&gt;,&lt;em&gt;d&lt;/em&gt;(&lt;em&gt;i &lt;/em&gt;,&lt;em&gt; j&lt;/em&gt;)， 代表中值滤波图像与原始图像的差值图像。做这样预处理的动机来源于之前传统方案的设计[7]。通过预处理操作，去除图像内容对检测性能的影响同时也起到了放大图像噪声信号的作用。预处理操作的效果图如图3所示，(a)(b)(c)三幅图像分布代表原始图像，差值图像以及中值滤波与原始图像的差值图像。从图3(c)中可以看出中值滤波后的差值图像中对原始图像内容的反映几乎去除。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-bce7d87ef51655bd68234e60c0bb352d.png" data-rawwidth="649" data-rawheight="322"&gt;&lt;/p&gt;&lt;p&gt;图4中展示了该工作提出的网络结构示意图。滤波层实现的是式(1)的操作，紧接着的网络结构与AlexNet结构类似，5个卷积层以及3个全连接层。根据文章中报告的实验结果，对于压缩的小尺寸图像（64x64、32x32）该方法实现了最好的检测准确率。为了测试滤波层的作用，作者在使用滤波层和不使用两种情况下进行了对比实验，实验结果显示滤波层对于检测准确率有7.22个百分点的提升。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-d083d32a63863eb10f2545efdd6770af.png" data-rawwidth="654" data-rawheight="186"&gt;基于Jiansheng Chen, Xiangui Kang等人的工作，Belhassen Bayar，Matthew C. Stamm[6]提出一种新的卷积结构。作者尝试利用新的卷积结构捕获图像操作过程中引入的图像临近像素之间相关关系的变化，于此同时尽可能压缩图像内容对于图像操作引入的像素相关关系的影响。为了实现这样的卷积结构设计，作者对卷积核的属性进行了限制，使得网络结构可以自动学习预测误差滤波器集合，从而抑制图像内容的影响同时捕获操作特性。限制条件如公式（2）所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-714ff005796be52c0a948e5ce6d9a124.png" data-rawwidth="373" data-rawheight="70"&gt;&lt;em&gt;w&lt;/em&gt;为新的卷积核，&lt;em&gt;w&lt;/em&gt;(0,0)为卷积核中心位置的数值。新的卷积结构只使用在第一层卷积中，从而实现图像预处理卷积核的自动学习。图5、图6分别展示了新的卷积结构和文章提出的网络结构图。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a52096ae8649630e0400df17074fe007.png" data-rawwidth="591" data-rawheight="404"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-582c6796d529cc413273e6c8613b8e6a.png" data-rawwidth="646" data-rawheight="264"&gt;&lt;p&gt;&lt;b&gt;2.3 重获取图像取证&lt;/b&gt;&lt;/p&gt;&lt;p&gt;重获取图像取证近几年开始受到取证工作者的关注。重获取操作是指原始图像被投影到新的媒介后被再次获取的过程。图7展示了常见的图像重获取操作流程。原始图像首先被投影到新得到媒介：电脑屏幕、手机屏幕、打印纸，然后对投影后的图像进行重新拍摄，形成重获取图像。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-76cb1b176e83e7755890c5bda11e5c77.png" data-rawwidth="648" data-rawheight="270"&gt;&lt;p&gt;重获取图像取证有重要的研究价值，对于篡改操作后的图像通常会在图像中留下指纹性的操作痕迹，消除这些痕迹的最简单的方式就是对篡改后的图像进行重新获取。因其操作的简易性被很多篡改者使用。重获取图像取证通过辨别图像是否经过重获取操作对图像操作历史的鉴别具有重要意义。另一方面，随着人脸识别身份系统的快速发展和广泛使用，一些不法分子试图通过一些手段欺骗身份识别系统，活体检测技术的使用为身份识别系统提供了一层保障，然后face2face系统的推出又为活体检测提出了挑战。重获取图像取证作为一种新的技术手段可以有效地增强身份识别系统的鲁棒性。&lt;/p&gt;&lt;p&gt;基于深度学习的方法，我们[8]提出了一种拉普拉斯卷积神经网络算法检测重获取图像。网络结构如图8所示，对于输入图像我们首先利用信号增强层放大重获取噪声信号，然后利用5个卷积层进行特征提取，最后使用全连接层作为特征分类层。我们提出的算法在不同尺寸的图像库上实现了95%以上的检测性能。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-a6d2f174115f956756296d6bda538025.png" data-rawwidth="648" data-rawheight="410"&gt;&lt;p&gt;&lt;b&gt;2.4 反反取证&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Jingjing Yu, Xiangui Kang等[9]将卷积神经网络应用在多类反反取证问题上。随着取证技术的发展，对抗取证的研究也随之兴起，称为反取证研究。反取证是针对于特定的取证技术提出使其失效的算法。该工作是针对于多种反取证算法进行取证，故而称为反反取证。文章提出的网络结构如下所示，四层卷积结构以及两层全连接结构。文章关注了四类反取证问题：JPEG压缩、中值滤波、重采样、对比度增强，根据文章报告的实验结果，平均检测准确率达到了96.9%。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-fe1da403857178c9fd967cb99f9cfae8.png" data-rawwidth="521" data-rawheight="203"&gt;&lt;p&gt;&lt;b&gt;2.5 隐写分析&lt;/b&gt;&lt;/p&gt;&lt;p&gt;把隐写分析列在这里，是因为隐写分析与取证领域存在极深的渊源。两者虽需要解决的具体问题不同，但是方法论本质上具有一致性，都是探寻微弱“噪声”信号的存在。隐写分析领域和图像取证领域的发展是相辅相成的，两者针对各自领域提出的有效算法通常在彼领域也能得到很好的应用。比如隐写分析领域常用的Rich Models、SPM算法都被应用于取证领域并取得了state-of-the-art的检测性能。&lt;/p&gt;&lt;p&gt;隐写分析是针对隐写问题发展而来的一种技术手段，目的是检测目标中是否包含隐藏信息。待检测目标中嵌入隐藏信息的比特率越低，意味着隐藏信息量越少，检测难度越大。传统的隐写分析都是基于特征提取加特征分类的两段论方案，为了更全面的刻画待测目标中的“微弱”信号，维度不断增加的高维特征被提出，例如Rich Models特征。特征分类方面为了加速高维隐写特征的分类，Fridrich课题组提出了针对隐写分析的特定分类器，集成分类器[10]。深度学习的发展为隐写分析提供了一种新的思路。Qian Y, Dong J等[11]首次将深度学习算法应用于隐写分析领域，并基于隐写分析的领域知识提出高斯激活函数，取得了和传统方案性能相当的检测效果；Guanshuo Xu, Yun-Qing Shi等[12]设计了一种新的网络结构，在网络结构中添加了绝对值层、BN层和全局pooling层，也取得了较好的检测效果。基于以上工作，两者又相继推出了后续工作。Qian Y, Dong J等[13,14]融合迁移学习的方法进一步提高了算法性能；Guanshuo Xu, Yun-Qing Shi等[15]提出了基于集成学习和集成分类的方案。&lt;/p&gt;&lt;p&gt;&lt;b&gt;图像取证深度学习之风何去何从&lt;/b&gt;&lt;/p&gt;&lt;p&gt;如今深度学习的如火如荼让各行各业的同胞摩拳擦掌。就取证领域而言，深度学习的探索之旅还处于小荷才露尖尖角的状态。如施云庆教授在IWDW2016中的谈话所言：“深度学习在取证领域中的进步相较于计算机视觉领域是很小的，如何进一步提升深度学习在取证中的检测性能仍然值得关注”。另外，取证领域的数据集规模相对于计算机视觉领域较小，对于数据驱动型的深度学习算法，更大规模的公开的全面的精确标注的数据集对于图像取证问题无疑是迫切需要的。&lt;/p&gt;&lt;p&gt;这段时间本文作者经过一些探索也取得了一些心得，在此和大家一起探讨。首先就网络的深度而言，浅层的网络结构已然可以得到较好的实验结果。当然网络的加深会对实验结果略有提升，但是并不能和增加层数带来的计算复杂度的提升成比例。其次，预处理操作并不是对于所有取证问题都适用，预处理操作在放大噪声信号的同时也相应的丢失了部分原始信息，对于深度学习数据驱动型算法而言，这些丢失的原始信息对于算法性能的影响比重如何暂时还无定论，所以预处理操作添加与否还需具体情况具体分析。&lt;/p&gt;&lt;p&gt;目前基于深度学习的图像取证研究还有许多问题需要去解决，更多的路需要去探索，本文作者欢迎读者的任何意见或者建议，并期待和大家一起探讨。&lt;/p&gt;&lt;p&gt;&lt;b&gt;参考文献&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[1] &lt;/b&gt;曹刚. 数字图像操作取证技术研究[D]. 北京交通大学, 2013.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[2]&lt;/b&gt; Piva A. An Overview on Image Forensics[J]. Isrn Signal Processing, 2013.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[3] &lt;/b&gt;Baroffio L, Bondi L, Bestagini P, et al. Camera identification with deep convolutional networks[J]. 2016.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[4]&lt;/b&gt; Huang F, Huang J, Shi Y Q. Detecting Double JPEG Compression With the Same Quantization Matrix[J]. IEEE Transactions on Information Forensics &amp;amp; Security, 2010.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[5] &lt;/b&gt;Chen J, Kang X, Liu Y, et al. Median Filtering Forensics Based on Convolutional Neural Networks[J]. Signal Processing Letters IEEE, 2015&lt;/p&gt;&lt;p&gt;&lt;b&gt;[6]&lt;/b&gt; Bayar B, Stamm M C. A Deep Learning Approach to Universal Image Manipulation Detection Using a New Convolutional Layer[C]// ACM Workshop on Information Hiding and Multimedia Security. ACM, 2016.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[7] &lt;/b&gt;Kang X, Stamm M C, Peng A, et al. Robust Median Filtering Forensics Using an Autoregressive Model[J]. IEEE Transactions on Information Forensics &amp;amp; Security, 2013.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[8] &lt;/b&gt;Peng P Y, Rong R N, Yao Z. Recapture Image Forensics Based On Laplacian Convolutional Neural Networks[C]// International Workshop on Digital-forensics and Watermaking, 2016&lt;/p&gt;&lt;p&gt;&lt;b&gt;[9]&lt;/b&gt; Jing J Y, Yi F Z, Jian H Y, et al. A Multi-purpose Image Counter-anti-forensic Method Using Convolutional Neural Networks[C] // International Workshop on Digital-forensics and Watermaking, 2016&lt;/p&gt;&lt;p&gt;&lt;b&gt;[10]&lt;/b&gt; Kodovsky J, Fridrich J, Holub V. Ensemble Classifiers for Steganalysis of Digital Media[J]. IEEE Transactions on Information Forensics &amp;amp; Security, 2012&lt;/p&gt;&lt;p&gt;&lt;b&gt;[11]&lt;/b&gt; Qian Y, Dong J, Wang W, et al. Deep learning for steganalysis via convolutional neural networks[C]//SPIE/IS&amp;amp;T Electronic Imaging. International Society for Optics and Photonics, 2015&lt;/p&gt;&lt;p&gt;&lt;b&gt;[12] &lt;/b&gt;Xu G, Wu H Z, Shi Y Q. Structural Design of Convolutional Neural Networks for Steganalysis[J]. IEEE Signal Processing Letters, 2016&lt;/p&gt;&lt;p&gt;&lt;b&gt;[13] &lt;/b&gt;Qian Y, Dong J, Wang W, et al. Learning Representations for Steganalysis from Regularized CNN Model with Auxiliary Tasks[C]//Proceedings of the 2015 International Conference on Communications, Signal Processing, and Systems. Springer Berlin Heidelberg, 2016&lt;/p&gt;&lt;p&gt;&lt;b&gt;[14]&lt;/b&gt; Qian Y, Dong J, Wang W, et al. Learning and transferring representations for image steganalysis using convolutional neural network[C]//Image Processing (ICIP), 2016 IEEE International Conference on. IEEE, 2016&lt;/p&gt;&lt;p&gt;&lt;b&gt;[15]&lt;/b&gt; Xu G, Wu H Z, Shi Y Q. Ensemble of CNNs for Steganalysis: An Empirical Study[C]//Proceedings of the 4th ACM Workshop on Information Hiding and Multimedia Security. ACM, 2016&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" class="" data-editable="true" data-title="@果果是枚开心果."&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-cce081b46b15055de2aa23c3163b44f8.png" data-rawwidth="115" data-rawheight="121"&gt;杨朋朋，&lt;/b&gt;就读于北京交通大学，信号与信息处理专业博士生二年级，导师倪蓉蓉教授。研究兴趣包括多媒体取证、隐写分析，深度学习。所在团队为教育部创新团队和科技部重点领域创新团队，负责人为赵耀教授。个人邮箱：ppyangforensics@gmail.com 个人主页：&lt;a href="http://www.ppyforensics.com/" data-editable="true" data-title="ppy – homepage"&gt;ppy – homepage&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325617&amp;amp;idx=1&amp;amp;sn=c941e03add12204f43f40a801e554413&amp;amp;chksm=f235a57bc5422c6d10602e1ebf87d231333b1ae869740c90e9092ab654dd92188b4890fffaff&amp;amp;scene=0#wechat_redirect" data-editable="true" data-title="深度学习在图像取证中的进展与趋势"&gt;深度学习在图像取证中的进展与趋势&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23341157&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Tue, 01 Nov 2016 11:15:31 GMT</pubDate></item><item><title>【Technical Review】ECCV16 Center Loss及其在人脸识别中的应用</title><link>https://zhuanlan.zhihu.com/p/23340343</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-a63a4a309c2b7d8915212e7fa885b760_r.png"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;&lt;b&gt;摘要&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在大家吐槽用softmax训练出来的人脸模型性能差，contrastive 和 triplet需要个中谜一样的采样方法之际。ECCV 2016 有篇文章提出了权衡的解决方案。通过添加center loss使得简单的softmax就能够训练出拥有内聚性的特征。該特点在人脸识别上尤为重要，从而使得在很少的数据情况下训练出来的模型也能有不俗的性能。本文尝试用一种比较容易理解的方式来解释这篇文章。&lt;/p&gt;&lt;p&gt;我觉得在开始正式介绍center loss之前，我有必要讲一个故事。请大家回答我，做人脸研究，最期待的事情是什么？来100块Titan……！咦，还不如换成银子呢。笔者认为，对于做人脸的人而言，最幸福的事情就是拿到全世界所有人的若干张人脸数据(画外音：地球人都知道!)。 各位看官先勿开喷，请听故事….假设我拿到了世界所有人的人脸数据，也训练好了一个所谓的映射空间(其实我更喜欢叫嵌入空间)…..此时我们觉得这个空间的画风应该是这样的(密集恐惧症慎入)….&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-09a5af94f9a1adf7654f6ce51ffc333b.png" data-rawwidth="610" data-rawheight="391"&gt;&lt;p&gt;咦？这不是海洋球么→_→，我书读得少你不要骗我？各位看官好眼力，这真的就是海洋球( T___T )。如果我们把一个球就看成一个人的话，球的中心就是类中心，直径就是他若干张照片带来的variance。理想情况我们当然认为每个人所代表的球应该差不多大，人与人之间也是能分开的，就跟海洋球一样。然后海洋球所在的池子呢，就是能容纳所有球的界。&lt;/p&gt;&lt;p&gt;梦做完了，醒醒，我翻了翻硬盘，也就有10W人的数据。现实总是那么骨感…，接下来发生的事情胖子可能会不太爱听,因为我们只有10W人的数据，但是池子还是很大，如果我们用普通的方法训练的话，海洋球就会填不满这个池子。最简单的解决方案就是，我们从50亿个海洋球，变成10W个足球(估计还是不够，没准得用月球( ¯ □ ¯ ))，对，你没听错，通过变成吃货长胖这种方式我们把损失49亿多海洋球弟兄的空间填满了。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-187c3319a609ab98627a8dfebf571366.png" data-rawwidth="621" data-rawheight="495"&gt;看官可能会问，填满了不就填满了么？有什么问题么？请大家注意，虽然那49亿多个兄弟不在你的训练集里面，但是有可能在你的测试集哟。所以当以前的两个离得比较近的海洋球兄弟进入到这个胖子的世界的时候(‘‘)(’’)， 这个新世界会要求新来的弟兄跟他们的国民一样胖，然后悲剧发生了。他们很有可能撞在了一起,以至于他们老妈都很有可能都分不清谁是谁(因为在小数据情况下的可分行并不能推广到全集)。如图三，如果用所有数据训练是可分的话，只用部分数据训练因为变胖问题，会使得两个不同的identity出现碰撞风险。从而会造成错误接受和错误拒绝。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-e713c59543cf38a3ece21dde30419add.png" data-rawwidth="617" data-rawheight="300"&gt;那么请问，什么样的方式才合理呢？鄙人认为，我心目中比较合理的海洋球摆放画风应该是这样的（7颗海洋球用来召唤神龙 (╯-_-)╯）：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-191ea6d0f8c0c02829610214d306308d.png" data-rawwidth="378" data-rawheight="229"&gt;&lt;p&gt;該画风的满足的条件就一个，那10W海洋球兄弟不允许吃胖。这样剩下来的空间就能够给他们每人一个大院子，这样一旦有新的朋友(49亿多个兄弟)来玩耍的时候，不至于撞得他们老妈都分不清楚是谁(至少从概率上极大的降低了)。&lt;/p&gt;&lt;p&gt;不好意思，主持人，故事讲得太爽，忘记我们是来讲一篇paper的了 (╯-_-)╯╧╧  。今天我们要讲的文章名字叫做A Discriminative Feature Learning Approach for Deep Face Recognition. 在放公式驱散一票读者之前，我觉得还是放点图来缓和一下作者和读者之间的紧张关系。论文中提出我们之前训练出来的features只是可分而已(下图左)，如果要得到一个判别性比较高的features应该是右边这种样子的。用我们的胖子理论，不好意思应该叫海洋球理论，大家就可以知道，并且理解为什么右图的特征更合理。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-61a0ceabc735c788bbe70785bcd36e01.png" data-rawwidth="592" data-rawheight="319"&gt;我们用论文中的另一个图来表达，海洋球理论的论据。我们在图6的左上图上画了三条线来模拟feature之间的角度(因为人脸verification基本上就是cosine距离)，我们可以看到，如果每一个类别吃的太胖，可能会导致他的类内距离可能比类间距离大，从而……你懂的(假分很高)。如果我们引入论文中的center loss并且给一个比较高的weight的时候，比如右下图，我基本找不到反例了。同时新的未知类别进来的时候也有更大的概率会和其他的类别分开(包含未知类和已知类)。在这center loss起到的作用就是减肥剂，怎么让一个比较胖的类内数据分布，变瘦，从而使得他有足够的院子容纳新的客人。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a63a4a309c2b7d8915212e7fa885b760.png" data-rawwidth="660" data-rawheight="480"&gt;以上文字给初学者品尝，接下来的文字将会上公式，请有耐心的初学者和大牛们接着往下看。整个softmax+center loss的定义如下&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-78038563ca566749fc6d0d84852256d3.png" data-rawwidth="619" data-rawheight="152"&gt;&lt;p&gt;我基本不太想解释一遍…….左边是softmax，右边嘛，就是约束当一个样本看到自己的类中心的时候，走上去打个招呼。这样才能瘦。主持人快来…..我编不下去了…..&lt;/p&gt;&lt;p&gt;可能大家会问了，为什么需要右边的约束才能达到这个效果呢？softmax为什么达不到这个效果？或者说softmax有啥缺点？&lt;/p&gt;&lt;p&gt;昂，简单来说就是伟大的softmax管杀不管埋。不对，应该说是只许自己胖，不准别人胖。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-34baa2b41cb5c124b8869739b5030090.png" data-rawwidth="506" data-rawheight="71"&gt;&lt;p&gt;各位看官请看softmax的更新函数，如果出现input的条件概率分数比较低的情况，他就会告诉所有的参数，\theta_j。你们碍着我长胖了，离我远点，然后乘机偷偷的向着input方向长胖一点(不改其吃货本质)。所以每个类都这么流氓的结果，就是大家都变成胖子，每个胖子的鼻子都贴着另一个胖子。&lt;/p&gt;&lt;p&gt;写到这本来想停笔了，但是wanda同学提了个问题。这玩意和contrastive loss和triplet loss之间有什么区别或者是联系没有呀？&lt;/p&gt;&lt;p&gt;上面的公式可以看成softmax + contrastive loss只不过只提供一个类内样本作为pair，而这个类内样本就是整个类的类中心。这在一定程度上解决了contrastive loss坑爹的采样问题，大家都知道contrastive loss需要坑爹的采样很多pair对，一不小心还会搞得positive pair和negative pair严重imbalance。&lt;/p&gt;&lt;p&gt;尽管triplet一定意义上解决了这个imbalance的问题，带来的后果是更复杂的采样策略。导致实际在使用过程中的trick实在太多，什么semi-hard example mining等等。Center loss相当于拆解了contrastive loss的两种情况，positive pair和negative pair。Center loss相当于把每一个进来的样本都自动配了一个positive sample(也就是他们类中心)， negative pair的inter-class separation则是是通过softmax来完成的，就是那么的简单粗暴有效。Wanda童鞋已经重复出很positive的结果并提供了mxnet code &lt;a href="https://github.com/pangyupo/mxnet_center_loss" data-editable="true" data-title="GitHub - pangyupo/mxnet_center_loss: implement center loss operator for mxnet" class=""&gt;GitHub - pangyupo/mxnet_center_loss: implement center loss operator for mxnet&lt;/a&gt;，欢迎试用。&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" class="" data-editable="true" data-title="@果果是枚开心果."&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-7d65c67344405c90976ad204ce1525bf.png" data-rawwidth="119" data-rawheight="123"&gt;&lt;p&gt;&lt;b&gt;祝浩(皮搋子狐狸)，&lt;/b&gt;3M Cogent Beijing R&amp;amp;D 高级算法工程师。本硕分别毕业于哈尔滨工业大学机械专业和北京师范大学计算机专业，并于2012年加入3M。14年拿到NICTA Scholarship 及其 top-up Scholarship，然后老板跑路。熟悉计算机视觉，神经影像和医学图像处理。在各种相关国际会议期刊上发表论文10余篇。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325602&amp;amp;idx=1&amp;amp;sn=9bc6071bf62a4ccdde1df4b4c5ae39f4&amp;amp;chksm=f235a568c5422c7ed7993b9d3b46bcd979d360605511284e4363a4a92f845019795569f8f3e9&amp;amp;scene=4#wechat_redirect" data-editable="true" data-title="【Technical Review】ECCV16 Center Loss及其在人脸识别中的应用" class=""&gt;【Technical Review】ECCV16 Center Loss及其在人脸识别中的应用&lt;/a&gt;\&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23340343&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Tue, 01 Nov 2016 10:25:47 GMT</pubDate></item><item><title>【高手之道】海康威视研究院ImageNet2016竞赛经验分享</title><link>https://zhuanlan.zhihu.com/p/23249000</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-4d69362ea1ffae8644165644861e0a7d_r.jpg"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;b&gt;摘要&lt;/b&gt;&lt;p&gt;海康威视研究院独家授权，分享ImageNet2016竞赛Scene Classification第一名，Object Detection第二名，Object Localization第二名，Scene Parsing第七名背后的技术修炼之道。下面为大家介绍海康威视研究院在本次ImageNet2016竞赛中的相关情况。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-4d69362ea1ffae8644165644861e0a7d.jpg"&gt;团队成员和分工如下：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-7f654278ede90b2148c14eae50088d41.jpg"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-e87993f06ae27890a1b57de718cda0bc.jpg"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-c0aa98650925e3ae481f6f6ac2b80863.jpg"&gt;数据增强对最后的识别性能和泛化能力都有着非常重要的作用。我们使用下面这些数据增强方法。第一，对颜色的数据增强，包括色彩的饱和度、亮度和对比度等方面，主要从Facebook的代码里改过来的。第二，PCA Jittering，最早是由Alex在他2012年赢得ImageNet竞赛的那篇NIPS中提出来的. 我们首先按照RGB三个颜色通道计算了均值和标准差，对网络的输入数据进行规范化，随后我们在整个训练集上计算了协方差矩阵，进行特征分解，得到特征向量和特征值，用来做PCA Jittering。第三，在图像进行裁剪和缩放的时候，我们采用了随机的图像差值方式。第四， Crop Sampling，就是怎么从原始图像中进行缩放裁剪获得网络的输入。比较常用的有2种方法：一是使用Scale Jittering，VGG和ResNet模型的训练都用了这种方法。二是尺度和长宽比增强变换，最早是Google提出来训练他们的Inception网络的。我们对其进行了改进，提出Supervised Data Augmentation方法。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-0e270114ad733a4ea09d978803ddc0a5.jpg"&gt;尺度和长宽比增强变换有个缺点，随机去选Crop Center的时候，选到的区域有时候并不包括真实目标的区域。这意味着，有时候使用了错误的标签去训练模型。如图所示，左下角的图真值标签是风车农场，但实际上裁剪的区域是蓝天白云，其中并没有任何风车和农场的信息。我们在Bolei今年CVPR文章的启发下，提出了有监督的数据增强方法。我们首先按照通常方法训练一个模型，然后用这个模型去生成真值标签的Class Activation Map（或者说Heat Map）, 这个Map指示了目标物体出现在不同位置的概率. 我们依据这个概率，在Map上随机选择一个位置，然后映射回原图，在原图那个位置附近去做Crop。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-cd9bbcf8f62066004ba20b2881c5d598.jpg"&gt;如图所示，对比原始的尺度和长宽比增强变换，我们方法的优点在于，我们根据目标物体出现在不同位置的概率信息，去选择不同的Crop区域，送进模型训练。通过引入这种有监督的信息，我们可以利用正确的信息来更好地训练模型，以提升识别准确率。 (+0.5~0.7)&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-4c54814d715fe5d86a4ca3ec90e9add4.jpg"&gt;场景数据集有800万样本，365个类别，各个类别的样本数非常不平衡，有很多类别的样本数达到了4万，也有很多类别的样本数还不到5000。这么大量的样本和非常不均匀的类别分布，给模型训练带来了难题。在去年冠军团队的Class-Aware Sampling方法的启发下，我们提出了Label Shuffling的类别平衡策略。在Class-Aware Sampling方法中，他们定义了2种列表，一是类别列表，一是每个类别的图像列表，对于365类的分类问题来说，就需要事先定义366个列表，很不方便。我们对此进行了改进，只需要原始的图像列表就可以完成同样的均匀采样任务。以图中的例子来说，步骤如下：首先对原始的图像列表，按照标签顺序进行排序；然后计算每个类别的样本数量，并得到样本最多的那个类别的样本数。根据这个最多的样本数，对每类随机都产生一个随机排列的列表；然后用每个类别的列表中的数对各自类别的样本数求余，得到一个索引值，从该类的图像中提取图像，生成该类的图像随机列表；然后把所有类别的随机列表连在一起，做个Random Shuffling，得到最后的图像列表，用这个列表进行训练。每个列表，到达最后一张图像的时候，然后再重新做一遍这些步骤，得到一个新的列表，接着训练。Label Shuffling方法的优点在于，只需要原始图像列表，所有操作都是在内存中在线完成，非常易于实现。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-b9fb06d90cc5074e4926a91fbbf7e80e.jpg"&gt;我们使用的另外一个方法是Label Smoothing，是今年Google的CVPR论文中提出来的方法。根据我们的混淆矩阵（Confusion Matrix）的分析，发现存在很多跨标签的相似性问题，这可能是由于标签模糊性带来的。所以，我们对混淆矩阵进行排序，得到跟每个标签最相近的4个标签，用它们来定义标签的先验分布，将传统的 one-hot标签，变成一个平滑过的soft标签。通过这种改进，我们发现可以从某种程度上降低过拟合问题。(+0.2~0.3)&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-c88ff71ef491bff70aa1f0dbd5fcd1d0.jpg"&gt;这边还有一些其他的技巧来提升性能，我们将其总结成一个原则：训练和测试要协调。在训练的时候，我们通常都需要做数据增强，在测试的时候，我们通常很少去做数据增强。这其中似乎有些不协调，因为你训练和测试之间有些不一致。通过我们的实验发现，如果你在训练的最后几个epoch，移除数据增强，然后跟传统一样测试，可以提升一点性能。同样，如果训练的时候一直使用尺度和长宽比增强数据增强，在测试的时候也同样做这个变化，随机取32个crop来测试，也可以在最后的模型上提升一点性能。还有一条，就是多尺度的训练，多尺度的测试。另外，值得指出的是，使用训练过程的中间结果，加入做测试，可以一定程度上降低过拟合。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-2ea607bca495fb43e5b476d482f9eca7.jpg"&gt;对于模型结构，没什么特别的改进，我们主要使用了Inception v3和Inception ResNet v2，以及他们加深加宽的版本。还用到了Wide ResNet 。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-38097e389268d90cfdf3ebb0e9e27b76.jpg"&gt;此次竞赛的语义分割任务非常具有挑战性。它一方面需要目标整体层面的信息，同时还需要每个像素的分类准确率。目前有很多语义分割的模型，但哪一种框架是最好的仍然是一个问题。我们设计了一个Mixed Context Network（MCN），它由一系列Mixed Context Blocks（MCB）堆叠而成。如图所示，每个MCB包括两个并行的卷积层：1个1x1卷积和1个3x3 Dilated卷积。Dilated卷积的采样率分别设置成了1,2,4,8,16。除了MCN之外，我们还在最后设计了一个Message Passing Network（MPN），来增强不同标签之间的空间一致性。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-d6f17746ab58e2aa71150fc1221894ee.jpg"&gt;CRF as RNN可以被加到网络的最后，与CNN一起联合训练。但是CRF as RNN比较耗费显存，尤其是在类别数比较大的时候（比如ADE20K有150类）。可以通过降低输入图像的分辨率来节省显存，但是这样做也会带来一些负面影响。为了解决这个问题，我们引入了一个新的比较省显存的模块，叫做MPN。在MPN中，我们首先将Score Map的通道从150降到了32，然后接了一个Permutohedral卷积层，用于做高维的高斯滤波。我们去掉了其中的平滑项，仅仅将1x1卷积层的特征和Permutohedral卷积层的特征连接，然后接一个3x3的卷积。实验证明，这样的结构也能较好地工作。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-93d428d5b2551202c7a487f04bcfadda.jpg"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-95865b813291bec665c1c8130cdf13e9.jpg"&gt;我们的检测和定位，都是基于Faster-RCNN这个框架。图中我们列出了所有用到的技巧。有很多技巧在以前的文献中都可以找到，比如多尺度的训练和测试，难样本挖掘，水平翻转和Box Voting。但我们自己也做了很多新的改进，比如样本均衡，Cascade RPN，预训练的Global Context等。至于网络结构，我们仅仅用了三个ResNet-101模型。一个来自于MSRA，一个来自于Facebook，还有一个是我们自己训练的Identity Mapping版的ResNet-101。我们最好的单模型结果，是源自我们自己训练的Identity Mapping版的ResNet-101。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-be5c4d4759d55c5271acadc3ad2d588b.jpg"&gt;我们设计了一个轻量级的Cascade RPN。2个RPN顺序堆叠在一起。 RPN 1使用滑窗Anchors，然后输出比较精确定位的Proposals。RPN 2使用RPN 1的Proposals作为Anchors。我们发现这个结构可以提升大中尺寸Proposals的定位精度，但不适合小的Proposals。所以在实际中，我们RPN1提取小的Proposals，RPN2提取大中尺寸的Proposals。&lt;b&gt;注:&lt;/b&gt; Proposals尺寸的阈值是64 * 64。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a5be0c98a1e6829652fc15c81a960e9f.jpg"&gt;另外一个改进就是限制正负Anchor的比例。在传统的RPN中，Batch Size通常是256，理想的正负Anchor比例是1。但是在实际使用中，这个比例往往会很大，通常会大于10。所以我们缩小了Batch Size，控制最大的比例为1.5。最小的Batch Size设置为32。对比实验表明，使用Cascade RPN和限制正负Anchor比例这两个策略，在ImageNet DET的验证集上，AR提升了5.4个点，Recall@0.7提升了9.5个点，而Recall@0.5只提升1个点。这说明Proposals的定位精度得到了显著的改善。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-4932aca2e45c04c8e55ef6de41059980.jpg"&gt;Global Context在去年Kaiming的论文中就已经提到，他们使用这个方法得到了1个点的mAP提升。我们也实现了自己的Global Context方法：除了在在RoI上做RoIPooling 之外，我们还对全图做了RoIPooling来获得全局特征。这个全局特征仅仅被用来分类，不参加bbox回归。我们实验发现，Global Context如果使用随机初始化，其性能提升有限。当我们采用预训练的参数进行精调之后，发现mAP的性能可以提升3.8个点。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-6ca5a27eebe9f4d573a3146a3e9b0a52.jpg"&gt;此外，我们发现，在1000类的LOC上预训练，然后再在DET数据上精调，可以 得到额外0.5个点的mAP提升。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-02d93f5ba2e2fd93a492a58591bee613.jpg"&gt;平衡采样是去年场景分类任务中所用到的一个技巧。我们也将它用来做检测任务。左侧是一个类别的列表，对于每个类别，我们又创建了一个图像列表。训练过程中，我们先从类别列表中选择一个类别，然后从这个类对应的图像列表中采样。和分类任务不同的是，检测任务中一张图像可能包含多个类别的目标。对于这种多标签的图像我们允许它们出现在多个类别的图像列表中。使用平衡采样技术，可以在VOC2007数据集上获得0.7的mAP提升。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-3b9ec5101b7a2248b03ed42135553ad0.jpg"&gt;集成上述所列的各项技术，我们的检测模型取得了SOTA的性能。在ImageNet DET任务中，我们以65.3的mAP获得了第二名。就单个模型而言，我们的模型能以少许优势排名第一。我们使用了相同的检测框架来完成ImageNet LOC任务。在最后的竞赛中，我们以8.7的定位误差排名第二。在PASCAL VOC 2012检测任务中，我们单模型获得了87.9的 mAP，超过了去年Kaiming的模型有4个点之多。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-4420bb556529bc294f99afd3c5109de9.jpg"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-208d59b4cfdcdfd80351d362dd8f5bfc.jpg"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-baa92663a10c48bbb763f36641ca58d8.jpg"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-6f7ff39710d8ee4f38841cceec4d15d2.jpg"&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" data-title="@果果是枚开心果." class="" data-editable="true"&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;海康威视研究院 &lt;/b&gt;是海康威视最重要的核心部门，主要致力于基础技术和前沿技术的探索和创新，在视音频编解码、视频图像处理、视频智能分析、云计算、大数据、云存储、人工智能等方面有深厚的技术积累，为海康威视核心产品和新兴业务拓展提供了有力的支撑，成为公司主营业务和创新业务发展的重要驱动力。研究院在KITTI、MOT、Pascal VOC、ImageNet等世界级人工智能竞赛中均获得过第一的好成绩。欢迎各位老师、学者、专家及其他业内人士，莅临杭州，参观交流。技术探讨、访问交流、求职招聘以及其他相关事宜，欢迎邮件联系谢迪博士：xiedi@hikvision.com。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325586&amp;amp;idx=1&amp;amp;sn=69a7e8482d884dda869290581a50a6d6&amp;amp;chksm=f235a558c5422c4eabe75f6db92fd13a3ca662d648676461914c9bfd1f4e22affe12430afce3&amp;amp;scene=0#wechat_redirect" data-editable="true" data-title="【高手之道】海康威视研究院ImageNet2016竞赛经验分享" class=""&gt;【高手之道】海康威视研究院ImageNet2016竞赛经验分享&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23249000&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Thu, 27 Oct 2016 18:09:30 GMT</pubDate></item></channel></rss>