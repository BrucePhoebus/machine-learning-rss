<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>程序员山居笔记 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/coding-future</link><description>主要和编程、算法有关，希望未来能有闲情逸致加上高级扯淡的内容。</description><lastBuildDate>Thu, 09 Feb 2017 00:17:23 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>100行Python为Prisma化的图片恢复原始色彩</title><link>https://zhuanlan.zhihu.com/p/21836208</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/4791027d7e93c79278d616f3dae224c7_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;首先感谢评论区中多位图像大牛指出的问题，代码并未把一张图片加上Prisma的效果，而是一个类似于Prisma的后处理。文章和标题都已作修改，如发现其他问题，欢迎指教，谢谢。&lt;/p&gt;&lt;p&gt;----------------- 正文分割线 -----------------&lt;/p&gt;&lt;p&gt;图像被&lt;a href="https://deepart.io/" data-editable="true" data-title="Deepart"&gt;Deepart&lt;/a&gt;等类似Prisma的工具处理后，可以加上各种风格。但可能存在一个问题，即原始图片的颜色也丢失了，例如：&lt;/p&gt;&lt;p&gt;纽约夜景图片 --&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/2d0076cdfc1d57db127997bfde942fbd.jpg" data-rawwidth="600" data-rawheight="338"&gt;&lt;p&gt;毕加索风格图片 --&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/ba14b947b225d9e5c59520a814376944.jpg" data-rawwidth="1179" data-rawheight="1536"&gt;合成图片 --&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/0d4e420fbb6aa742a7873ac3b0a6913c.jpg" data-rawwidth="600" data-rawheight="338"&gt;&lt;p&gt;合成后的图片中，原始图片的颜色丢失掉了，下面有一个简单的办法，可以将合成图片的样式与原始图片的色彩合成，让图片既具有毕加索的风格，又保留原图的颜色。&lt;/p&gt;&lt;p&gt;--- &lt;a href="https://github.com/pavelgonchar/color-independent-style-transfer" data-editable="true" data-title="源代码传送门" class=""&gt;Github传送门&lt;/a&gt; ---&lt;/p&gt;&lt;p&gt;效果如下：&lt;/p&gt;&lt;p&gt;重新处理后的毕加索风格纽约夜景图片 --&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/4791027d7e93c79278d616f3dae224c7.jpg" data-rawwidth="600" data-rawheight="338"&gt;&lt;/p&gt;&lt;p&gt;--- 下面是代码和注释 ---&lt;/p&gt;&lt;p&gt;首先加载以下库：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;import skimage.io
import tensorflow as tf
from tensorflow.python.framework import ops, dtypes
import numpy as np
from matplotlib import pyplot as plt
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;定义一些常量：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;flags = tf.app.flags
FLAGS = flags.FLAGS

""" minsk.jpg是原始图片文件(338 * 600) """
flags.DEFINE_string('original', 'New_York_night.jpg', 'Original Image')
""" tmp_950_color.jpg是包含某种风格的图片文件(338 * 600) """
flags.DEFINE_string('styled', 'New_York_night_picasso.jpg', 'Styled Image')

""" Tensor占位符，后面用Feed来计算 """
original = tf.placeholder("float", [1, 338, 600, 3])
styled = tf.placeholder("float", [1, 338, 600, 3])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;定义互转RGB和YUV格式的方法：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;def rgb2yuv(rgb):
    """
    Convert RGB image into YUV https://en.wikipedia.org/wiki/YUV
    """
    rgb2yuv_filter = tf.constant(
        [[[[0.299, -0.169, 0.499],
           [0.587, -0.331, -0.418],
            [0.114, 0.499, -0.0813]]]])
    rgb2yuv_bias = tf.constant([0., 0.5, 0.5])

    temp = tf.nn.conv2d(rgb, rgb2yuv_filter, [1, 1, 1, 1], 'SAME')
    temp = tf.nn.bias_add(temp, rgb2yuv_bias)

    return temp


def yuv2rgb(yuv):
    """
    Convert YUV image into RGB https://en.wikipedia.org/wiki/YUV
    """
    yuv = tf.mul(yuv, 255)
    yuv2rgb_filter = tf.constant(
        [[[[1., 1., 1.],
           [0., -0.34413999, 1.77199996],
            [1.40199995, -0.71414, 0.]]]])
    yuv2rgb_bias = tf.constant([-179.45599365, 135.45983887, -226.81599426])
    temp = tf.nn.conv2d(yuv, yuv2rgb_filter, [1, 1, 1, 1], 'SAME')
    temp = tf.nn.bias_add(temp, yuv2rgb_bias)
    temp = tf.maximum(temp, tf.zeros(temp.get_shape(), dtype=tf.float32))
    temp = tf.minimum(temp, tf.mul(
        tf.ones(temp.get_shape(), dtype=tf.float32), 255))
    temp = tf.div(temp, 255)
    return temp
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;定义拼接图像的方法，可以把两张图片水平连接起来：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;def concat_images(imga, imgb):
    """
    Combines two color image ndarrays side-by-side.
    """
    ha, wa = imga.shape[:2]
    hb, wb = imgb.shape[:2]
    max_height = np.max([ha, hb])
    total_width = wa + wb
    new_img = np.zeros(shape=(max_height, total_width, 3), dtype=np.float32)
    new_img[:ha, :wa] = imga
    new_img[:hb, wa:wa + wb] = imgb
    return new_img&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;准备就绪，开始主要流程。&lt;/p&gt;&lt;p&gt;在TensorFlow的Session开始前，首先定义一些必要的Operation和Tensor&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;""" 把含有风格的图像styled转换成yuv格式的灰度图styled_grayscale_yuv """
styled_grayscale = tf.image.rgb_to_grayscale(styled)
styled_grayscale_rgb = tf.image.grayscale_to_rgb(styled_grayscale)
styled_grayscale_yuv = rgb2yuv(styled_grayscale_rgb)

""" 把需要添加风格的原始图像转换成yuv格式original_yuv """
original_yuv = rgb2yuv(original)

""" 
组合图像：
1. styled_grayscale_yuv的Y分量
2. original_yuv的U分量
3. original_yuv的V分量
"""
combined_yuv = tf.concat(3, [tf.split(3, 3, styled_grayscale_yuv)[0], tf.split(3, 3, original_yuv)[1], tf.split(3, 3, original_yuv)[2]])

""" 转换成RGB格式 """
combined_rbg = yuv2rgb(combined_yuv)

""" 初始化 """
init = tf.initialize_all_variables()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后正式开始Session，完成Operations：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;with tf.Session() as sess:
    sess.run(tf.initialize_all_variables())

    """ 读取需要添加风格的原始图片 """
    original_image = skimage.io.imread(FLAGS.original) / 255.0
    original_image = original_image.reshape((1, 338, 600, 3))

    """ 读取含有风格的图片 """
    styled_image = skimage.io.imread(FLAGS.styled) / 255.0
    styled_image = styled_image.reshape((1, 338, 600, 3))

    """ 为原始图片添加上风格 """
    combined_rbg_ = sess.run(combined_rbg, feed_dict={original: original_image, styled: styled_image})

    """ 拼接几幅图片并保存，做个对比 """
    summary_image = concat_images(original_image.reshape((338, 600, 3)), styled_image.reshape((338, 600, 3)))
    summary_image = concat_images(summary_image, combined_rbg_[0])
    plt.imsave("results.jpg", summary_image)
&lt;/code&gt;&lt;/pre&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21836208&amp;pixel&amp;useReferer"/&gt;</description><author>王若愚</author><pubDate>Tue, 02 Aug 2016 18:13:56 GMT</pubDate></item><item><title>非监督学习算法--K均值聚类</title><link>https://zhuanlan.zhihu.com/p/21558539</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/729eb965e0ada616d3cd58adcc4c7013_r.png"&gt;&lt;/p&gt;本文介绍最常见的非监督学习算法「K均值(K-means)」，思路和仿真主要参考Ng的课程「Machine Learning」以及课程作业的Matlab的实现。&lt;p&gt;&lt;a href="https://github.com/wrymax/machine-learning-assignments/tree/master/week8/machine-learning-ex7/ex7" data-editable="true" data-title="猛戳下载本文Matlab实现" class=""&gt;猛戳下载本文Matlab实现&lt;/a&gt;，其中ex7.m包含了k-means的代码，ex7_pca.m包含了PCA的代码。&lt;/p&gt;&lt;p&gt;以下是正文。&lt;/p&gt;&lt;p&gt;&lt;b&gt;K-means的意义和使用场景：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在无任何先验分类知识的情况下，自动发现数据集的分类。例如：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;在大量文本中发现隐含的话题；&lt;/li&gt;&lt;li&gt;发现图像中包含的颜色种类；&lt;/li&gt;&lt;li&gt;从销售数据中发现不同特征顾客的分类。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;👇下面简述算法步骤、仿真以及算法中的问题 👇&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.1 算法步骤&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;假设希望将训练数据集&lt;equation&gt;x^{(i)} (i = 1, 2, 3,..., m)&lt;/equation&gt;分为K类；&lt;/li&gt;&lt;li&gt;在&lt;equation&gt;x^{(i)} (i = 1, 2, 3,..., m)&lt;/equation&gt;中，随机选择K个作为初始分类的图心(centroids) &lt;equation&gt;\mu _{1}, \mu _{2}, \mu _{3},..., \mu _{K}&lt;/equation&gt;；&lt;/li&gt;&lt;li&gt;遍历&lt;equation&gt;x^{(i)}&lt;/equation&gt;，计算出和每个&lt;equation&gt;x&lt;/equation&gt;距离最近的图心&lt;equation&gt;\mu ^{(i)}&lt;/equation&gt;，记录当前&lt;equation&gt;x&lt;/equation&gt;属于第&lt;equation&gt;i&lt;/equation&gt;类；&lt;/li&gt;&lt;li&gt;遍历K种分类，分别计算上一步中，划归其中的所有&lt;equation&gt;x&lt;/equation&gt;点的中心点，将该点设置为本分类中心的图心；&lt;/li&gt;&lt;li&gt;迭代上两步，直到图心位置收敛。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;1.2 Matlab仿真：K-means过程&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/87d35141e80060aea0eecb2f094454a6.jpg" data-rawwidth="570" data-rawheight="493"&gt;&lt;p&gt;                                         (我做了个gif动图，可能需要戳一下它才会动起来)&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.3 随机初始化 Random Initialisation &lt;/b&gt;&lt;/p&gt;&lt;p&gt;由于初始的图心是随机选择的，K-means可能陷入局部最优而导致最终的图心无法收敛到合适的位置。可以使用随机初始化来解决这个问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;多次运行K-means算法，计算&lt;equation&gt;c^{(1)},..., c^{(m)}, \mu _{1},..., \mu _{k}&lt;/equation&gt;；&lt;/li&gt;&lt;li&gt;计算Cost Function &lt;equation&gt;J = (c^{(1)},..., c^{(m)}, \mu _{1},..., \mu _{k})&lt;/equation&gt;，函数代表了聚类的失真程度；&lt;/li&gt;&lt;li&gt;选择J最小的那一组初始化以及最终的计算结果。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;1.4 选择聚类数量&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;使用「肘方法」Elbow Method&lt;/li&gt;&lt;ul&gt;&lt;li&gt;逐渐增加K，并分别计算Cost Function J，寻找J较小的K，如图：&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/392ddcffe91d6e90c0316e0e5e5cb47d.jpg" data-rawwidth="620" data-rawheight="321"&gt;&lt;li&gt;理论上，当分类数量K增大时，J将逐渐变小；&lt;/li&gt;&lt;li&gt;但也可能陷入局部最优的问题，导致k增大时，J反而增大。此时需要重新随机初始化后再次计算。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;Ng推荐的办法：向自己提问「我为什么要使用K-means」？充分理解聚类的需求以及聚类后能向下游贡献什么东西，往往能从中发现真正合适的聚类数量。例如下图：&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/deee28966bfec9af3579e48b3118f7c4.jpg" data-rawwidth="620" data-rawheight="329"&gt;&lt;li&gt;横轴为衣服店顾客的身高，纵轴为顾客的体重。当你理解了顾客可能分为「S、M、L」三类，或者「XS、S、M、L、XL」五类时，最终选择的分类数量可能比较make sense，而不是完全依赖Cost Function算出一个10类或者4类，最终并没有太大实际意义。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;1.5 实践：使用K-means压缩图像&lt;/b&gt;&lt;/p&gt;&lt;p&gt;思路：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;对一个RGB图像执行K-means算法，寻找能描述图像的16种主要颜色分类；&lt;/li&gt;&lt;li&gt;将每个像素点聚类到这16种颜色分类中，并分别替换为对应分类的颜色；&lt;/li&gt;&lt;li&gt;对每个像素使用颜色的索引来代替3维的RGB亮度值，由此可以将图像的大小压缩到&lt;equation&gt;\frac{1}{6}&lt;/equation&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;仿真图：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/037cf01978ec1bedb8ae69ebf5505221.jpg" data-rawwidth="620" data-rawheight="342"&gt;代码在文章开始处的Github链接中，执行ex7即可观测到结果。&lt;/p&gt;&lt;p&gt;如需转载，请附上原链接，谢谢。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21558539&amp;pixel&amp;useReferer"/&gt;</description><author>王若愚</author><pubDate>Mon, 11 Jul 2016 17:31:53 GMT</pubDate></item><item><title>SVM支持向量机</title><link>https://zhuanlan.zhihu.com/p/21481541</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/52c8d933d79140e2fb6710994a5d581d_r.jpg"&gt;&lt;/p&gt;本文从概念和一些算法细节上介绍Support Vector Machine支持向量机的原理和功能。通过对比Logistic Regression，期望对SVM有一个快速的入门介绍。实现代码可参考：&lt;a href="https://github.com/wrymax/machine-learning-assignments/tree/master/week7/machine-learning-ex6/ex6" data-editable="true" data-title="SVM示例"&gt;SVM示例&lt;/a&gt;，在Matlab / Octave中加载，执行ex6和ex6_spam即可。&lt;b&gt;1) 目的和场景&lt;/b&gt;SVM是一种经典的机器学习算法，主要解决数据分类问题。原理是基于训练大量已有的数据和已知的分类情况，计算出一个预测模型。然后可以在模型上跑新的数据，从而判断新数据应当归于哪一类。SVM适用于线性和非线性的场景。SVM的典型使用场景如：
&lt;ol&gt;&lt;li&gt;房价估算&lt;ol&gt;&lt;li&gt;根据过去十年来房价和房屋面积、卧室数量、当地消费水平等等各种因素数据，将房屋分为「豪宅」、「中等」、「经济型住房」、「贫民窟」等几类；&lt;/li&gt;&lt;li&gt;使用SVM训练这些数据得出一个模型，可以用来预测在新的条件下，某个住房可以被划归到哪种分类，价值区间多少。&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;垃圾邮件分类器：&lt;ol&gt;&lt;li&gt;获取可疑的spam email关键词列表，例如：Buy、now等（实际Spam Corpus可以参考使用&lt;a href="https://spamassassin.apache.org/publiccorpus/" data-editable="true" data-title="Apache Spam Assassin"&gt;Apache Spam Assassin&lt;/a&gt;）；&lt;/li&gt;&lt;li&gt;收集大量的spam和非spam邮件数据，将其中包含的可疑spam关键词找出并标记在特征向量中，用SVM训练这些数据，得出一个模型，用来判断一封新的邮件是否为一个垃圾邮件。&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;b&gt;2) 核心思路&lt;/b&gt;2.1 SVM的思路可以由Logistic Regression演变而来，依然是通过Gradient Descent计算CostFunction的全局极小值，得到theta矩阵，作为分类模型。
如图：
&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/31d0113a2e3d05cc56d4044a8d37e828.jpg" data-rawwidth="620" data-rawheight="337"&gt;2.2 SVM和Logistic Regression的不同之处：
&lt;ol&gt;&lt;li&gt;把LR中的Sigmoid函数 &lt;equation&gt;log(h_{\theta }(x) ) = log(g(z))&lt;/equation&gt; 被修改为上图中的函数 &lt;equation&gt;cost_{0} (z)&lt;/equation&gt; 和 &lt;equation&gt;cost_{1} (z)&lt;/equation&gt; 其中&lt;equation&gt;z = \theta^{T}X &lt;/equation&gt;&lt;ol&gt;&lt;li&gt;当分类结果 y = 1 时，期望z &amp;gt;= 1，此时&lt;equation&gt;cost_{1} (z)&lt;/equation&gt; = 0，总体cost function达到最小&lt;/li&gt;&lt;li&gt;当分类结果 y = 0 时，期望z &amp;lt;= -1，此时&lt;equation&gt;cost_{0} (z)&lt;/equation&gt; = 0，总体cost function达到最小&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;关于Regularisation参数C：&lt;ol&gt;&lt;li&gt;Logistic Regression在cost function的后半部分上乘以一个lambda（一个较小的值，例如0.03）&lt;/li&gt;&lt;li&gt;SVM在cost function的前半部分乘以一个较大的C（可以理解为1 / lambda，例如1, 100, 1000）&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;2.3 Kernel 核函数
&lt;ol&gt;&lt;li&gt;SVM引入Kernel的概念，将&lt;equation&gt;z = \theta^{T}X &lt;/equation&gt;中的X变为一个f = g(x)，形式变为：&lt;equation&gt;z = \theta^{T} f&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;最常见的Kernel&lt;ol&gt;&lt;li&gt;Linear Kernel 线性核&lt;ol&gt;&lt;li&gt;实际上就是不使用Kernel，直接使用&lt;equation&gt;\theta^{T} X&lt;/equation&gt;作为cost function的自变量&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Gaussian Kernel 高斯核&lt;ol&gt;&lt;li&gt;对X套用高斯函数，如下图&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/5cdc926f1d45540203539c5b8a905eb9.png" data-rawwidth="3636" data-rawheight="1996"&gt;&lt;li&gt;图例参数说明&lt;ol&gt;&lt;li&gt;「l」是「landmark」的缩写，在实际编程中，直接把training set放置到landmark位置上即可&lt;/li&gt;&lt;li&gt;高斯核函数 f 实际上计算了x和某个landmark l之间的近似度，这个近似度用欧氏距离来描述（计算向量内积）&lt;/li&gt;&lt;li&gt;&lt;equation&gt;\sigma &lt;/equation&gt;用于控制高斯核的陡峭程度，&lt;equation&gt;\sigma &lt;/equation&gt;越大，函数越平滑&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;其他核函数&lt;ol&gt;&lt;li&gt;Polynomial Kernel&lt;/li&gt;&lt;li&gt;String Kernel&lt;/li&gt;&lt;li&gt;chi-square kernel&lt;/li&gt;&lt;li&gt;histogram kernel&lt;/li&gt;&lt;li&gt;intersection kernel&lt;/li&gt;&lt;li&gt;根据Ng的说法，这些看起来很屌，但他自己基本没怎么用过… 大多数情况下，都是用Gaussian Kernel&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;2.4 Logistic Regression、SVM和神经网络的使用场景对比
假设 n 为特征的数量，m为训练集的数量
&lt;ol&gt;&lt;li&gt;当n比m大得多时，如n = 10000，m = 1000，建议使用Logistic Regression，或者SVM + 线性核&lt;/li&gt;&lt;li&gt;如果n很小，m不大不小，例如n = 1 ~1000, m = 10 ~ 10000，建议使用SVM + 高斯核&lt;/li&gt;&lt;li&gt;如果n很小，m很大，例如n = 1 ~1000, m = 50000+，建议添加更多的特征，然后使用Logistic Regression，或者SVM + 线性核&lt;/li&gt;&lt;li&gt;各种情况下，神经网络都工作的不错，但是比SVM训练的速度慢一些&lt;/li&gt;&lt;/ol&gt;&lt;b&gt;3) 实践中的一些注意事项
&lt;/b&gt;&lt;ol&gt;&lt;li&gt;需要通过多次交叉检验来确定合适的regularisation参数C和高斯核参数&lt;equation&gt;\sigma &lt;/equation&gt;，方法：&lt;ol&gt;&lt;li&gt;设定一组备选的C和&lt;equation&gt;\sigma &lt;/equation&gt;，用他们两两组合循环训练SVM model&lt;/li&gt;&lt;li&gt;用训练好的model跑Cross Validation数据集，计算errors&lt;/li&gt;&lt;li&gt;找出errors最低的一组C和sigma&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;几个好用的SVM工具包&lt;ol&gt;&lt;li&gt;liblinear&lt;/li&gt;&lt;li&gt;libsvm&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21481541&amp;pixel&amp;useReferer"/&gt;</description><author>王若愚</author><pubDate>Mon, 04 Jul 2016 12:39:20 GMT</pubDate></item><item><title>应用机器学习算法的一些具体建议</title><link>https://zhuanlan.zhihu.com/p/21449423</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/01d6654b5392b00f4e777791d7e80c48_r.jpg"&gt;&lt;/p&gt;本文来自于学习Ng的Machine Learning课程笔记。学习时用英文记录，写本文时加上了部分中文解释，尽可能的口语化了。另外部分名词可能翻译的不太对，如果您发现了请不吝指正，谢谢。具体实现代码请参考：&lt;a href="https://github.com/wrymax/machine-learning-assignments/tree/master/week6/machine-learning-ex5/ex5" class=""&gt;https://github.com/wrymax/machine-learning-assignments/tree/master/week6/machine-learning-ex5/ex5&lt;/a&gt;使用Matlab / Octave加载，运行ex5即可。本文主要指出了一些机器学习实践中的技巧，coding前必备。Let's get started.&lt;b&gt;关键名词：&lt;/b&gt;&lt;ol&gt;&lt;li&gt;训练数据集 Training Set&lt;/li&gt;&lt;li&gt;训练数据集的代价函数 Jtrain(theta)&lt;/li&gt;&lt;li&gt;交叉验证数据集 Cross Validation&lt;/li&gt;&lt;li&gt;交叉验证数据集的代价函数 Jcv(theta)&lt;/li&gt;&lt;li&gt;测试数据集 Test&lt;/li&gt;&lt;li&gt;测试数据集的代价函数 Jtest(theta)&lt;/li&gt;&lt;li&gt;预测误差、代价 error（代价函数的计算结果）&lt;/li&gt;&lt;li&gt;偏差 Bias&lt;/li&gt;&lt;li&gt;特征多样性、方差 Variance&lt;/li&gt;&lt;li&gt;欠拟合 Under-Fitting&lt;/li&gt;&lt;li&gt;过拟合 Over-Fitting&lt;/li&gt;&lt;li&gt;正则化 Regularisation &lt;/li&gt;&lt;li&gt;查准率 Precision&lt;/li&gt;&lt;li&gt;召回率 Recall&lt;/li&gt;&lt;li&gt;F值 F Score&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;核心概念&lt;/b&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;Machine Learning Diagnostic 机器学习诊断&lt;/b&gt;&lt;ol&gt;&lt;li&gt;To get better avenue, you may:&lt;ol&gt;&lt;li&gt;Collect larger training examples 收集更多的训练数据 =&amp;gt; 解决overfitting&lt;/li&gt;&lt;li&gt;Get additional features 获得和使用额外的特征 =&amp;gt; 解决high bias&lt;/li&gt;&lt;li&gt;Add polynomial features 添加多项式特征 =&amp;gt; 解决high bias&lt;/li&gt;&lt;li&gt;Reduce features 减少特征 =&amp;gt; 解决overfitting&lt;/li&gt;&lt;li&gt;Increase lambda 增大lambda =&amp;gt; 解决overfitting&lt;/li&gt;&lt;li&gt;Decrease lambda =&amp;gt; 解决underfitting&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;However, it may takes you 6 month to train data, and nothing to gain.&lt;/li&gt;&lt;li&gt;Machine Learning Diagnostic is: 机器学习诊断思路如下：&lt;ol&gt;&lt;li&gt;A test that you can run to gain insight what is / isn’t working with a learning algorithm, and gain guidance as to how best to improve its performance. 这是一种测试方法，你可以据此尝试并改进算法的性能&lt;/li&gt;&lt;li&gt;It takes time to implement. 这得花点时间。。&lt;/li&gt;&lt;li&gt;It sometimes rule out certain courses of action (changes to your learning algorithm) as being unlikely to improve its performance significantly. 有可能这并没有什么卵用&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Training/Testing Procedure 训练/测试过程&lt;ol&gt;&lt;li&gt;Split dataset into 7:3, 70% of which is going to be the training set, 30% of which is going to be the testing set. 切分数据集：70%作为训练数据，30%作为测试数据&lt;/li&gt;&lt;li&gt;Learn parameter theta from training data. 使用训练数据习得你需要的theta&lt;/li&gt;&lt;li&gt;Compute test set error: 使用刚刚训练出来的Cost Function来跑一下测试数据集，得到测试数据集的误差&lt;ol&gt;&lt;li&gt;Jtest(theta) = CostFunction(training_data) =&amp;gt; the square deviation equation &lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Model Selection and Train/Validation/Test Sets 模型选择与训练/校验/测试数据集&lt;/b&gt;&lt;ol&gt;&lt;li&gt;Degree of Polynomial 多项式级数（一个我们需要关注的指标），这个东西就是下图中的「d」&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/4a1f050207306becba5c5573957d0625.png" data-rawwidth="3796" data-rawheight="2060"&gt;&lt;li&gt;如上图，我们从d = 1到d = 10，分别用test数据集计算Cost Function，然后选取一个最优化的d（上图中选择了d = 5），后面几部讲了具体怎么实施这个过程&lt;/li&gt;&lt;li&gt;Evaluating your hypothesis 评估你的预测函数&lt;ol&gt;&lt;li&gt;Training Set - 60% of data set 把60%的数据设置为训练数据集&lt;/li&gt;&lt;li&gt;Cross Validation Set (CV) - 20% of data set 把20%的数据设置为交叉验证数据集&lt;/li&gt;&lt;li&gt;Test Set - 20% of data set 把20%的数据设置为测试数据集&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Use Validation Set to select the model 使用交叉验证数据集来选择模型&lt;ol&gt;&lt;li&gt;Compute cost function J(theta) by CV in different degrees&lt;/li&gt;&lt;li&gt;Choose the minimal one as the target degree, as below, d = 4&lt;/li&gt;&lt;li&gt;Estimate generalisation erro for test set Jtest(theta(4))&lt;/li&gt;&lt;li&gt;以上这段英文就是步骤1 - 3了&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Bias vs. Variance 偏差 vs. 特征多样性（or方差？）&lt;/b&gt;&lt;ol&gt;&lt;li&gt;Bias =&amp;gt; 特征值以外的代价函数偏差&lt;ol&gt;&lt;li&gt;High Bias =&amp;gt; high lambda =&amp;gt; Under-fitting&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Variance =&amp;gt; 特征多样性&lt;ol&gt;&lt;li&gt;High Variance =&amp;gt; low lambda =&amp;gt; Overfitting&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Diagnosing Bias vs. Variance 诊断：偏差高低和特征值大小&lt;ol&gt;&lt;li&gt;Set coordinates of (degree_of_polynomial_d, error); 设置一个横轴为特征数量（多项式维度）d，纵轴为预测误差error的坐标系&lt;/li&gt;&lt;li&gt;Train the 60% data set, draw a curve which will converge when d goes bigger; 用60%的数据集作为训练集合，计算J(theta)，当多项式维度d（特征数）增大时，error会减少，向0收敛&lt;/li&gt;&lt;li&gt;Run prediction with Cross-Validation-Set by trained model, there will be an overfitting point, after which Jcv(theta) will continuously arise. 使用从训练集中获得的模型，计算交叉验证数据集的预测误差，会发现过拟合问题。&lt;/li&gt;&lt;li&gt;如下图：&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/d1a6d5dfd58b7cdbd0410487a5596c4f.png" data-rawwidth="3744" data-rawheight="2000"&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Question 问题来了...&lt;/b&gt;&lt;ol&gt;&lt;li&gt;How can we figure out it is suffered from &lt;b&gt;Bias&lt;/b&gt;, or from &lt;b&gt;Variance&lt;/b&gt;? 我们如何知道模型的问题出在偏差还是特征多样性上？&lt;ol&gt;&lt;li&gt;Bias =&amp;gt; Underfitting, as on the left part of the diagram 偏差往往和欠拟合相关，如图左半部分，特征数过少&lt;ol&gt;&lt;li&gt;Jtraining(theta) will be high 训练集的代价函数error会很大&lt;/li&gt;&lt;li&gt;Jcv(theta) ≈ Jtraining(theta) 交叉验证的代价函数error约等于训练集，也很大&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Variance =&amp;gt; Overfitting, as on the right part of the diagram 特征多样性往往和过拟合相关，如图右半部分，特征数量过多&lt;ol&gt;&lt;li&gt;Jtraining(theta) will be low 训练集的代价函数error越来越低&lt;/li&gt;&lt;li&gt;Jcv(theta) &amp;gt;&amp;gt;( much higher than ) Jtraining(theta) 交叉验证数据集的代价函数error在经过一个极小值后开始上升，最终远大于训练集的错误。这是典型的过拟合特征：对新数据的fitting性能非常差。&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;如下图：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/a0db6ba0b74fd2185a95d4ff0974c079.png" data-rawwidth="3788" data-rawheight="1920"&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Choosing the regularisation parameter lambda 选择正则化参数lambda&lt;ol&gt;&lt;li&gt;Try lambda from small to large, like 从小到大，尝试lambda&lt;ol&gt;&lt;li&gt;0&lt;/li&gt;&lt;li&gt;0.01&lt;/li&gt;&lt;li&gt;0.02&lt;/li&gt;&lt;li&gt;0.04&lt;/li&gt;&lt;li&gt;0.08&lt;/li&gt;&lt;li&gt;...&lt;/li&gt;&lt;li&gt;10&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Pick up best fitting lambda of Cross-Validation cost function, say Theta(5) 选择一个对交叉验证数据集的代价函数拟合最好的lambda值，例如theta(5)&lt;/li&gt;&lt;li&gt;Compute J(theta) by the test data set 计算测试数据集的代价函数&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/2d7c1a31ce3a19e9aadc47c4ac0b73aa.png" data-rawwidth="3764" data-rawheight="2024"&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Bias/Variance as a function of the regularisation parameter lambda 以lambda为参数的偏差/方差函数&lt;/b&gt;&lt;ol&gt;&lt;li&gt;When lambda is 0 当lambda为0时&lt;ol&gt;&lt;li&gt;you can fit the training set relatively well, since there is no regularisation. 训练集会拟合的相对不错，因为没有做任何的正则化&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;When lambda is small 当lambda很小时&lt;ol&gt;&lt;li&gt;You get a small value of Jtrain 训练集的预测误差也很小&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;When lambda goes large 当lambda变大时&lt;ol&gt;&lt;li&gt;The bias becomes larger, so Jtrain goes much larger 偏差越来越大，训练集的预测误差会显著增大&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/b473417172cd29c59f9a5655c6889a16.png" data-rawwidth="3820" data-rawheight="2016"&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Learning Curves 学习曲线&lt;/b&gt;&lt;ol&gt;&lt;li&gt;When training set number 「m」grows&lt;ol&gt;&lt;li&gt;It is much harder to fit, so training error grows 训练集增大，代价函数越来越难拟合所有的数据集，error会增大&lt;/li&gt;&lt;li&gt;As examples grows, it does better at generalising to new examples, so Cross-Validation error decreases 训练集增大时，代价函数归纳新元素的性能会更好，因此cross-validation的错误率会下降&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/c92244a19933e73a4ed2dc2a8d93911f.jpg" data-rawwidth="620" data-rawheight="326"&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;High Bias 高偏差的情况&lt;ol&gt;&lt;li&gt;High Bias means low Variance, so h(theta) would a low-dimensional function, which cannot fit all the dataset well 高偏差代表预测多项式的维度过低，因此很难预测整个数据集&lt;/li&gt;&lt;li&gt;When m is small, Jcv is high and Jtrain is low, while both of them will converge to similar value when dataset grows large enough 当数据集的总数很小时，交叉验证集的预测误差很大，训练数据集的预测误差很小；但当m越来越大，两者将越来越接近&lt;/li&gt;&lt;li&gt;Both the error of Jcv and Jtrain would be fairly HIGH 交叉验证和训练数据集的J都会很大&lt;/li&gt;&lt;li&gt;Conclusion 结论&lt;ol&gt;&lt;li&gt;If a learning algorithm is suffering from high bias, getting more training data will not(by itself) help much 当学习算法存在高偏差问题时，训练更多的数据无法解决问题（J会收敛于一个很高的错误值，不再下降）&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/a2561bd221232925c4625490b7ba316f.jpg" data-rawwidth="620" data-rawheight="330"&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;High Variance 高特征多样性的情况&lt;ol&gt;&lt;li&gt;High Variance means low lambda and the polynomial hypothesis function has many many features 高特征多样性说明正则化参数lambda很小，此外预测多项式有非常多的特征&lt;/li&gt;&lt;li&gt;When m is small 当训练数据量m很小时&lt;ol&gt;&lt;li&gt;Jtrain is small; as m grows up, Jtrain becomes larger too, but the training set error would still be pretty low 训练集的J也很小；但是当训练数据越来越多时，由于预测函数的维度过高，拟合开始变得困难，Jtrain逐渐上升，但是仍然是一个非常小的数值&lt;/li&gt;&lt;li&gt;Jcv is large; as m grows up, Jtrain becomes smaller and coverage to a value similar with Jtrain 交叉验证数据集的J很大（高特征多样性带来的过拟合会导致预测函数对样本外的数据点预测偏差很大）；当m增大时，Jcv会逐渐下降&lt;/li&gt;&lt;li&gt;The indicative diagnostic that we have a high variance problem 我们遇到高特征多样性问题的一个象征性指标：&lt;ol&gt;&lt;li&gt;With m becomes larger, there is a large gap between the training error and cross-validation error 随着m增大，训练数据集的预测误差和交叉验证数据集的预测误差之间会存在一个很大的空白&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Conclusion 结论&lt;ol&gt;&lt;li&gt;If learning algorithm is suffering from high variance, getting moe training data is likely to help 当学习算法存在高特征多样性为题是，使用更多的训练数据可能会有帮助&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/ffe4cfa386a14c901823696c246575f3.jpg" data-rawwidth="620" data-rawheight="330"&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Deciding what to try next (revisited) 决定下一步做什么&lt;/b&gt;&lt;ol&gt;&lt;li&gt;When debugging a learning algorithm, you find your model makes unacceptably large errors in its prediction, what to do next? 当调试一个学习算法时，你发现你的预测模型得出了不可接受的高误差，下一步该怎么办？&lt;ol&gt;&lt;li&gt;Get more training examples 使用更多的训练数据&lt;ol&gt;&lt;li&gt;When Jcv is much higher than Jtrain, it fixes high variance 当Jcv比Jtrain大的多时，它可以解决高特征多样性的问题&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Try smaller sets of features 尝试减少特征数量&lt;ol&gt;&lt;li&gt;It fixes high variance problem too 减少一部分用处不大的特征可以解决高特征多样性的问题&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Try getting additional features 尝试使用额外的特征&lt;ol&gt;&lt;li&gt;Maybe the model is under-fitting (high bias), try additional features can make the model fitting training set better 高误差也有可能是因为特征数量太少了，因此使用额外的特征可以解决高偏差的问题&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Try adding polynomial features 尝试添加多项式特征 (x1 * x1, x2 * x2, x1 * x2, etc)&lt;ol&gt;&lt;li&gt;Which also solves high bias problem 也是可以解决高偏差的问题（欠拟合）&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Try decreasing lambda 尝试降低正则化参数lambda&lt;ol&gt;&lt;li&gt;Which also solves high bias problem 也是可以解决高偏差的问题（欠拟合）&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Try increasing lambda 尝试增加正则化参数lambda&lt;ol&gt;&lt;li&gt;Which also solves high variance problem 也是可以解决高特征数量的问题（过拟合）&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Neural Networks and overfitting 神经网络与过拟合&lt;ol&gt;&lt;li&gt;Small neural network 小型神经网络&lt;ol&gt;&lt;li&gt;fewer parameters, more prone to under-fitting 网络层数少、神经元数量少，更易导致欠拟合&lt;/li&gt;&lt;li&gt;Computationally cheaper 计算资源消耗少&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Large neural network 大型神经网络&lt;ol&gt;&lt;li&gt;Type1. few layers, lot of units 类型1：层数少，每层的神经元多&lt;/li&gt;&lt;li&gt;Type2. few units, lot of layers 类型2：层数多，每层的神经元少&lt;/li&gt;&lt;li&gt;more parameters, more prone to over-fitting 参数多，更易导致过拟合&lt;/li&gt;&lt;li&gt;Computationally more expensive 计算资源消耗多&lt;/li&gt;&lt;li&gt;Use regularisation to address overfitting 使用正则化来解决过拟合问题&lt;/li&gt;&lt;li&gt;Try one layer, two layers and three layers.. and compute Jcv(theta) to decide how many layers you will use 尝试1层、2层、3层… 并计算交叉验证代价函数，据此来选择最合适的神经网络层数&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;b&gt;Machine Learning System Design 机器学习系统设计&lt;/b&gt;&lt;b&gt;（本段中楼主懒癌发作不想打字。。。直接贴Ng的截图了）&lt;/b&gt;&lt;ol&gt;&lt;li&gt;Building a Spam Classifier 构建一个垃圾邮件分类器&lt;ol&gt;&lt;li&gt;Prioritising What to Work On &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/230490f942998f0541070e26c91f0dfa.png" data-rawwidth="1832" data-rawheight="838"&gt;&lt;/li&gt;&lt;li&gt;Recommended Approach 推荐的方法&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/5c6966a18a75ba8633f77780dbaf382e.png" data-rawwidth="1756" data-rawheight="722"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/81a7af0b71a42cf0752b0ef9784ac2ab.png" data-rawwidth="3636" data-rawheight="1872"&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Handling Skewed Data 处理歪曲/偏斜的数据&lt;ol&gt;&lt;li&gt;Error Metrics for Skewed Data 歪曲数据的错误度量方法&lt;ol&gt;&lt;li&gt;查准率 Precision&lt;ol&gt;&lt;li&gt;Precision = TruePositive / (No. of predicted positive)&lt;/li&gt;&lt;li&gt;No. of predicted positive = TruePositive + FalsePositive&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;召回率 Recall&lt;ol&gt;&lt;li&gt;Recall = TruePositive / (No. of actual positive)&lt;/li&gt;&lt;li&gt;No. of actual positive = TruePositive + FalseNegative&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Trading off Precision and Recall 权衡查准率和召回率&lt;ol&gt;&lt;li&gt;High Precision and Low Recall: Suppose we want to predict y = 1 only if very confident &lt;ol&gt;&lt;li&gt;Predict 1 if h(x) &amp;gt;= 0.9&lt;/li&gt;&lt;li&gt;Predict 0 if h(x) &amp;lt; 0.9&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;High Recall and Low Precision: Suppose we want to predict y = 0 only if very confident&lt;ol&gt;&lt;li&gt;Predict 1 if h(x) &amp;gt;= 0.3&lt;/li&gt;&lt;li&gt;Predict 0 if h(x) &amp;lt; 0.3&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/490a48d1d7c4c3ddb1c507066c99301c.png" data-rawwidth="3788" data-rawheight="2020"&gt;&lt;li&gt;F Score F值&lt;ol&gt;&lt;li&gt;For making a decision which pair of Precision and Recall is better&lt;/li&gt;&lt;li&gt;Measure P and R on the &lt;b&gt;cross validation set&lt;/b&gt;, and choose the value of threshold which maximises F score, as below&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/8c912d6f5472417b4bf5a45724cc3108.png" data-rawwidth="3756" data-rawheight="1952"&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Using Large Data Sets 使用大数据集&lt;ol&gt;&lt;li&gt;首先问自己两个问题：&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Question1: Can a human expert accurately predict y from given features x?  人类砖家使用目前的特征指标，能否准确预测出结果？&lt;ol&gt;&lt;li&gt;e.g. Can a realtor predict the housing price simply based on housing sizes? 例如：一个房地产经纪人是否能仅仅基于房屋面积预测出房价？&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Question2: Can we get a large training set to train with many features? 我们能不能搞到更多的数据来训练如此多的特征？&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;如果可以，那么：&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Use a learning algorithm with many parameters ( many features / many hidden layers )  多设置一些特征，或者神经网络中的隐藏层&lt;/li&gt;&lt;li&gt;Use a very large training set 尽量多的使用训练数据&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21449423&amp;pixel&amp;useReferer"/&gt;</description><author>王若愚</author><pubDate>Wed, 29 Jun 2016 14:15:41 GMT</pubDate></item><item><title>神经网络的学习 / 训练过程</title><link>https://zhuanlan.zhihu.com/p/21381359</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/17db6ca9a6df9e33f302b13568e97dda_r.jpg"&gt;&lt;/p&gt;以下为部分学习笔记。&lt;p&gt;具体实现代码参考: &lt;a href="https://github.com/wrymax/machine-learning-assignments/tree/master/week5/machine-learning-ex4/ex4"&gt;https://github.com/wrymax/machine-learning-assignments/tree/master/week5/machine-learning-ex4/ex4&lt;/a&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;Cost Function 代价函数&lt;/b&gt;&lt;ol&gt;&lt;li&gt;Important Parameters:&lt;ol&gt;&lt;li&gt;L =&amp;gt; Total number of layers in network&lt;/li&gt;&lt;li&gt;Sl =&amp;gt; Number of units ( not counting bias unit ) in layer l&lt;/li&gt;&lt;li&gt;As below, L = 4, S1 = 3, S2 = 5, S4 = SL = 4&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/f168eae6991fd9681da794b9fc3e0b12.png" data-rawwidth="626" data-rawheight="342"&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Two Classification Methods&lt;ol&gt;&lt;li&gt;Binary Classification 二元分类&lt;ol&gt;&lt;li&gt;y = 0 or 1&lt;/li&gt;&lt;li&gt;SL = K = 1 ( One output unit )&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Multi-class Classification 多元分类&lt;ol&gt;&lt;li&gt;y is logical vectors, which uses 1 to denote the class&lt;/li&gt;&lt;li&gt;SL = K, K &amp;gt;= 3 ( K output units )&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/62e9dc13f799ec5ec0cb8df167179063.png" data-rawwidth="1890" data-rawheight="992"&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;The Cost Function&lt;ol&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/70270b37927da2357329ae77eef65d6a.png" data-rawwidth="1854" data-rawheight="1038"&gt;&lt;li&gt;J(theta) sum up the cost function in logistic regression of ALL Layers.&lt;/li&gt;&lt;li&gt;Regularisation sum up all Theta elements between each two layers.&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Back Propagation 向后传播&lt;/b&gt;&lt;ol&gt;&lt;li&gt;Compute Gradient 用于计算梯度( CostFunction对Theta的偏微分 )&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/eba219186b9387a30205698194cc0ffe.png" data-rawwidth="1894" data-rawheight="1014"&gt;&lt;/li&gt;&lt;li&gt;Algorithm 算法解释&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/d750e6a3a3cf8706181b653b3efe3d22.png" data-rawwidth="1894" data-rawheight="1032"&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Back Propagation in Practice 向后传播实践技巧&lt;/b&gt;&lt;ol&gt;&lt;li&gt;Learning Algorithm 学习算法&lt;ol&gt;&lt;li&gt;initialTheta&lt;/li&gt;&lt;li&gt;costFunction&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Unrolling Parameters 展开参数&lt;ol&gt;&lt;li&gt;Change matrices into vectors &lt;/li&gt;&lt;li&gt;Change vectors into matrices&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Gradient Checking 梯度检查&lt;ol&gt;&lt;li&gt;Use numerical estimate method to compute derivatives&lt;/li&gt;&lt;li&gt;Pros:&lt;ol&gt;&lt;li&gt;It can check is derivatives are correct&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Cons:&lt;ol&gt;&lt;li&gt;It is super slow. &lt;/li&gt;&lt;li&gt;When you make sure back propagation gives similar values as gradient, just turn off it.&lt;/li&gt;&lt;li&gt;Be sure to disable gradient checking code before training your classifier. Or the training process would be super slow.&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Random Initialisation 随机初始化&lt;ol&gt;&lt;li&gt;“Zero Initialisation" does not work in neural network.&lt;/li&gt;&lt;li&gt;Random Initialisation: Symmetry breaking&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Put things together&lt;ol&gt;&lt;li&gt;Training a neural network&lt;ol&gt;&lt;li&gt;Pick a network architecture&lt;ol&gt;&lt;li&gt;Number of input units: Dimension of features x(i)&lt;/li&gt;&lt;li&gt;Number of output units: Number of Classes&lt;/li&gt;&lt;li&gt;Layers:&lt;ol&gt;&lt;li&gt;Number of layers&lt;/li&gt;&lt;li&gt;Units in each layer&lt;ol&gt;&lt;li&gt;Same units number in each layer&lt;/li&gt;&lt;li&gt;Usually the more units the better&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Randomly initialise weights&lt;ol&gt;&lt;li&gt;Small values near zero&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Implement forward propagation to get prediction for any x(i)&lt;/li&gt;&lt;li&gt;Implement code to compote cose function J(theta)&lt;/li&gt;&lt;li&gt;Implement backprop to compute partial derivatives of J(theta)&lt;ol&gt;&lt;li&gt;for i = 1:m&lt;ol&gt;&lt;li&gt;Perform forward propagation and back-propagation using example (x(i), y(i))&lt;/li&gt;&lt;li&gt;Get activations a(l) and delta(l) for l = 2,…,L&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21381359&amp;pixel&amp;useReferer"/&gt;</description><author>王若愚</author><pubDate>Mon, 20 Jun 2016 00:26:30 GMT</pubDate></item></channel></rss>